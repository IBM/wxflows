id	title	text
0	Creating a script asset with CPDCTL in Watson Machine Learning	"Learn to create a script asset with the CPDCTL command line interface.  
CPDCTL_VERSION = """" ! curl -L https://github.com/IBM/cpdctl/releases/download/{CPDCTL_VERSION}/cpdctl_linux_amd64.tar.gz --output cpdctl_linux_amd64.tar.gz ! tar xzf cpdctl_linux_amd64.tar.gz %%capture %env PATH={CWD}:{PATH} ! cpdctl version URL = ""<<CPD URL>>"" USERNAME = ""<<CPD user name>>"" APIKEY = ""<<CPD user API key>>"" ! cpdctl config context cpd-demo --url {URL} --username {USERNAME} --apikey {APIKEY} project_id = os.getenv('PROJECT_ID') with open('<test_script>', 'w') as f: f.write(""echo OK"") ! cpdctl asset script create --file <test_script> --project-id {project_id} ! cpdctl asset search --type-name script --query 'test.sh' --project-id {project_id} SCRIPT_ASSET_ID = """" ! cpdctl asset delete --asset-id {SCRIPT_ASSET_ID} --project-id {project_id}  
Install the CPDCTL library.  
Check for the available versions on the GitHub releases page.  
Extract the downloaded archive.  
Ensure that the cpdctl archive is in the current working directory and the system PATH.  
Check that the CLI is accessible.  
Optional: The following steps are not required if you are running the script in a Cloud Pak for Data environment runtime.  
Open your script.  
Create script asset.  
When you run this line of code, you receive the asset ID in its output.  
List the script assets in the project.  
Optional: You can delete the script with the following code.  
Parent topic: Managing AI lifecycle with CPDCTL"
1	Batch deployment input details for Decision Optimization models	"Follow these rules when you are specifying input details for batch deployments of Decision Optimization models.  
Data type summary table:"
2	Batch deployment input details for Decision Optimization models	"Data sources
Input/output inline data:  
Inline input data is converted to CSV files and used by the engine. CSV output data is converted to output inline data. Base64-encoded raw data is supported as input and output.  
Input/output data references:  
Tabular data is loaded from CSV, XLS, XLSX, JSON files or database data sources supported by the WDP connection library, converted to CSV files, and used by the engine. CSV output data is converted to tabular data and saved to CSV, XLS, XLSX, JSON files, or database data sources supported by the WDP connection library. Raw data can be loaded and saved from or to any file data sources that are supported by the WDP connection library. No support for compressed files. The environment variables parameter of deployment jobs is not applicable.  
If you are specifying input/output data references programmatically:  
""output_data"": [ { ""id"":"".*\\.csv""}] ""output_data_references"": [ {""id"":"".*"", ""type"": ""s3"", ""connection"": {...}, ""location"": { ""bucket"": ""do-wml"", ""path"": ""${job_id}/${attachment_name}"" }}]  
Data source reference type depends on the asset type. Refer to the Data source reference types section in Adding data assets to a deployment space.  
For S3 or Db2, connection details must be specified in the input_data_references.connection parameter, in the deployment job’s payload.  
For S3 or Db2, location details such as table name, bucket name, or path must be specified in the input_data_references.location.path parameter, in the deployment job’s payload.  
For data_asset, a managed asset can be updated or created. For creation, you can set the name and description for the created asset.  
You can use a pattern in ID or connection properties. For example, see the following code snippet:  
To collect all output CSV as inline data:  
To collect job output in a particular S3 folder:  
Note:  
Support for s3 and db2 values for scoring.input_data_references.type and scoring.output_data_references.type is deprecated and will be removed in the future. Use connection_asset or data_asset instead. See the documentation for the Watson Machine Learning REST API or Watson Machine Learning Python client library for details and examples."
3	Batch deployment input details for Decision Optimization models	"Data sources
For more information, see Model input and output data adaptation.  
Parent topic: Batch deployment input details by framework"
4	Importing space and project assets into deployment spaces	"You can import assets that you export from a deployment space or a project (either a project export or a Git archive) into a new or existing deployment space. This way, you can add assets or update existing assets (for example, replacing a model with its newer version) to use for your deployments.  
You can import a space or a project export file to a new deployment space or an existing deployment space to populate the space with assets.  
Tip: The export file can come from a Git-enabled project and a Watson Studio project. To create the file to export, create a compressed file for the project that contains the assets to import. Then, follow the steps for importing the compressed file into a new or existing space."
5	Importing space and project assets into deployment spaces	"Importing a space or a project to a new deployment space
To import a space or a project when you are creating a new deployment space:  
Click New deployment space. Enter the details for the space. For more information, see Creating deployment spaces. In the Upload space assets section, upload the exported compressed file that contains data assets and click Create.  
The assets from the exported file is added as space assets."
6	Importing space and project assets into deployment spaces	"Importing a space or a project to an existing deployment space
To import a space or a project into an existing space:  
From your deployment space, click the import and export space () icon. From the list, select Import space. Add your compressed file that contains assets from a Watson Studio project or deployment space. Tip: If the space that you are importing is encrypted, enter the password in the Password field. After your asset is imported, click Done.  
The assets from the exported file is added as space assets."
7	Importing space and project assets into deployment spaces	"Resolving issues with asset duplication
The importing mechanism compares assets that exist in your space with the assets that are being imported. If it encounters an asset with the same name and of the same type:  
If the asset type supports revisions, the importing mechanism creates a new revision of the existing asset and fixes the new revision. If the asset type does not support revisions, the importing mechanism fixes the existing asset.  
This table describes how import works to resolve cases where assets are duplicated between the import file and the existing space.  
Warning: Multiple assets of the same name in an existing space or multiple assets of the same name in an import file are not fully supported scenarios. The import works as described for the scenarios listed in the table, but you cannot use versioning capabilities specific to the import.  
For more information, see Managing asset versions.  
Existing deployments get updated differently, depending on deployment type:  
If a batch deployment was created by using the previous version of the asset, the next invocation of the batch deployment job will refer to the updated state of the asset. If an online deployment was created by using the previous version of the asset, the next ""restart"" of the deployment refers to the updated state of the asset."
8	Importing space and project assets into deployment spaces	"Learn more
For an example of how to export and import a deployment space by using the Watson Machine Learning Python client library, view or download this sample notebook. To learn about adding other types of assets to a space, refer to Assets in deployment spaces. To learn about exporting assets from a deployment space, refer to Exporting space assets.  
Parent topic: Deployment spaces"
9	Selecting an AutoAI model	AutoAI automatically prepares data, applies algorithms, and attempts to build model pipelines that are best suited for your data and use case. Learn how to evaluate the model pipelines so that you can save one as a model.
10	Selecting an AutoAI model	"Reviewing experiment results
During AutoAI training, your data set is split to a training part and a hold-out part. The training part is used by the AutoAI training stages to generate the AutoAI model pipelines and cross-validation scores that are used to rank them. After AutoAI training, the hold-out part is used for the resulting pipeline model evaluation and computation of performance information such as ROC curves and confusion matrices, which are shown in the leaderboard. The training/hold-out split ratio is 90/10.  
As the training progresses, you are presented with a dynamic infographic and leaderboard. Hover over nodes in the infographic to explore the factors that pipelines share and their unique properties. For a guide to the data in the infographic, click the Legend tab in the information panel. Or, to see a different view of the pipeline creation, click the Experiment details tab of the notification panel, then click Switch views to view the progress map. In either view, click a pipeline node to view the associated pipeline in the leaderboard. The leaderboard contains model pipelines that are ranked by cross-validation scores."
11	Selecting an AutoAI model	"View the pipeline transformations
Hover over a node in the infographic to view the transformations for a pipeline. The sequence of data transformations consists of a pre-processing transformer and a sequence of data transformers, if feature engineering was performed for the pipeline. The algorithm is determined by model selection and optimization steps during AutoAI training.  
See Implementation details to review the technical details for creating the pipelines."
12	Selecting an AutoAI model	"View the leaderboard
Each model pipeline is scored for various metrics and then ranked. The default ranking metric for binary classification models is the area under the ROC curve. For multi-class classification models the default metric is accuracy. For regression models, the default metric is the root mean-squared error (RMSE). The highest-ranked pipelines display in a leaderboard, so you can view more information about them. The leaderboard also provides the option to save select model pipelines after you review them.  
You can evaluate the pipelines as follows:  
Click a pipeline in the leaderboard to view more detail about the metrics and performance. Click Compare to view how the top pipelines compare. Sort the leaderboard by a different metric."
13	Selecting an AutoAI model	"View the leaderboard
Viewing the confusion matrix
One of the details you can view for a pipeline for a binary classification experiment is a Confusion matrix.  
The confusion matrix is based on the holdout data, which is the portion of the training dataset that is not used for training the model pipeline but only used to measure its performance on data that was not seen during training.  
In a binary classification problem with a positive class and a negative class, the confusion matrix summarizes the pipeline model’s positive and negative predictions in four quadrants depending on their correctness regarding the positive or negative class labels of the holdout data set.  
For example, the Bank sample experiment seeks to identify customers that take promotions that are offered to them. The confusion matrix for the top-ranked pipeline is:  
Click the items in the navigation menu to view other details about the selected pipeline. For example, Feature importance shows which data features contribute most to your prediction output."
14	Selecting an AutoAI model	"Save a pipeline as a model
When you are satisfied with a pipeline, save it using one of these methods:  
Click Save model to save the candidate pipeline as a model to your project so you can test and deploy it. Click Save as notebook to create and save an auto-generated notebook to your project. You can review the code or run the experiment in the notebook."
15	Selecting an AutoAI model	"Next steps
Promote the trained model to a deployment space so that you can test it with new data and generate predictions."
16	Selecting an AutoAI model	"Learn more
AutoAI implementation details  
Parent topic: AutoAI overview"
17	Deploying a tuned foundation model	Deploy a tuned model so you can add it to a business workflow and start to use foundation models in a meaningful way.
18	Deploying a tuned foundation model	"Before you begin
The tuning experiment that you used to tune the foundation model must be finished. For more information, see Tuning a foundation model."
19	Deploying a tuned foundation model	"Deploy a tuned model
To deploy a tuned model, complete the following steps:  
From the navigation menu, expand Projects, and then click All projects.  
Click to open your project.  
From the Assets tab, click the Experiments asset type.  
Click to open the tuning experiment for the model you want to deploy.  
From the Tuned models list, find the completed tuning experiment, and then click New deployment.  
Name the tuned model.  
The name of the tuning experiment is used as the tuned model name if you don't change it. The name has a number after it in parentheses, which counts the deployments. The number starts at one and is incremented by one each time you deploy this tuning experiment.  
Optional: Add a description and tags.  
In the Target deployment space field, choose a deployment space.  
The deployment space must be associated with a machine learning instance that is in the same account as the project where the tuned model was created.  
If you don't have a deployment space, choose Create a new deployment space, and then follow the steps in Creating deployment spaces.  
For more information, see What is a deployment space?  
In the Deployment serving name field, add a label for the deployment.  
The serving name is used in the URL for the API endpoint that identifies your deployment. Adding a name is helpful because the human-readable name that you add replaces a long, system-generated ID that is assigned otherwise.  
The serving name also abstracts the deployment from its service instance details. Applications can refer to this name which allows for the underlying service instance to be changed without impacting users.  
The name can have up to 36 characters. The supported characters are [a-z,0-9,_].  
The name must be unique across the IBM Cloud region. You might be prompted to change the serving name if the name you choose is already in use.  
Tip: Select View deployment in deployment space after creating. Otherwise, you need to take more steps to find your deployed model.  
Click Deploy.  
After the tuned model is promoted to the deployment space and deployed, a copy of the tuned model is stored in your project as a model asset."
20	Deploying a tuned foundation model	"Deploy a tuned model
What is a deployment space?
When you create a new deployment, a tuned model is promoted to a deployment space, and then deployed. A deployment space is separate from the project where you create the asset. A deployment space is associated with the following services that it uses to deploy assets:  
Watson Machine Learning: A product with tools and services you can use to build, train, and deploy machine learning models. This service hosts your turned model.  
IBM Cloud Object Storage: A secure platform for storing structured and unstructured data. Your deployed model asset is stored in a Cloud Object Storage bucket that is associated with your project.  
For more information, see Deployment spaces."
21	Deploying a tuned foundation model	"Testing the deployed model
The true test of your tuned model is how it responds to input that follows tuned-for patterns.  
You can test the tuned model from one of the following pages:  
Prompt Lab: A tool with an intuitive user interface for prompting foundation models. You can customize the prompt parameters for each input. You can also save the prompt as a notebook so you can interact with it programmatically. Deployment space: Useful when you want to test your model programmatically. From the API Reference tab, you can find information about the available endpoints and code examples. You can also submit input as text and choose to return the output or in a stream, as the output is generated. However, you cannot change the prompt parameters for the input text.  
To test your tuned model, complete the following steps:  
From the navigation menu, select Deployments.  
Click the name of the deployment space where you deployed the tuned model.  
Click the name of your deployed model.  
Follow the appropriate steps based on where you want to test the tuned model:  
From Prompt Lab:  
Click Open in Prompt Lab, and then choose the project where you want to work with the model.  
Prompt Lab opens and the tuned model that you deployed is selected from the Model field.  
In the Try section, add a prompt to the Input field that follows the prompt pattern that your tuned model is trained to recognize, and then click Generate.  
For more information about how to use the prompt editor, see Prompt Lab.  
From the deployment space:  
Click the Test tab.  
In the Input data field, add a prompt that follows the prompt pattern that your tuned model is trained to recognize, and then click Generate.  
You can click View parameter settings to see the prompt parameters that are applied to the model by default. To change the prompt parameters, you must go to the Prompt Lab."
22	Deploying a tuned foundation model	"Learn more
Tuning a foundation model  
Security and privacy for foundation models  
Parent topic: Deploying foundation model assets"
23	AutoAI Overview	"The AutoAI graphical tool analyzes your data and uses data algorithms, transformations, and parameter settings to create the best predictive model. AutoAI displays various potential models as model candidate pipelines and rank them on a leaderboard for you to choose from.  
Note:  
You can use a data asset that is saved as a Feature Group (beta) but the metadata is not used to populate the AutoAI experiment settings."
24	AutoAI Overview	"AutoAI data use
These limits are based on the default compute configuration of 8 CPU and 32 GB.  
AutoAI classification and regression experiments:  
You can upload a file up to 1 GB for AutoAI experiments. If you connect to a data source that exceeds 1 GB, only the first 1 GB of records is used.  
AutoAI time series experiments:  
If the data source contains a timestamp column, AutoAI samples the data at a uniform frequency. For example, data can be in increments of one minute, one hour, or one day. The specified timestamp is used to determine the lookback window to improve the model accuracy.  
Note:  
If the file size is larger than 1 GB, AutoAi sorts the data in descending time order and only the first 1 GB is used to train the experiment.  
If the data source does not contain a timestamp column, ensure AutoAI samples the data at uniform intervals and sorts the data in ascending time order. An ascending sort order means that the value in the first row is the oldest, and the value in the last row is the most recent.  
Note: If the file size is larger than 1 GB, truncate the file size so it is smaller than 1 GB."
25	AutoAI Overview	"AutoAI process
Using AutoAI, you can build and deploy a machine learning model with sophisticated training features and no coding. The tool does most of the work for you.  
To view the code that created a particular experiment, or interact with the experiment programmatically, you can save an experiment as a notebook.  
AutoAI automatically runs the following tasks to build and evaluate candidate model pipelines:  
Data pre-processing Automated model selection Automated feature engineering Hyperparameter optimization  
For additional detail on each of these phases, including links to associated research papers and descriptions of the algorithms applied to create the model pipelines, see AutoAI implementation details."
26	AutoAI Overview	"AutoAI process
Data pre-processing
Most data sets contain different data formats and missing values, but standard machine learning algorithms work only with numbers and no missing values. Therefore, AutoAI applies various algorithms or estimators to analyze, clean, and prepare your raw data for machine learning. This technique automatically detects and categorizes values based on features, such as data type: categorical or numerical. Depending on the categorization, AutoAI uses hyper-parameter optimization to determine the best combination of strategies for missing value imputation, feature encoding, and feature scaling for your data."
27	AutoAI Overview	"AutoAI process
Automated model selection
AutoAI uses automated model selection to identify the best model for your data. This novel approach tests potential models against small subsets of the data and ranks them based on accuracy. AutoAI then selects the most promising models and increases the size of the data subset until it identifies the best match. This approach saves time and improves performance by gradually narrowing down the potential models based on accuracy.  
For information on how to handle automatically-generated pipelines to select the best model, refer to Selecting an AutoAI model."
28	AutoAI Overview	"AutoAI process
Automated feature engineering
Feature engineering identifies the most accurate model by transforming raw data into a combination of features that best represent the problem. This unique approach explores various feature construction choices in a structured, nonexhaustive manner, while progressively maximizing model accuracy by using reinforcement learning. This technique results in an optimized sequence of transformations for the data that best match the algorithms of the model selection step."
29	AutoAI Overview	"AutoAI process
Hyperparameter optimization
Hyperparameter optimization refines the best performing models. AutoAI uses a novel hyperparameter optimization algorithm for certain function evaluations, such as model training and scoring, that are typical in machine learning. This approach quickly identifies the best model despite long evaluation times at each iteration."
30	AutoAI Overview	"Next steps
AutoAI tutorial: Build a Binary Classification Model  
Parent topic: Analyzing data and building models"
31	SPSS predictive analytics data preparation algorithms in notebooks	"Descriptives provides efficient computation of the univariate and bivariate statistics and automatic data preparation features on large scale data. It can be used widely in data profiling, data exploration, and data preparation for subsequent modeling analyses.  
The core statistical features include essential univariate and bivariate statistical summaries, univariate order statistics, metadata information creation from raw data, statistics for visualization of single fields and field pairs, data preparation features, and data interestingness score and data quality assessment. It can efficiently support the functionality required for automated data processing, user interactivity, and obtaining data insights for single fields or the relationships between the pairs of fields inclusive with a specified target.  
Python example code:"
32	SPSS predictive analytics data preparation algorithms in notebooks	"Descriptives Selection Strategy
When the number of field pairs is too large (for example, larger than the default of 1000), SelectionStrategy is used to limit the number of pairs for which bivariate statistics will be computed. The strategy involves 2 steps:  
Limit the number of pairs based on the univariate statistics. Limit the number of pairs based on the core association bivariate statistics.  
Notice that the pair will always be included under the following conditions:  
The pair consists of a predictor field and a target field. The pair of predictors or targets is enforced."
33	SPSS predictive analytics data preparation algorithms in notebooks	"Smart Data Preprocessing
The Smart Data Preprocessing (SDP) engine is an analytic component for data preparation. It consists of three separate modules: relevance analysis, relevance and redundancy analysis, and smart metadata (SMD) integration.  
Given the data with regular fields, list fields, and map fields, relevance analysis evaluates the associations of input fields with targets, and selects a specified number of fields for subsequent analysis. Meanwhile, it expands list fields and map fields, and extracts the selected fields into regular column-based format.  
Due to the efficiency of relevance analysis, it's also used to reduce the large number of fields in wide data to a moderate level where traditional analytics can work.  
SmartDataPreprocessingRelevanceAnalysis exports these outputs:  
JSON file, containing model information new column-based data the related data model  
Python example code:"
34	SPSS predictive analytics data preparation algorithms in notebooks	"Sparse Data Convertor
Sparse Data Convertor (SDC) converts regular data fields into list fields. You just need to specify the fields that you want to convert into list fields, then SDC will merge the fields according to their measurement level. It will generate, at most, three kinds of list fields: continuous list field, categorical list field, and map field.  
Python example code:"
35	SPSS predictive analytics data preparation algorithms in notebooks	"Binning
You can use this function to derive one or more new binned fields or to obtain the bin definitions used to determine the bin values.  
Python example code:"
36	SPSS predictive analytics data preparation algorithms in notebooks	"Hex Binning
You can use this function to calculate and assign hexagonal bins to two fields.  
Python example code:"
37	SPSS predictive analytics data preparation algorithms in notebooks	"Complex Sampling
The complexSampling function selects a pseudo-random sample of records from a data source.  
The complexSampling function performs stratified sampling of incoming data using simple exact sampling and simple proportional sampling. The stratifying fields are specified as input and the sampling counts or sampling ratio for each of the strata to be sampled must also be provided. Optionally, the record counts for each strata may be provided to improve performance.  
Python example code:"
38	SPSS predictive analytics data preparation algorithms in notebooks	"Count and Sample
The countAndSample function produces a pseudo-random sample having a size approximately equal to the 'samplingCount' input.  
The sampling is accomplished by calling the SamplingComponent with a sampling ratio that's computed as 'samplingCount / totalRecords' where 'totalRecords' is the record count of the incoming data.  
Python example code:"
39	SPSS predictive analytics data preparation algorithms in notebooks	"MR Sampling
The mrsampling function selects a pseudo-random sample of records from a data source at a specified sampling ratio. The size of the sample will be approximately the specified proportion of the total number of records subject to an optional maximum. The set of records and their total number will vary with random seed. Every record in the data source has the same probability of being selected.  
Python example code:"
40	SPSS predictive analytics data preparation algorithms in notebooks	"Sampling Model
The samplingModel function selects a pseudo-random percentage of the subsequence of input records defined by every Nth record for a given step size N. The total sample size may be optionally limited by a maximum.  
When the step size is 1, the subsequence is the entire sequence of input records. When the sampling ratio is 1.0, selection becomes deterministic, not pseudo-random.  
Note that with distributed data, the samplingModel function applies the selection criteria independently to each data split. The maximum sample size, if any, applies independently to each split and not to the entire data source; the subsequence is started fresh at the start of each split.  
Python example code:"
41	SPSS predictive analytics data preparation algorithms in notebooks	"Sequential Sampling
The sequentialSampling function is similar to the samplingModel function. It also selects a pseudo-random percentage of the subsequence of input records defined by every Nth record for a given step size N. The total sample size may be optionally limited by a maximum.  
When the step size is 1, the subsequence is the entire sequence of input records. When the sampling ratio is 1.0, selection becomes deterministic, not pseudo-random. The main difference between sequentialSampling and samplingModel is that with distributed data, the sequentialSampling function applies the selection criteria to the entire data source, while the samplingModel function applies the selection criteria independently to each data split.  
Python example code:  
Parent topic: SPSS predictive analytics algorithms"
42	SPSS predictive analytics forecasting using data preparation for time series data in notebooks	"Data preparation for time series data (TSDP) provides the functionality to convert raw time data (in Flattened multi-dimensional format, which includes transactional (event) based and column-based data) into regular time series data (in compact row-based format) which is required by the subsequent time series analysis methods.  
The main job of TSDP is to generate time series in terms of the combination of each unique value in the dimension fields with metric fields. In addition, it sorts the data based on the timestamp, extracts metadata of time variables, transforms time series with another time granularity (interval) by applying an aggregation or distribution function, checks the data quality, and handles missing values if needed.  
Python example code:"
43	SPSS predictive analytics forecasting using data preparation for time series data in notebooks	"TimeSeriesDataPreparationConvertor
This is the date/time convertor API that's used to provide some functionalities of the date/time convertor inside TSDP for applications to use. There are two use cases for this component:  
Compute the time points between a specified start and end time. In this case, the start and end time both occur after the first observation in the previous TSDP's output. Compute the time points between a start index and end index referring to the last observation in the previous TSDP's output."
44	SPSS predictive analytics forecasting using data preparation for time series data in notebooks	"Temporal causal modeling
Temporal causal modeling (TCM) refers to a suite of methods that attempt to discover key temporal relationships in time series data by using a combination of Granger causality and regression algorithms for variable selection.  
Python example code:"
45	SPSS predictive analytics forecasting using data preparation for time series data in notebooks	"Temporal Causal Auto Regressive Model
Autoregressive (AR) models are built to compute out-of-sample forecasts for predictor series that aren't target series. These predictor forecasts are then used to compute out-of-sample forecasts for the target series.  
Model produced by TemporalCausal  
TemporalCausal exports outputs:  
a JSON file that contains TemporalCausal model information an XML file that contains multi series model  
Python example code:"
46	SPSS predictive analytics forecasting using data preparation for time series data in notebooks	"Temporal Causal Outlier Detection
One of the advantages of building TCM models is the ability to detect model-based outliers. Outlier detection refers to a capability to identify the time points in the target series with values that stray too far from their expected (fitted) values based on the TCM models."
47	SPSS predictive analytics forecasting using data preparation for time series data in notebooks	"Temporal Causal Root Cause Analysis
The root cause analysis refers to a capability to explore the Granger causal graph in order to analyze the key/root values that resulted in the outlier in question."
48	SPSS predictive analytics forecasting using data preparation for time series data in notebooks	"Temporal Causal Scenario Analysis
Scenario analysis refers to a capability of the TCM models to ""play-out"" the repercussions of artificially setting the value of a time series. A scenario is the set of forecasts that are performed by substituting the values of a root time series by a vector of substitute values."
49	SPSS predictive analytics forecasting using data preparation for time series data in notebooks	"Temporal Causal Summary
TCM Summary selects Top N models based on one model quality measure. There are five model quality measures: Root Mean Squared Error (RMSE), Root Mean Squared Percentage Error (RMSPE), Bayesian Information Criterion (BIC), Akaike Information Criterion (AIC), and R squared (RSQUARE). Both N and the model quality measure can be set by the user."
50	SPSS predictive analytics forecasting using data preparation for time series data in notebooks	"Time Series Exploration
Time Series Exploration explores the characteristics of time series data based on some statistics and tests to generate preliminary insights about the time series before modeling. It covers not only analytic methods for expert users (including time series clustering, unit root test, and correlations), but also provides an automatic exploration process based on a simple time series decomposition method for business users.  
Python example code:"
51	SPSS predictive analytics forecasting using data preparation for time series data in notebooks	"Reverse Data preparation for time series data
Reverse Data preparation for time series data (RTSDP) provides functionality that converts the compact row based (CRB) format that's generated by TimeSeriesDataPreperation (TSDP) or TemporalCausalModel (TCM Score) back to the flattened multidimensional (FMD) format.  
Python example code:  
Parent topic: SPSS predictive analytics algorithms"
52	Geospatial data analysis	"You can use the geospatio-temporal library to expand your data science analysis in Python notebooks to include location analytics by gathering, manipulating and displaying imagery, GPS, satellite photography and historical data.  
The gespatio-temporal library is available in all IBM Watson Studio Spark with Python runtime environments."
53	Geospatial data analysis	"Key functions
The geospatio-temporal library includes functions to read and write data, topological functions, geohashing, indexing, ellipsoidal and routing functions.  
Key aspects of the library include:  
All calculated geometries are accurate without the need for projections. The geospatial functions take advantage of the distributed processing capabilities provided by Spark. The library includes native geohashing support for geometries used in simple aggregations and in indexing, thereby improving storage retrieval considerably. The library supports extensions of Spark distributed joins. The library supports the SQL/MM extensions to Spark SQL."
54	Geospatial data analysis	"Getting started with the library
Before you can start using the library in a notebook, you must register STContext in your notebook to access the st functions.  
To register STContext:"
55	Geospatial data analysis	"Next steps
After you have registered STContext in your notebook, you can begin exploring the spatio-temporal library for:  
Functions to read and write data Topological functions Geohashing functions Geospatial indexing functions Ellipsoidal functions Routing functions  
Check out the following sample Python notebooks to learn how to use these different functions in Python notebooks:  
Use the spatio-temporal library for location analytics Use spatial indexing to query spatial data Spatial queries in PySpark  
Parent topic: Notebooks and scripts"
56	Updating a deployment	"After you create an online or a batch deployment, you can still update your deployment details and update the assets that are associated with your deployment.  
For more information, see:  
Update deployment details Update assets associated with a deployment"
57	Updating a deployment	"Updating deployment details
You can update general deployment details, such as deployment name, description, metadata, and tags by using one of these methods:  
Update deployment details from the UI. Update deployment details by using the Patch API command."
58	Updating a deployment	"Updating deployment details
Updating deployment details from the UI
From the Deployments tab of your deployment space, click the action menu for the deployment and choose Edit settings.  
Update the details and then click Save.  
Tip: You can also update a deployment from the information sheet for the deployment."
59	Updating a deployment	"Updating deployment details
Updating deployment details by using the Patch API command
Use the Watson Machine Learning API Patch command to update deployment details.  
For example, to update a description for deployment:  
Notes:  
For <operation type>, use ""add"", ""remove"", or ""replace""."
60	Updating a deployment	"Updating assets associated with a deployment
After you create an online or batch deployment, you can update the deployed asset from the same endpoint. For example, if you have a better performing model, you can replace the deployed model with the improved version. When the update is complete, the new model is available from the REST API endpoint.  
Before you update an asset, make sure that these conditions are true:  
The framework of the new model is compatible with the existing deployed model.  
The input schema exists and matches for the new and deployed model.  
Caution: Failure to follow these conditions can result in a failed deployment.  
For more information, see Updating an asset from the deployment space UI.  
For more information, see Updating an asset by using the Patch API command."
61	Updating a deployment	"Updating assets associated with a deployment
Updating an asset from the deployment space UI
From the Deployments tab of your deployment space, click the action menu for the deployment and choose Edit. Click Replace asset. From the Select an asset dialog box, select the asset that you want to replace the current asset with and click Select asset. Click Save.  
Important: Make sure that the new asset is compatible with the deployment."
62	Updating a deployment	"Updating assets associated with a deployment
Updating an asset by using the Patch API command
Use the Watson Machine Learning API Patch command to update any supported asset.  
Use this method to patch a model for an online deployment.  
For example, patch a model with ID 6f01d512-fe0f-41cd-9a52-1e200c525c84 in space ID f2ddb8ce-7b10-4846-9ab0-62454a449802:  
A successful output response looks like this:  
Notes:  
""status"": {       ""message"": {         ""text"": ""The input schema of the asset being patched does not match with the currently deployed asset. Please ensure that the score payloads are up to date as per the asset being patched.""       },  
For <operation type>, use ""add"", ""remove"", or ""replace"".  
The initial state for the PATCH API output is ""updating"". Keep polling the status until it changes to ""ready"", then retrieve the deployment meta.  
Only the ASSET attribute can be specified for the asset patch. Changing any other attribute results in an error.  
The schema of the current model and the model being patched is compared to the deployed asset. A warning message is returned in the output of the Patch request API if the two don't match. For example, if a mismatch is detected, you can find this information in the output response.  
For more information, see Updating software specifications by using the API.  
Parent topic: Managing predictive deployments"
63	Set up your system	Before you can use IBM Federated Learning, ensure that you have the required hardware, software, and dependencies.
64	Set up your system	"Core requirements by role
Each entity that participates in a Federated Learning experiment must meet the requirements for their role."
65	Set up your system	"Core requirements by role
Admin software requirements
Designate an admin for the Federated Learning experiment. The admin must have:  
Access to the platform with Watson Studio and Watson Machine Learning enabled. You must create a Watson Machine Learning service instance. A project for assembling the global model. You must associate the Watson Machine Learning service instance with your project."
66	Set up your system	"Core requirements by role
Party hardware and software requirements
Each party must have a system that meets these minimum requirements.  
Note: Remote parties participating in the same Federated Learning experiment can use different hardware specs and architectures, as long as they each meet the minimum requirement.  
x86 64-bit PPC Mac M-series 4 GB memory or greater  
Linux Mac OS/Unix Windows  
A supported Python version and a machine learning framework. The Watson Machine Learning Python client.  
If you are using Linux, run pip install 'ibm-watson-machine-learning[fl-rt22.2-py3.10]'. If you are using Mac OS with M-series CPU and Conda, download the installation script and then run ./install_fl_rt22.2_macos.sh <name for new conda environment>."
67	Set up your system	"Network requirements
An outbound connection from the remote party to aggregator is required. Parties can use firewalls that restrict internal connections with each other."
68	Set up your system	"Data sources requirements
Data must comply with these requirements.  
Data must be in a directory or storage repository that is accessible to the party that uses them. Each data source for a federate model must have the same features. IBM Federated Learning supports horizontal federated learning only. Data must be in a readable format, but the formats can vary by data source. Suggested formats include:  
Hive Excel CSV XML Database  
Parent topic: Creating a Federated Learning experiment"
69	Deploying a prompt template	Deploy a prompt template so you can add it to a business workflow or so you can evaluate the prompt template to measure performance.
70	Deploying a prompt template	"Before you begin
Save a prompt template that contains at least one variable as a project asset. See Building reusable prompts."
71	Deploying a prompt template	"Promote a prompt template to a deployment space
To deploy a prompt template, complete the following steps:  
Open the project containing the prompt template. Click Promote to space for the template.  
In the Target deployment space field, choose a deployment space or create a new space. Note the following:  
The deployment space must be associated with a machine learning instance that is in the same account as the project where the prompt template was created.  
If you don't have a deployment space, choose Create a new deployment space, and then follow the steps in Creating deployment spaces.  
If you plan to evaluate the prompt template in the space, the recommended Deployment stage type for the space is Production. For more information on evaluating, see Evaluating a prompt template in a deployment space.  
Note: The deployment space stage cannot be changed after the space is created.  
Tip: Select View deployment in deployment space after creating. Otherwise, you need to take more steps to find your deployed asset.  
From the Assets tab of the deployment space, click Deploy. You create an online deployment, which means you can send data to the endpoint and receive a response in real-time.  
Optional: In the Deployment serving name field, add a unique label for the deployment.  
The serving name is used in the URL for the API endpoint that identifies your deployment. Adding a name is helpful because the human-readable name that you add replaces a long, system-generated unique ID that is assigned otherwise.  
The serving name also abstracts the deployment from its service instance details. Applications refer to this name, which allows for the underlying service instance to be changed without impacting users.  
The name can have up to 36 characters. The supported characters are [a-z,0-9,_].  
The name must be unique across the IBM Cloud region. You might be prompted to change the serving name if the name you choose is already in use."
72	Deploying a prompt template	"Testing the deployed prompt template
After the deployment successfully completes, click the deployment name to view the deployment.  
API reference tab includes the API endpoints and code snippets that you need to add this prompt template to an application. Test tab supports testing the prompt template. Enter test data as text, streamed text, or in a JSON file.  
If the watsonx.governance service is enabled, you also see these tabs:  
Evaluate provides the tools for evaluating the prompt template in the space. Click Activate to choose the dimensions to evaluate. For details, see Evaluating prompt templates. AI Factsheets displays all of the metadata that is collected for the prompt template. Use these details for tracking the prompt template for governance and compliance goals. See Tracking prompt templates.  
For more information, see Deployment spaces."
73	Deploying a prompt template	"Learn more
Tracking prompt templates  
Evaluating a prompt template in a deployment space  
Security and privacy for foundation models  
Follow the tutorial Quick start: Evaluate and track a prompt template to evaluate and track a sample prompt template  
See the blog post IBM watsonx.governance for management and performance assurance of AI models — A prescriptive approach for tips on how to operationalize business applications and workflows with AI inference using the deployed prompt template in a notebook  
Parent topic: Deploying and managing assets"
74	SPSS predictive analytics algorithms for scoring	"A PMML-compliant scoring engine supports:  
PMML-compliant models (4.2 and earlier versions) produced by various vendors, except for Baseline Model, ScoreCard Model, Sequence Model, and Text Model. Refer to the Data Mining Group (DMG) web site for a list of supported models. Non-PMML models produced by IBM SPSS products: Discriminant and Bayesian networks PMML 4.2 transformations completely  
Different kinds of models can produce various scoring results. For example:  
Classification models (those with a categorical target: Bayes Net, General Regression, Mining, Naive Bayes, k-Nearest Neighbor, Neural Network, Regression, Ruleset, Support Vector Machine, and Tree) produce: Regression models (those with a continuous target: General Regression, Mining, k-Nearest Neighbor, Neural Network, Regression, and Tree) produce predicted values; some also produce standard errors. Cox regression (in General Regression) produces predicted survival probability and cumulative hazard values. Tree models also produce Node ID. Clustering models produce Cluster ID and Cluster affinity. Anomaly Detection (represented as Clustering) produces anomaly index and top reasons. Association models produce Consequent, Rule ID, and confidence for top matching rules.  
Predicted values Probabilities Confidence values  
Python example code:  
Parent topic: SPSS predictive analytics algorithms"
75	Generating accurate output	Foundation models sometimes generate output that is not factually accurate. If factual accuracy is important for your project, set yourself up for success by learning how and why these models might sometimes get facts wrong and how you can ground generated output in correct facts.
76	Generating accurate output	"Why foundation models get facts wrong
Foundation models can get facts wrong for a few reasons:  
Pre-training builds word associations, not facts Pre-training data sets contain out-of-date facts Pre-training data sets do not contain esoteric or domain-specific facts and jargon Sampling decoding is more likely to stray from the facts"
77	Generating accurate output	"Why foundation models get facts wrong
Pre-training builds word associations, not facts
During pre-training, a foundation model builds up a vocabulary of words (tokens) encountered in the pre-training data sets. Also during pre-training, statistical relationships between those words become encoded in the model weights.  
For example, ""Mount Everest"" often appears near ""tallest mountain in the world"" in many articles, books, speeches, and other common pre-training sources. As a result, a pre-trained model will probably correctly complete the prompt ""The tallest mountain in the world is "" with the output ""Mount Everest.""  
These word associations can make it seem that facts have been encoded into these models too. For very common knowledge and immutable facts, you might have good luck generating factually accurate output using pre-trained foundation models with simple prompts like the tallest-mountain example. However, it is a risky strategy to rely on only pre-trained word associations when using foundation models in applications where accuracy matters."
78	Generating accurate output	"Why foundation models get facts wrong
Pre-training data sets contain out-of-date facts
Collecting pre-training data sets and performing pre-training runs can take a significant amount of time, sometimes months. If a model was pre-trained on a data set from several years ago, the model vocabulary and word associations encoded in the model weights won't reflect current world events or newly popular themes. For this reason, if you submit the prompt ""The most recent winner of the world cup of football (soccer) is "" to a model pre-trained on information a few years old, the generated output will be out of date."
79	Generating accurate output	"Why foundation models get facts wrong
Pre-training data sets do not contain esoteric or domain-specific facts and jargon
Common foundation model pre-training data sets, such as The Pile (Wikipedia), contain hundreds of millions of documents. Given how famous Mount Everest is, it's reasonable to expect a foundation model to have encoded a relationship between ""tallest mountain in the world"" and ""Mount Everest"". However, if a phenomenon, person, or concept is mentioned in only a handful of articles, chances are slim that a foundation model would have any word associations about that topic encoded in its weights. Prompting a pre-trained model about information that was not in its pre-training data sets is unlikely to produce factually accurate generated output."
80	Generating accurate output	"Why foundation models get facts wrong
Sampling decoding is more likely to stray from the facts
Decoding is the process a model uses to choose the words (tokens) in the generated output:  
Greedy decoding always selects the token with the highest probability Sampling decoding selects tokens pseudo-randomly from a probability distribution  
Greedy decoding generates output that is more predictable and more repetitive. Sampling decoding is more random, which feels ""creative"". If, based on pre-training data sets, the most likely words to follow ""The tallest mountain is "" are ""Mount Everest"", then greedy decoding could reliably generate that factually correct output, whereas sampling decoding might sometimes generate the name of some other mountain or something that's not even a mountain."
81	Generating accurate output	"How to ground generated output in correct facts
Rather than relying on only pre-trained word associations for factual accuracy, provide context in your prompt text."
82	Generating accurate output	"How to ground generated output in correct facts
Use context in your prompt text to establish facts
When you prompt a foundation model to generate output, the words (tokens) in the generated output are influenced by the words in the model vocabulary and the words in the prompt text. You can use your prompt text to boost factually accurate word associations.  
Here's a prompt to cause a model to complete a sentence declaring your favorite color:  
Given that only you know what your favorite color is, there's no way the model could reliably generate the correct output.  
Instead, a color will be selected from colors mentioned in the model's pre-training data:  
If greedy decoding is used, whichever color appears most frequently with statements about favorite colors in pre-training content will be selected. If sampling decoding is used, a color will be selected randomly from colors mentioned most often as favorites in the pre-training content.  
Here's a prompt that includes context to establish the facts:  
If you prompt a model with text that includes factually accurate context like this, then the output the model generates will be more likely to be accurate.  
For more examples of including context in your prompt, see these samples:  
Sample 4a - Answer a question based on an article Sample 4b - Answer a question based on an article"
83	Generating accurate output	"How to ground generated output in correct facts
Use less ""creative"" decoding
When you include context with the needed facts in your prompt, using greedy decoding is likely to generate accurate output. If you need some variety in the output, you can experiment with sampling decoding with low values for parameters like Temperature, Top P, and Top K. However, using sampling decoding increases the risk of inaccurate output."
84	Generating accurate output	"Retrieval-augmented generation
The retrieval-augmented generation pattern scales out the technique of pulling context into prompts. If you have a knowledge base, such as process documentation in web pages, legal contracts in PDF files, a database of products for sale, a GitHub repository of C++ code files, or any other collection of information, you can use the retrieval-augmented generation pattern to generate factually accurate output based on the information in that knowledge base.  
Retrieval-augmented generation involves three basic steps:  
Search for relevant content in your knowledge base Pull the most relevant content into your prompt as context Send the combined prompt text to the model to generate output  
For more information, see: Retrieval-augmented generation  
Parent topic: Prompt tips"
85	Compute options for model training and scoring	"When you train or score a model or function, you choose the type, size, and power of the hardware configuration that matches your computing needs.  
Default hardware configurations Compute usage"
86	Compute options for model training and scoring	"Default hardware configurations
Choose the hardware configuration for your Watson Machine Learning asset when you train the asset or when you deploy it."
87	Compute options for model training and scoring	"Compute usage for Watson Machine Learning assets
Deployments and scoring consume compute resources as capacity unit hours (CUH) from the Watson Machine Learning service.  
To check the total monthly CUH consumption for your Watson Machine Learning services, from the navigation menu, select Administration -> Environment runtimes.  
Additionally, you can monitor the monthly resource usage in each specific deployment space. To do that, from your deployment space, go to the Manage tab and then select Resource usage. The summary shows CUHs used by deployment type: separately for AutoAI deployments, Federated Learning deployments, batch deployments, and online deployments."
88	Compute options for model training and scoring	"Compute usage for Watson Machine Learning assets
Compute usage details
The rate of consumed CUHs is determined by the computing requirements of your deployments. It is based on such variables as:  
type of deployment type of framework complexity of scoring Scaling a deployment to support more concurrent users and requests also increases CUH consumption. As many variables affect resource consumption for a deployment, it is recommended that you run tests on your models and deployments to analyze CUH consumption.  
The way that online deployments consume capacity units is based on framework. For some frameworks, CUHs are charged for the number of hours that the deployment asset is active in a deployment space. For example, SPSS models in online deployment mode that run for 24 hours a day, seven days a week, consume CUHs and are charged for that period. An active online deployment has no idle time. For other frameworks, CUHs are charged according to scoring duration. Refer to the CUH consumption table for details on how CUH usage is calculated.  
Compute time is calculated to the millisecond, with a 1-minute minimum for each distinct operation. For example:  
A training run that takes 12 seconds is billed as 1 minute A training run that takes 83.555 seconds is billed exactly as calculated"
89	Compute options for model training and scoring	"Compute usage for Watson Machine Learning assets
CUH consumption by deployment and framework type
CUH consumption is calculated by using these formulas:"
90	Compute options for model training and scoring	"Learn more
Deploying assets Watson Machine Learning service Monitoring account resource usage  
Parent topic: Managing predictive deployments"
91	Data skipping for Spark SQL	"Data skipping can significantly boost the performance of SQL queries by skipping over irrelevant data objects or files based on a summary metadata associated with each object.  
Data skipping uses the open source Xskipper library for creating, managing and deploying data skipping indexes with Apache Spark. See Xskipper - An Extensible Data Skipping Framework.  
For more details on how to work with Xskipper see:  
Quick Start Guide Demo Notebooks  
In addition to the open source features in Xskipper, the following features are also available:  
Geospatial data skipping Encrypting indexes Data skipping with joins (for Spark 3 only) Samples showing these features"
92	Data skipping for Spark SQL	"Geospatial data skipping
You can also use data skipping when querying geospatial data sets using geospatial functions from the spatio-temporal library.  
To benefit from data skipping in data sets with latitude and longitude columns, you can collect the min/max indexes on the latitude and longitude columns. Data skipping can be used in data sets with a geometry column (a UDT column) by using a built-in Xskipper plugin.  
The next sections show you to work with the geospatial plugin."
93	Data skipping for Spark SQL	"Geospatial data skipping
Setting up the geospatial plugin
To use the plugin, load the relevant implementations using the Registration module. Note that you can only use Scala in applications in IBM Analytics Engine powered by Apache Spark, not in Watson Studio.  
import com.ibm.xskipper.stmetaindex.filter.STMetaDataFilterFactory import com.ibm.xskipper.stmetaindex.index.STIndexFactory import com.ibm.xskipper.stmetaindex.translation.parquet.{STParquetMetaDataTranslator, STParquetMetadatastoreClauseTranslator} import io.xskipper._ Registration.addIndexFactory(STIndexFactory) Registration.addMetadataFilterFactory(STMetaDataFilterFactory) Registration.addClauseTranslator(STParquetMetadatastoreClauseTranslator) Registration.addMetaDataTranslator(STParquetMetaDataTranslator) from xskipper import Xskipper from xskipper import Registration Registration.addMetadataFilterFactory(spark, 'com.ibm.xskipper.stmetaindex.filter.STMetaDataFilterFactory') Registration.addIndexFactory(spark, 'com.ibm.xskipper.stmetaindex.index.STIndexFactory') Registration.addMetaDataTranslator(spark, 'com.ibm.xskipper.stmetaindex.translation.parquet.STParquetMetaDataTranslator') Registration.addClauseTranslator(spark, 'com.ibm.xskipper.stmetaindex.translation.parquet.STParquetMetadatastoreClauseTranslator')  
For Scala:  
For Python:"
94	Data skipping for Spark SQL	"Geospatial data skipping
Index building
To build an index, you can use the addCustomIndex API. Note that you can only use Scala in applications in IBM Analytics Engine powered by Apache Spark, not in Watson Studio.  
import com.ibm.xskipper.stmetaindex.implicits._ // index the dataset val xskipper = new Xskipper(spark, dataset_path) xskipper .indexBuilder() // using the implicit method defined in the plugin implicits .addSTBoundingBoxLocationIndex(""location"") // equivalent //.addCustomIndex(STBoundingBoxLocationIndex(""location"")) .build(reader).show(false) xskipper = Xskipper(spark, dataset_path) # adding the index using the custom index API xskipper.indexBuilder() \ .addCustomIndex(""com.ibm.xskipper.stmetaindex.index.STBoundingBoxLocationIndex"", ['location'], dict()) \ .build(reader) \ .show(10, False)  
For Scala:  
For Python:"
95	Data skipping for Spark SQL	"Geospatial data skipping
Supported functions
The list of supported geospatial functions includes the following:  
ST_Distance ST_Intersects ST_Contains ST_Equals ST_Crosses ST_Touches ST_Within ST_Overlaps ST_EnvelopesIntersect ST_IntersectsInterior"
96	Data skipping for Spark SQL	"Encrypting indexes
If you use a Parquet metadata store, the metadata can optionally be encrypted using Parquet Modular Encryption (PME). This is achieved by storing the metadata itself as a Parquet data set, and thus PME can be used to encrypt it. This feature applies to all input formats, for example, a data set stored in CSV format can have its metadata encrypted using PME.  
In the following section, unless specified otherwise, when referring to footers, columns, and so on, these are with respect to metadata objects, and not to objects in the indexed data set.  
Index encryption is modular and granular in the following way:  
Each index can either be encrypted (with a per-index key granularity) or left in plain text Footer + object name column: Footer + metadata column can either be:  
Footer column of the metadata object which in itself is a Parquet file contains, among other things: Object name column stores the names of all indexed objects.  
Schema of the metadata object, which reveals the types, parameters and column names for all indexes collected. For example, you can learn that a BloomFilter is defined on column city with a false-positive probability of 0.1. Full path to the original data set or a table name in case of a Hive metastore table.  
Both encrypted using the same key. This is the default. In this case, the plain text footer configuration for the Parquet objects comprising the metadata in encrypted footer mode, and the object name column is encrypted using the selected key.  
Both in plain text. In this case, the Parquet objects comprising the metadata are in plain text footer mode, and the object name column is not encrypted.  
If at least one index is marked as encrypted, then a footer key must be configured regardless of whether plain text footer mode is enabled or not. If plain text footer is set then the footer key is used only for tamper-proofing. Note that in that case the object name column is not tamper proofed.  
If a footer key is configured, then at least one index must be encrypted.  
Before using index encryption, you should check the documentation on PME and make sure you are familiar with the concepts.  
Important: When using index encryption, whenever a `key` is configured in any Xskipper API, it's always the label `NEVER the key itself`."
97	Data skipping for Spark SQL	"Encrypting indexes
Important: When using index encryption, whenever a `key` is configured in any Xskipper API, it's always the label `NEVER the key itself`.  
To use index encryption:  
Follow all the steps to make sure PME is enabled. See PME.  
Perform all regular PME configurations, including Key Management configurations.  
Create encrypted metadata for a data set:  
Follow the regular flow for creating metadata. Configure a footer key. If you wish to set a plain text footer + object name column, set io.xskipper.parquet.encryption.plaintext.footer to true (See samples below). In IndexBuilder, for each index you want to encrypt, add the label of the key to use for that index.  
To use metadata during query time or to refresh existing metadata, no setup is necessary other than the regular PME setup required to make sure the keys are accessible (literally the same configuration needed to read an encrypted data set)."
98	Data skipping for Spark SQL	"Samples
The following samples show metadata creation using a key named k1 as a footer + object name key, and a key named k2 as a key to encrypt a MinMax for temp, while also creating a ValueList for city, which is left in plain text. Note that you can only use Scala in applications in IBM Analytics Engine powered by Apache Spark, not in Watson Studio.  
// index the dataset val xskipper = new Xskipper(spark, dataset_path) // Configuring the JVM wide parameters val jvmComf = Map( ""io.xskipper.parquet.mdlocation"" -> md_base_location, ""io.xskipper.parquet.mdlocation.type"" -> ""EXPLICIT_BASE_PATH_LOCATION"") Xskipper.setConf(jvmConf) // set the footer key val conf = Map( ""io.xskipper.parquet.encryption.footer.key"" -> ""k1"") xskipper.setConf(conf) xskipper .indexBuilder() // Add an encrypted MinMax index for temp .addMinMaxIndex(""temp"", ""k2"") // Add a plaintext ValueList index for city .addValueListIndex(""city"") .build(reader).show(false) xskipper = Xskipper(spark, dataset_path) # Add JVM Wide configuration jvmConf = dict([ (""io.xskipper.parquet.mdlocation"", md_base_location), (""io.xskipper.parquet.mdlocation.type"", ""EXPLICIT_BASE_PATH_LOCATION"")]) Xskipper.setConf(spark, jvmConf) # configure footer key conf = dict([(""io.xskipper.parquet.encryption.footer.key"", ""k1"")]) xskipper.setConf(conf) # adding the indexes xskipper.indexBuilder() \ .addMinMaxIndex(""temp"", ""k1"") \ .addValueListIndex(""city"") \ .build(reader) \ .show(10, False)  
For Scala:  
For Python  
If you want the footer + object name to be left in plain text mode (as mentioned above), you need to add the configuration parameter:"
99	Data skipping for Spark SQL	"Samples
For Scala:  
For Python  
If you want the footer + object name to be left in plain text mode (as mentioned above), you need to add the configuration parameter:  
// index the dataset val xskipper = new Xskipper(spark, dataset_path) // Configuring the JVM wide parameters val jvmComf = Map( ""io.xskipper.parquet.mdlocation"" -> md_base_location, ""io.xskipper.parquet.mdlocation.type"" -> ""EXPLICIT_BASE_PATH_LOCATION"") Xskipper.setConf(jvmConf) // set the footer key val conf = Map( ""io.xskipper.parquet.encryption.footer.key"" -> ""k1"", ""io.xskipper.parquet.encryption.plaintext.footer"" -> ""true"") xskipper.setConf(conf) xskipper .indexBuilder() // Add an encrypted MinMax index for temp .addMinMaxIndex(""temp"", ""k2"") // Add a plaintext ValueList index for city .addValueListIndex(""city"") .build(reader).show(false) xskipper = Xskipper(spark, dataset_path) # Add JVM Wide configuration jvmConf = dict([ (""io.xskipper.parquet.mdlocation"", md_base_location), (""io.xskipper.parquet.mdlocation.type"", ""EXPLICIT_BASE_PATH_LOCATION"")]) Xskipper.setConf(spark, jvmConf) # configure footer key conf = dict([(""io.xskipper.parquet.encryption.footer.key"", ""k1""), (""io.xskipper.parquet.encryption.plaintext.footer"", ""true"")]) xskipper.setConf(conf) # adding the indexes xskipper.indexBuilder() \ .addMinMaxIndex(""temp"", ""k1"") \ .addValueListIndex(""city"") \ .build(reader) \ .show(10, False)  
For Scala:  
For Python"
100	Data skipping for Spark SQL	"Data skipping with joins (for Spark 3 only)
With Spark 3, you can use data skipping in join queries such as:  
This example shows a star schema based on the TPC-H benchmark schema (see TPC-H) where lineitem is a fact table and contains many records, while the orders table is a dimension table which has a relatively small number of records compared to the fact tables.  
The above query has a predicate on the orders tables which contains a small number of records which means using min/max will not benefit much from data skipping.  
Dynamic data skipping is a feature which enables queries such as the above to benefit from data skipping by first extracting the relevant l_orderkey values based on the condition on the orders table and then using it to push down a predicate on l_orderkey that uses data skipping indexes to filter irrelevant objects.  
To use this feature, enable the following optimization rule. Note that you can only use Scala in applications in IBM Analytics Engine powered by Apache Spark, not in Watson Studio.  
  import com.ibm.spark.implicits.   spark.enableDynamicDataSkipping()    from sparkextensions import SparkExtensions    SparkExtensions.enableDynamicDataSkipping(spark)  
For Scala:  
For Python:  
Then use the Xskipper API as usual and your queries will benefit from using data skipping.  
For example, in the above query, indexing l_orderkey using min/max will enable skipping over the lineitem table and will improve query performance."
101	Data skipping for Spark SQL	"Support for older metadata
Xskipper supports older metadata created by the MetaIndexManager seamlessly. Older metadata can be used for skipping as updates to the Xskipper metadata are carried out automatically by the next refresh operation.  
If you see DEPRECATED_SUPPORTED in front of an index when listing indexes or running a describeIndex operation, the metadata version is deprecated but is still supported and skipping will work. The next refresh operation will update the metadata automatically."
102	Monitoring the experiment and saving the model	"Any party or admin with collaborator access to the experiment can monitor the experiment and save a copy of the model.  
As the experiment runs, you can check the progress of the experiment. After the training is complete, you can view your results, save and deploy the model, and then test the model with new data."
103	Monitoring the experiment and saving the model	"Monitoring the experiment
When all parties run the party connector script, the experiment starts training automatically. As the training runs, you can view a dynamic diagram of the training progress. For each round of training, you can view the four stages of a training round:  
Sending model: Federated Learning sends the model metrics to each party. Training: The process of training the data locally. Each party trains to produce a local model that is fused. No data is exchanged between parties. Receiving models: After training is complete, each party sends its local model to the aggregator. The data is not sent and remains private. Aggregating: The aggregator combines the models that are sent by each of the remote parties to create an aggregated model."
104	Monitoring the experiment and saving the model	"Saving your model
When the training is complete, a chart that displays the model accuracy over each round of training is drawn. Hover over the points on the chart for more information on a single point's exact metrics.  
A Training rounds table shows details for each training round. The table displays the participating parties' average accuracy of their model training for each round.  
When you are done with the viewing, click Save model to project to save the Federated Learning model to your project."
105	Monitoring the experiment and saving the model	"Saving your model
Rerun the experiment
You can rerun the experiment as many times as you need in your project.  
Note:  
If you encounter errors when rerunning an experiment, see Troubleshoot for more details."
106	Monitoring the experiment and saving the model	"Saving your model
Deploying your model
After you save your Federated Learning model, you can deploy and score the model like other machine learning models in a Watson Studio platform.  
See Deploying models for more details.  
Parent topic: Creating a Federated Learning experiment"
107	Handling Pipeline errors	You can specify how to respond to errors in a pipeline globally, with an error policy, and locally, by overriding the policy on the node level. You can also create a custom error-handling response.
108	Handling Pipeline errors	"Setting global error policy
The error policy sets the default behavior for errors in a pipeline. You can override this behavior for any node in the pipeline.  
To set the global error policy:  
Click the Manage default settings icon on the toolbar. Choose the default response to an error under the Error policy: You can optionally create a custom error-handling response for a flow failure.  
Fail pipeline on error stops the flow and initiates an error-handling flow.  
Continue pipeline on error tries to continue running the pipeline.  
Note: Continue pipeline on error affects nodes that use the default error policy and does not affect node-specific error policies."
109	Handling Pipeline errors	"Specifying an error response
If you opt for Fail pipeline on error for either the global error policy or for a node-specific policy, you can further specify what happens on failure. For example, if you check the Show icon on nodes that are linked to an error-handling pipeline, an icon flags a node with an error to help debug the flow."
110	Handling Pipeline errors	"Specifying a node-specific error policy
You can override the default error policy for any node in the pipeline.  
Click a node to open the configuration pane. Check the option to Override default error policy with:  
Fail pipeline on error Continue pipeline on error"
111	Handling Pipeline errors	"Viewing all node policies
To view all node-specific error handling for a pipeline:  
Click Manage default settings on the toolbar. Click the view all node policies link under Error policy.  
A list of all nodes in the pipeline show which nodes use the default policy, and which override the default policy. Click a node name to see the policy details. Use the view filter to show:  
All error policies: all nodes Default policy: all nodes that use the default policy Override default policy: all nodes that override the default policy Fail pipeline on error: all nodes that stop the flow on error Continue pipeline on error: all nodes that try to continue the flow on error"
112	Handling Pipeline errors	"Running the Fail on error flow
If you specify that the flow fails on error, a secondary error handling flow starts when an error is encountered."
113	Handling Pipeline errors	"Adding a custom error response
If Create custom error handling response is checked on default settings for error policy, you can add an error handling node to the canvas so you can configure a custom error response. The response applies to all nodes configured to fail when an error occurs.  
Parent topic: Creating a pipeline"
114	Configuring a classification or regression experiment	AutoAI offers experiment settings that you can use to configure and customize your classification or regression experiments.
115	Configuring a classification or regression experiment	"Experiment settings overview
After you upload the experiment data and select your experiment type and what to predict, AutoAI establishes default configurations and metrics for your experiment. You can accept these defaults and proceed with the experiment or click Experiment settings to customize configurations. By customizing configurations, you can precisely control how the experiment builds the candidate model pipelines.  
Use the following tables as a guide to experiment settings for classification and regression experiments. For details on configuring a time series experiment, see Building a time series experiment."
116	Configuring a classification or regression experiment	"Prediction settings
Most of the prediction settings are on the main General page. Review or update the following settings."
117	Configuring a classification or regression experiment	"Prediction settings
Data fairness settings
Click the Fairness tab to evaluate your experiment for fairness in predicted outcomes. For details on configuring fairness detection, see Applying fairness testing to AutoAI experiments."
118	Configuring a classification or regression experiment	"Data source settings
The General tab of data source settings provides options for configuring how the experiment consumes and processes the data for training and evaluating the experiment."
119	Configuring a classification or regression experiment	"Runtime settings
Review experiment settings or change the compute resources that are allocated for running the experiment."
120	Configuring a classification or regression experiment	"Next steps
Configure a text analysis experiment  
Parent topic: Building an AutoAI model"
121	Sharing notebooks with a URL	"You can create a URL to share the last saved version of a notebook on social media or with people outside of Watson Studio. The URL shows a read-only view of the notebook. Anyone who has the URL can view or download the notebook.  
Required permissions:  
You must have the Admin or Editor role in the project to share a notebook URL. The shared notebook shows the author of the shared version and when the notebook version was last updated."
122	Sharing notebooks with a URL	"Sharing a notebook URL
To share a notebook URL:  
Open the notebook in edit mode.  
If necessary, add code to hide sensitive code cells.  
Create a saved version of the notebook by clicking File > Save Version.  
Click the Share icon () from the notebook action bar.  
Select to share the link.  
Choose a sharing option:  
Choose Only text and output to hide all code cells. Choose All content excluding sensitive code cells to hide code cells that you marked as sensitive. Choose All content, including code to show everything, even code cells that you marked as sensitive. Make sure that you remove your credential and other sensitive information before you choose this option and every time before you save a new version of the notebook.  
Copy the link or choose a social media site on which to share the URL.  
Note: The URL remains valid while the project and notebook exist and while the notebook is shared. If you unshare the notebook, the URL becomes invalid. When you unshare, and then re-share the notebook, the URL will be the same again."
123	Sharing notebooks with a URL	"Updating a shared notebook
To update a shared notebook:  
Open the notebook in edit mode. Make changes to the notebook. Create a new version of the notebook by clicking File > Save Version.  
Note: Clicking File > Save saves your changes but it doesn't create a new version of the notebook; the shared URL still points to the older version of the notebook."
124	Sharing notebooks with a URL	"Unsharing a notebook URL
To unshare a notebook URL:  
Open the notebook in edit mode.  
Click the Share icon () from the notebook action bar.  
Unselect the Share with anyone who has the link toggle.  
Parent topic: Managing the lifecycle of notebooks and scripts"
125	Foundation model lifecycle	"To help you discover and use the latest and best foundation models from IBM and other open source providers, the foundation models that are available for prompting from watsonx.ai are refreshed regularly.  
Foundation models that are built by IBM are continuously updated and improved. As new versions of IBM foundation models are introduced, older versions remain available for you to use for at least 90 days after an updated model is introduced.  
Similarly, as newer and more-effective open source models become available, less useful models are removed from watsonx.ai to make room for them. You are given at least 30 days notice before foundation models from open source providers are removed from watsonx.ai."
126	Foundation model lifecycle	"Modifications to IBM foundation models
IBM foundation models are periodically modified by IBM to improve the foundation model performance or security. A modification is a model refresh that might include new capabilities or fixes, but does not meet IBM's criteria to warrant a version update.  
Foundation model modifications do not disrupt the watsonx.ai service. You can check the current full version number for an IBM foundation model at any time from the model card. The version number consists of three digits that identify the version, modification, and fix numbers that are associated with the IBM foundation model. For more information about versioning, see IBM Software product versioning explained.  
Any applications that inference an IBM foundation model that is modified will pick up the modifications, including any changes in performance or in the output that is generated by the model."
127	Foundation model lifecycle	"Foundation model deprecation
During the deprecation period, you can continue to inference the deprecated foundation model. However, a message is returned with the foundation model output to notify you about the upcoming model removal.  
A deprecated foundation model can also be constricted. When a deprecated model is in the constricted state, it means the model can be inferenced, but cannot be tuned, trained, or deployed.  
When a foundation model is deprecated, the following steps are taken to inform you about the deprecation:  
The foundation model is highlighted in the product user interface with a warning icon . A tooltip indicates that the deprecated model is scheduled for withdrawal. The deprecation is announced in the What’s new topic of the product documentation. The release note clearly states the deprecation date and withdrawal date for the foundation model. The Deprecated foundation models table is updated to show the foundation model that is being deprecated, the dates of deprecation and withdrawal, and a suitable alternative foundation model for you to consider as a replacement."
128	Foundation model lifecycle	"Adjustments after foundation model removal
If any of the following saved resources submit input to a foundation model that was withdrawn, you must choose a supported alternative foundation model to use.  
Prompt template asset Prompt session asset Notebook asset  
For more information about working with saved prompt assets, see Saving your work.  
For more information about how to change the foundation model that is inferenced from a notebook asset, see Inferencing a foundation model with a notebook.  
Parent topic: Supported foundation models"
129	Exporting space assets from deployment spaces	"You can export assets from a deployment space so that you can share the space with others or reuse the assets in another space.  
For a list of assets that you can export from space, refer to Assets in a deployment space."
130	Exporting space assets from deployment spaces	"Exporting space assets from the UI
Important:  
To avoid problems with importing the space, export all dependencies together with the space. For more information, see Exporting a project.  
To export space assets from the UI:  
From your deployment space, click the import and export space () icon. From the list, select Export space. Click New export file. Specify a file name and an optional description. Tip: To encrypt sensitive data in the exported archive, type the password in the Password field. Select the assets that you want to export with the space. Click Create to create the export file. After the space is exported, click the download () to save the file.  
You can reuse this space by choosing Create a space from a file when you create a new space."
131	Exporting space assets from deployment spaces	"Exporting space assets by using the cpdctl tool
By using the cpdctl tool, you can export these types of assets:  
wml_model wml_model_definition wml_pipeline wml_function wml_experiment  
Refer to this example code:  
The following output is written to the /tmp/space.zip file:  
For more information, see cpdctl tool documentation."
132	Exporting space assets from deployment spaces	"Learn more
Importing spaces and projects into existing deployment spaces.  
Parent topic: Deployment spaces"
133	ibm-watson-studio-lib for R	"The ibm-watson-studio-lib library for R provides access to assets. It can be used in notebooks that are created in the notebook editor or in RStudio in a project. ibm-watson-studio-lib provides support for working with data assets and connections, as well as browsing functionality for all other asset types.  
There are two kinds of data assets:  
Stored data assets refer to files in the storage associated with the current project. The library can load and save these files. For data larger than one megabyte, this is not recommended. The library requires that the data is kept in memory in its entirety, which might be inefficient when processing huge data sets. Connected data assets represent data that must be accessed through a connection. Using the library, you can retrieve the properties (metadata) of the connected data asset and its connection. The functions do not return the data of a connected data asset. You can either use the code that is generated for you when you click Read data on the Code snippets panel to access the data or you must write your own code.  
Note: The ibm-watson-studio-lib functions do not encode or decode data when saving data to or getting data from a file. Additionally, the ibm-watson-studio-lib functions can't be used to access connected folder assets (files on a path to the project storage)."
134	ibm-watson-studio-lib for R	"Setting up the ibm-watson-studio-lib library
The ibm-watson-studio-lib library for R is pre-installed and can be imported directly in a notebook in the notebook editor. To use the ibm-watson-studio-lib library in your notebook, you need the ID of the project and the project token.  
To insert the project token to your notebook:  
library(ibmWatsonStudioLib) wslib <- access_project_or_space(list(""token""=""<ProjectToken>""))  
Click the More icon on your notebook toolbar and then click Insert project token.  
If a project token exists, a cell is added to your notebook with the following information:  
<ProjectToken> is the value of the project token.  
If you are told in a message that no project token exists, click the link in the message to be redirected to the project's Access Control page where you can create a project token. You must be eligible to create a project token. For details, see Manually adding the project token.  
To create a project token:  
From the Manage tab, select the Access Control page, and click New access token under Access tokens. Enter a name, select Editor role for the project, and create a token. Go back to your notebook, click the More icon on the notebook toolbar and then click Insert project token."
135	ibm-watson-studio-lib for R	"The ibm-watson-studio-lib functions
The ibm-watson-studio-lib library exposes a set of functions that are grouped in the following way:  
Get project information Get authentication token Fetch data Save data Get connection information Get connected data information Access assets by ID instead of name Access project storage directly Spark support Browse project assets"
136	ibm-watson-studio-lib for R	"The ibm-watson-studio-lib functions
Get project information
While developing code, you might not know the exact names of data assets or connections. The following functions provide lists of assets, from which you can pick the relevant ones. In all examples, you can use wslib$show(assets) to pretty-print the list. The index of each item is printed in front of the item.  
# Import the lib library(""ibmWatsonStudioLib"") wslib <- access_project_or_space(list(""token""=""<ProjectToken>"")) assets <- wslib$list_connections() wslib$show(assets) connprops <- wslib$get_connection(assets[0])  
list_connections()  
This function returns a list of the connections. The list of returned connections is not sorted by any criterion and can change when you call the function again. You can pass a list item instead of a name to get_connection function.  
list_connected_data()  
This function returns the connected data assets. The list of returned connected data assets is not sorted by any criterion and can change when you call the function again. You can pass a list item instead of a name to the get_connected_data function.  
list_stored_data()  
This function returns a list of the stored data assets (data files). The list of returned data assets is not sorted by any criterion and can change when you call the function again. You can pass a list item instead of a name to the load_data and save_data functions.  
Note: A heuristic is applied to distinguish between connected data assets and stored data assets. However, there may be cases where a data asset of the wrong kind appears in the returned lists.  
wslib$here By using this entry point, you can retrieve metadata about the project that the lib is working with. The entry point wslib$here provides the following functions:  
get_name()  
This function returns the name of the project.  
get_description()  
This function returns the description of the project.  
get_ID()  
This function returns the ID of the project.  
get_storage()  
This function returns storage information for the project."
137	ibm-watson-studio-lib for R	"The ibm-watson-studio-lib functions
Get authentication token
Some tasks require an authentication token. For example, if you want to run your own requests against the Watson Data API, you need an authentication token.  
You can use the following function to get the bearer token:  
get_current_token()  
For example:  
This function returns the bearer token that is currently used by the ibm-watson-studio-lib library."
138	ibm-watson-studio-lib for R	"The ibm-watson-studio-lib functions
Fetch data
You can use the following functions to fetch data from a stored data asset (a file) in your project.  
load_data(asset_name_or_item, attachment_type_or_item = NULL)  
This function loads the data of a stored data asset into a bytes buffer. The function is not recommended for very large files.  
The function takes the following parameters:  
# Import the lib library(""ibmWatsonStudioLib"") wslib <- access_project_or_space(list(""token""=""<ProjectToken>"")) # Fetch the data from a file my_file <- wslib$load_data(""MyFile.csv"") # Read the CSV data file into a data frame df <- read.csv(text = rawToChar(my_file)) head(df)  
asset_name_or_item: (Required) Either a string with the name of a stored data asset or an item like those returned by list_stored_data().  
attachment_type_or_item: (Optional) Attachment type to load. A data asset can have more than one attachment with data. Without this parameter, the default attachment type, namely data_asset is loaded. Specify this parameter if the attachment type is not data_asset. For example, if a plain text data asset has an attached profile from Natural Language Analysis, this can be loaded as attachment type data_profile_nlu.  
Here is an example that shows you how to load the data of a data asset:  
download_file(asset_name_or_item, file_name = NULL, attachment_type_or_item = NULL)  
This function downloads the data of a stored data asset and stores it in the specified file in the file system of your runtime. The file is overwritten if it already exists.  
The function takes the following parameters:  
# Import the lib library(""ibmWatsonStudioLib"") wslib <- access_project_or_space(list(""token""=""<ProjectToken>"")) # Let's assume you have a R script ""helpers.R"" with helper functions on your local machine. # Upload the script to your project using the Data Panel on the right. # Download the script to the file system of your runtime wslib$download_file(""helpers.R"") # Source the script to use the contained functions, e.g. ‘my_func’, in your notebook. source(""helpers.R"") my_func()"
139	ibm-watson-studio-lib for R	"The ibm-watson-studio-lib functions
Fetch data
asset_name_or_item: (Required) Either a string with the name of a stored data asset or an item like those returned by list_stored_data().  
file_name: (Optional) The name of the file that the downloaded data is stored to. It defaults to the asset's attachment name.  
attachment_type_or_item: (Optional) The attachment type to download. A data asset can have more than one attachment with data. Without this parameter, the default attachment type, namely data_asset is downloaded. Specify this parameter if the attachment type is not data_asset. For example, if a plain text data asset has an attached profile from Natural Language Analysis, this can be downlaoded loaded as attachment type data_profile_nlu.  
Here is an example that shows you how to you can use download_file to make your custom R script available in your notebook:"
140	ibm-watson-studio-lib for R	"The ibm-watson-studio-lib functions
Save data
The functions to store data in your project storage do multiple things:  
Store the data in project storage Add the data as a data asset (by creating an asset or overwriting an existing asset) to your project so you can see the data in the data assets list in your project. Associate the asset with the file in the storage.  
You can use the following functions to save data:  
save_data(asset_name_or_item, data, overwrite = NULL, mime_type = NULL, file_name = NULL)  
This function saves data in memory to the project storage.  
The function takes the following parameters:  
# Import the lib library(""ibmWatsonStudioLib"") wslib <- access_project_or_space(list(""token""=""<ProjectToken>"")) # let's assume you have a data frame df which contains the data # you want to save as a csv file csv <- capture.output(write.csv(df, row.names=FALSE), type=""output"") csv_raw <- charToRaw(paste0(csv, collapse='\n')) wslib$save_data(""my_asset_name.csv"", csv_raw) # the function returns a list which contains the asset_name, asset_id, file_name and additional information upon successful saving of the data  
asset_name_or_item: (Required) The name of the created asset or list item that is returned by list_stored_data(). You can use the item if you like to overwrite an existing file.  
data: (Required) The data to upload. The expected data type is raw.  
overwrite: (Optional) Overwrites the data of a stored data asset if it already exists. Defaults to FALSE. If an asset item is passed instead of a name, the behavior is to overwrite the asset.  
mime_type: (Optional) The MIME type for the created asset. By default the MIME type is determined from the asset name suffix. If you use asset names without a suffix, specify the MIME type here. For example mime_type=application/text for plain text data. This parameter is ignored when overwriting an asset."
141	ibm-watson-studio-lib for R	"The ibm-watson-studio-lib functions
Save data
file_name: (Optional) The file name to be used in the project storage. The data is saved in the storage associated with the project. When creating a new asset, the file name is derived from the asset name, but might be different. If you want to access the file directly, you can specify a file name. This parameter is ignored when overwriting an asset.  
Here is an example that shows you how to save data to a file:  
upload_file(file_path, asset_name = NULL, file_name = NULL, overwrite = FALSE, mime_type = NULL)  
This function saves data in the file system in the runtime to a file associated with your project.  
The function takes the following parameters:  
# Import the lib library(""ibmWatsonStudioLib"") wslib <- access_project_or_space(list(""token""=""<ProjectToken>"")) # Let's assume you have downloaded a file and want to save it # in your project. download.file(""https://some/url/data_file.csv"", ""data_file.csv"") wslib$upload_file(""data_file.csv"") # The function returns a list which contains the asset_name, asset_id, file_name and additional information upon successful saving of the data.  
file_path: (Required) The path to the file in the file system.  
asset_name: (Optional) The name of the data asset that is created. It defaults to the name of the file to be uploaded.  
file_name: (Optional) The name of the file that is created in the storage associated with the project. It defaults to the name of the file to be uploaded.  
overwrite: (Optional) Overwrites an existing file in storage. Defaults to FALSE.  
mime_type: (Optional) The MIME type for the created asset. By default the MIME type is determined from the asset name suffix. If you use asset names without a suffix, specify the MIME type here. For example mime_type='application/text' for plain text data. This parameter is ignored when overwriting an asset.  
Here is an example that shows you how you can upload a file to the project:"
142	ibm-watson-studio-lib for R	"The ibm-watson-studio-lib functions
Get connection information
You can use the following function to access the connection metadata of a given connection.  
get_connection(name_or_item)  
This function returns the properties (metadata) of a connection which you can use to fetch data from the connection data source. Use wslib$show(connprops) to view the properties. The special key ""."" in the returned list item provides information about the connection asset.  
The function takes the following required parameter:  
name_or_item: Either a string with the name of a connection or an item like those returned by list_connections().  
Note that when you work with notebooks, you can click Read data on the Code snippets panel to generate code to load data from a connection into a pandas DataFrame for example."
143	ibm-watson-studio-lib for R	"The ibm-watson-studio-lib functions
Get connected data information
You can use the following function to access the metadata of a connected data asset.  
get_connected_data(name_or_item)  
This function returns the properties of a connected data asset, including the properties of the underlying connection. Use wslib$show() to view the properties. The special key ""."" in the returned list provides information about the data and the connection assets.  
The function takes the following required parameter:  
name_or_item: Either a string with the name of a connected data asset or an item like those returned by list_connected_data().  
Note that when you work with notebooks, you can click Read data on the Code snippets panel to generate code to load data from a connected data asset into a pandas DataFrame for example."
144	ibm-watson-studio-lib for R	"The ibm-watson-studio-lib functions
Access asset by ID instead of name
You should preferably always access data assets and connections by a unique name. Asset names are not necessarily always unique and the ibm-watson-studio-lib functions will raise an exception when a name is ambiguous. You can rename data assets in the UI to resolve the conflict.  
Accessing assets by a unique ID is possible but is discouraged as IDs are valid only in the current project and will break code when transferred to a different project. This can happen for example, when projects are exported and re-imported. You can get the ID of a connection, connected or stored data asset by using the corresponding list function, for example list_connections().  
The entry point wslib$by_id provides the following functions:  
get_connection(asset_id)  
This function accesses a connection by the connection asset ID.  
get_connected_data(asset_id)  
This function accesses a connected data asset by the connected data asset ID.  
load_data(asset_id, attachment_type_or_item = NULL)  
This function loads the data of a stored data asset by passing the asset ID. See load_data() for a decsription of the other parameters you can pass.  
save_data(asset_id, data, overwrite = NULL, mime_type = NULL, file_name = NULL)  
This function saves data to a stored data asset by passing the asset ID. This implies overwrite=TRUE. See save_data() for a description of the other parameters you can pass.  
download_file(asset_id, file_name = NULL, attachment_type_or_item = NULL)  
This function downloads the data of a stored data asset by passing the asset ID. See download_file() for a description of the other parameters you can pass."
145	ibm-watson-studio-lib for R	"The ibm-watson-studio-lib functions
Access project storage directly
You can fetch data from project storage and store data in project storage without synchronizing the project assets using the entry point wslib$storage.  
The entry point wslib$storage provides the following functions:  
fetch_data(filename)  
This function returns the data in a file as a bytes buffer. The file does not need to be registered as data asset.  
The function takes the following required parameter:  
filename: The name of the file in the project.  
store_data(filename, data, overwrite = FALSE)  
This function saves data in memory to storage, but does not create a new data asset. The function returns a list which contains the file name, file path and additional information. Use Use wslib$show() to print the information.  
The function takes the following parameters:  
filename: (Required) The name of the file in the project storage. data: (Required) The data to save as a raw object. overwrite: (Optional) Overwrites the data of a file in storage if it already exists. By default, this is set to false.  
download_file(storage_filename, local_filename = NULL)  
This function downloads the data in a file in storage and stores it in the specified local file. The local file is overwritten if it already existed.  
The function takes the following parameters:  
storage_filename: (Required) The name of the file in storage to download. local_filename: (Optional) The name of the file in the local file system of your runtime to downloaded the file to. Omit this parameter to use the storage file name.  
register_asset(storage_path, asset_name = NULL, mime_type = NULL)  
This function registers the file in storage as a data asset in your project. This operation fails if a data asset with the same name already exists. You can use this function if you have very large files that you cannot upload via save_data(). You can upload large files directly to the IBM Cloud Object Storage bucket of your project, for example via the UI, and then register them as data assets using register_asset().  
The function takes the following parameters:"
146	ibm-watson-studio-lib for R	"The ibm-watson-studio-lib functions
Access project storage directly
The function takes the following parameters:  
storage_path: (Required) The path of the file in storage. asset_name: (Optional) The name of the created asset. It defaults to the file name. mime_type: (Optional) The MIME type for the created asset. By default the MIME type is determined from the asset name suffix. Use this parameter to specify a MIME type if your file name does not have a file extension or if you want to set a different MIME type.  
Note: You can register a file several times as a different data asset. Deleting one of those assets in the project also deletes the file in storage, which means that other asset references to the file might be broken."
147	ibm-watson-studio-lib for R	"The ibm-watson-studio-lib functions
Spark support
The entry point wslib$spark provides functions to access files in storage with Spark.  
The entry point wslib$spark provides the following functions:  
provide_spark_context(sc)  
Use this function to enable Spark support.  
The function takes the following required parameter:  
library(ibmWatsonStudioLib) wslib <- access_project_or_space(list(""token""=""<ProjectToken>"")) wslib$spark$provide_spark_context(sc)  
sc: The SparkContext. It is provided in the notebook runtime.  
The following example shows you how to set up Spark support:  
get_data_url(asset_name)  
This function returns a URL to access a file in storage from Spark via Hadoop.  
The function takes the following required parameter:  
asset_name: The name of the asset.  
storage.get_data_url(file_name)  
This function returns a URL to access a file in storage from Spark via Hadoop. The function expects the file name and not the asset name.  
The function takes the following required parameter:  
file_name: The name of a file in the project storage."
148	ibm-watson-studio-lib for R	"The ibm-watson-studio-lib functions
Browse project assets
The entry point wslib$assets provides generic, read-only access to assets of any type. For selected asset types, there are dedicated functions that provide additional data.  
The following naming conventions apply:  
Functions named list_<something> return a list of named lists. Each contained list represents one asset and includes a small set of properties (metadata) that identifies the asset. Functions named get_<something> return a single named list with the properties for the asset.  
To pretty-print a list or list of named lists, use wslib$show().  
The functions expect either the name of an asset, or an item from a list as the parameter. By default, the functions return only a subset of the available asset properties. By setting the parameter raw_info=TRUE, you can get the full set of asset properties.  
The entry point wslib$assets provides the following functions:  
list_assets(asset_type, name = NULL, query = NULL, selector = NULL, raw_info = FALSE)  
This function lists all assets for the given type with respect to the given constraints.  
The function takes the following parameters:  
# Import the lib library(""ibmWatsonStudioLib"") wslib <- access_project_or_space(list(""token""=""<ProjectToken>"")) # List all assets in the project all_assets <- wslib$assets$list_assets(""asset"") wslib$show(all_assets) # List all data assets with name 'MyFile.csv' assets_by_name <- wslib$assets$list_assets(""data_asset"", name = ""MyFile.csv"") # List all data assets whose name starts with ""MyF"" assets_by_query <- wslib$assets$list_assets(""data_asset"", query = ""asset.name:(MyF*)"") # List all data assets which are larger than 1MB sizeFilter <- function(asset) asset$metadata$size > 1000000 large_assets <- wslib$assets$list_assets(""data_asset"", selector = sizeFilter, raw_info = TRUE) wslib$show(large_assets) # List all notebooks notebooks <- wslib$assets$list_assets(""notebook"")"
149	ibm-watson-studio-lib for R	"The ibm-watson-studio-lib functions
Browse project assets
asset_type: (Required) The type of the assets to list, for example data_asset. See list_asset_types() for a list of the available asset types. Use asset type asset for the list of all available assets in the project.  
name: (Optional) The name of the asset to list. Use this parameter if more than one asset with the same name exists. You can only specify either name and query.  
query: (Optional) A query string that is passed to the Watson Data API to search for assets. You can only specify either name and query.  
selector: (Optional) A custom filter function on the candidate asset list items. If the selector function returns TRUE, the asset is included in the returned asset list.  
raw_info: (Optional) Returns all of the available metadata. By default, the parameter is set to FALSE and only a subset of the properties is returned.  
Examples of using the list_assets function:  
list_asset_types(raw_info = FALSE)  
This function lists all available asset types.  
The function can take the following parameter:  
raw_info: (Optional) Returns the full set of metadata. By default, the parameter is FALSE and only a subset of the properties is returned.  
list_datasource_types(raw_info = FALSE)  
This function lists all available data source types.  
The function can take the following parameter:  
raw_info: (Optional) Returns the full set of metadata. By default, the parameter is FALSE and only a subset of the properties is returned.  
get_asset(name_or_item, asset_type=None, raw_info = FALSE)  
The function returns the metadata of an asset.  
The function takes the following parameters:  
notebooks <- wslib$assets$list_assets(""notebook"") wslib$show(notebooks) notebook <- wslib$assets$get_asset(notebooks[[1]]) wslib$show(notebook)  
name_or_item: (Required) The name of the asset or an item like those returned by list_assets()  
asset_type: (Optional) The type of the asset. If the parameter name_or_item contains a string for the name of the asset, setting asset_type is required."
150	ibm-watson-studio-lib for R	"The ibm-watson-studio-lib functions
Browse project assets
asset_type: (Optional) The type of the asset. If the parameter name_or_item contains a string for the name of the asset, setting asset_type is required.  
raw_info: (Optional) Returns the full set of metadata. By default, the parameter is FALSE and only a subset of the properties is returned.  
Example of using the list_assets and get_asset functions:  
get_connection(name_or_item, with_datasourcetype=False, raw_info = FALSE)  
This function returns the metadata of a connection.  
The function takes the following parameters:  
name_or_item: (Required) The name of the connection or an item like those returned by list_connections() with_datasourcetype: (Optional) Returns additional information about the data source type of the connection. raw_info: (Optional) Returns the full set of metadata. By default, the parameter is FALSE and only a subset of the properties is returned.  
get_connected_data(name_or_item, with_datasourcetype=False, raw_info = FALSE)  
This function returns the metadata of a connected data asset.  
The function takes the following parameters:  
name_or_item: (Required) The name of the connected data asset or an item like those returned by list_connected_data() with_datasourcetype: (Optional) Returns additional information about the data source type of the associated connected data asset. raw_info: (Optional) Returns the full set of metadata. By default, the parameter is FALSE and only a subset of the properties is returned.  
get_stored_data(name_or_item, raw_info = FALSE)  
This function returns the metadata of a stored data asset.  
The function takes the following parameters:  
name_or_item: (Required) The name of the stored data asset or an item like those returned by list_stored_data() raw_info: (Optional) Returns the full set of metadata. By default, the parameter is FALSE and only a subset of the properties is returned.  
list_attachments(name_or_item_or_asset, asset_type=None, raw_info = FALSE)  
This function returns a list of the attachments of an asset.  
The function takes the following parameters:"
151	ibm-watson-studio-lib for R	"The ibm-watson-studio-lib functions
Browse project assets
list_attachments(name_or_item_or_asset, asset_type=None, raw_info = FALSE)  
This function returns a list of the attachments of an asset.  
The function takes the following parameters:  
assets <- wslib$list_stored_data() wslib$show(assets) asset <- assets[[1]] attachments <- wslib$assets$list_attachments(asset) wslib$show(attachments) buffer <- wslib$load_data(asset, attachments[[1]])  
name_or_item_or_asset: (Required) The name of the asset or an item like those returned by list_stored_data() or get_asset().  
asset_type: (Optional) The type of the asset. It defaults to type data_asset.  
raw_info: (Optional) Returns the full set of metadata. By default, the parameter is FALSE and only a subset of the properties is returned.  
Example of using the list_attachments function to read an attachment of a stored data asset:  
Parent topic: Using ibm-watson-studio-lib"
152	Data imputation in AutoAI experiments	Data imputation is the means of replacing missing values in your data set with substituted values. If you enable imputation, you can specify how missing values are interpolated in your data.
153	Data imputation in AutoAI experiments	"Imputation by experiment type
Imputation methods depend on the type of experiment that you build.  
For classification and regression you can configure categorical and numerical imputation methods. For timeseries problems, you can choose from a set of imputation methods to apply to numerical columns. When the experiment runs, the best performing method from the set is applied automatically. You can also specify a specific value as a replacement value."
154	Data imputation in AutoAI experiments	"Enabling imputation
To view and set imputation options:  
Click Experiment settings when you configure your experiment. Click the Data source option. Click Enable data imputation. Note that if you do not explicitly enable data imputation but your data source has missing values, AutoAI warns you and applies default imputation methods. See imputation details. Select options in the Imputation section. Optionally set a threshold for the percentage of imputation acceptable for a column of data. If the percentage of missing values exceeds the specified threshold, the experiment fails. To resolve, update the data source or adjust the threshold."
155	Data imputation in AutoAI experiments	"Configuring imputation for classification and regression experiments
Choose one of these methods for imputing missing data in binary classification, multiclass classification, or regression experiments. Note that you can have one method for completing values for text-based (categorical) data and another for numerical data."
156	Data imputation in AutoAI experiments	"Configuring imputation for timeseries experiments
Choose some or all of these methods. When multiple methods are selected, the best-performing method is automatically applied for the experiment.  
Note: Imputation is not supported for date or time values."
157	Data imputation in AutoAI experiments	"Next steps
Data imputation implementation details for time series experiments  
Parent topic: AutoAI overview"
158	Building a time series experiment	Use AutoAI to create a time series experiment to predict future activity, such as stock prices or temperatures, over a specified date or time range.
159	Building a time series experiment	"Time series overview
A time series experiment is a method of forecasting that uses historical observations to predict future values. The experiment automatically builds many pipelines using machine learning models, such as random forest regression and Support Vector Machines (SVMs), as well as statistical time series models, such as ARIMA and Holt-Winters. Then, the experiment recommends the best pipeline according to the pipeline performance evaluated on a holdout data set or backtest data sets.  
Unlike a standard AutoAI experiment, which builds a set of pipelines to completion then ranks them. A time series experiment evaluates pipelines earlier in the process and only completes and test the best-performing pipelines.  
For details on the various stages of training and testing a time series experiment, see Time series implementation details."
160	Building a time series experiment	"Predicting anomalies in a time series experiment
You can configure your time series experiment to predict anomalies (outliers) in your data or predictions. To configure anomaly prediction for your experiment, follow the steps in Creating a time series anomaly prediction model."
161	Building a time series experiment	"Using supporting features to improve predictions
When you configure your time series experiment, you can choose to specify supporting features, also known as exogenous features. Supporting features are features that influence or add context to the prediction target. For example, if you are forecasting ice cream sales, daily temperature would be a logical supporting feature that would make the forecast more accurate."
162	Building a time series experiment	"Using supporting features to improve predictions
Leveraging future values for supporting features
If you know the future values for the supporting features, you can leverage those future values when you deploy the model. For example, if you are training a model to forecast future t-shirt sales, you can include promotional discounts as a supporting feature to enhance the prediction. Inputting the future value of the promotion then makes the forecast more accurate."
163	Building a time series experiment	"Data requirements
These are the current data requirements for training a time series experiment:  
The training data must be a single file in CSV format.  
The file must contain one or more time series columns and optionally contain a timestamp column. For a list of supported date/time formats, see AutoAI time series implementation details.  
If the data source contains a timestamp column, ensure that the data is sampled at uniform frequency. That is, the difference in timestamps of adjacent rows is the same. For example, data can be in increments of 1 minute, 1 hour, or one day. The specified timestamp is used to determine the lookback window to improve the model accuracy.  
Note:  
If the file size is larger than 1 GB, sort the data in descending order by the timestamp, and only the first 1 GB is used to train the experiment.  
If the data source does not contain a timestamp column, ensure that the data is sampled at regular intervals and sorted in ascending order according to the sample date/time. That is, the value in the first row is the oldest, and the value in the last row is the most recent.  
Note: If the file size is larger than 1 GB, truncate the file so it is smaller than 1 GB.  
Select what data to use when training the final pipelines. If you choose to include training data only, the generated notebooks will include a cell for retrieving the holdout data used to evaluate each pipeline.  
Choose data from your project or upload it from your file system or from the asset browser, then click Continue. Click the preview icon , after the data source name to review your data. Optionally, you can add a second file as holdout data for testing the trained pipelines."
164	Building a time series experiment	"Configuring a time series experiment
When you configure the details for an experiment, click Yes to Enable time series and complete the experiment details.  
The prediction summary shows you the experiment type and the metric that is selected for optimizing the experiment."
165	Building a time series experiment	"Configuring experiment settings
To configure more details for your time series experiment, click Experiment settings."
166	Building a time series experiment	"Configuring experiment settings
General prediction settings
On the General panel for prediction settings, you can optionally change the metric used to optimize the experiment or specify the algorithms to consider or the number of pipelines to generate."
167	Building a time series experiment	"Configuring experiment settings
Time series configuration details
On the Time series pane for prediction settings, configure the details for how to train the experiment and generate predictions."
168	Building a time series experiment	"Configuring data source settings
To configure details for your input data, click Experiment settings and select Data source."
169	Building a time series experiment	"Configuring data source settings
General data source settings
On the General panel for data source settings, you can modify your dataset to interpolate missing values, split your dataset into training and holdout data, and input supporting features."
170	Building a time series experiment	"Configuring time series data
To configure the time series data, you can adjust the settings for the time series data that is related to backtesting the experiment. Backtesting provides a means of validating a time-series model by using historical data.  
In a typical machine learning experiment, you can hold back part of the data randomly to test the resulting model for accuracy. To validate a time series model, you must preserve the time order relationship between the training data and testing data.  
The following steps describe the backtest method:  
The training data length is determined based on the number of backtests, gap length, and holdout size. To learn more about these parameters, see Building a time series experiment. Starting from the oldest data, the experiment is trained using the training data. The experiment is evaluated on the first validation data set. If the gap length is non-zero, any data in the gap is skipped over. The training data window is advanced by increasing the holdout size and gap length to form a new training set. A fresh experiment is trained with this new data and evaluated with the next validation data set. The prior two steps are repeated for the remaining backtesting periods.  
To adjust the backtesting configuration:  
Open Experiment settings. From Data sources, click the Time series. (Optional): Adjust the settings as shown in the table.  
The visualization for the configuration settings illustrates the backtesting flow. The graphic is interactive, so you can manipulate the settings from the graphic or from the configuration fields. For example, by adjusting the gap length, you can see model validation results on earlier time periods of the data without increasing the number of backtests."
171	Building a time series experiment	"Interpreting the experiment results
After you run your time series experiment, you can examine the resulting pipelines to get insights into the experiment details. Pipelines that use Supporting features are indicated by SUP enhancement tag to distinguish them from pipelines that don’t use these features. To view details:  
Hover over nodes on the visualization to get details about the pipelines as they are being generated. Toggle to the Progress Map view to see a different view of the training process. You can hover over each node in the process for details. After the final pipelines are completed and written to the leaderboard, you can click a pipeline to see the performance details. Click View discarded pipelines to view the algorithms that are used for the pipelines that are not selected as top performers. Save the experiment code as notebook that you can review. Save a particular pipeline as a notebook that you can review.  
Watch this video to see how to run a time series experiment and create a model in a Jupyter notebook using training and holdout data.  
Video disclaimer: Some minor steps and graphical elements in this video might differ from your platform.  
This video provides a visual method to learn the concepts and tasks in this documentation."
172	Building a time series experiment	"Next steps
Follow a step-by-step tutorial to train a univariate time series model to predict minimum temperatures by using sample data. Follow a step-by-step tutorial to train a time series experiment with supporting features. Learn about scoring a deployed time series model. Learn about using the API for AutoAI time series experiments."
173	Building a time series experiment	"Additional resources
For an introduction to forecasting with AutoAI time series experiments, see the blog post Right on time(series): Introducing Watson Studio’s AutoAI Time Series. For more information about creating a time series experiment, see this blog post about creating a new time series experiment. Read a blog post about adding supporting features to a time series experiment. Review a sample notebook for a time series experiment with supporting features. Read a blog post about adding supporting features to a time series experiment using the API."
174	Building a time series experiment	"Next steps
Tutorial: AutoAI univariate time series experiment Tutorial: AutoAI supporting features time series experiment Time series experiment implementation details Scoring a time series model  
Parent topic: AutoAI overview"
175	Customizing with third-party and private Python libraries	"If your model requires custom components such as user-defined transformers, estimators, or user-defined tensors, you can create a custom software specification that is derived from a base, or a predefined specification. Python functions and Python scripts also support custom software specifications.  
You can use custom software specification to reference any third-party libraries, user-created Python packages, or both. Third-party libraries or user-created Python packages must be specified as package extensions so that they can be referenced in a custom software specification.  
You can customize deployment runtimes in these ways:  
Define customizations in a Watson Studio project and then promote them to a deployment space Create package extensions and custom software specifications in a deployment space by using the Watson Machine Learning Python client  
For more information, see Troubleshooting."
176	Customizing with third-party and private Python libraries	"Defining customizations in a Watson Studio project and then promoting them to a deployment space
Environments in Watson Studio projects can be customized to include third-party libraries that can be installed from Anaconda or from the PyPI repository.  
For more information, see Environments.  
As part of custom environment creation, these steps are performed internally (visible to the user):  
A package extension that contains the details of third-party libraries is created in conda YAML format. A custom software specification with the same name as the custom environment is created and the package extension that is created is associated with this custom software specification.  
The models or Python functions/scripts created with the custom environment must reference the custom software specification when they are saved in Watson Machine Learning repository in the project scope."
177	Customizing with third-party and private Python libraries	"Defining customizations in a Watson Studio project and then promoting them to a deployment space
Propagating software specifications and package extensions from projects to deployment spaces
To export custom software specifications and package extensions that were created in a Watson Studio project to a deployment space:  
From your project interface, click the Manage tab. Select Environments. Click the Templates tab. From your custom environment's Options menu, select Promote to space.  
Alternatively, when you promote any model or Python function that is associated with a custom environment from a Watson Studio project to a deployment space, the associated custom software specification and package extension is also promoted to the deployment space.  
If you want to update software specifications and package extensions after you promote them to deployment space, follow these steps:  
In the deployment space, delete the software specifications, package extensions, and associated models (optional) by using the Watson Machine Learning Python client. In a project, promote the model, function, or script that is associated with the changed custom software specification and package extension to the space.  
Software specifications are also included when you import a project or space that includes one."
178	Customizing with third-party and private Python libraries	"Creating package extensions and custom software specifications in a deployment space by using the Watson Machine Learning Python client
You can use the Watson Machine Learning APIs or Python client to define a custom software specification that is derived from a base specification.  
High-level steps to create a custom software specification that uses third-party libraries or user-created Python packages:  
Optional: Save a conda YAML file that contains a list of third-party libraries or save a user-created Python library and create a package extension.  
Note: This step is not required if the model does not have any dependency on a third-party library or a user-created Python library.  
Create a custom software specification  
Add a reference of the package extensions to the custom software specification that you created."
179	Customizing with third-party and private Python libraries	"Creating package extensions and custom software specifications in a deployment space by using the Watson Machine Learning Python client
Saving a conda YAML file that contains a list of third-party libraries
To save a conda YAML file that contains a list of third-party libraries as a package extension and create a custom software specification that is linked to the package extension:"
180	Customizing with third-party and private Python libraries	"Creating package extensions and custom software specifications in a deployment space by using the Watson Machine Learning Python client
Saving a conda YAML file that contains a list of third-party libraries
metadata = { wml_client.spaces.ConfigurationMetaNames.NAME: 'examples-create-software-spec', wml_client.spaces.ConfigurationMetaNames.DESCRIPTION: 'For my models' } space_details = wml_client.spaces.store(meta_props=metadata) space_uid = wml_client.spaces.get_id(space_details) # set the default space wml_client.set.default_space(space_uid) # see available meta names for software specs print('Available software specs configuration:', wml_client.software_specifications.ConfigurationMetaNames.get()) wml_client.software_specifications.list() asset_id = 'undefined' pe_asset_id = 'undefined' pe_metadata = { wml_client.package_extensions.ConfigurationMetaNames.NAME: 'My custom library', # optional: # wml_client.software_specifications.ConfigurationMetaNames.DESCRIPTION: wml_client.package_extensions.ConfigurationMetaNames.TYPE: 'conda_yml' } name: add-regex-package dependencies: - regex pe_asset_details = wml_client.package_extensions.store( meta_props=pe_metadata, file_path='customlibrary.yaml' ) pe_asset_id = wml_client.package_extensions.get_id(pe_asset_details) # Get the id of the base software specification base_id = wml_client.software_specifications.get_id_by_name('default_py3.9') # create the metadata for software specs ss_metadata = { wml_client.software_specifications.ConfigurationMetaNames.NAME: 'Python 3.9 with pre-installed ML package', wml_client.software_specifications.ConfigurationMetaNames.DESCRIPTION: 'Adding some custom libraries like regex', # optional wml_client.software_specifications.ConfigurationMetaNames.BASE_SOFTWARE_SPECIFICATION: {'guid': base_id}, wml_client.software_specifications.ConfigurationMetaNames.PACKAGE_EXTENSIONS: [{'guid': pe_asset_id}] } # store the software spec ss_asset_details = wml_client.software_specifications.store(meta_props=ss_metadata) # get the id of the new asset asset_id = wml_client.software_specifications.get_id(ss_asset_details) # view new software specification details import pprint as pp ss_asset_details = wml_client.software_specifications.get_details(asset_id) print('Package extensions', pp.pformat("
181	Customizing with third-party and private Python libraries	"Creating package extensions and custom software specifications in a deployment space by using the Watson Machine Learning Python client
Saving a conda YAML file that contains a list of third-party libraries
= wml_client.software_specifications.get_id(ss_asset_details) # view new software specification details import pprint as pp ss_asset_details = wml_client.software_specifications.get_details(asset_id) print('Package extensions', pp.pformat( ss_asset_details['entity']['software_specification']['package_extensions'] ))"
182	Customizing with third-party and private Python libraries	"Creating package extensions and custom software specifications in a deployment space by using the Watson Machine Learning Python client
Saving a conda YAML file that contains a list of third-party libraries
Authenticate and create the client.  
Refer to Authentication.  
Create and set the default deployment space, then list available software specifications.  
Create the metadata for package extensions to add to the base specification.  
Create a yaml file that contains the list of packages and then save it as customlibrary.yaml.  
Example yaml file:  
For more information, see Examples of customizations.  
Store package extension information.  
Create the metadata for the software specification and store the software specification."
183	Customizing with third-party and private Python libraries	"Creating package extensions and custom software specifications in a deployment space by using the Watson Machine Learning Python client
Saving a user-created Python library and creating a package extension
For more information, see Requirements for using custom components in models.  
To save a user-created Python package as a package extension and create a custom software specification that is linked to the package extension:"
184	Customizing with third-party and private Python libraries	"Creating package extensions and custom software specifications in a deployment space by using the Watson Machine Learning Python client
Saving a user-created Python library and creating a package extension
metadata = { wml_client.spaces.ConfigurationMetaNames.NAME: 'examples-create-software-spec', wml_client.spaces.ConfigurationMetaNames.DESCRIPTION: 'For my models' } space_details = wml_client.spaces.store(meta_props=metadata) space_uid = wml_client.spaces.get_id(space_details) # set the default space wml_client.set.default_space(space_uid) # see available meta names for software specs print('Available software specs configuration:', wml_client.software_specifications.ConfigurationMetaNames.get()) wml_client.software_specifications.list() asset_id = 'undefined' pe_asset_id = 'undefined' pe_metadata = { wml_client.package_extensions.ConfigurationMetaNames.NAME: 'My Python library', # optional: # wml_client.software_specifications.ConfigurationMetaNames.DESCRIPTION: wml_client.package_extensions.ConfigurationMetaNames.TYPE: 'pip.zip' } python_lib_file_path=""my-python-library-0.1.zip"" pe_asset_details = wml_client.package_extensions.store( meta_props=pe_metadata, file_path=python_lib_file_path ) pe_asset_id = wml_client.package_extensions.get_id(pe_asset_details) # Get the id of the base software specification base_id = wml_client.software_specifications.get_id_by_name('default_py3.9') # create the metadata for software specs ss_metadata = { wml_client.software_specifications.ConfigurationMetaNames.NAME: 'Python 3.9 with pre-installed ML package', wml_client.software_specifications.ConfigurationMetaNames.DESCRIPTION: 'Adding some custom libraries like regex', # optional wml_client.software_specifications.ConfigurationMetaNames.BASE_SOFTWARE_SPECIFICATION: {'guid': base_id}, wml_client.software_specifications.ConfigurationMetaNames.PACKAGE_EXTENSIONS: [{'guid': pe_asset_id}] } # store the software spec ss_asset_details = wml_client.software_specifications.store(meta_props=ss_metadata) # get the id of the new asset asset_id = wml_client.software_specifications.get_id(ss_asset_details) # view new software specification details import pprint as pp ss_asset_details = wml_client.software_specifications.get_details(asset_id) print('Package extensions',"
185	Customizing with third-party and private Python libraries	"Creating package extensions and custom software specifications in a deployment space by using the Watson Machine Learning Python client
Saving a user-created Python library and creating a package extension
the new asset asset_id = wml_client.software_specifications.get_id(ss_asset_details) # view new software specification details import pprint as pp ss_asset_details = wml_client.software_specifications.get_details(asset_id) print('Package extensions', pp.pformat( ss_asset_details['entity']['software_specification']['package_extensions'] ))"
186	Customizing with third-party and private Python libraries	"Creating package extensions and custom software specifications in a deployment space by using the Watson Machine Learning Python client
Saving a user-created Python library and creating a package extension
Authenticate and create the client.  
Refer to Authentication.  
Create and set the default deployment space, then list available software specifications.  
Create the metadata for package extensions to add to the base specification.  
Note:  
You can specify pip_zip only as a value for the wml_client.package_extensions.ConfigurationMetaNames.TYPE metadata property.  
Specify the path of the user-created Python library.  
For more information, see Requirements for using custom components in models.  
Store package extension information.  
Create the metadata for the software specification and store the software specification."
187	Customizing with third-party and private Python libraries	"Troubleshooting
When a conda yml based custom library installation fails with this error: Encountered error while installing custom library, try these alternatives:  
name: <conda yml file name> dependencies: - numpy - pip: - pandas==1.2.5  
Use a different version of the same package that is available in Anaconda for the concerned Python version.  
Install the library from the pypi repository, by using pip. Edit the conda yml installation file contents:  
Parent topic: Customizing deployment runtimes"
188	AutoAI libraries for Python	The autoai-lib library for Python contains a set of functions that help you to interact with IBM Watson Machine Learning AutoAI experiments. Using the autoai-lib library, you can review and edit the data transformations that take place in the creation of the pipeline. Similarly, you can use the autoai-ts-libs library to interact with pipeline notebooks for time series experiments.
189	AutoAI libraries for Python	"Installing autoai-lib or autoai-ts-libs for Python
Follow the instructions in Installing custom libraries to install autoai-lib or autoai-ts-libs."
190	AutoAI libraries for Python	"Installing autoai-lib or autoai-ts-libs for Python
Using autoai-lib and autoai-ts-libs for Python
The autoai-lib and autoai-ts-libs library for Python contain functions that help you to interact with IBM Watson Machine Learning AutoAI experiments. Using the autoai-lib library, you can review and edit the data transformations that take place in the creation of classification and regression pipelines. Using the autoai-ts-libs library, you can review the data transformations that take place in the creation of time series (forecast) pipelines."
191	AutoAI libraries for Python	"Installing autoai-lib or autoai-ts-libs for Python
Installing autoai-lib and autoai-ts-libs for Python
Follow the instructions in Installing custom libraries to install autoai-lib and autoai-ts-libs."
192	AutoAI libraries for Python	"Installing autoai-lib or autoai-ts-libs for Python
Configuring library-level logging
You can configure the library logger autoai_libs for your requirements. For example, you can define and attach handlers or configure filters to customize log details such as warning and error handling.  
If you do not update the configuration you will get the default behavior for logging. For example, messages of severity warning and higher (that is warning, error, and critical) will be printed to stderr (standard error) without any special formatting. For example, warning messages display as WARNING with no special formatting. For more information on how to configure logging, and to view examples, refer to the documentation for autoai-lib."
193	AutoAI libraries for Python	"The autoai-lib functions
The instantiated project object that is created after you import the autoai-lib library exposes these functions:  
Selects a subset of columns of a numpy array  
Usage:  
Removes spaces and special characters from string columns of an input numpy array X.  
Usage:  
Given a numpy array and a reference list of missing values for it, replaces missing values with a special value (typically a special missing value such as np.nan).  
Usage:  
Given a numpy array and a reference list of known values for each column, replaces values that are not part of a reference list with a special value (typically np.nan). This method is typically used to remove labels for columns in a test data set that has not been seen in the corresponding columns of the training data set.  
Usage:  
Converts a 1-D numpy array of strings that represent booleans to floats and replaces missing values with np.nan. Also changes type of array from 'object' to 'float'.  
Usage:  
This transformer is a wrapper for categorical imputer. Internally it currently uses sklearn SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)  
Usage:  
This method is a wrapper for categorical encoder. If encoding parameter is 'ordinal', internally it currently uses sklearn OrdinalEncoder. If encoding parameter is 'onehot', or 'onehot-dense' internally it uses sklearn OneHotEncoder  
Usage:  
Transforms a float64 numpy array to float32.  
Usage:  
Given numpy array X and dtypes_list that denotes the types of its columns, it replaces columns of strings that represent floats (type 'float_str' in dtypes_list) to columns of floats and replaces their missing values with np.nan.  
Usage:  
This method is a wrapper for numerical imputer.  
Usage:  
This parameter is a wrapper for scaling of numerical variables. It currently uses sklearn StandardScaler internally.  
Usage:  
Rearranges columns or rows of a numpy array based on a list of indexes.  
Usage:"
194	AutoAI libraries for Python	"The autoai-lib functions
Feature transformation
These methods apply to the feature transformations described in AutoAI implementation details.  
For unary stateless functions, such as square or log, use TA1.  
Usage:  
For binary stateless functions, such as sum, product, use TA2.  
Usage:  
For unary state-based transformations (with fit/transform) use, such as frequent count.  
Usage:  
For binary state-based transformations (with fit/transform) use, such as group-by.  
Usage:  
For a transform that applies at the data level, such as PCA, use TAM.  
Usage:  
TGen is a general wrapper and can be used for most functions (might not be most efficient though).  
Usage:  
Feature selection, type 1 (using pairwise correlation between each feature and target.)  
Usage:  
Feature selection, type 2.  
Usage:"
195	AutoAI libraries for Python	"The autoai-ts-libs functions
The combination of transformers and estimators are designed and chosen for each pipeline by the AutoAI Time Series system. Changing the transformers or the estimators in the generated pipeline notebook can cause unexpected results or even failure. We do not recommend you change the notebook for generated pipelines, thus we do not currently offer the specification of the functions for the autoai-ts-libs library."
196	AutoAI libraries for Python	"Learn more
Selecting an AutoAI model  
Parent topic: Saving an AutoAI generated notebook"
197	Hyperparameter definitions	"Definitions of hyperparameters used in the experiment training. One or more of these hyperparameter options might be used, depending on your framework and fusion method.  
Parent topic: Frameworks, fusion methods, and Python versions"
198	Entity extraction	"The Watson Natural Language Processing Entity extraction models extract entities from input text.  
For details, on available extraction types, refer to these sections:  
Machine-learning-based extraction for general entities Machine-learning-based extraction for PII entities Rule-based extraction for general entities Rule-based extraction for PII entities"
199	Entity extraction	"Machine-learning-based extraction for general entities
The machine-learning-based extraction models are trained on labeled data for the more complex entity types such as person, organization and location.  
Capabilities  
The entity models extract entities from the input text. The following types of entities are recognized:  
Date Duration Facility Geographic feature Job title Location Measure Money Ordinal Organization Person Time  
Available workflows and blocks differ, depending on the runtime used."
200	Entity extraction	"Machine-learning-based extraction for general entities
Machine-learning-based workflows for general entities in Runtime 23.1
Workflow names  
entity-mentions_transformer-workflow_multilingual_slate.153m.distilled: this workflow can be used on both CPUs and GPUs. entity-mentions_transformer-workflow_multilingual_slate.153m.distilled-cpu: this workflow is optimized for CPU-based runtimes.  
Supported languages  
Entity extraction is available for the following languages.  
For a list of the language codes and the corresponding language, see Language codes:  
ar, cs, da, de, en, es, fi, fr, he, hi, it, ja, ko, nb, nl, nn, pt, ro, ru, sk, sv, tr, zh-cn  
Code sample  
Output of the code sample:"
201	Entity extraction	"Machine-learning-based extraction for PII entities
Block names entity-mentions_bilstm_en_pii  
The entity-mentions_bilstm_en_pii machine-learning based extraction model is trained on labeled data for types person and location.  
Capabilities  
The entity-mentions_bilstm_en_pii block recognizes the following types of entities:  
Dependencies on other blocks  
The following block must run before you can run the entity-mentions_bilstm_en_pii block:  
syntax_izumo_en_stock  
Code sample  
Output of the code sample:"
202	Entity extraction	"Rule-based extraction for general entities
The rule-based model entity-mentions_rbr_xx_stock identifies syntactically regular entities.  
Block name entity-mentions_rbr_xx_stock  
Capabilities  
Rule-based extraction handles syntactically regular entity types. The entity block extract entities from the input text. The following types of entities are recognized:  
PhoneNumber EmailAddress Number Percent IPAddress HashTag TwitterHandle URLDate  
Supported languages  
Entity extraction is available for the following languages. For a list of the language codes and the corresponding language, see Language codes.  
ar, cs, da, de, en, es, fi, fr, he, hi, it, ja, ko, nb, nl, nn, pt, ro, ru, sk, sv, tr, zh-cn, zh-tw  
Dependencies on other blocks  
None  
Code sample  
Output of the code sample:"
203	Entity extraction	"Rule-based extraction for PII entities
The rule-based model entity-mentions_rbr_multi_pii handles the majority of the types by identifying common formats of PII entities and performing possible checksum or validations as appropriate for each entity type. For example, credit card number candidates are validated using the Luhn algorithm.  
Block name entity-mentions_rbr_multi_pii  
Capabilities  
The entity block entity-mentions_rbr_multi_pii recognizes the following types of entities:  
Some PII entity type names are country-specific. The _ in the following entity types is a placeholder for a country code.  
BankAccountNumber.BBAN._ : These are more variable national bank account numbers and the extraction is mostly language-specific without a general checksum algorithm. BankAccountNumber.IBAN._ : Highly standardized IBANs are supported in a language-independent way and with a checksum algorithm. NationalNumber.NationalID._: These national IDs don’t have a (published) checksum algorithm, and are being extracted on a language-specific basis. NationalNumber.Passport._ : Checksums are implemented only for the countries where a checksum algorithm exists. These are specifically extracted language with additional context restrictions. NationalNumber.TaxID._ : These IDs don't have a (published) checksum algorithm, and are being extracted on a language-specific basis.  
Which entity types are available for which languages and which country code to use is listed in the following table.  
Dependencies on other blocks  
None  
Code sample  
Output of the code sample:  
Parent topic: Watson Natural Language Processing task catalog"
204	Importing models to a deployment space	"Import machine learning models trained outside of IBM Watson Machine Learning so that you can deploy and test the models. Review the model frameworks that are available for importing models.  
Here, to import a trained model means:  
Store the trained model in your Watson Machine Learning repository Optional: Deploy the stored model in your Watson Machine Learning service  
and repository means a Cloud Object Storage bucket. For more information, see Creating deployment spaces.  
You can import a model in these ways:  
Directly through the UI By using a path to a file By using a path to a directory Import a model object  
For more information, see Importing models by ML framework.  
For more information, see Things to consider when you import models.  
For an example of how to add a model programmatically by using the Python client, refer to this notebook:  
Use PMML to predict iris species.  
For an example of how to add a model programmatically by using the REST API, refer to this notebook:  
Use scikit-learn to predict diabetes progression"
205	Importing models to a deployment space	"Available ways to import models, per framework type
This table lists the available ways to import models to Watson Machine Learning, per framework type."
206	Importing models to a deployment space	"Available ways to import models, per framework type
Adding a model by using UI
Note:  
If you want to import a model in the PMML format, you can directly import the model .xml file.  
To import a model by using UI:  
From the Assets tab of your space in Watson Machine Learning, click Import assets. Select Local file and then select Model. Select the model file that you want to import and click Import.  
The importing mechanism automatically selects a matching model type and software specification based on the version string in the .xml file."
207	Importing models to a deployment space	"Available ways to import models, per framework type
Importing a model object
Note:  
This import method is supported by a limited number of ML frameworks. For more information, see Available ways to import models, per framework type.  
To import a model object:  
If your model is located in a remote location, follow Downloading a model that is stored in a remote location. Store the model object in your Watson Machine Learning repository. For more information, see Storing model in Watson Machine Learning repository."
208	Importing models to a deployment space	"Available ways to import models, per framework type
Importing a model by using a path to a file
Note:  
This import method is supported by a limited number of ML frameworks. For more information, see Available ways to import models, per framework type.  
To import a model by using a path to a file:  
!cp <saved model> <target directory> !cd <target directory> !tar -zcvf <saved model>.tar.gz <saved model> assets/ <saved model> variables/ variables/variables.data-00000-of-00001 variables/variables.index  
If your model is located in a remote location, follow Downloading a model that is stored in a remote location to download it.  
If your model is located locally, place it in a specific directory:  
For Scikit-learn, XGBoost, Tensorflow, and PyTorch models, if the downloaded file is not a .tar.gz archive, make an archive:  
The model file must be at the top-level folder of the directory, for example:  
Use the path to the saved file to store the model file in your Watson Machine Learning repository. For more information, see Storing model in Watson Machine Learning repository."
209	Importing models to a deployment space	"Available ways to import models, per framework type
Importing a model by using a path to a directory
Note:  
This import method is supported by a limited number of ML frameworks. For more information, see Available ways to import models, per framework type.  
To import a model by using a path to a directory:  
!cp <saved model> <target directory> !cd <target directory> assets/ <saved model> variables/ variables/variables.data-00000-of-00001 variables/variables.index  
If your model is located in a remote location, refer to Downloading a model stored in a remote location.  
If your model is located locally, place it in a specific directory:  
For scikit-learn, XGBoost, Tensorflow, and PyTorch models, the model file must be at the top-level folder of the directory, for example:  
Use the directory path to store the model file in your Watson Machine Learning repository. For more information, see Storing model in Watson Machine Learning repository."
210	Importing models to a deployment space	"Available ways to import models, per framework type
Downloading a model stored in a remote location
Follow this sample code to download your model from a remote location:"
211	Importing models to a deployment space	"Things to consider when you import models
To learn more about importing a specific model type, see:  
Models saved in PMML format Spark MLlib models Scikit-learn models XGBoost models TensorFlow models PyTorch models  
To learn more about frameworks that you can use with Watson Machine Learning, see Supported frameworks."
212	Importing models to a deployment space	"Things to consider when you import models
Models saved in PMML format
The only available deployment type for models that are imported from PMML is online deployment. The PMML file must have the .xml file extension. PMML models cannot be used in an SPSS stream flow. The PMML file must not contain a prolog. Depending on the library that you are using when you save your model, a prolog might be added to the beginning of the file by default. For example, if your file contains a prolog string such as spark-mllib-lr-model-pmml.xml, remove the string before you import the PMML file to the deployment space.  
Depending on the library that you are using when you save your model, a prolog might be added to the beginning of the file by default, like in this example:  
You must remove that prolog before you can import the PMML file to Watson Machine Learning."
213	Importing models to a deployment space	"Things to consider when you import models
Spark MLlib models
Only classification and regression models are available. Custom transformers, user-defined functions, and classes are not available."
214	Importing models to a deployment space	"Things to consider when you import models
Scikit-learn models
.pkl and .pickle are the available import formats. To serialize or pickle the model, use the joblib package. Only classification and regression models are available. Pandas Dataframe input type for predict() API is not available. The only available deployment type for scikit-learn models is online deployment."
215	Importing models to a deployment space	"Things to consider when you import models
XGBoost models
.pkl and .pickle are the available import formats. To serialize or pickle the model, use the joblib package. Only classification and regression models are available. Pandas Dataframe input type for predict() API is not available. The only available deployment type for XGBoost models is online deployment."
216	Importing models to a deployment space	"Things to consider when you import models
TensorFlow models
.pb, .h5, and .hdf5 are the available import formats. To save or serialize a TensorFlow model, use the tf.saved_model.save() method. tf.estimator is not available. The only available deployment types for TensorFlow models are: online deployment and batch deployment."
217	Importing models to a deployment space	"Things to consider when you import models
PyTorch models
torch.onnx.export(<model object>, <prediction/training input data>, ""<serialized model>.onnx"", verbose=True, input_names=<input tensor names>, output_names=<output tensor names>)  
The only available deployment type for PyTorch models is online deployment.  
For a Pytorch model to be importable to Watson Machine Learning, it must be previously exported to .onnx format. Refer to this code."
218	Importing models to a deployment space	"Storing a model in your Watson Machine Learning repository
Use this code to store your model in your Watson Machine Learning repository:  
Notes:  
torch.onnx.export(net, x, 'lin_reg1.onnx', verbose=True, keep_initializers_as_inputs=True, opset_version=9)  
Depending on the model framework used, <your model> can be the actual model object, a full path to a saved model file, or a path to a directory where the model file is located. For more information, see Available ways to import models, per framework type.  
For a list of available software specifications to use as <software specification name>, use the client.software_specifications.list() method.  
For a list of available model types to use as model_type, refer to Software specifications and hardware specifications for deployments.  
When you export a Pytorch model to the .onnx format, specify the keep_initializers_as_inputs=True flag and set opset_version to 9 (Watson Machine Learning deployments use the caffe2 ONNX runtime that doesn't support opset versions higher than 9).  
To learn more about how to create the <your credentials> dictionary, refer to Watson Machine Learning authentication.  
Parent topic: Assets in deployment spaces"
219	Federated Learning XGBoost tutorial for UI	"This tutorial demonstrates the usage of Federated Learning with the goal of training a machine learning model with data from different users without having users share their data. The steps are done in a low code environment with the UI and with an XGBoost framework.  
In this tutorial you learn to:  
Step 1: Start Federated Learning as the admin Step 2: Train model as a party  
Before you begin Start the aggregator  
Step 3: Save and deploy the model online Step 4: Score the model  
Notes:  
This is a step-by-step tutorial for running a UI driven Federated Learning experiment. To see a code sample for an API driven approach, go to Federated Learning XGBoost samples. In this tutorial, admin refers to the user that starts the Federated Learning experiment, and party refers to one or more users who send their model results after the experiment is started by the admin. While the tutorial can be done by the admin and multiple parties, a single user can also complete a full run through as both the admin and the party. For a simpler demonstrative purpose, in the following tutorial only one data set is submitted by one party. For more information on the admin and party, see Terminology."
220	Federated Learning XGBoost tutorial for UI	"Step 1: Start Federated Learning
In this section, you learn to start the Federated Learning experiment."
221	Federated Learning XGBoost tutorial for UI	"Step 1: Start Federated Learning
Before you begin
Log in to IBM Cloud. If you don't have an account, create one with any email.  
Create a Watson Machine Learning service instance if you do not have it set up in your environment.  
Log in to watsonx.  
Use an existing project or create a new one. You must have at least admin permission.  
Associate the Watson Machine Learning service with your project.  
In your project, click the Manage > Service & integrations. Click Associate service. Select your Watson Machine Learning instance from the list, and click Associate; or click New service if you do not have one to set up an instance.  
Log in to your Cloud Pak for Data cluster.  
Install the Watson Machine Learning image if you do not have it set up in your environment.  
Log in to your IBM Cloud Pak for Data host.  
Use an existing project or create a new one. You must have at least editor permission."
222	Federated Learning XGBoost tutorial for UI	"Step 1: Start Federated Learning
Start the aggregator
Create the Federated learning experiment asset:  
Click the Assets tab in your project.  
Click New asset > Train models on distributed data.  
Type a Name for your experiment and optionally a description.  
Verify the associated Watson Machine Learning instance under Select a machine learning instance. If you don't see a Watson Machine Learning instance associated, follow these steps:  
Click Associate a Machine Learning Service Instance.  
Select an existing instance and click Associate, or create a New service.  
Click Reload to see the associated service.  
Click Next.  
Configure the experiment.  
On the Configure page, select a Hardware specification.  
Under the Machine learning framework dropdown, select scikit-learn.  
For the Model type, select XGBoost.  
For the Fusion method, select XGBoost classification fusion  
Define the hyperparameters.  
Set the value for the Rounds field to 5.  
Accept the default values for the rest of the fields.  
Click Next.  
Select remote training systems.  
Click Add new systems.  
Give your Remote Training System a name.  
Under Allowed identities, select the user that will participate in the experiment, and then click Add. You can add as many allowed identities as participants in this Federated Experiment training instance. For this tutorial, choose only yourself.Any allowed identities must be part of the project and have at leastAdmin permission.  
When you are finished, click Add systems.  
Return to the Select remote training systems page, verify that your system is selected, and then click Next.  
Review your settings, and then click Create.  
Watch the status. Your Federated Learning experiment status is Pending when it starts. When your experiment is ready for parties to connect, the status will change to Setup – Waiting for remote systems. This may take a few minutes."
223	Federated Learning XGBoost tutorial for UI	"Step 2: Train model as a party
adult.csv adult_sklearn_data_handler.py rts_<RTS Name>_<RTS ID>.py  
Ensure that you are using the same Python version as the admin. Using a different Python version might cause compatibility issues. To see Python versions compatible with different frameworks, see Frameworks and Python version compatibility.  
Create a new local directory.  
Download the Adult data set into the directory with this command: wget https://api.dataplatform.cloud.ibm.com/v2/gallery-assets/entries/5fcc01b02d8f0e50af8972dc8963f98e/data -O adult.csv.  
Download the data handler by running wget https://raw.githubusercontent.com/IBMDataScience/sample-notebooks/master/Files/adult_sklearn_data_handler.py -O adult_sklearn_data_handler.py.  
Install Watson Machine Learning.  
If you are using Linux, run pip install 'ibm-watson-machine-learning[fl-rt22.2-py3.10]'. If you are using Mac OS with M-series CPU and Conda, download the installation script and then run ./install_fl_rt22.2_macos.sh <name for new conda environment>.You now have the party connector script, mnist_keras_data_handler.py, mnist-keras-test.pkl and mnist-keras-train.pkl, data handler in the same directory.  
Go back to the Federated Learning experiment page, where the aggregator is running. Click View Setup Information.  
Click the download icon next to the remote training system, and select Party connector script.  
Ensure that you have the party connector script, the Adult data set, and the data handler in the same directory. If you run ls -l, you should see:  
In the party connector script:  
""data"": { ""name"": ""AdultSklearnDataHandler"", ""path"": ""./adult_sklearn_data_handler.py"", ""info"": { ""txt_file"": ""./adult.csv"" }, },  
Authenticate using any method.  
Put in these parameters for the ""data"" section:  
where:  
name: Class name defined for the data handler. path: Path of where the data handler is located. info: Create a key value pair for the file type of local data set, or the path of your data set."
224	Federated Learning XGBoost tutorial for UI	"Step 2: Train model as a party
name: Class name defined for the data handler. path: Path of where the data handler is located. info: Create a key value pair for the file type of local data set, or the path of your data set.  
Run the party connector script: python3 rts_<RTS Name>_<RTS ID>.py.  
When all participating parties connect to the aggregator, the aggregator facilitates the local model training and global model update. Its status is Training. You can monitor the status of your Federated Learning experiment from the user interface.  
When training is complete, the party receives a Received STOP message on the party.  
Now, you can save the trained model and deploy it to a space."
225	Federated Learning XGBoost tutorial for UI	"Step 3: Save and deploy the model online
In this section, you learn how to save and deploy the model that you trained.  
Save your model.  
In your completed Federated Learning experiment, click Save model to project. Give your model a name and click Save. Go to your project home.  
Create a deployment space, if you don't have one.  
From the navigation menu , click Deployments.  
Click New deployment space.  
Fill in the fields, and click Create.  
Promote the model to a space.  
Return to your project, and click the Assets tab. In the Models section, click the model to view its details page. Click Promote to space. Choose a deployment space for your trained model. Select the Go to the model in the space after promoting it option. Click Promote.  
When the model displays inside the deployment space, click New deployment.  
Select Online as the Deployment type. Specify a name for the deployment. Click Create."
226	Federated Learning XGBoost tutorial for UI	"Step 4: Score the model
In this section, you learn to create a Python function to process the scoring data to ensure that it is in the same format that was used during training. For comparison, you will also score the raw data set by calling the Python function that we created."
227	Federated Learning XGBoost tutorial for UI	"Step 4: Score the model
def adult_scoring_function(): import pandas as pd from ibm_watson_machine_learning import APIClient wml_credentials = { ""url"": ""https://us-south.ml.cloud.ibm.com"", ""apikey"": ""<API KEY>"" } client = APIClient(wml_credentials) client.set.default_space('<SPACE ID>') # converts scoring input data format to pandas dataframe def create_dataframe(raw_dataset): fields = raw_dataset.get(""input_data"")[0].get(""fields"") values = raw_dataset.get(""input_data"")[0].get(""values"") raw_dataframe = pd.DataFrame( columns = fields, data = values ) return raw_dataframe # reuse preprocess definition from training data handler def preprocess(training_data): """""" Performs the following preprocessing on adult training and testing data: * Drop following features: 'workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country' * Map 'race', 'sex' and 'class' values to 0/1 * ' White': 1, ' Amer-Indian-Eskimo': 0, ' Asian-Pac-Islander': 0, ' Black': 0, ' Other': 0 * ' Male': 1, ' Female': 0 * Further details in Kamiran, F. and Calders, T. Data preprocessing techniques for classification without discrimination * Split 'age' and 'education' columns into multiple columns based on value :param training_data: Raw training data :type training_data: `pandas.core.frame.DataFrame :return: Preprocessed training data :rtype: `pandas.core.frame.DataFrame` """""" if len(training_data.columns)==15: # drop 'fnlwgt' column training_data = training_data.drop(training_data.columns[2], axis='columns') training_data.columns = ['age', 'workclass', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'class'] # filter out columns unused in training, and reorder columns training_dataset = training_data[['race', 'sex', 'age', 'education-num', 'class']] # map 'sex' and 'race' feature values based on sensitive attribute privileged/unpriveleged groups training_dataset['sex'] ="
228	Federated Learning XGBoost tutorial for UI	"Step 4: Score the model
columns training_dataset = training_data[['race', 'sex', 'age', 'education-num', 'class']] # map 'sex' and 'race' feature values based on sensitive attribute privileged/unpriveleged groups training_dataset['sex'] = training_dataset['sex'].map({' Female': 0, ' Male': 1}) training_dataset['race'] = training_dataset['race'].map({' Asian-Pac-Islander': 0, ' Amer-Indian-Eskimo': 0, ' Other': 0, ' Black': 0, ' White': 1}) # map 'class' values to 0/1 based on positive and negative classification training_dataset['class'] = training_dataset['class'].map({' <=50K': 0, ' >50K': 1}) training_dataset['age'] = training_dataset['age'].astype(int) training_dataset['education-num'] = training_dataset['education-num'].astype(int) # split age column into category columns for i in range(8): if i != 0: training_dataset['age' + str(i)] = 0 for index, row in training_dataset.iterrows(): if row['age'] < 20: training_dataset.loc[index, 'age1'] = 1 elif ((row['age'] < 30) & (row['age'] >= 20)): training_dataset.loc[index, 'age2'] = 1 elif ((row['age'] < 40) & (row['age'] >= 30)): training_dataset.loc[index, 'age3'] = 1 elif ((row['age'] < 50) & (row['age'] >= 40)): training_dataset.loc[index, 'age4'] = 1 elif ((row['age'] < 60) & (row['age'] >= 50)): training_dataset.loc[index, 'age5'] = 1 elif ((row['age'] < 70) & (row['age'] >= 60)): training_dataset.loc[index, 'age6'] = 1 elif row['age'] >= 70: training_dataset.loc[index, 'age7'] = 1 # split age column into multiple columns training_dataset['ed6less'] = 0 for i in range(13): if i >= 6: training_dataset['ed' + str(i)] = 0 training_dataset['ed12more'] = 0 for index,"
229	Federated Learning XGBoost tutorial for UI	"Step 4: Score the model
multiple columns training_dataset['ed6less'] = 0 for i in range(13): if i >= 6: training_dataset['ed' + str(i)] = 0 training_dataset['ed12more'] = 0 for index, row in training_dataset.iterrows(): if row['education-num'] < 6: training_dataset.loc[index, 'ed6less'] = 1 elif row['education-num'] == 6: training_dataset.loc[index, 'ed6'] = 1 elif row['education-num'] == 7: training_dataset.loc[index, 'ed7'] = 1 elif row['education-num'] == 8: training_dataset.loc[index, 'ed8'] = 1 elif row['education-num'] == 9: training_dataset.loc[index, 'ed9'] = 1 elif row['education-num'] == 10: training_dataset.loc[index, 'ed10'] = 1 elif row['education-num'] == 11: training_dataset.loc[index, 'ed11'] = 1 elif row['education-num'] == 12: training_dataset.loc[index, 'ed12'] = 1 elif row['education-num'] > 12: training_dataset.loc[index, 'ed12more'] = 1 training_dataset.drop(['age', 'education-num'], axis=1, inplace=True) # move class column to be last column label = training_dataset['class'] training_dataset.drop('class', axis=1, inplace=True) training_dataset['class'] = label return training_dataset def score(raw_dataset): try: # create pandas dataframe from input raw_dataframe = create_dataframe(raw_dataset) # reuse preprocess from training data handler processed_dataset = preprocess(raw_dataframe) # drop class column processed_dataset.drop('class', inplace=True, axis='columns') # create data payload for scoring fields = processed_dataset.columns.values.tolist() values = processed_dataset.values.tolist() scoring_dataset = {client.deployments.ScoringMetaNames.INPUT_DATA: [{'fields': fields, 'values': values}]} print(scoring_dataset) # score data prediction = client.deployments.score('<MODEL DEPLOYMENT ID>', scoring_dataset) return prediction except Exception as e: return {'error': repr(e)} return score # stores python function in space meta_props = { client.repository.FunctionMetaNames.NAME: 'Adult Income Scoring Function', client.repository.FunctionMetaNames.SOFTWARE_SPEC_ID: software_spec_id } stored_function ="
230	Federated Learning XGBoost tutorial for UI	"Step 4: Score the model
return {'error': repr(e)} return score # stores python function in space meta_props = { client.repository.FunctionMetaNames.NAME: 'Adult Income Scoring Function', client.repository.FunctionMetaNames.SOFTWARE_SPEC_ID: software_spec_id } stored_function = client.repository.store_function(meta_props=meta_props, function=adult_scoring_function) function_id = stored_function['metadata']['id'] # create online deployment for fucntion meta_props = { client.deployments.ConfigurationMetaNames.NAME: ""Adult Income Online Scoring Function"", client.deployments.ConfigurationMetaNames.ONLINE: {} } online_deployment = client.deployments.create(function_id, meta_props=meta_props) function_deployment_id = online_deployment['metadata']['id'] import pandas as pd # read adult csv dataset adult_csv = pd.read_csv('./adult.csv', dtype='category') # use 10 random rows for scoring sample_dataset = adult_csv.sample(n=10) fields = sample_dataset.columns.values.tolist() values = sample_dataset.values.tolist() raw_dataset = {client.deployments.ScoringMetaNames.INPUT_DATA: [{'fields': fields, 'values': values}]} prediction = client.deployments.score(function_deployment_id, raw_dataset) print(prediction)"
231	Federated Learning XGBoost tutorial for UI	"Step 4: Score the model
Define the Python function as follows. The function loads the scoring data in its raw format and processes the data exactly as it was done during training. Then, score the processed data.  
Replace the variables in the previous Python function:  
API KEY: Your IAM API key. To create a new API key, go to the IBM Cloud website, and click Create an IBM Cloud API key under Manage > Access(IAM) > API keys. SPACE ID: ID of the Deployment space where the adult income deployment is running. To see your space ID, go to Deployment spaces > YOUR SPACE NAME > Manage. Copy the Space GUID. MODEL DEPLOYMENT ID: Online deployment ID for the adult income model. To see your model ID, you can see it by clicking the model in your project. It is in both the address bar and the information pane.  
Get the Software Spec ID for Python 3.9. For list of other environments run client.software_specifications.list(). software_spec_id = client.software_specifications.get_id_by_name('default_py3.9')  
Store the Python function into your Watson Studio space.  
Create an online deployment by using the Python function.  
Download the Adult Income data set. This is reused as our scoring data.  
Score the adult income data by using the Python function created."
232	Federated Learning XGBoost tutorial for UI	"Step 4: Score the model
Next steps
Creating your Federated Learning experiment.  
Parent topic: Federated Learning tutorial and samples"
233	Deleting deployment spaces	"Delete existing deployment spaces that you don't require anymore.  
Important:  
Before you delete a deployment space, you must delete all the deployments that are associated with it. Only a project admin can delete a deployment space. For more information, see Deployment space collaborator roles and permissions.  
To remove a deployment space, follow these steps:  
From the navigation menu, click Deployments. In the deployments list, click the Spaces tab and find the deployment space that you want to delete. Hover over the deployment space, select the menu () icon, and click Delete. In the confirmation dialog box, click Delete."
234	Deleting deployment spaces	"Learn more
To learn more about how to clean up a deployment space and delete it programmatically, refer to:  
Notebook on managing machine learning artifacts Notebook on managing spaces  
Parent topic: Deployment spaces"
235	Creating online deployments in Watson Machine Learning	Create an online deployment for machine learning models, generative AI assets, or Python functions to generate predictions or results in real time. For example, create an online deployment for a classification model that is trained to predict whether a bank customer enrolls in a promotion. Submitting new customer data to the deployment endpoint returns a prediction in real time.
236	Creating online deployments in Watson Machine Learning	"Deployable asset types for online deployments
You can create online deployments for these types of assets:  
Functions Models Prompt templates Scripts  
AutoAI models Decision Optimization models Federated learning models Models that are imported from a file Models that are created with supported frameworks Tuned foundation models SPSS Modeler models Watson Machine Learning Accelerator models"
237	Creating online deployments in Watson Machine Learning	"Ways to create an online deployment
You can create an online deployment one of these ways:  
Use a no-code approach to Create an online deployment from a deployment space. Use code to Create an online deployment programmatically by using notebooks.  
After you create an online deployment, an endpoint URL is generated. You can use the endpoint URL to test the deployment or to include the deployment in your applications."
238	Creating online deployments in Watson Machine Learning	"Creating an online deployment from a deployment space
Follow these steps to create your online deployment from a deployment space:  
Promote or add the asset that you want to deploy to a space. For more information, see Assets in a deployment space.  
From the Assets page, click Deploy from the action menu.  
From the deployment details page, click New deployment.  
Choose Online as the deployment type.  
Provide a name and an optional description for the deployment.  
Use the Serving name field to specify a name for your deployment instead of deployment ID.  
Note:  
The serving name must be distinctive within the namespace. The serving name must contain only these characters: [a-z,0-9,_] and must be a maximum 36 characters long. The serving name works only as part of the prediction URL. Sometime, you may have to use the deployment ID.  
Optional: Select a hardware specification if you're deploying Python functions, Tensorflow models, or models with custom software specifications. For example, if you are scaling a deployment, you might want to increase the hardware resources.  
Restriction:  
If you want to use a custom hardware specification, you must create and select the custom hardware specification by using the hardware_specifications.store function from the Python client or Environments API. You cannot create or select a custom hardware specification from the user interface in a deployment space.  
Optional: Select a software specification that is compatible with your asset. You can select software specifications for functions only.  
Click Create to create the deployment."
239	Creating online deployments in Watson Machine Learning	"Creating an online deployment from a deployment space
Testing an online deployment
Follow these steps to test your online deployment:  
From the Deployments tab of your space, click the deployment name. Click the Test tab to enter data and get a prediction or other response from the deployed asset.  
The way that you provide test data for an online deployment depends on the type of asset you deployed. For most assets, you can directly enter test data into a form, upload a file that contains test data, or paste the test data in as JSON code. For example, following is the test interface for a classification model:  
Note:  
For providing test data:  
If your deployed model has a defined schema, your test data must use the same schema. For example, for a classification model, enter test data by using the same columns, omitting the prediction column. For a deployed prompt template, the form for testing the deployment mirrors the input/output form that you used to create the prompt.  
When you submit JSON code as the payload, or input data, for a deployment, your input data must match the requirements of the deployed asset.  
For example, for a model with a schema, the fields must match the column headers for the data, and the values must contain the data (in the same order). Use this format:  
Refer to this example:  
However, to enter input data for a deployed prompt template, the structure mirrors the input and output for prompting the model. For example, see the following code snippet:  
Note:  
For formatting JSON input:  
All strings are enclosed in double quotation marks. The Python notation for dictionaries looks similar, but Python strings in single quotation marks are not accepted in the JSON data. Indicate missing values with null."
240	Creating online deployments in Watson Machine Learning	"Creating an online deployment from a deployment space
Retrieving the endpoint for an online deployment
Follow these steps to retrieve the endpoint URL for your online deployment so that you can access the deployment from your applications:  
From the Deployments tab of your space, click the deployment name. In the API Reference tab, find the private and public endpoint links. You can also find code snippets in various programming languages such as Python, Java, and more to include the endpoint details in an application.  
Note:  
If you added Serving name when you created your online deployment, you see two endpoint URLs. The first URL contains the deployment ID, and the second URL contains your serving name. You can use either one of these URLs with your deployment."
241	Creating online deployments in Watson Machine Learning	"Creating an online deployment from a deployment space
Accessing the online deployment details
Follow these steps to review or update deployment details:  
From the Deployments tab of your space, click a deployment name. Click the Deployment details tab to access information that is related to your online deployment."
242	Creating online deployments in Watson Machine Learning	"Creating online deployment programmatically by using notebooks
You can create an online deployment programmatically by using:  
Watson Machine Learning REST API. Watson Machine Learning Python client library.  
To access sample notebooks that use demonstrate how to create and manage deployments by using Watson Machine Learning Python client, see Machine learning samples and examples."
243	Creating online deployments in Watson Machine Learning	"Creating online deployment programmatically by using notebooks
Testing your online deployment programmatically
To test your online deployment programmatically, you must prepare a payload that matches the schema of the existing model requirements of the deployed asset. For example, the input data (payload) for a classification model must match the schema of the deployed model. The following sample for a classification model provides test data with columns and values that exactly match the model schema:"
244	Creating online deployments in Watson Machine Learning	"Creating online deployment programmatically by using notebooks
Retrieving the endpoint for an online deployment programmatically
To retrieve the endpoint URL of your online deployment from a notebook:  
List the deployments by calling the Python client method client.deployments.list(). Find the row with your deployment. The deployment endpoint URL is listed in the url column."
245	Creating online deployments in Watson Machine Learning	"Learn more
To learn how to create a batch deployment, see Creating a batch deployment.  
To learn how to manage deployment jobs, and update, scale, or delete an online deployment, see Managing assets.  
To learn more about deployment endpoints, see Endpoint URLs.  
Parent topic: Managing predictive deployments"
246	Creating a custom component for use in the pipeline	"A Watson Pipelines custom component runs a script that you write. You can use custom components to share reusable scripts between pipelines.  
You create custom components as project assets. You can then use the components in pipelines you create in that project. You can create as many custom components for pipelines as needed. Currently, you must create a custom component programmatically, by using a Python function."
247	Creating a custom component for use in the pipeline	"Creating a component as a project asset
To create a custom component, use the Python client to authenticate with IBM Watson Pipelines, code the component, then publish the component to the specified project. After it is available in the project, you can assign the component to a node in a pipeline and run it as part of a pipeline flow.  
This example demonstrates the process of publishing a component that adds two numbers together, then assigns the component to a pipeline node.  
# Install libraries ! pip install ibm-watson-pipelines # Authentication from ibm_watson_pipelines import WatsonPipelines apikey = '' project_id = 'your_project_id' client = WatsonPipelines.from_apikey(apikey) # Define the function of the component # If you define the input parameters, users are required to # input them in the UI def add_two_numbers(a: int, b: int) -> int: print('Adding numbers: {} + {}.'.format(a, b)) return a + b + 10 # Other possible functions might be sending a Slack message, # or listing directories in a storage volume, and so on. # Publish the component client.publish_component( name='Add numbers', # Appears in UI as component name func=add_two_numbers, description='Custom component adding numbers', # Appears in UI as component description project_id=project_id, overwrite=True, # Overwrites an existing component with the same name )  
Publish a function as a component with the Python client. Run the following code in a Jupyter Notebook in a project of IBM watsonx.  
To generate a new API key:  
Go to the IBM Cloud home page Click Manage > Access (IAM) Click API keys Click Create  
Drag the node called Run Pipelines component under Run to the canvas.  
Choose the name of the component that you want to use.  
Connect and run the node as part of a pipeline job."
248	Creating a custom component for use in the pipeline	"Manage pipeline components
Use these Python client methods to manage custom pipeline components.  
Parent topic: Creating a pipeline"
249	Extracting sentiment with a custom transformer model	"You can train your own models for sentiment extraction based on the Slate IBM Foundation model. This pretrained model can be find-tuned for your use case by training it on your specific input data.  
The Slate IBM Foundation model is available only in Runtime 23.1.  
Note: Training transformer models is CPU and memory intensive. Depending on the size of your training data, the environment might not be large enough to complete the training. If you run into issues with the notebook kernel during training, create a custom notebook environment with a larger amount of CPU and memory, and use that to run your notebook. Use a GPU-based environment for training and also inference time, if it is available to you. See Creating your own environment template.  
Input data format for training Loading the pretrained model resources Training the model Applying the model on new data"
250	Extracting sentiment with a custom transformer model	"Input data format for training
You need to provide a training and development data set to the training function. The development data is usually around 10% of the training data. Each training or development sample is represented as a JSON object. It must have a text and a labels field. The text represents the training example text, and the labels field is an array, which contains exactly one label of positive, neutral, or negative.  
The following is an example of an array with sample training data:  
The training and development data sets are created as data streams from arrays of JSON objects. To create the data streams, you might use the utility method prepare_data_from_json:"
251	Extracting sentiment with a custom transformer model	"Loading the pretrained model resources
The pretrained Slate IBM Foundation model needs to be loaded before it passes to the training algorithm. In addition, you need to load the syntax analysis models for the languages that are used in your input texts.  
To load the model:"
252	Extracting sentiment with a custom transformer model	"Training the model
For all options that are available for configuring sentiment transformer training, enter:  
The train_transformer method creates a workflow model, which automatically runs syntax analysis and the trained sentiment classification. In a subsequent step, enable language detection so that the workflow model can run on input text without any prerequisite information.  
The following is a sample call using the input data and pretrained model from the previous section (Training the model):"
253	Extracting sentiment with a custom transformer model	"Applying the model on new data
After you train the model on a data set, apply the model on new data by using the run() method, as you would use on any of the existing pre-trained blocks.  
Sample code:  
Parent topic: Creating your own models"
254	Batch deployment input details for Scikit-learn and XGBoost models	"Follow these rules when you are specifying input details for batch deployments of Scikit-learn and XGBoost models.  
Data type summary table:"
255	Batch deployment input details for Scikit-learn and XGBoost models	"Data source
If you are specifying input/output data references programmatically:  
Data source reference type depends on the asset type. Refer to the Data source reference types section in Adding data assets to a deployment space.  
Notes:  
The environment variables parameter of deployment jobs is not applicable. For connections of type Cloud Object Storage or Cloud Object Storage (infrastructure), you must configure Access key and Secret key, also known as HMAC credentials,  
Parent topic: Batch deployment input details by framework"
256	Applying fairness testing to AutoAI experiments	Evaluate an experiment for fairness to ensure that your results are not biased in favor of one group over another.
257	Applying fairness testing to AutoAI experiments	"Limitations
Fairness evaluations are not supported for time series experiments."
258	Applying fairness testing to AutoAI experiments	"Evaluating experiments and models for fairness
When you define an experiment and produce a machine learning model, you want to be sure that your results are reliable and unbiased. Bias in a machine learning model can result when the model learns the wrong lessons during training. This scenario can result when insufficient data or poor data collection or management results in a poor outcome when the model generates predictions. It is important to evaluate an experiment for signs of bias to remediate them when necessary and build confidence in the model results.  
AutoAI includes the following tools, techniques, and features to help you evaluate and remediate an experiment for bias.  
Definitions and terms Applying fairness test for an AutoAI experiment in the UI Applying fairness test for an AutoAI experiment in a notebook Evaluating results Bias mitigation"
259	Applying fairness testing to AutoAI experiments	"Definitions and terms
Fairness Attribute - Bias or Fairness is typically measured by using a fairness attribute such as gender, ethnicity, or age.  
Monitored/Reference Group - Monitored group are those values of fairness attribute for which you want to measure bias. Values in the monitored group are compared to values in the reference group. For example, if Fairness Attribute=Gender is used to measure bias against females, then the monitored group value is “Female” and the reference group value is “Male”.  
Favourable/Unfavourable outcome - An important concept in bias detection is that of favorable and unfavorable outcome of the model. For example, Claim approved might be considered a favorable outcome and Claim denied might be considered as an unfavorable outcome.  
Disparate impact - The metric used to measure bias (computed as the ratio of percentage of favorable outcome for the monitored group to the percentage of favorable outcome for the reference group). Bias is said to exist if the disparate impact value is less than a specified threshold.  
For example, if 80% of insurance claims that are made by males are approved but only 60% of claims that are made by females are approved, then the disparate impact is: 60/80 = 0.75. Typically, the threshold value for bias is 0.8. As this disparate impact ratio is less than 0.8, the model is considered to be biased.  
Note when the disparate impact ratio is greater than 1.25 [inverse value (1/disparate impact) is under the threshold 0.8] it is also considered as biased."
260	Applying fairness testing to AutoAI experiments	"Watch a video about evaluating and improving fairness
Watch this video to see how to evaluate a machine learning model for fairness to ensure that your results are not biased.  
Video disclaimer: Some minor steps and graphical elements in this video might differ from your platform.  
This video provides a visual method to learn the concepts and tasks in this documentation."
261	Applying fairness testing to AutoAI experiments	"Applying fairness test for an AutoAI experiment in the UI
Open Experiment Settings.  
Click the Fairness tab.  
Enable options for fairness. The options are as follows:  
Fairness evaluation: Enable this option to check each pipeline for bias by calculating the disparate impact ration. This method tracks whether a pipeline shoes a tendency to provide a favorable (preferred) outcome for one group more often than another. Fairness threshold: Set a fairness threshold to determine whether bias exists in a pipeline based on the value of the disparate impact ration. The default is 80, which represents a disparate impact ratio less than 0.80. Favorable outcomes: Specify the value from your prediction column that would be considered favorable. For example, the value might be ""approved"", ""accepted"" or whatever fits your prediction type. Automatic protected attribute method: Choose how to evaluate features that are a potential source of bias. You can specify automatic detection, in which case AutoAI detects commonly protected attributes, including: sex, ethnicity, marital status, age, and zip or postal code. Within each category, AutoAI tries to determine a protected group. For example, for the sex category, the monitored group would be female.  
Note: In automatic mode, it is likely that a feature is not identified correctly as a protected attribute if it has untypical values, for example, being in a language other than English. Auto-detect is only supported for English.  
Manual protected attribute method: Manually specify an outcome and supply the protected attribute by choosing from a list of attributes. Note when you manually supply attributes, you must then define a group and specify whether it is likely to have the expected outcomes (the reference group) or should be reviewed to detect variance from the expected outcomes (the monitored group).  
For example, this image shows a set of manually specified attribute groups for monitoring.  
Save the settings to apply and run the experiment to apply the fairness evaluation to your pipelines.  
Notes:  
For multiclass models, you can select multiple values in the prediction column to classify as favorable or not. For regression models, you can specify a range of outcomes that are considered to be favorable or not. Fairness evaluations are not currently available for time series experiments."
262	Applying fairness testing to AutoAI experiments	"Applying fairness test for an AutoAI experiment in the UI
List of automatically detected attributes for measuring fairness
When automatic detection is enabled, AutoAI will automatically detect the following attributes if they are present in the training data. The attributes must be in English.  
age citizen_status color disability ethnicity gender genetic_information handicap language marital political_belief pregnancy religion veteran_status"
263	Applying fairness testing to AutoAI experiments	"Applying fairness test for an AutoAI experiment in a notebook
You can perform fairness testing in an AutoAI experiment that is trained in a notebook and extend the capabilities beyond what is provided in the UI."
264	Applying fairness testing to AutoAI experiments	"Applying fairness test for an AutoAI experiment in a notebook
Bias detection example
In this example, by using the Watson Machine Learning Python API (ibm-watson-machine-learning), the optimizer configuration for bias detection is configured with the following input, where:  
name - experiment name prediction_type - type of the problem prediction_column - target column name fairness_info - bias detection configuration"
265	Applying fairness testing to AutoAI experiments	"Evaluating results
You can view the evaluation results for each pipeline.  
From the Experiment summary page, click the filter icon for the Pipeline leaderboard. Choose the Disparate impact metrics for your experiment. This option evaluates one general metric and one metric for each monitored group. Review the pipeline metrics for disparate impact to determine whether you have a problem with bias or just to determine which pipeline performs better for a fairness evaluation.  
In this example, the pipeline that was ranked first for accuracy also has a disparate income score that is within the acceptable limits."
266	Applying fairness testing to AutoAI experiments	"Bias mitigation
If bias is detected in an experiment, you can mitigate it by optimizing your experiment by using ""combined scorers"": accuracy_and_disparate_impact or r2_and_disparate_impact, both defined by the open source LALE package.  
Combined scorers are used in the search and optimization process to return fair and accurate models.  
For example, to optimize for bias detection for a classification experiment:  
Open Experiment Settings. On the Predictions page, choose to optimize Accuracy and disparate impact in the experiment. Rerun the experiment.  
The Accuracy and disparate impact metric creates a combined score for accuracy and fairness for classification experiments. A higher score indicates better performance and fairness measures. If the disparate impact score is between 0.9 and 1.11 (an acceptable level), the accuracy score is returned. Otherwise, a disparate impact value lower than the accuracy score is returned, with a lower (negative) value which indicates a fairness gap.  
Read this Medium blog post on Bias detection in AutoAI."
267	Applying fairness testing to AutoAI experiments	"Bias mitigation
Next steps
Troubleshooting AutoAI experiments  
Parent topic: AutoAI overview"
268	Compute resource options for the notebook editor in projects	"When you run a notebook in the notebook editor in a project, you choose an environment template, which defines the compute resources for the runtime environment. The environment template specifies the type, size, and power of the hardware configuration, plus the software configuration. For notebooks, environment templates include a supported language of Python and R.  
Types of environments Runtime releases CPU environment templates Spark environment templates GPU environment templates Default hardware specifications for scoring models with Watson Machine Learning Data files in notebook environments Compute usage by service Runtime scope Changing environments"
269	Compute resource options for the notebook editor in projects	"Types of environments
You can use these types of environments for running notebook:  
Anaconda CPU environments for standard workloads. Spark environments for parallel processing that is provided by the platform or by other services. GPU environments for compute-intensive machine learning models.  
Most environment types for notebooks have default environment templates so you can get started quickly. Otherwise, you can create custom environment templates."
270	Compute resource options for the notebook editor in projects	"Runtime releases
The default environments for notebooks are added as an affiliate of a runtime release and prefixed with Runtime followed by the release year and release version.  
A runtime release specifies a list of key data science libraries and a language version, for example Python 3.10. All environments of a runtime release are built based on the library versions defined in the release, thus ensuring the consistent use of data science libraries across all data science applications.  
The Runtime 23.1 release is available for Python 3.10 and R 4.2.  
While a runtime release is supported, IBM will update the library versions to address security requirements. Note that these updates will not change the <Major>.<Minor> versions of the libraries, but only the <Patch> versions. This ensures that your notebook assets will continue to run."
271	Compute resource options for the notebook editor in projects	"Runtime releases
Library packages included in Runtimes
For specific versions of popular data science library packages included in Watson Studio runtimes refer to these tables:  
In addition to the libraries listed in the tables, runtimes include many other useful libraries. To see the full list, select the Manage tab in your project, then click Templates, select the Environments tab, and then click on one of the listed environments."
272	Compute resource options for the notebook editor in projects	"CPU environment templates
You can select any of the following default CPU environment templates for notebooks. The default environment templates are listed under Templates on the Environments page on the Manage tab of your project.  
DO Indicates that the environment templates includes the CPLEX and the DOcplex libraries to model and solve decision optimization problems that exceed the complexity that is supported by the Community Edition of the libraries in the other default Python environments. See Decision Optimization notebooks.  
NLP Indicates that the environment templates includes the Watson Natural Language Processing library with pre-trained models for language processing tasks that you can run on unstructured data. See Using the Watson Natural Language Processing library. This default environment should be large enough to run the pre-trained models.  
Stop all active CPU runtimes when you don't need them anymore, to prevent consuming extra capacity unit hours (CUHs). See CPU idle timeout."
273	Compute resource options for the notebook editor in projects	"CPU environment templates
Notebooks and CPU environments
When you open a notebook in edit mode in a CPU runtime environment, exactly one interactive session connects to a Jupyter kernel for the notebook language and the environment runtime that you select. The runtime is started per single user and not per notebook. This means that if you open a second notebook with the same environment template in the same project, a second kernel is started in the same runtime. Runtime resources are shared by the Jupyter kernels that you start in the runtime. Runtime resources are also shared if the CPU has GPU.  
If you want to avoid sharing runtimes but want to use the same environment template for multiple notebooks in a project, you should create custom environment templates with the same specifications and associate each notebook with its own template.  
If necessary, you can restart or reconnect to the kernel. When you restart a kernel, the kernel is stopped and then started in the same session again, but all execution results are lost. When you reconnect to a kernel after losing a connection, the notebook is connected to the same kernel session, and all previous execution results which were saved are available."
274	Compute resource options for the notebook editor in projects	"Spark environment templates
You can select any of the following default Spark environment templates for notebooks. The default environment templates are listed under Templates on the Environments page on the Manage tab of your project.  
Stop all active Spark runtimes when you don't need them anymore, to prevent consuming extra capacity unit hours (CUHs). See Spark idle timeout."
275	Compute resource options for the notebook editor in projects	"Spark environment templates
Large Spark environments
If you have the Watson Studio Professional plan, you can create custom environment templates for larger Spark environments.  
Professional plan users can have up to 35 executors and can choose from the following options for both driver and executor:  
The CUH rate per hour increases by 0.5 for every vCPU that is added. For example, 1x Driver: 3vCPU with 12GB of RAM and 4x Executors: 2vCPU with 8GB of RAM amounts to (3 + (4 * 2)) = 11 vCPUs and 5.5 CUH."
276	Compute resource options for the notebook editor in projects	"Spark environment templates
Notebooks and Spark environments
You can select the same Spark environment template for more than one notebook. Every notebook associated with that environment has its own dedicated Spark cluster and no resources are shared.  
When you start a Spark environment, extra resources are needed for the Jupyter Enterprise Gateway, Spark Master, and the Spark worker daemons. These extra resources amount to 1 vCPU and 2 GB of RAM for the driver and 1 GB RAM for each executor. You need to take these extra resources into account when selecting the hardware size of a Spark environment. For example: if you create a notebook and select Default Spark 3.3 & Python 3.10, the Spark cluster consumes 3 vCPU and 12 GB RAM but, as 1 vCPU and 4 GB RAM are required for the extra resources, the resources remaining for the notebook are 2 vCPU and 8 GB RAM."
277	Compute resource options for the notebook editor in projects	"Spark environment templates
File system on a Spark cluster
If you want to share files across executors and the driver or kernel of a Spark cluster, you can use the shared file system at /home/spark/shared.  
If you want to use your own custom libraries, you can store them under /home/spark/shared/user-libs/. There are four subdirectories under /home/spark/shared/user-libs/ that are pre-configured to be made available to Python and R or Java runtimes.  
The following tables lists the pre-configured subdirectories where you can add your custom libaries.  
To share libraries across a Spark driver and executors:  
Download your custom libraries or JAR files to the appropriate pre-configured directory. Restart the kernel from the notebook menu by clicking Kernel > Restart Kernel. This loads your custom libraries or JAR files in Spark.  
Note that these libraries are not persisted. When you stop the environment runtime and restart it again later, you need to load the libraries again."
278	Compute resource options for the notebook editor in projects	"GPU environment templates
You can select the following GPU environment template for notebooks. The environment templates are listed under Templates on the Environments page on the Manage tab of your project.  
The GPU environment template names indicate the accelerator power. The GPU environment templates include the Watson Natural Language Processing library with pre-trained models for language processing tasks that you can run on unstructured data. See Using the Watson Natural Language Processing library.  
~ Indicates that the environment template requires the Watson Studio Professional plan. See Offering plans.  
Stop all active GPU runtimes when you don't need them anymore, to prevent consuming extra capacity unit hours (CUHs). See GPU idle timeout."
279	Compute resource options for the notebook editor in projects	"GPU environment templates
Notebooks and GPU environments
GPU environments for notebooks are available only in the Dallas IBM Cloud service region.  
You can select the same Python and GPU environment template for more than one notebook in a project. In this case, every notebook kernel runs in the same runtime instance and the resources are shared. To avoid sharing runtime resources, create multiple custom environment templates with the same specifications and associate each notebook with its own template."
280	Compute resource options for the notebook editor in projects	"Default hardware specifications for scoring models with Watson Machine Learning
When you invoke the Watson Machine Learning API within a notebook, you consume compute resources from the Watson Machine Learning service as well as the compute resources for the notebook kernel.  
You can select any of the following hardware specifications when you connect to Watson Machine Learning and create a deployment."
281	Compute resource options for the notebook editor in projects	"Data files in notebook environments
If you are working with large data sets, you should store the data sets in smaller chunks in the IBM Cloud Object Storage associated with your project and process the data in chunks in the notebook. Alternatively, you should run the notebook in a Spark environment.  
Be aware that the file system of each runtime is non-persistent and cannot be shared across environments. To persist files in Watson Studio, you should use IBM Cloud Object Storage. The easiest way to use IBM Cloud Object Storage in notebooks in projects is to leverage the project-lib package for Python or the project-lib package for R."
282	Compute resource options for the notebook editor in projects	"Compute usage by service
The notebook runtimes consumes compute resources as CUH from Watson Studio, while running default or custom environments. You can monitor the Watson Studio CUH consumption in the project on the Resource usage page on the Manage tab of the project.  
Notebooks can also consume CUH from the Watson Machine Learning service when the notebook invokes the Watson Machine Learning to score a model. You can monitor the total monthly amount of CUH consumption for the Watson Machine Learning service on the Resource usage page on the Manage tab of the project."
283	Compute resource options for the notebook editor in projects	"Compute usage by service
Track CUH consumption for Watson Machine Learning in a notebook
To calculate capacity unit hours consumed by a notebook, run this code in the notebook:  
For example:  
returns 5.49 CUH  
For details, see the Service Instances section of the IBM Watson Machine Learning API documentation."
284	Compute resource options for the notebook editor in projects	"Runtime scope
Environment runtimes are always scoped to an environment template and a user within a project. If different users in a project work with the same environment, each user will get a separate runtime.  
If you select to run a version of a notebook as a scheduled job, each scheduled job will always start in a dedicated runtime. The runtime is stopped when the job finishes."
285	Compute resource options for the notebook editor in projects	"Changing the environment of a notebook
You can switch environments for different reasons, for example, you can:  
Select an environment with more processing power or more RAM Change from using an environment without Spark to a Spark environment  
You can only change the environment of a notebook if the notebook is unlocked. You can change the environment:  
From the notebook opened in edit mode:  
Save your notebook changes. Click the Notebook Info icon () from the notebook toolbar and then click Environment. Select another template with the compute power and memory capacity from the list. Select Change environment. This stops the active runtime and starts the newly selected environment.  
From the Assets page of your project:  
Select the notebook in the Notebooks section, click Actions > Change Environment and select another environment. The kernel must be stopped before you can change the environment. This new runtime environment will be instantiated the next time the notebook is opened for editing.  
In the notebook job by editing the job template. See Editing job settings."
286	Compute resource options for the notebook editor in projects	"Next steps
Creating a notebook Creating your own environment template Customizing an environment template Stopping active notebook runtimes"
287	Compute resource options for the notebook editor in projects	"Learn more
Monitoring account resource usage  
Parent topic: Compute resources for tools"
288	Text embedding generation	"Use the embedding models and embeddings API that are available from watsonx.ai to create text embeddings that capture the meaning of sentences or passages for use in your generative AI applications.  
Converting text into text embeddings helps with document comparison, question-answering, and in retrieval-augmented generation (RAG) tasks, where you need to retrieve relevant content quickly.  
For more information about using embeddings in a RAG use case, see Using text embeddings to ground prompts in factual information. For more information about the supported embedding models, see Supported embedding models. For more information about the embeddings API, see Text embeddings API reference."
289	Text embedding generation	"What are text embeddings?
A text embedding is a numerical representation of a sentence or passage as a vector of real-valued numbers. By converting sentences to number vectors, operations on sentences become more like math equations, which is something computers can quickly, and can do well.  
When an embedding model creates a vector representation of a sentence, the embedding model assigns values that capture the semantic meaning of the sentence. The embedding model also positions the vector within a multidimensional space based on its assigned values. The size of the dimensional space varies by model, which means the exact vector values vary also. However, all models position the vectors such that sentences with similar meanings are nearer to one another.  
Most embedding models generate vectors in so many dimensions, ranging from hundreds to thousands of dimensions, that it's impossible to visualize. Hypothetically, if an embedding model were to generate a 3-dimensional vector, it might look as follows.  
A few things to notice about the image:  
The position of each sentence within the 3-dimensional space is defined by its 3-value array. The two sentences that share the subject of artwork (Jan bought a painting of dogs playing cards and The Degas reproduction is hanging in the den) are nearest to each other. The third sentence, I took my dogs for a walk, is nearer to the sentence about the dogs painting because the sentences share the keyword dogs. However, a shared keyword affects the positioning of the vectors less than a similarity in meaning. The values in the arrays in the image, such as 0.021, are fictional. The values in an array for higher-dimensional models are more precise, for example, 0.020891575.  
You can store generated vectors in a vector database. When the same embedding model is used to convert all of the sentences in the database, the vector store can leverage the inherent groupings and relationships that exist among the sentences based on their vector values to return relevant search results quickly.  
Unlike traditional indexes that store text and rely on keyword search for information retrieval, vector stores support semantic searches that retrieve information that is similar in meaning. For example, where keyword search checks only whether the keyword is present, semantic search weighs the context in which the keyword is used, which typically produces better search results."
290	Text embedding generation	"Learn more
Supported embedding models Using text embeddings to ground prompts in factual information  
Parent topic: Coding generative AI solutions"
291	Federated Learning architecture	IBM Federated Learning has two main components: the aggregator and the remote training parties.
292	Federated Learning architecture	"Aggregator
The aggregator is a model fusion processor. The admin manages the aggregator.  
The aggregator runs the following tasks:  
Runs as a platform service in regions Dallas, Frankfurt, London, or Tokyo. Starts with a Federated Learning experiment."
293	Federated Learning architecture	"Party
A party is a user that provides model input to the Federated Learning experiment aggregator. The party can be:  
on any system that can run the Watson Machine Learning Python client and compatible with Watson Machine Learning frameworks.  
Note:  
The system does not have to be specifically IBM watsonx. For a list of system requirements, see Set up your system.  
running on the system in any geographical location. You are recommended to locate each party in the same region where the data is to avoid data extraction out of different regions.  
This illustration shows the architecture of IBM Federated Learning.  
A Remote Training System is used to authenticate the party's identity to the aggregator during training."
294	Federated Learning architecture	"User workflow
The data scientist: A party connects to the aggregator on their system, which can be remote. An admin controls the Federated Learning experiment by:  
Identifies the data sources. Creates an initial ""untrained"" model. Creates a data handler file. These tasks might overlap with a training party entity.  
Configuring the experiment to accommodate remote parties. Starting the aggregator.  
This illustration shows the actions that are associated with each role in the Federated Learning process.  
Parent topic: Get started"
295	Creating the initial model	"Parties can create and save the initial model before training by following a set of examples.  
Save the Tensorflow model Save the Scikit-learn model Save the Pytorch model  
Consider the configuration examples that match your model type."
296	Creating the initial model	"Save the Tensorflow model
If you choose Tensorflow as the model framework, you need to save a Keras model as the SavedModel format. A Keras model can be saved in SavedModel format by using tf.keras.model.save().  
To compress your files, run the command zip -r mymodel.zip model_architecture. The contents of your .zip file must contain:"
297	Creating the initial model	"Save the Scikit-learn model
SKLearn classification SKLearn regression SKLearn Kmeans"
298	Creating the initial model	"Save the Scikit-learn model
SKLearn Kmeans
You need to create a .zip file that contains your model in pickle format by running the command zip mymodel.zip model_architecture.pickle. The contents of your .zip file must contain:"
299	Creating the initial model	"Save the PyTorch model
You need to create a .zip file containing your model in pickle format. Run the command zip mymodel.zip model_architecture.pt. The contents of your .zip file should contain:  
Parent topic: Creating a Federated Learning experiment"
300	Authenticating for programmatic access to a project or space	To work with foundation models in IBM watsonx.ai programmatically, you must authenticate and identify your project or deployment space.
301	Authenticating for programmatic access to a project or space	"Credentials
To use the watsonx.ai API, you need an IBM Cloud Identity and Access Management (IAM) token. You can use your IBM Cloud API key to generate an IAM token. You are responsible for refreshing the token when it expires.  
To use the watsonx.ai Python library, you can use either an IBM Cloud API key or an IBM Cloud IAM token."
302	Authenticating for programmatic access to a project or space	"Credentials
IBM Cloud API key
You can create the IBM Cloud API key in various ways.  
For more information, see Creating an API key.  
The following sample code snippet shows how an API key is used for authentication when using the Python library."
303	Authenticating for programmatic access to a project or space	"Credentials
IBM Cloud IAM token
For many tasks that you want to perform from the API, you need to both authenticate and prove that you are allowed to perform the task. To show that you are authorized to perform tasks, you need an IAM token.  
When you use the REST API directly, you must generate the IAM token and manage its expiration.  
For more information, see Generating an IBM Cloud IAM token.  
When you use the Python library, you can choose to use an API key or an IAM token. If you create the IAM token yourself and pass the token to the library, then you must manage refreshing the token before it expires. Alternatively, you can pass your API key to the APIClient in the library and allow the client to generate the IAM token for you. When the client generates the token, it also manages refreshing the token as needed.  
The following sample code snippet shows how an IAM token is used for authenticating a REST request.  
The following sample code snippet shows how an IAM token is used for authentication when using the Python library."
304	Authenticating for programmatic access to a project or space	"Project or deployment space details
The project ID or deployment space ID that is associated with the foundation model that you want to work with is required in Python library requests and in the data payload for most of the API methods.  
The project must have an instance of Watson Machine Learning associated with it. The Watson Machine Learning service hosts the deployed watsonx.ai foundation models.  
To get the ID of a project, complete the following steps:  
From your project, click the Manage tab. Copy the project ID from the Details section of the General page.  
To get the ID of a deployment space, complete the following steps:  
From your deployment, click the Manage tab. Copy the space GUID from the Details section of the General page.  
Parent topic: Coding genertaive AI solutions"
305	Importing scripts into a notebook	"If you want to streamline your notebooks, you can move some of the code from your notebooks into a script that your notebook can import. For example, you can move all helper functions, classes, and visualization code snippets into a script, and the script can be imported by all of the notebooks that share the same runtime.  Without all of the extra code, your notebooks can more clearly communicate the results of your analysis.  
To import a script from your local machine to a notebook and write to the script from the notebook, use one of the following options:  
Copy the code from your local script file into a notebook cell.  
For Python:  
At the beginning of this cell, add %%writefile myfile.py to save the code as a Python file to your working directory. Notebooks that use the same runtime can also import this file.  
The advantage of this method is that the code is available in your notebook, and you can edit and save it as a new Python script at any time.  
For R:  
If you want to save code in a notebook as an R script to the working directory, you can use the writeLines(myfile.R) function.  
Save your local script file in Cloud Object Storage and then make the file available to the runtime by adding it to the runtime's local file system. This is only supported for Python.  
Click the Upload asset to project icon (), and then browse the script file or drag it into your notebook sidebar. The script file is added to Cloud Object Storage bucket associated with your project. Make the script file available to the Python runtime by adding the script to the runtime's local file system:  
f = open('<myScript>.py', 'wb') f.write(streaming_body_1.read()) f.close() import <myScript>  
Click the Code snippets icon (), and then select Read data.  
Click Select data from project and then select Data asset.  
From the list of data assets available in your project's COS, select your script and then click Select. .  
Click an empty cell in your notebook and then from the Load as menu in the notebook sidebar select Insert StreamingBody object.  
Write the contents of the StreamingBody object to a file in the local runtime`s file system:  
This opens a file with write access and calls the write method to write to the file.  
Import the script:"
306	Importing scripts into a notebook	"Write the contents of the StreamingBody object to a file in the local runtime`s file system:  
This opens a file with write access and calls the write method to write to the file.  
Import the script:  
To import the classes to access the methods in a script in your notebook, use the following command:  
from <python file name> import <class name> source(""./myCustomFunctions.R"") ## available in base R source_url(""<insert URL here>"") ## available in devtools  
For Python:  
For R:  
To source an R script from the web:  
Parent topic: Libraries and scripts"
307	Batch deployment input details for Tensorflow models	"Follow these rules when you are specifying input details for batch deployments of Tensorflow models.  
Data type summary table:"
308	Batch deployment input details for Tensorflow models	"Data sources
Input or output data references:  
Local or managed assets from the space Connected (remote) assets: Cloud Object Storage  
If you are specifying input/output data references programmatically:  
Data source reference type depends on the asset type. Refer to the Data source reference types section in Adding data assets to a deployment space.  
Notes:  
The environment variables parameter of deployment jobs is not applicable. For connections of type Cloud Object Storage or Cloud Object Storage (infrastructure), you must configure Access key and Secret key, also known as HMAC credentials.  
Parent topic: Batch deployment input details by framework"
309	Python library	"You can inference and tune foundation models in IBM watsonx.ai programmatically by using the Python library. For more information, See Foundation models Python library.  
You can also work with watsonx.ai foundation models from LangChain. For more information, see IBM extension in LangChain."
310	Python library	"Using the Python library
The ibm-watsonx-ai Python library is available on PyPI from the url: https://pypi.org/project/ibm-watsonx-ai/.  
You can install the ibm-watsonx-ai Python library in your development environment by using the following command:  
If you already have the library installed, include the -U parameter to pick up any updates and work with the latest version of the library."
311	Python library	"Prerequisites
You need credentials and a project ID or deployment space ID before you can use the Python library. For more information, see Authenticating for programmatic access to a project or space."
312	Python library	"Learn from available sample notebooks
Sample notebooks are available that you can use as a guide as you create notebooks of your own to do common tasks such as inferencing or tuning a foundation model. See the following topics for more information:  
Inferencing a foundation model with a notebook Tuning a foundation model with a notebook  
Parent topic: Coding generative AI solutions"
313	Planning for AI governance	Plan how to use watsonx.governance to accelerate responsible, transparent, and explainable AI workflows with an AI governance solution that provides end-to-end monitoring for machine learning and generative AI models.
314	Planning for AI governance	"Governance capabilities
Note:  
To govern metadata from foundation models curated by IBM, you must have watsonx.ai provisioned.  
Consider these watsonx.governance capabilities as you plan your governance strategy:  
Collect metadata in factsheets about machine learning models and prompt templates for generative AI foundation models. Monitor machine learning deployments for fairness, drift, and quality to ensure that your models are meeting specified standards. Monitor generative AI assets for breaches of toxic language thresholds or detection of personal identifiable information. Evaluate prompt templates with metrics designed to measure performance and to test for the presence of prohibited content, such as hateful speech. Collect model health data that includes data size, latency, and throughput to help you assess performance issues and manage resource consumption. Use the AI risk atlas as a guide to the challenges of AI solutions so you can plan risk mitigation and meet your compliance and regulatory goals. Assign a single risk score to tracked models to indicate the relative impact of the associated model. For example, a model that predicts sensitive information such as a credit score might be assigned a high risk score. Use the automated transaction analysis tools to improve transparency and explainability for your AI assets. For example, see how a feature contributes to a prediction and test what-if scenarios to explore different outcomes. Optionally integrate with IBM OpenPages Model Risk Governance to collect metadata about foundation models as well as machine learning models to help you achieve your governance goals. You can also use OpenPages to develop workflows that support your governance processes."
315	Planning for AI governance	"Planning for governance
Consider these governance strategies:  
Build your governance team Set up your governance structures Manage collaboration with roles and access control Develop a communication plan Implement a simple solution Plan for more complex solutions"
316	Planning for AI governance	"Planning for governance
Build a governance team
Consider the expertise that you need on your governance team. A typical governance plan might include the following roles. Sometimes, the same person might fill multiple roles. In other cases, a role might represent a team of people.  
Model owner: The owner creates an AI use case to track a solution to a business need. The owner requests the model or prompt template, manages the approval process, and tracks the solution through the AI Lifecycle. Model developer or Data scientist: The developer works with the data in a data set or a large language model (LLM) and creates the machine learning model or LLM prompt template. Model validator: the validator tests the solution to determine whether it meets the goals that are stated in the AI use case. Risk and compliance manager: The risk manager determines the policies and compliance thresholds for the AI use case. For example, the risk manager might determine the rules to apply for testing a solution for fairness or for screening output for hateful and abusive speech. MLOps engineer: The MLOps engineer moves a solution from a pre-production (test) environment to a production environment when a solution is deemed ready to be fully deployed. App developer: Following deployment, and app developer runs evaluations against the deployment to monitor how the solution performs against the metric threshold set by the risk and compliance owner. If performance drops below specified thresholds, the app developer works with the other stakeholders to address problems and update the model or prompt template."
317	Planning for AI governance	"Planning for governance
Set up a governance structure
After you identify roles and assembling a team, plan your governance structure.  
Create an inventory for storing AI use cases. An inventory is where you store and view AI use cases and the factsheets that are associated with the governed assets. Depending on your governance requirements, store all use cases in a single inventory, or create multiple inventories for your governance efforts. Create projects for collaboration. If you are using IBM tools, create a Watson Studio project. The project can hold the data that is required to train or test the AI solution and the model or prompt template that is governed. Use the access control to restrict access to the approved collaborators. Create a pre-production deployment space. Use the space to test your model or prompt template by using test data. Like a project, a space provides access control features so you can include the required collaborators. Configure test and validation evaluations. Provide the model or prompt template details and configure a set of evaluations to test the performance of your solution. For example, you might test a machine learning model for dimensions such as fairness, quality, and drift, and test a prompt template against metrics such as perplexity (how accurate the output is), or toxicity (whether the output contains hateful or abusive speech). By testing on known (labeled) data, you can evaluate the performance before you move a solution to production. Configure a production space. When the model or prompt template is ready to be deployed to a production environment, move the solution and all dependencies to a production space. A production space typically has a tighter access control list. Configure evaluations for the deployed model. Provide the model details and configure evaluations for the solution. You can now test against live data rather than test data. It is important to monitor your solution so that you are alerted if thresholds are crossed, indicating a potential problem with the deployed solution."
318	Planning for AI governance	"Planning for governance
Manage collaboration for governance
Watsonx.governance is built on a collaborative platform to allow for all approved team members to contribute to the goals of solving business problems.  
To plan for collaboration, consider how to manage access to the inventories, projects, and spaces you use for governance.  
Use roles along with access control features to ensure that your team has appropriate access to meet goals."
319	Planning for AI governance	"Planning for governance
Develop a communication plan
Some of the workflow around defining an AI use case and moving assets through the lifecycle rely on effective communication. Decide how your team will communicate and establish the details. For example:  
Will you use email for decision-making or a messaging tool such as Slack? Is there a formal process for adding comments to an asset as it moves through a workflow?  
Create your communication plan and share it with your team."
320	Planning for AI governance	"Planning for governance
Implement a simple governance solution
As you roll out your governance strategy, start with a simple implementation, then consider how to build incrementally to a more comprehensive solution. The simplest implementation requires an AI use case in an inventory, with an asset moving from request to production.  
For the most straightforward implementation of AI governance, create an AI use case for tracking assets. An AI use case in an inventory consists of a set of factsheets containing lineage, history, and other relevant information about a model's lifecycle. Add data scientists, data engineers, and other users as collaborators.  
AI use case owners can request and track assets:  
Business users create AI use cases in the inventory to request machine-learning models or LLM prompt templates. Data scientists associate the trained asset with an AI use case to track lifecycle activities.  
AI factsheets accumulate information about the model or prompt templates in the following ways:  
All actions that are associated with the tracked asset are automatically saved, including deployments and evaluations. All changes to input data assets are automatically saved. Data scientists can add tags, supporting documentation, and other information. Data scientists can associate challenger models with the AI use cases to compare model performance.  
Validators and other stakeholders review AI factsheets to ensure compliance and certify asset progress from development to production. They can also generate reports from the factsheets to print, share, or archive details."
321	Planning for AI governance	"Planning for governance
Plan for more complex solutions
You can extend your AI governance implementation at any time. Consider these options to extend governance:  
MLOps engineers can extend model tracking to include external models that are created with third-party machine learning models. MLOps engineers can add custom properties to factsheets to track more information. Compliance analysts can customize the default report templates to generate tailored reports for the organization."
322	Planning for AI governance	"Governing assets that are created locally or externally
Watsonx.governance provides the tools for you to govern assets you created using IBM tools, such as machine learning models created by using AutoAI or foundation model prompt templates created in a watsonx project. You can also govern machine learning models that are created by using non-IBM tools, such as Microsoft Azure or Amazon Web Services. As you develop your governance plan, consider these differences:  
IBM assets developed with tools such as Watson Studio are available for governance earlier in the lifecycle. You can track the factsheet for a local asset from the Development phase, and have visibility into details such as the training data and creation details from an earlier stage. An inventory owner or administrator must enable governance for external models. When governance is enabled for external models, they can be added to an AI use case explicitly. If you track an external model in the develop phase, then lifecycle activities for the validate and operate phases are tracked automatically.  
For a list of supported machine learning model providers, see Supported machine learning providers."
323	Planning for AI governance	"Next steps
To begin governance, follow the steps in Provisioning and launching IBM watsonx.governance to provision Watson OpenScale with AI Factsheets.  
Parent topic: Watsonx.governance overview"
324	Deployments dashboard	The deployments dashboard provides an aggregate view of deployment activity available to you, across spaces. You can get a broad view of deployment activity such as the status of job runs or a list of online deployments. You can also use filters and views to focus on specific job runs or category of runs such as failed runs. ModelOps or DevOps users can review and monitor the activity for an organization.
325	Deployments dashboard	"Accessing the Deployments dashboard
From the navigation menu, click Deployments. If you don't have any deployment spaces, you are prompted to create a space. This following illustration shows an example of the Deployments dashboard:  
The dashboard view has two tabs:  
Activity: Use the Activity tab to review all of the deployment activity across spaces. You can sort and filter this view to focus on a particular type of activity, such as failed deployments, or jobs with active runs. You can also review metrics such as the number of deployment spaces with active deployments. Spaces: Use the Spaces tab to list all the spaces that you can access. You can read the overview information, such as the number of deployments and job runs in a space, or click a space name to view details and create deployments or jobs."
326	Deployments dashboard	"Viewing activity
View the overview information for finished runs, active runs, or online deployments, or drill down to view details."
327	Deployments dashboard	"Viewing activity
Finished runs
The Finished runs section shows activity in jobs over a specified time interval. The default is to view finished jobs for the last 8 hours. It shows jobs that are completed, canceled, or failed across all of your deployment spaces within the specified time frame. Click View finished runs to view a list of runs.  
The view provides more information on the finished runs and a visualization that shows run times.  
Filter the view to focus on a particular type of activity:  
Jobs with active runs - Shows jobs that have active runs (running, started, or queued) across all spaces you can access. Active runs - Shows runs that are in the running, started, or queued state across all jobs you can access. Jobs with finished runs - Shows jobs with runs that are completed, canceled, or failed. Finished runs - Shows runs that are completed, canceled, or failed."
328	Deployments dashboard	"Viewing activity
Active runs
The Active runs section displays runs that are currently running or are in the starting or queued state. Click View active runs to view a list of the runs."
329	Deployments dashboard	"Viewing activity
Online deployments
The Deployments section shows all online and R-Shiny deployments, which are sorted into categories for by status. Click View deployments to view the list of deployments that you can access.  
From any view, you can start from the overview and drill down to see more information for a particular job or run. You can also filter the view to focus on a particular type of deployment."
330	Deployments dashboard	"Viewing spaces
View a list of spaces that you can access, with overview information such as number of deployments and collaborators. Click the name of a space to view details or add assets, and to create new deployments or jobs. Use filters to modify the view from the default list of all spaces to show Active spaces, with deployments or jobs, or Inactive spaces, with no deployments or jobs."
331	Deployments dashboard	"Next steps
Use spaces to organize your deployment activity.  
Parent topic: Deploying and managing assets"
332	Tuning Studio	"Tune a foundation model with the Tuning Studio to guide an AI foundation model to return useful output.  
To run training experiments, you must have the Admin or Editor role in a project.  
The Tuning Studio is not available with all plans or in all data centers. See Watson Machine Learning plans and Regional availability for services and features.  
Tabular: JSON, JSONL. For details, see Data formats.  
Note: You can use the same training data file with one or more tuning experiments.  
You use the Tuning Studio to create a tuned version of an existing foundation model.  
   
Watch a video that explains when and why to tune a foundation model.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
   
Foundation models are AI models that are pretrained on terabytes of data from across the internet and other public resources. They are unrivaled in their ability to predict the next best word and generate language. While language-generation can be useful for brainstorming and spurring creativity, foundation models typically need to be guided to achieve concrete tasks. Model tuning, and other techniques, such as retrieval-augmented generation, help you to use foundation models in meaningful ways for your business.  
With the Tuning Studio, you can tune a smaller foundation model to improve its performance on natural language processing tasks such as classification, summarization, and generation. Tuning can help a smaller foundation model achieve results comparable to larger models in the same model family. By tuning and deploying the smaller model, you can reduce long-term inference costs.  
Much like prompt engineering, tuning a foundation model helps you to influence the content and format of the foundation model output. Knowing what to expect from a foundation model is essential if you want to plug the step of inferencing a foundation model into a business workflow.  
The following diagram illustrates how tuning a foundation model can help you guide the model to generate useful output. You provide labeled data that illustrates the format and type of output that you want the model to return, which helps the foundation model to follow the established pattern.  
You can tune a foundation model to optimize the model's ability to do many things, including:  
Generate new text in a specific style Generate text that summarizes or extracts information in a certain way Classify text"
333	Tuning Studio	"You can tune a foundation model to optimize the model's ability to do many things, including:  
Generate new text in a specific style Generate text that summarizes or extracts information in a certain way Classify text  
To learn more about when tuning a model is the right approach, see When to tune a foundation model."
334	Tuning Studio	"Workflow
Tuning a model involves the following tasks:  
Engineer prompts that work well with the model you want to use.  
Find the largest foundation model that works best for the task. Experiment until you understand which prompt formats show the most potential for getting good results from the model.  
Tuning doesn't mean you can skip prompt engineering altogether. Experimentation is necessary to find the right foundation model for your use case. Tuning means you can do the work of prompt engineering once and benefit from it again and again.  
You can use the Prompt Lab to experiment with prompt engineering. For help, see Prompt Lab.  
Create training data to use for model tuning.  
Create a tuning experiment to tune the model.  
Evaluate the tuned model.  
If necessary, change the training data or the experiment parameters and run more experiments until you're satisfied with the results.  
Deploy the tuned model.  
Submit inference requests to the tuned model."
335	Tuning Studio	"Foundation model tuning costs
The cost of tuning a foundation model is measured in capacity unit hours, which measures the compute resource consumption of the tuning experiment. For more information, see Capacity Unit Hours metering.  
The cost of inferencing a tuned model is measured in resource units. The rate depends on the model's billing class. A prompt-tuned foundation model has the same billing class as the foundation model that it tunes. For more information, see Resource unit metering."
336	Tuning Studio	"Learn more
When to tune Methods for tuning Choosing a foundation model to tune Tuning a model Evaluate the results of a tuning experiment"
337	Tuning Studio	"Get started
Quick start: Tune a foundation model Sample notebook: Tune a model to classify CFPB documents in watsonx Sample notebook: Prompt tuning for multi-class classification with watsonx  
Parent topic: Developing generative AI solutions"
338	Detecting entities with a custom dictionary	"If you have a fixed set of terms that you want to detect, like a list of product names or organizations, you can create a dictionary. Dictionary matching is very fast and resource-efficient.  
Watson Natural Language Processing dictionaries contain advanced matching capabilities that go beyond a simple string match, including:  
Dictionary terms can consist of a single token, for example wheel, or multiple tokens, for example, steering wheel. Dictionary term matching can be case-sensitive or case-insensitive. With a case-sensitive match, you can ensure that acronyms, like ABS don't match terms in the regular language, like abs that have a different meaning. You can specify how to consolidate matches when multiple dictionary entries match the same text. Given the two dictionary entries, Watson and Watson Natural Language Processing, you can configure which entry should match in ""I like Watson Natural Language Processing"": either only Watson Natural Language Processing, as it contains Watson, or both. You can specify to match the lemma instead of enumerating all inflections. This way, the single dictionary entry mouse will detect both mouse and mice in the text. You can attach a label to each dictionary entry, for example Organization category to include additional metadata in the match.  
All of these capabilities can be configured, so you can pick the right option for your use case."
339	Detecting entities with a custom dictionary	"Types of dictionary files
Watson Natural Language Processing supports two types of dictionary files:  
Arthur Allen Albert Alexa ""label"", ""entry"" ""ORGANIZATION"", ""NASA"" ""COUNTRY"", ""USA"" ""ACTOR"", ""Christian Bale""  
Term list (ending in .dict)  
Example of a term list:  
Table (ending in .csv)  
Example of a table:  
You can use multiple dictionaries during the same extraction. You can also use both types at the same time, for example, run a single extraction with three dictionaries, one term list and two tables."
340	Detecting entities with a custom dictionary	"Creating dictionary files
Begin by creating a module directory inside your notebook. This is a directory inside the notebook file system that will be used temporarily to store your dictionary files.  
To create dictionary files in your notebook:  
Create a module directory. Note that the name of the module folder cannot contain any dashes as this will cause errors.import os import watson_nlp module_folder = ""NLP_Dict_Module_1"" os.makedirs(module_folder, exist_ok=True) Create dictionary files, and store them in the module directory. You can either read in an external list or CSV file, or you can create dictionary files like so:# Create a term list dictionary term_file = ""names.dict"" with open(os.path.join(module_folder, term_file), 'w') as dictionary: dictionary.write('Bruce') dictionary.write('\n') dictionary.write('Peter') dictionary.write('\n') # Create a table dictionary table_file = 'Places.csv' with open(os.path.join(module_folder, table_file), 'w') as places: places.write(""\""label\"", \""entry\"""") places.write(""\n"") places.write(""\""SIGHT\"", \""Times Square\"""") places.write(""\n"") places.write(""\""PLACE\"", \""5th Avenue\"""") places.write(""\n"")"
341	Detecting entities with a custom dictionary	"Loading the dictionaries and configuring matching options
The dictionaries can be loaded using the following helper methods.  
To load a single dictionary, use watson_nlp.toolkit.rule_utils.DictionaryConfig (<dictionary configuration>) To load multiple dictionaries, use watson_nlp.toolkit.rule_utils.DictionaryConfig.load_all([<dictionary configuration>)])  
For each dictionary, you need to specify a dictionary configuration. The dictionary configuration is a Python dictionary, with the following attributes:  
Code sample"
342	Detecting entities with a custom dictionary	"Training a model that contains dictionaries
After you have loaded the dictionaries, create a dictionary model and train the model using the RBR.train() method. In the method, specify:  
The module directory The language of the dictionary entries The dictionaries to use  
Code sample"
343	Detecting entities with a custom dictionary	"Applying the model on new data
After you have trained the dictionaries, apply the model on new data using the run() method, as you would use on any of the existing pre-trained blocks.  
Code sample  
Output of the code sample:  
To show the labels or the name of the dictionary:  
Output showing the labels:  
Parent topic: Creating your own models"
344	Compute resource options for SPSS Modeler in projects	"When you run an SPSS Modeler flow in a project, you choose an environment template for the runtime environment. The environment template specifies the type, size, and power of the hardware configuration, plus the software template.  
Types of environments Default environment templates Compute usage Changing the runtime"
345	Compute resource options for SPSS Modeler in projects	"Types of environments
You can use this type of environment with SPSS Modeler:  
Default SPSS Modeler CPU environments for standard workloads"
346	Compute resource options for SPSS Modeler in projects	"Default environment templates
You can select any of the following default environment templates for SPSS Modeler in a project. The included environment templates are listed under Templates on the Environments page on the Manage tab of your project.  
After selecting an environment, any other SPSS Modeler flows opened in that project will use the same runtime. The hardware configuration of the available SPSS Modeler environments is preset and cannot be changed."
347	Compute resource options for SPSS Modeler in projects	"Compute usage in projects
SPSS Modeler consumes compute resources as CUH from the Watson Studio service in projects.  
You can monitor the Watson Studio CUH consumption on the Resource usage page on the Manage tab of your project."
348	Compute resource options for SPSS Modeler in projects	"Changing the SPSS Modeler runtime
If you notice that processing is very slow, you can restart SPSS Modeler and select a larger environment runtime.  
To change the SPSS Modeler environment runtime:  
Save any data from your current session before switching to another environment. Stop the active SPSS Modeler runtime under Tool runtimes on the Environments page on the Manage tab of your project. Restart SPSS Modeler and select another environment with the compute power and memory capacity that better meets your requirements."
349	Compute resource options for SPSS Modeler in projects	"Learn more
SPSS Modeler  
Parent topic: Choosing compute resources for tools"
350	Run a sample pipeline to compare models	"Download a pre-populated project with the assets that you need to run a sample pipeline. The pipeline compares two AutoAI experiments and compares the output, selecting the best model and deploying it as a web service.  
The sample Train AutoAI and reference model creates a pre-populated project with the assets that you need to run a pre-built pipeline that trains models by using a sample data set. After the setup and configuration tasks, you can run the sample pipeline to automate the following sequence:  
Copy sample assets into a space. Run a notebook and an AutoAI experiment simultaneously, on a common training data set. Run another notebook to compare the results from the previous nodes and select the best model, ranked for accuracy. Copy the winning model to a space and create a web service deployment for the selected model.  
After the run completes, you can inspect the output in the pipeline editor, and then switch to the associated deployment space to view and test the resulting deployment."
351	Run a sample pipeline to compare models	"Learning goals
After running this sample, you will know how to:  
Configure a Watson Pipeline Run a Watson Pipeline"
352	Run a sample pipeline to compare models	"Downloading the sample
Follow these steps to create the sample project from the Resource hub so you can test the capabilities of IBM Watson Pipelines:  
Open the Train AutoAI and reference model sample from the Resource hub. Click Create project to create the project. Open the project and follow the instructions on the in the Readme file to set up the pipeline assets."
353	Run a sample pipeline to compare models	"The sample pipeline components
The sample project includes:  
Pre-built sample Watson Pipeline Data set called german_credit_data_biased_training.csv used for training a model to predict credit risk Data set called german_credit_test_data.csv used to test the deployed model Notebook called reference-model-training-notebook that trains an AutoAI experiment and saves the best pipeline as a model Notebook called select-winning-model that compares the models and chooses the best to save to the designated deployment space"
354	Run a sample pipeline to compare models	"Getting started with the sample
To run the sample pipeline, you need to do these setup tasks:  
Create a deployment space, for example, dev-space which you need when you run the notebooks. From the navigation menu, select Deployments > View All Spaces > New deployment space. Complete the required fields.  
Note:  
Make sure you associate a Watson Machine Learning instance with the space or the pipeline run will fail.  
From the Assets page of the sample project, open the reference-model-training-notebook and follow the steps in the Set up the environment section to acquire and insert an api_key variable as your credentials.  
After you insert your credentials, click File > Save as version to save the updated notebook to your project.  
Do the same for the select-winning-model notebook to add credentials and save the updated version of the notebook."
355	Run a sample pipeline to compare models	"Exploring the pipeline
After you complete the set up tasks, open the sample pipeline On-boarding - Train AutoAI and reference model and select the best from the Assets page of the sample project.  
You will see the sample pipeline:"
356	Run a sample pipeline to compare models	"Exploring the pipeline
Viewing node configuration
As you explore the sample pipeline, double-click the various nodes to view their configuration. For example, if you click the first node for copying an asset, you will see this configuration:  
The node that copies the data asset to a deployment space is configured by using a pipeline parameter. The pipeline parameter creates a placeholder for the space that you created to use for this pipeline. When you run the pipeline, you are prompted to choose the space."
357	Run a sample pipeline to compare models	"Exploring the pipeline
Running the pipeline
When you are ready to run the pipeline, click the Run icon and choose Trial job. You are prompted to choose the deployment space for the pipeline and create or supply an API key for the pipeline if one is not already available.  
As the pipeline runs, you will see status notifications about the progress of the run. Nodes that are processed successfully are marked with a checkmark."
358	Run a sample pipeline to compare models	"Exploring the pipeline
Viewing the output
When the job completes, click Pipeline output for the run to see a summary of pipeline processes. You can click to expand each section and view the details for each operation."
359	Run a sample pipeline to compare models	"Viewing the deployment in your space
After you are done exploring the pipeline and its output, you can view the assets that were created in the space you designated for the pipeline.  
Open the space. You can see that the models and training data were copied to the space. The winning model is tagged as selected_model."
360	Run a sample pipeline to compare models	"Viewing the deployment in your space
Viewing the deployment
The last step of the pipeline created a web service deployment for the selected model. Click the Deployments tab to view the deployment."
361	Run a sample pipeline to compare models	"Viewing the deployment in your space
Testing the deployment
You can test the deployment to see the predictions that the model generates.  
Click the deployment name to view the details. Click the Test tab. Enter this JSON data into the Input form. The payload (input) must match the schema for the model but do not include the prediction column.  
Clicking Predict returns this prediction, indicating a low credit risk for this customer."
362	Run a sample pipeline to compare models	"Next steps
Create a pipeline by using your own assets.  
Parent topic: Watson Pipelines"
363	Creating a time series anomaly prediction (Beta)	"Create a time series anomaly prediction experiment to train a model that can detect anomalies, or unexpected results, when the model predicts results based on new data.  
Tech preview This is a technology preview and is not yet supported for use in production environments."
364	Creating a time series anomaly prediction (Beta)	"Detecting anomalies in predictions
You can use anomaly prediction to find outliers in model predictions. Consider the following scenarios for training a time series model with anomaly prediction. For example, suppose you have operational metrics from monitoring devices that were collected in the date range of 2022.1.1 through 2022.3.31. You are confident that no anomalies exist in the data for that period, even if the data is unlabeled. You can use a time series anomaly prediction experiment to:  
Train model candidate pipelines and auto-select the top-ranked model candidate Deploy a selected model to predict new observations if:  
A new time point is an anomaly (for example, an online score predicts a time point 2022.4.1 that is outside of the expected range) A new time range has anomalies (for example, a batch score predicts values of 2022.4.1 to 2022.4.7, outside the expected range)"
365	Creating a time series anomaly prediction (Beta)	"Working with a sample
To create an AutoAI Time series experiment with anomaly prediction that uses a sample:  
Create an AutoAI experiment.  
Select Resource hub sample.  
Click the tile for Electricity usage anomalies sample data.  
Follow the prompts to configure and run the experiment.  
Review the details about the pipelines and explore the visualizations."
366	Creating a time series anomaly prediction (Beta)	"Configuring a time series experiment with anomaly prediction
Load the data for your experiment.  
Restriction: You can upload only a single data file for an anomaly prediction experiment. If you upload a second data file (for holdout data) the Anomaly prediction option is disabled, and only the Forecast option is available. By default, Anomaly prediction experiments use a subset of the training data for validation.  
Click Yes to Enable time series.  
Select Anomaly prediction as the experiment type.  
Configure the feature columns from the data source that you want to predict based on the previous values. You can specify one or more columns to predict.  
Select the date/time column.  
The prediction summary shows you the experiment type and the metric that is selected for optimizing the experiment."
367	Creating a time series anomaly prediction (Beta)	"Configuring experiment settings
To configure more details for your time series experiment, open the Experiment settings pane. Options that are not available for anomaly prediction experiments are unavailable."
368	Creating a time series anomaly prediction (Beta)	"Configuring experiment settings
General prediction settings
On the General panel for prediction settings, configure details for training the experiment."
369	Creating a time series anomaly prediction (Beta)	"Configuring experiment settings
Time series configuration details
On the Time series pane for prediction settings, configure the details for how to train the experiment and generate predictions."
370	Creating a time series anomaly prediction (Beta)	"Configuring data source settings
To configure details for your input data, open the Experiment settings panel and select the Data source."
371	Creating a time series anomaly prediction (Beta)	"Configuring data source settings
General data source settings
On the General panel for data source settings, you can choose options for how to use your experiment data."
372	Creating a time series anomaly prediction (Beta)	"Reviewing the experiment results
When you run the experiment, the progress indicator displays the pathways to pipeline creation. Ranked pipelines are listed on the leaderboard. Pipeline score represents how well the pipeline performed for the optimizing metric.  
The Experiment summary tab displays a visualization of how metrics performed for the pipeline.  
Use the metric filter to focus on particular metrics. Hover over the name of a metric to view details.  
Click a pipeline name to view details. On the Model evaluation page, you can review a table that summarizes details about the pipeline.  
The rows represent five evaluation metrics: Area under ROC, Precision, Recall, F1, Average precision. The columns represent four synthesized anomaly types: Level shift, Trend, Localized extreme, Variance. Each value in a cell is an average of the metric based on three iterations of evaluation on the synthesized anomaly type."
373	Creating a time series anomaly prediction (Beta)	"Reviewing the experiment results
Evaluation metrics:
These metrics are used to evaluate a pipeline:"
374	Creating a time series anomaly prediction (Beta)	"Reviewing the experiment results
Anomaly types
These are the anomaly types AutoAI detects."
375	Creating a time series anomaly prediction (Beta)	"Saving a pipeline as a model
To save a model candidate pipeline as a machine learning model, select Save as model for the pipeline you prefer. The model is saved as a project asset. You can promote the model to a space and create a deployment for it."
376	Creating a time series anomaly prediction (Beta)	"Saving a pipeline as a notebook
To review the code for a pipeline, select Save as notebook for a pipeline. An automatically generated notebook is saved as a project asset. Review the code to explore how the pipeline was generated.  
For details on the methods used in the pipeline code, see the documentation for the autoai-ts-libs library."
377	Creating a time series anomaly prediction (Beta)	"Scoring the model
After you save a pipeline as a model, then promote the model to a space, you can score the model to generate predictions for input, or payload, data. Scoring the model and interpreting the results is similar to scoring a binary classification model, as the score presents one of two possible values for each prediction:  
1 = no anomaly detected -1 = anomaly detected"
378	Creating a time series anomaly prediction (Beta)	"Scoring the model
Deployment details
Note these requirements for deploying an anomaly prediction model.  
The schema for the deplyment input data must match the schema for the training data except for the prediction, or target column. The order of the fields for model scoring must be the same as the order of the fields in the training data schema."
379	Creating a time series anomaly prediction (Beta)	"Scoring the model
Deployment example
The following is valid input for an anomaly prediction model:  
The score for this input is [1,1,-1,1] where -1 means the value is an anomaly and 1 means the prediction is in the normal range."
380	Creating a time series anomaly prediction (Beta)	"Implementation details
These algorithms support anomaly prediction in time series experiments.  
The algorithms are organized in these categories:  
Forecasting: Algorithms for detecting anomalies using time series forecasting methods Relationship: Algorithms for detecting anomalies by analyzing the relationship among data points Window: Algorithms for detecting anomalies by applying transformations and ML techniques to rolling windows"
381	Creating a time series anomaly prediction (Beta)	"Learn more
Saving an AutoAI generated notebook (Watson Machine Learning)  
Parent topic: Building a time series experiment"
382	Sample foundation model prompts for common tasks	Try these samples to learn how different prompts can guide foundation models to do common tasks.
383	Sample foundation model prompts for common tasks	"How to use this topic
Explore the sample prompts in this topic:  
Copy and paste the prompt text and input parameter values into the Prompt Lab in IBM watsonx.ai See what text is generated. See how different models generate different output. Change the prompt text and parameters to see how results vary.  
There is no one right way to prompt foundation models. But patterns have been found, in academia and industry, that work fairly reliably. Use the samples in this topic to build your skills and your intuition about prompt engineering through experimentation.  
   
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video chapters [ 0:11 ] Introduction to prompts and Prompt Lab [ 0:33 ] Key concept: Everything is text completion [ 1:34 ] Useful prompt pattern: Few-shot prompt [ 1:58 ] Stopping criteria: Max tokens, stop sequences [ 3:32 ] Key concept: Fine-tuning [ 4:32 ] Useful prompt pattern: Zero-shot prompt [ 5:32 ] Key concept: Be flexible, try different prompts [ 6:14 ] Next steps: Experiment with sample prompts"
384	Sample foundation model prompts for common tasks	"Samples overview
You can find samples that prompt foundation models to generate output that supports the following tasks:  
Classification Extraction Generation Question answering (QA) Summarization Code generation and conversion Dialog Translation  
The following table shows the foundation models that are used in task-specific samples. A checkmark (✓) indicates that the model is used in a sample for the associated task. You can click See sample to go to the sample prompt.  
The following table summarizes the available sample prompts."
385	Sample foundation model prompts for common tasks	"Classification
Classification is useful for predicting data in distinct categories. Classifications can be binary, with two classes of data, or multi-class. A classification task is useful for categorizing information, such as customer feedback, so that you can manage or act on the information more efficiently."
386	Sample foundation model prompts for common tasks	"Classification
Sample with a zero-shot prompt: Classify a message
Scenario: Given a message that is submitted to a customer-support chatbot for a cloud software company, classify the customer's message as either a question or a problem. Depending on the class assignment, the chat is routed to the correct support team for the issue type.  
Model choice Models that are instruction-tuned can generally complete this task with this sample prompt.  
Suggestions: mt0-xxl-13b, flan-t5-xxl-11b, flan-ul2-20b, or mixtral-8x7b-instruct-v01  
Decoding Greedy. The model must return one of the specified class names; it cannot be creative and make up new classes.  
Stopping criteria  
Specify two stop sequences: ""Question"" and ""Problem"". After the model generates either of those words, it should stop. With such short output, the Max tokens parameter can be set to 5.  
Prompt text Paste this zero-shot prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click Generate to see the result."
387	Sample foundation model prompts for common tasks	"Classification
Sample with a few-shot prompt: Classify a message in freeform mode
Scenario: Given a message that is submitted to a customer-support chatbot for a cloud software company, classify the customer's message as either a question or a problem description. Based on the class type, the chat can be routed to the correct support team.  
Model choice With few-shot examples of both classes, models such as mixtral-8x7b-instruct-v01 can complete this task well.  
Decoding Greedy. The model must return one of the specified class names; it cannot be creative and make up new classes.  
Stopping criteria  
Specify two stop sequences: ""Question"" and ""Problem"". After the model generates either of those words, it should stop. With such short output, the Max tokens parameter can be set to 5.  
Prompt text Paste this few-shot prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click Generate to see the result."
388	Sample foundation model prompts for common tasks	"Classification
Sample with a few-shot prompt: Classify a message in structured mode
Scenario: Given a message that is submitted to a customer-support chatbot for a cloud software company, classify the customer's message as either a question or a problem description. Based on the class type, the chat can be routed to the correct support team.  
Model choice With few-shot examples of both classes, models such as mixtral-8x7b-instruct-v01 can complete this task well.  
Decoding Greedy. The model must return one of the specified class names, not be creative and make up new classes.  
Stopping criteria  
Specify two stop sequences: ""Question"" and ""Problem"". After the model generates either of those words, it should stop. With such short output, the Max tokens parameter can be set to 5.  
Set up section Paste these headers and examples into the Examples area of the Set up section:  
Try section Paste this message in the Try section:  
Select the model and set parameters, then click Generate to see the result."
389	Sample foundation model prompts for common tasks	"Classification
Sample: Classify a Japanese message
Scenario: Given a message that is submitted to a customer-support chatbot for a Japanese cloud software company, classify the customer's message as either a question or a problem description. Based on the class type, the chat can be routed to the correct support team.  
Model choice The elyza-japanese-llama-2-7b-instruct model can classify prompt input text that is written in Japanese.  
AI guardrails Disable the AI guardrails feature. The feature is supported with English text only. It might flag as inappropriate content that is not inappropriate.  
Decoding Greedy. The model must return one of the specified class names; it cannot be creative and make up new classes.  
Stopping criteria  
Specify two stop sequences: 問題 for problem and 質問 for question. After the model generates either of those words, it should stop. If you want to lower the value in the Max tokens parameter, do not lower the value below 7 tokens. Japanese characters use more tokens than the same words in English.  
Prompt text Paste this few-shot prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click Generate to see the result. The sample prompt text is a Japanese translation of the English prompt text in Sample 1b."
390	Sample foundation model prompts for common tasks	"Extracting details
Extraction tasks can help you to find key terms or mentions in data based on the semantic meaning of words rather than simple text matches."
391	Sample foundation model prompts for common tasks	"Extracting details
Sample: Extract details from a complaint
Scenario: Given a complaint from a customer who had trouble booking a flight on a reservation website, identify the factors that contributed to this customer's unsatisfactory experience.  
Model choices flan-ul2-20b  
Decoding Greedy. We need the model to return words that are in the input; the model cannot be creative and make up new words.  
Stopping criteria The list of extracted factors will not be long, so set the Max tokens parameter to 50.  
Prompt text Paste this zero-shot prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click Generate to see the result."
392	Sample foundation model prompts for common tasks	"Generating natural language
Generation tasks are what large language models do best. Your prompts can help guide the model to generate useful language."
393	Sample foundation model prompts for common tasks	"Generating natural language
Sample with a few-shot prompt: Generate a numbered list on a theme in freeform mode
Scenario: Generate a numbered list on a particular theme.  
Model choice The mixtral-8x7b-instruct-v01 foundation model was trained to recognize and handle special characters, such as the newline character, well. This model is a good choice when you want your generated text to be formatted a specific way with special characters.  
Decoding Sampling. This is a creative task. Set the following parameters:  
Temperature: 0.7 Top P: 1 Top K: 50 Random seed: 9045 (To get different output each time you click Generate, specify a different value for the Random seed parameter or clear the parameter.)  
Stopping criteria  
To make sure the model stops generating text after one list, specify a stop sequence of two newline characters. To do that, click the Stop sequence text box, press the Enter key twice, and then click Add sequence. The list will not be very long, so set the Max tokens parameter to 50.  
Prompt text Paste this few-shot prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click Generate to see the result."
394	Sample foundation model prompts for common tasks	"Generating natural language
Sample with a few-shot prompt: Generate a numbered list on a theme in structured mode
Scenario: Generate a numbered list on a particular theme.  
Model choice The mixtral-8x7b-instruct-v01 foundation model was trained to recognize and handle special characters, such as the newline character, well. This model is a good choice when you want your generated text to be formatted a specific way with special characters.  
Decoding Sampling. This scenario is a creative one. Set the following parameters:  
Temperature: 0.7 Top P: 1 Top K: 50 Random seed: 9045 (To generate different results, specify a different value for the Random seed parameter or clear the parameter.)  
Stopping criteria  
To make sure that the model stops generating text after one list, specify a stop sequence of two newline characters. To do that, click in the Stop sequence text box, press the Enter key twice, and then click Add sequence. The list will not be long, so set the Max tokens parameter to 50.  
Set up section Paste these headers and examples into the Examples area of the Set up section:  
Try section Paste this input in the Try section:  
Select the model and set parameters, then click Generate to see the result."
395	Sample foundation model prompts for common tasks	"Generating natural language
Sample with a zero-shot prompt: Generate a numbered list on a particular theme
Scenario: Ask the model to play devil's advocate. Describe a potential action and ask the model to list possible downsides or risks that are associated with the action.  
Model choice The granite-13b-instruct model was trained to recognize and handle special characters, such as the newline character, well. The granite-13b-instruct-v2 model is a good choice when you want your generated text to be formatted in a specific way with special characters.  
Decoding Greedy. The model must return the most predictable content based on what's in the prompt; the model cannot be too creative.  
Stopping criteria The summary might run several sentences, so set the Max tokens parameter to 60.  
Prompt text Paste this prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click Generate to see the result."
396	Sample foundation model prompts for common tasks	"Question answering
Question-answering tasks are useful in help systems and other scenarios where frequently asked or more nuanced questions can be answered from existing content.  
To help the model return factual answers, implement the retrieval-augmented generation pattern. For more information, see Retrieval-augmented generation."
397	Sample foundation model prompts for common tasks	"Question answering
Sample: Answer a question based on an article in freeform mode
Scenario: The website for an online seed catalog has many articles to help customers plan their garden and ultimately select which seeds to purchase. A new widget is being added to the website to answer customer questions based on the contents of the article the customer is viewing. Given a question that is related to an article, answer the question based on the article.  
Model choice Models that are instruction-tuned, such as flan-t5-xxl-11b, flan-ul2-20b, mixtral-8x7b-instruct-v01, or mt0-xxl-13b can generally complete this task with this sample prompt.  
Decoding Greedy. The answers must be grounded in the facts in the article, and if there is no good answer in the article, the model should not be creative and make up an answer.  
Stopping criteria To cause the model to return a one-sentence answer, specify a period ""."" as a stop sequence. The Max tokens parameter can be set to 50.  
Prompt text Paste this zero-shot prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click Generate to see the result.  
You can experiment with asking other questions too, such as:  
""How large do tomato plants get?"" ""Do tomato plants prefer shade or sun?"" ""Is it easy to grow tomatoes?""  
Try out-of-scope questions too, such as:  
""How do you grow cucumbers?"""
398	Sample foundation model prompts for common tasks	"Question answering
Sample: Answer a question based on an article in structured mode
Scenario: The website for an online seed catalog has many articles to help customers plan their garden and ultimately select which seeds to purchase. A new widget is being added to the website to answer customer questions based on the contents of the article the customer is viewing. Given a question related to a particular article, answer the question based on the article.  
Model choice Models that are instruction-tuned, such as flan-t5-xxl-11b, flan-ul2-20b, mixtral-8x7b-instruct-v01, or mt0-xxl-13b can generally complete this task with this sample prompt.  
Decoding Greedy. The answers must be grounded in the facts in the article, and if there is no good answer in the article, the model should not be creative and make up an answer.  
Stopping criteria To cause the model to return a one-sentence answer, specify a period ""."" as a stop sequence. The Max tokens parameter can be set to 50.  
Set up section Paste this text into the Instruction area of the Set up section:  
Try section In the Try section, add an extra test row so you can paste each of these two questions in a separate row:  
Select the model and set parameters, then click Generate to see two results."
399	Sample foundation model prompts for common tasks	"Question answering
Sample: Answer a question based on a document with Granite
Scenario: You are creating a chatbot that can answer user questions. When a user asks a question, you want the agent to answer the question with information from a specific document.  
Model choice Models that are instruction-tuned, such as granite-13b-instruct-v2, can complete the task with this sample prompt.  
Decoding Greedy. The answers must be grounded in the facts in the document, and if there is no good answer in the article, the model should not be creative and make up an answer.  
Stopping criteria Use a Max tokens parameter of 50.  
Prompt text Paste this zero-shot prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click Generate to see the result."
400	Sample foundation model prompts for common tasks	"Question answering
Sample: Answer general knowledge questions in Japanese
Scenario: Answer general questions about finance in Japanese.  
Model choice The granite-8b-japanese model can be used for multiple tasks, including text generation, summarization, question and answering, classification, and extraction.  
Decoding Greedy. This sample is answering questions, so we don't want creative output.  
Stopping criteria  
Set the Max tokens parameter to 500 to allow for many turns in the dialog. Add a stop sequence of two newline characters to prevent the foundation model from returning overly long responses. To do that, click in the Stop sequence text box, press the Enter key twice, and then click Add sequence.  
Prompt text The model was tuned for question-answering with examples in the following format:  
In English, the template reads as follows:  
Paste this prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, replace {your-input} with your query or request, and then click Generate to see the result.  
For example, this prompt asks about interest rates."
401	Sample foundation model prompts for common tasks	"Summarization
Summarization tasks save you time by condensing large amounts of text into a few key pieces of information."
402	Sample foundation model prompts for common tasks	"Summarization
Sample with a zero-shot prompt: Summarize a meeting transcript
Scenario: Given a meeting transcript, summarize the main points as meeting notes so those notes can be shared with teammates who did not attend the meeting.  
Model choice Models that are instruction-tuned can generally complete this task with this sample prompt. Suggestions: flan-t5-xxl-11b, flan-ul2-20b, or mixtral-8x7b-instruct-v01.  
Decoding Greedy. The model must return the most predictable content based on what's in the prompt; the model cannot be too creative.  
Stopping criteria The summary might run several sentences, so set the Max tokens parameter to 60.  
Prompt text Paste this zero-shot prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click Generate to see the result."
403	Sample foundation model prompts for common tasks	"Summarization
Sample with a few-shot prompt: Summarize a meeting transcript in freeform mode
Scenario: Given a meeting transcript, summarize the main points as meeting notes so those notes can be shared with teammates who did not attend the meeting.  
Model choice With few-shot examples, most models can complete this task well. Try mixtral-8x7b-instruct-v01.  
Decoding Greedy. The model must return the most predictable content based on what's in the prompt, not be too creative.  
Stopping criteria  
To make sure that the model stops generating text after the summary, specify a stop sequence of two newline characters. To do that, click in the Stop sequence text box, press the Enter key twice, and then click Add sequence. Set the Max tokens parameter to 60.  
Prompt text Paste this few-shot prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click Generate to see the result."
404	Sample foundation model prompts for common tasks	"Summarization
Sample with a few-shot prompt: Summarize a meeting transcript in structured mode
Scenario: Given a meeting transcript, summarize the main points in a bulleted list so that the list can be shared with teammates who did not attend the meeting.  
Model choice The mixtral-8x7b-instruct-v01 foundation model was trained to recognize and handle special characters, such as the newline character, well. This model is a good choice when you want your generated text to be formatted in a specific way with special characters.  
Decoding Greedy. The model must return the most predictable content based on what's in the prompt; the model cannot be too creative.  
Stopping criteria  
To make sure that the model stops generating text after one list, specify a stop sequence of two newline characters. To do that, click in the Stop sequence text box, press the Enter key twice, and then click Add sequence. Set the Max tokens parameter to 60.  
Set up section Paste these headers and examples into the Examples area of the Set up section:  
Try section Paste this message in the Try section:  
Select the model and set parameters, then click Generate to see the result."
405	Sample foundation model prompts for common tasks	"Code generation and conversion
Foundation models that can generate and convert programmatic code are great resources for developers. They can help developers to brainstorm and troubleshoot programming tasks."
406	Sample foundation model prompts for common tasks	"Code generation and conversion
Sample: Generate programmatic code from instructions
Scenario: You want to generate code from instructions. Namely, you want to write a function in the Python programming language that returns a sequence of prime numbers that are lower than the number that is passed to the function as a variable.  
Model choice Models that can generate code, such as codellama-34b-instruct-hf and mixtral-8x7b-instruct-v01 can generally complete this task when a sample prompt is provided.  
Decoding Greedy. The answer must be a valid code snippet. The model cannot be creative and make up an answer.  
Stopping criteria To stop the model after it returns a single code snippet, specify <end of code> as the stop sequence. The Max tokens parameter can be set to 1,000.  
Prompt text Paste this code snippet into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click Generate to see the result.  
The output contains Python code similar to the following snippet:  
Be sure to test the generated code to verify that it works as you expect.  
For example, if you run reversed(""good morning""), the result is 'gninrom doog'.  
Note: The foundation model might generate code that is taken directly from its training data. As a result, generated code might require attribution. You are responsible for ensuring that any generated code that you use is properly attributed, if necessary."
407	Sample foundation model prompts for common tasks	"Code generation and conversion
Sample: Convert code from one programming language to another
Scenario: You want to convert code from one programming language to another. Namely, you want to convert a code snippet from C++ to Python.  
Model choice Models that can generate code, such as codellama-34b-instruct-hf and mixtral-8x7b-instruct-v01 can generally complete this task when a sample prompt is provided.  
Decoding Greedy. The answer must be a valid code snippet. The model cannot be creative and make up an answer.  
Stopping criteria To stop the model after it returns a single code snippet, specify <end of code> as the stop sequence. The Max tokens parameter can be set to 300.  
Prompt text Paste this code snippet into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click Generate to see the result.  
This prompt includes an example input and output pair. The input is C++ code and the output is the equivalent function in Python code.  
The C++ code snippet to be converted is included next. It is a function that counts the number of arithmetic progressions with the sum S and common difference of D, where S and D are integer values that are passed as parameters.  
The final part of the prompt identifies the language that you want the C++ code snippet to be converted into.  
The output contains Python code similar to the following snippet:  
The generated Python code functions the same as the C++ function included in the prompt.  
Test the generated Python code to verify that it works as you expect.  
Remember, the foundation model might generate code that is taken directly from its training data. As a result, generated code might require attribution. You are responsible for ensuring that any generated code that you use is properly attributed, if necessary."
408	Sample foundation model prompts for common tasks	"Dialog
Dialog tasks are helpful in customer service scenarios, especially when a chatbot is used to guide customers through a workflow to reach a goal."
409	Sample foundation model prompts for common tasks	"Dialog
Sample: Converse with llama-2 chat
Scenario: Generate dialog output like a chatbot.  
Model choice Like other foundation models, Llama 2 (in both the 70 billion and 13 billion sizes) can be used for multiple tasks. But both Llama 2 models are optimized for dialog use cases. The llama-2-70b-chat and llama-2-13b-chat are fine-tuned for the [INST]<<SYS>><</SYS>>[/INST] prompt format. For more information about this prompt format, see How to prompt Llama 2.  
Decoding Greedy. This sample is answering general knowledge, factual questions, so we don't want creative output.  
Stopping criteria Set the Max tokens parameter to 100.  
The template for Llama 2 uses an end-of-string (EOS) token (</s>) to delimit the end of the agent's response. The model output stops generating text when the </s> EOS token is encountered.  
Prompt text Paste this prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click Generate to see the result.  
After the initial output is generated, continue the dialog to ask about more planets, wrapping your user messages in the same [INST][/INST] structure. You don't need to add an <s> token to the start of follow-up questions.  
Tip: Alternatively, you can switch to chat mode and enter the user input directly. The Prompt Lab inserts the expected syntax that identifies the instruction and user input segments for you.  
The output looks something like this:  
Notice a few important points about the output that the model generates as you go:"
410	Sample foundation model prompts for common tasks	"Dialog
Sample: Converse with llama-2 chat
The output looks something like this:  
Notice a few important points about the output that the model generates as you go:  
The so-called system instruction, in the <<SYS>><</SYS>> tags, continues to influence the output at each dialog turn without having to repeat the system instruction. In other words, the assistant responses continue to be prepended with ""Dear user, "". In true dialog fashion, the assistant response to the user input ""How many moons are there?"" takes into account the previous user input ""I have a question about the Earth."" and returns the number of moons orbiting Earth. Also in proper dialog form, the assistant responses continue to follow the topic of the conversation, which is the number of moons. (Otherwise, the generated output to the vague user message ""And Mars?"" could wander off in any direction.) Caution: Newline (carriage-return) characters especially, and spaces to a lesser extent, in the prompt text can have a dramatic impact on the output generated. When you use Llama 2 for chat use cases, follow the recommended prompt template format as closely as possible. Do not use the [INST]<<SYS>><</SYS>>[/INST] prompt format when you use Llama 2 for any other tasks besides chat."
411	Sample foundation model prompts for common tasks	"Dialog
Sample: Converse with granite-13b-chat-v1
Scenario: Generate dialog output like a chatbot.  
Model choice Like other foundation models, granite-13b-chat-v1 can be used for multiple tasks. However, it is optimized for carrying on a dialog.  
Decoding Greedy. This sample is answering general knowledge, factual questions, so we don't want creative output.  
Stopping criteria  
A helpful feature of the model is the inclusion of a special token that is named END_KEY at the end of each response. When some generative models return a response to the input in fewer tokens than the maximum number allowed, they can repeat patterns from the input. This model prevents such repetition by incorporating a reliable stop sequence for the prompt. Add END_KEY as the stop sequence. Set the Max tokens parameter to 200 so the model can return a complete answer.  
Prompt text The model expects the input to follow a specific pattern.  
Start the input with an instruction. For example, the instruction might read as follows:  
You are a helpful AI language model developed by IBM. Your primary function is to assist users in answering questions, generating text, and engaging in conversation. You should not produce output that discriminates based on race, religion, gender identity, or sexual orientation.  
Next, add lines to capture the question and answer pattern with the following syntax:  
Human: content of the question Assistant: new line for the model's answer  
You can replace the terms Human and Assistant with other terms.  
If you're using version 1, do not include any trailing white spaces after the Assistant: label, and be sure to add a new line.  
Paste the following prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click Generate to see the result.  
Tip: Alternatively, you can switch to chat mode and enter the user input directly. The Prompt Lab inserts the expected syntax that identifies the instruction and user input segments for you.  
After the initial output is generated, continue the dialog by asking a follow-up question. For example, if the output describes how a bill becomes a law in the United States, you can ask about how laws are made in other countries.  
A few notes about using this sample with the model:"
412	Sample foundation model prompts for common tasks	"Dialog
Sample: Converse with granite-13b-chat-v1
A few notes about using this sample with the model:  
The prompt input outlines the chatbot scenario and describes the personality of the AI assistant. The description explains that the assistant should indicate when it doesn't know an answer. It also directs the assistant to avoid discussing unethical topics. The assistant is able to respond to a follow-up question that relies on information from an earlier exchange in the same dialog. The model expects the input to follow a specific pattern. The generated response from the model is clearly indicated by the keyword END_KEY. You can use this keyword as a stop sequence to help the model generate succinct responses."
413	Sample foundation model prompts for common tasks	"Dialog
Sample: Converse with granite-13b-chat-v2
Scenario: Generate dialog output like a chatbot.  
Model choice Use granite-13b-chat-v2 to carry on a dialog.  
Decoding  
Use sampling decoding with the default sampling settings. Set the repetition penalty to 1.2.  
Stopping criteria  
Set the Max tokens parameter to 200 so the model returns a complete answer, but tries to be as concise as possible.  
Prompt text To improve model safety and reduce bias, add a system prompt as part of the user input. The system prompt can establish some ground rules for the dialog. For example:  
You are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. Keep your answers short and to the point.  
Remember to use the prompt template format that is expected by the model.  
<|system|> system prompt <|user|> content of the question <|assistant|> new line for the model's answer  
If you want to submit a few-shot prompt to this model, you can add the system prompt, and then the examples, followed by the prompt text to be inferenced.  
Paste the following prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click Generate to see the result.  
Tip: Alternatively, you can switch to chat mode and enter the user input directly. The Prompt Lab inserts the expected syntax that identifies the instruction and user input segments for you.  
Do not include any trailing white spaces after the <|assistant|> label, and be sure to add a new line.  
After the initial output is generated, you can continue the dialog by asking a follow-up question. For example, you can ask about the moons of other planets.  
And continue the conversation with another follow-up question.  
If the model output is too long, you can try specifying a stop sequence of two newline characters by clicking the Stop sequence text box, pressing the Enter key twice, and then clicking Add sequence. However, the repetition penalty is usually enough to keep the model on track.  
A few notes about using this sample with the model:"
414	Sample foundation model prompts for common tasks	"Dialog
Sample: Converse with granite-13b-chat-v2
A few notes about using this sample with the model:  
The system prompt establishes some general guardrails for the model. The assistant is able to respond to a follow-up question that relies on information from an earlier exchange in the same dialog. The model expects the input to follow a specific pattern, and can be sensitive to misplaced whitespaces."
415	Sample foundation model prompts for common tasks	"Dialog
Sample: Converse in Japanese with granite-8b-japanese
Scenario: Generate Japanese dialog output like a chatbot.  
Model choice The granite-8b-japanese foundation model can be used to participate in a dialog in Japanese. The granite-8b-japanese foundation model works best when you use the same prompt format as was used during model training.  
Decoding Greedy. This sample is answering general knowledge, factual questions, so we don't want creative output.  
Stopping criteria  
Set the Max tokens parameter to 500 to allow for many turns in the dialog. Add a stop sequence of two newline characters to prevent the foundation model from returning overly long responses. To do that, click in the Stop sequence text box, press the Enter key twice, and then click Add sequence.  
Prompt text Paste this prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click Generate to see the result.  
This template includes a system prompt that says “You are an honest and talented AI assistant. Please follow the user's instructions as accurately as possible.”  
For example, to request information about investing from the foundation model, you can submit the following prompt, where the user input is “Please tell me about investment.”  
After the initial output is generated, you can continue the dialog by asking follow-up questions. Wrap the message as follows:  
For example, you might ask “Are there any risks?”  
Notice a few important points about the output that the model generates as you go:  
The instruction continues to influence the output at each dialog turn without having to repeat the system instruction. The assistant responses continue to follow the topic of the conversation."
416	Sample foundation model prompts for common tasks	"Dialog
Sample: Converse in Arabic with jais-13b-chat
Scenario: Generate Arabic and English dialog output like a chatbot.  
Model choice Use jais-13b-chat to participate in a conversation.  
Decoding  
Use greedy decoding with the default sampling settings.  
Stopping criteria  
Set the Max tokens parameter to 900 so the model can return a complete answer and handle follow-up questions.  
Prompt text To improve model safety and reduce bias, add a system prompt as part of the user input. When the model was trained, it used a system prompt with the following text:  
The system prompt in English is as follows:  
Tip: Alternatively, you can switch to chat mode and enter the user input directly. The Prompt Lab inserts the system prompt, the instruction, and user input segments with the expected syntax for you.  
Replace {Question} with the user input that you want the foundation model to answer to start the chat.  
For example, you can ask the following question:  
هل يوجد للأرض أقمار؟  
The English translation is: Does the Earth have any moons?  
After the initial output is generated, you can continue the dialog by asking a follow-up question. Use the same syntax for the follow-up question."
417	Sample foundation model prompts for common tasks	"Translation
Use models that can do natural language translation tasks to translate text from one natural language to another."
418	Sample foundation model prompts for common tasks	"Translation
Sample: Translate text from Japanese to English
Scenario: Translate text that is written in Japanese into English.  
Model choice The elyza-japanese-llama-2-7b-instruct model can translate text from Japanese to English and from English to Japanese.  
AI guardrails Disable the AI guardrails feature. The feature is supported with English text only. It might flag as inappropriate content that is not inappropriate.  
Decoding Greedy. The model must return the same text, only translated. The model cannot be creative.  
Stopping criteria Increase the number of allowed tokens by changing the Max tokens parameter value to 500.  
Prompt text Paste the following prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click Generate to see the result.  
The sample prompt text overlaps with text that is used in Sample 8c.  
The instruction says “Please translate to English”. One example of Japanese text being translated into English is included."
419	Sample foundation model prompts for common tasks	"Translation
Sample: Translate text from Spanish to English
Scenario: Translate text that is written in Spanish into English.  
Model choice The mixtral-8x7b-instruct-v01 model can translate text from French, German, Italian, or Spanish to English. This sample prompts the model to translate from Spanish to English.  
AI guardrails Disable the AI guardrails feature. The feature is supported with English text only. It might flag as inappropriate content that is not inappropriate.  
Decoding Greedy. The model must return the same text, only translated. The model cannot be creative.  
Stopping criteria  
Be sure to include a stop sequence for this model. Otherwise, the model might continue to generate new sentences and translations, even when the instruction tells it not to. To stop the model after one sentence, add a period (.) as the stop sequence. Set the Max tokens parameter value to 200.  
Prompt text Paste the following prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click Generate to see the result."
420	Sample foundation model prompts for common tasks	"Translation
Sample: Translate text from English to Japanese
Scenario: Translate text that is written in English into Japanese.  
Model choice The granite-8b-japanese model can translate text from Japanese to English and from English to Japanese.  
AI guardrails Disable the AI guardrails feature. The feature is supported with English text only. It might flag as inappropriate content that is not inappropriate.  
Decoding Greedy. The model must return the same text, only translated. The model cannot be creative.  
Stopping criteria Increase the number of allowed tokens by changing the Max tokens parameter value to 500.  
Prompt text Paste the following prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click Generate to see the result."
421	Sample foundation model prompts for common tasks	"Translation
Sample: Translate text from French to English
Scenario: Translate text that is written in French into English.  
Model choice The granite-20b-multilingual model understands English, German, Spanish, French, and Portuguese. This sample prompts the model to translate text from French to English.  
AI guardrails Disable the AI guardrails feature. The feature is supported with English text only. It might flag as inappropriate content that is not inappropriate.  
Decoding Greedy. The model must return the same text, only translated. The model cannot be creative.  
Stopping criteria Set the Max tokens parameter value to 200.  
Prompt text Paste the following prompt text into the freeform prompt editor in Prompt Lab, select the model, set parameters, and then click Generate to see the result.  
   
Parent topic: Prompt Lab"
422	Detecting entities with a custom transformer model	"If you don't have a fixed set of terms or you cannot express entities that you like to detect as regular expressions, you can build a custom transformer model. The model is based on the pretrained Slate IBM Foundation model.  
When you use the pretrained model, you can build multi-lingual models. You don't have to have separate models for each language.  
You need sufficient training data to achieve high quality (2000 – 5000 per entity type). If you have GPUs available, use them for training.  
Note:  
Training transformer models is CPU and memory intensive. The predefined environments are not large enough to complete the training. Create a custom notebook environment with a larger amount of CPU and memory, and use that to run your notebook. If you have GPUs available, it's highly recommended to use them. See Creating your own environment template."
423	Detecting entities with a custom transformer model	"Input data format
The training data is represented as an array with multiple JSON objects. Each JSON object represents one training instance, and must have a text and a mentions field. The text field represents the training sentence text, and mentions is an array of JSON objects with the text, type, and location of each mention:  
Example:"
424	Detecting entities with a custom transformer model	"Training your model
The transformer algorithm is using the pretrained Slate model. The pretrained Slate model is only available in Runtime 23.1.  
To get the options available for configuring Transformer training, enter:  
Sample code"
425	Detecting entities with a custom transformer model	"Applying the model on new data
Apply the trained transformer workflow model on new data by using the run() method, as you would use on any of the existing pre-trained blocks.  
Code sample"
426	Detecting entities with a custom transformer model	"Storing and loading the model
The custom transformer model can be stored as any other model as described in Saving and loading custom models, using ibm_watson_studio_lib.  
To load the custom transformer model, extra steps are required:  
import zipfile model_zip = 'trained_workflow_file' model_folder = 'trained_workflow_folder' wslib.download_file('trained_workflow', file_name=model_zip) with zipfile.ZipFile(model_zip, 'r') as zip_ref: zip_ref.extractall(model_folder) trained_workflow = watson_nlp.load(model_folder)  
Ensure that you have an access token on the Access control page on the Manage tab of your project. Only project admins can create access tokens. The access token can have Viewer or Editor access permissions. Only editors can inject the token into a notebook.  
Add the project token to the notebook by clicking More > Insert project token from the notebook action bar and then run the cell.  
By running the inserted hidden code cell, a wslib object is created that you can use for functions in the ibm-watson-studio-lib library. For information on the available ibm-watson-studio-lib functions, see Using ibm-watson-studio-lib for Python.  
Download and extract the model to your local runtime environment:  
Load the model from the extracted folder:  
Parent topic: Creating your own models"
427	Scoring a time series model	After you save an AutoAI time series pipeline as a model, you can deploy and score the model to forecast new values.
428	Scoring a time series model	"Deploying a time series model
After you save a model to a project, follow the steps to deploy the model:  
Find the model in the project asset list. Promote the model to a deployment space. Promote payload data to the deployment space. From the deployment space, create a deployment."
429	Scoring a time series model	"Scoring considerations
To this point, deploying a time series model follows the same steps as deploying a classification or regression model. However, because of the way predictions are structured and generated in a time series model, your input must match your model structure. For example, the way you structure your payload depends on whether you are predicting a single result (univariate) or multiple results (multivariate).  
Note these high-level considerations:  
To get the first forecast window row or rows after the last row in your data, send an empty payload. To get the next value, send the result from the empty payload request as your next scoring request, and so on. You can send multiple rows as input, to build trends and predict the next value after a trend. If you have multiple prediction columns, you need to include a value for each of them in your scoring request"
430	Scoring a time series model	"Scoring an online deployment
If you create an online deployment, you can pass the payload data by using an input form or by submitting JSON code. This example shows how to structure the JSON code to generate predictions."
431	Scoring a time series model	"Scoring an online deployment
Predicting a single value
In the simplest case, given this sample data, you are trying to forecast the next step of value1 with a forecast window of 1, meaning each prediction will be a single step (row).  
You must pass a blank entry as the input data to request the first prediction, which is structured like this:  
The output that is returned predicts the next step in the model:  
The next input passes the result of the previous output to predict the next step:"
432	Scoring a time series model	"Scoring an online deployment
Predicting multiple values
In this case, you are predicting two targets, value1 and value2.  
The input data must still pass a blank entry to request the first prediction. The next input would be structured like this:"
433	Scoring a time series model	"Predicting based on new observations
If instead of predicting the next row based on the prior step you want to enter new observations, enter the input data like this for a univariate model:  
Enter new observations like this for a multivariate model:  
Where 2, 4, and 6 are observations for value1 and 1, 3, 5 are observations for value2."
434	Scoring a time series model	"Scoring a time series model with Supporting features
After you deploy your model, you can go to the page detailing your deployment to get prediction values. Choose one of the following ways to test your deployment:"
435	Scoring a time series model	"Scoring a time series model with Supporting features
Using existing input values
You can use existing input values in your data set to obtain prediction values. Click Predict to obtain a set of prediction values. The total number of prediction values in the output is defined by prediction horizon that you previously set during the experiment configuration stage."
436	Scoring a time series model	"Scoring a time series model with Supporting features
Using new input values
You can choose to populate the spreadsheet with new input values or use JSON code to obtain a prediction.  
To add input data to the New observations (optional) spreadsheet, select the Input tab and do one of the following:  
Add pre-existing .csv file containing new observations from your local directory by clicking Browse local files. Download the input file template by clicking Download CSV template, enter values, and upload the file. Use an existing data asset from your project by clicking Search in space. Manually enter input observations in the spreadsheet.  
You can also provide future values for Supporting features if you previously enabled your experiment to leverage these values during the experiment configuration stage. Make sure to add these values to the Future supporting features (optional) spreadsheet.  
To add input data using JSON code, select the Paste JSON tab and do one of the following:  
Add pre-existing JSON file containing new observations from your local directory by clicking Browse local files. Use an existing data asset from your project by clicking Search in space. Manually enter or paste JSON code into the editor.  
In this code sample, the prediction column is pollution, and the supporting features are temp and press."
437	Scoring a time series model	"Next steps
Saving an AutoAI generated notebook (Watson Machine Learning)  
Parent topic: Building a time series experiment"
438	Tracking assets in an AI use case	"Track machine learning models or prompt templates in AI use cases to capture details about them in factsheets. Use the information collected in the AI use case to monitor the progress of assets through the AI lifecycle, from request to production.  
Define an AI use case to identify a business problem and request a solution. A solution might be a predictive machine learning model or a generative AI prompt template. When an asset is developed, associate it with the use case to capture details about the asset in factsheets. As the asset moves through the AI lifecycle, from development to testing and then to production, the factsheets collect the data to support governance or compliance goals."
439	Tracking assets in an AI use case	"Creating approaches to compare ways to solve a problem
Each AI use case can contain at least one approach. An approach is one facet of the solution to the business problem represented by the AI use case. For example, you might create two approaches to compare by using different frameworks for predictive models to see which one performs best. Or, created approaches to track several prompt templates in a use case.  
Approaches also capture version information. The same version number is applied to all assets in an approach. If you have a stable version of an asset, you might maintain that version in an approach and create a new approach for the next round of iteration and experimentation.  
This use case includes three approaches for organizing three prompt templates for an insurance claims processing use case:"
440	Tracking assets in an AI use case	"Adding assets to a use case
You can track these assets in an AI use case:  
Prompt templates include the prompt input for a foundation model and variables that are defined to make the prompt reusable for generating new output. Machine learning models that are created by using a Watson Machine Learning tool such as AutoAI or SPSS Modeler. External models are models that are created in Jupyter Notebooks or models that are created by using a third-party machine learning provider."
441	Tracking assets in an AI use case	"Learn more
Use a sample project to try out watsonx.governance features with the tutorial Quick start: Evaluate and track a prompt template.  
Parent topic: Governing assets in AI use cases"
442	Compute resource options for Watson Pipelines	When you run a pipeline in a project, most resources are tracked and billed for the components included in the pipeline. For example, running a notebook in a pipeline consumes resources for that notebook according to the environment specified. One resource explicitly charged for a pipeline is execution time running a Bash script node.
443	Compute resource options for Watson Pipelines	"Default hardware configurations
You can specify a hardware configuration to use for executing a Bash script. The configuration you choose dictates the rate of CUH consumption billed to your Watson Studio plan."
444	Compute resource options for Watson Pipelines	"Default hardware configurations
Capacity units per hour for Bash script
Notes:  
The runtimes for Bash scripts stop automatically when processing is complete. Resource consumption is measured in seconds, with a minimum of 1 second. For example, if the execution time is 52.1 seconds, the charge is for 53 seconds."
445	Compute resource options for Watson Pipelines	"Compute usage in projects
Watson Pipelines consumes compute resources as CUH from the Watson Studio service.  
You can monitor the total monthly amount of CUH consumption for the Watson Machine Learning service on the Resource usage page on the Manage tab of your project."
446	Compute resource options for Watson Pipelines	"Learn more
Compute resource options for assets and deployments in spaces Monitoring account resource usage  
Parent topic: Choosing compute resources for tools"
447	Managing AI use cases	Manage the options for AI use cases. For example, create or manage an inventory for storing and reviewing AI use cases, customize report templates, or manage attachments.
448	Managing AI use cases	"Creating an inventory for AI use cases
You must have Admin rights to create and manage an inventory. For more information, see Collaboration roles.  
Click AI use cases from the navigation menu.  
Click the settings icon .  
Click Inventories to see the Inventories settings.  
Click New inventory.  
Assign a name, add an optional description, and associate a Cloud Object Storage instance."
449	Managing AI use cases	"Adding collaborators to an inventory
Inventories are meant to be collaborative so that multiple people that perform different roles can contribute to governance of key assets. To add collaborators to an inventory:  
Click AI use cases from the navigation menu. Click the settings icon . Click Inventories to see the Inventories settings. In the options menu for the inventory, click Set collaborators. Click Add collaborators to add collaborators individually, or by user group. Assign a role of Admin, Editor, or Viewer. Collaborators are added to the list for the inventory. You can remove or change the assigned access as needed."
450	Managing AI use cases	"Managing external models, report templates, and attachments
You can extend inventory management to include the ability to govern external models, customize report templates, and manage attachments for factsheets."
451	Managing AI use cases	"Enabling governance of external models
Enable governance for models that are created in notebooks or outside of Cloud Pak for Data. Track the results of model evaluations and model details in factsheets. You can store external models in the Platform assets catalog or choose an inventory.  
Click AI use cases from the navigation menu. Click the settings icon . Click General to see the General settings. Click the toggle to enable External model management. Select an inventory for tracking external models. Select an owner, then click Apply.  
Note: External models are listed under AI governance in the main navigation menu.  
MRG-Factsheet-API-Access"
452	Managing AI use cases	"Enabling governance of external models
Managing report templates
Manage report templates to customize the reports inventory users can create.  
For details, see Managing report templates."
453	Managing AI use cases	"Enabling governance of external models
Managing attachments
Create and manage attachment groups for AI use cases to provide the structure for users to attach supporting files to enrich a use case or a factsheet. For example, if you want every use case to include approval documents, you can create a group to define placeholders for those documents in each use case. Users can then upload the documents to those placeholder slots.  
For more information, see Managing attachments."
454	Managing AI use cases	"Learn more
Parent topic: Governing assets in AI use cases"
455	Managing attachments for AI use cases	Create attachment groups and define attachment slots for an AI use case or factsheet.
456	Managing attachments for AI use cases	"Adding attachment groups
If you have the manage_catalog role for an inventory, you can define attachment groups and manage attachment definitions for the AI use cases or factsheets in the inventory. Use an attachment group to organize a set of related attachment facts and render them together. Attachments can provide supporting information and extra details for a use case. Data scientists might want to attach visualizations from their model. Model requesters might want to attach a file of requirements to describe a business need."
457	Managing attachments for AI use cases	"Adding attachment groups
Supported attachment types
The following file types are supported for file attachments:  
Image file types:  
png jpg jpeg  
Other file types:  
html csv docx pdf zip text file Markdown"
458	Managing attachments for AI use cases	"Adding attachment groups
Creating an attachment group
Open the AI uses cases settings and click the Attachments tab. If you do not see this tab, you might have insufficient access. Choose whether to add an attachment group to an AI use case or to the factsheet template. Click Add group. Enter a name and an optional description. When you define the attachment group, an identifier is created from the name of the group. The identifier can be used for programmatic access to the group. Click Show identifier to view and edit the ID. Save your changes to create the attachment group."
459	Managing attachments for AI use cases	"Adding attachment groups
Adding attachment facts to a group
From an attachment group, add attachment fact definitions that specify how a user can add an attachment to a factsheet. Attachment definitions display as available slots in the attachment section for a use case or factsheet.  
Use the up and down arrow keys to reorder attachments in the list.  
In this example, an attachment group for approvals defines attachment facts for approvals from risk and compliance and from the model validator.  
When you save your attachment fact definitions, an attachment slot and description display on the use case or factsheet for attaching a file. A pin icon indicates an available attachment slot. Any user with at least edit access to the use case or factsheet can upload attachments.  
Parent topic: Managing AI use cases"
460	Creating an SPSS Modeler batch job with multiple data sources	"In an SPSS Modeler flow, it's common to have multiple import and export nodes, where multiple import nodes can be fetching data from one or more relational databases. Learn how to use Watson Machine Learning to create an SPSS Modeler batch job with multiple data sources from relational databases.  
Note:  
The examples use IBM Db2 and IBM Db2 Warehouse, referred to in examples as dashdb."
461	Creating an SPSS Modeler batch job with multiple data sources	"Connecting to multiple relational databases as input to a batch job
The number of import nodes in an SPSS Modeler flow can vary. You might use as many as 60 or 70. However, the number of distinct connections to databases in these cases are just a few, though the table names that are accessed through the connections vary. Rather than specifying the details for every table connection, the approach that is described here focuses on the database connections. Therefore, the batch jobs accept a list of data connections or references by node name that are mapped to connection names in the SPSS Modeler flow's import nodes.  
For example, assume that if a flow has 30 nodes, only three database connections are used to connect to 30 different tables. In this case, you submit three connections (C1, C2, and C3) to the batch job. C1, C2, and C3 are connection names in the import node of the flow and the node name in the input of the batch job.  
When a batch job runs, the data reference for a node is provided by mapping the node name with the connection name in the import node. This example illustrates the steps for creating the mapping.  
The following diagram shows the flow from model creation to job submission:  
Limitation: The connection reference for a node in a flow is overridden by the reference that is received from the batch job. However, the table name in the import or export node is not overridden."
462	Creating an SPSS Modeler batch job with multiple data sources	"Deployment scenario with example
In this example, an SPSS model is built by using 40 import nodes and a single output. The model has the following configuration:  
Connections to three databases: 1 Db2 Warehouse (dashDB) and 2 Db2. The import nodes are read from 40 tables (30 from Db2 Warehouse and 5 each from the Db2 databases). A single output table is written to a Db2 database."
463	Creating an SPSS Modeler batch job with multiple data sources	"Deployment scenario with example
Example
These steps demonstrate how to create the connections and identify the tables.  
{ ""name"": ""SPSS Drug Model"", ""label_column"": ""label"", ""type"": ""spss-modeler_18.1"", ""runtime"": { ""href"": ""/v4/runtimes/spss-modeler_18.1"" }, ""space"": { ""href"": ""/v4/spaces/<space_id>"" }, ""schemas"": { ""input"": [ { ""id"": ""dashdb_conn"", ""fields"": [] }, { ""id"": ""db2_conn1 "", ""fields"": [] } , { ""id"": ""db2_conn2"", ""fields"": [] } ], ""output"": [{ ""id"": ""db2_conn2 "",""fields"": [] }] } }  
Create a connection in your project.  
To run the SPSS Modeler flow, you start in your project and create a connection for each of the three databases your model connects to. You then configure each import node in the flow to point to a table in one of the connected databases.  
For this example, the database connections in the project are named dashdb_conn, db2_conn1, and db2_conn2.  
Configure Data Asset to import nodes in your SPSS Modeler flow with connections.  
Configure each node in the flow to reference one of the three connections you created (dashdb_conn, db2_conn1, and db2_conn2), then specify a table for each node.  
Note: You can change the name of the connection at the time of the job run. The table names that you select in the flow are referenced when the job runs. You can't overwrite or change them.  
Save the SPSS model to the Watson Machine Learning repository.  
For this example, it's helpful to provide the input and output schema when you are saving the model. It simplifies the process of identifying each input when you create and submit the batch job in the Watson Studio user interface. Connections that are referenced in the Data Asset nodes of the SPSS Modeler flow must be provided in the node name field of the input schema. To find the node name, double-click the Data Asset import node in your flow to open its properties:  
Note:"
464	Creating an SPSS Modeler batch job with multiple data sources	"Deployment scenario with example
Example
Note:  
SPSS models that are saved without schemas are still supported for jobs, but you must enter node name fields manually and provide the data asset when you submit the job.  
This code sample shows how to save the input schema when you save the model (Endpoint: POST /v4/models).  
Note: The number of fields in each of these connections doesn't matter. They’re not validated or used. What's important is the number of connections that are used.  
Create the batch deployment for the SPSS model.  
For SPSS models, the creation process of the batch deployment job is the same. You can submit the deployment request with the model that was created in the previous step.  
Submit SPSS batch jobs.  
You can submit a batch job from the Watson Studio user interface or by using the REST API. If the schema is saved with the model, the Watson Studio user interface makes it simple to accept input from the connections specified in the schema. Because you already created the data connections, you can select a connected data asset for each node name field that displays in the Watson Studio user interface as you define the job.  
The name of the connection that is created at the time of job submission can be different from the one used at the time of model creation. However, it must be assigned to the node name field."
465	Creating an SPSS Modeler batch job with multiple data sources	"Deployment scenario with example
Submitting a job when schema is not provided
If the schema isn't provided in the model metadata at the time the model is saved, you must enter the import node name manually. Further, you must select the data asset in the Watson Studio user interface for each connection. Connections that are referenced in the Data Asset import nodes of the SPSS Modeler flow must be provided in the node name field of the import/export data references.  
This code sample demonstrates how to specify the connections for a job that is submitted by using the REST API (Endpoint: /v4/deployment_jobs).  
Parent topic: Batch deployment input details by framework"
466	Language detection	"The Watson Natural Language Processing Language Detection identifies the language of input text.  
Block name lang-detect_izumo_multi_stock  
Supported languages  
The Language Detection block is able to detect the following languages:  
af, ar, bs, ca, cs, da, de, el, en, es, fi, fr, he, hi, hr, it, ja, ko, nb, nl, nn, pl, pt, ro, ru, sk, sr, sv, tr, zh_cn, zh_tw  
Capabilities  
Use this block to detect the language of an input text.  
Dependencies on other blocks  
None  
Code sample  
Output of the code sample:  
Parent topic: Watson Natural Language Processing task catalog"
467	Using Python functions to work with Cloud Object Storage	"To access and work with data that is in IBM Cloud Object Storage, you can use Python functions from a notebook.  
With your IBM Cloud Object Storage credentials, you can access and load data from IBM Cloud Object Storage to use in a notebook. This data can be any object of type file-like-object, for example, byte buffers or string buffers. The data that you upload can reside in a different IBM Cloud Object Storage bucket than the project's bucket.  
You can also upload data from a local system into IBM Cloud Object Storage from within a notebook. This data can be a compressed file or Pickle object.  
See Working With IBM Cloud Object Storage In Python for more information."
468	Using Python functions to work with Cloud Object Storage	"Learn more
Use ibm-watson-studio-lib for Python to interact with Watson Studio projects and project assets. The library also contains functions that simplify fetching files from IBM Cloud Object Storage.  
Control access to COS buckets  
Parent topic: Loading and accessing data in a notebook"
469	Using the API to work with foundation models	"You can inference and tune foundation models in IBM watsonx.ai programmatically by using the watsonx.ai API.  
For more information, see API reference documentation."
470	Using the API to work with foundation models	"Tasks that you can do by using the API
You can use the watsonx.ai API to do the following things:  
Get useful details about the available foundation models, such as a model's maximum context length Check how many tokens a model will calculate for your prompt input Inference foundation models Prompt-tune a foundation model"
471	Using the API to work with foundation models	"Prerequisites
To use the watsonx.ai API, you need an IBM Cloud Identity and Access Management (IAM) token. You also need to include the project ID or deployment space ID that is associated with the foundation model in the data payload for most of the API methods.  
For more information, see Authenticating for programmatic access to a project or space.  
You must specify the {model_id} for the foundation model that you want to use.  
You can use the List the available foundation models method to get the ID for a foundation model.  
For a list of the model IDs for the foundation models that are included with watsonx.ai, see Foundation model IDs for APIs.  
To submit inference requests to a tuned foundation model, you use the inference API endpoint that includes the deployment ID or serving name for the model deployment. The {model_id} is not required with this type of request because only one model is supported by the deployment.  
Specify the date on which you created and tested your code in the version parameter that is required with each request. For example, version=2024-03-14."
472	Using the API to work with foundation models	"Inferencing a foundation model
The method that you use to inference a foundation model differs depending on whether the foundation model is associated with a deployment.  
To inference a foundation model that is provided with watsonx.ai, use the Text generation method. To inference a tuned foundation model or to send an inference by using a prompt template, use the Deployments>Infer text method.  
You can prompt a foundation model by using one of the following text generation methods:  
Infer text: Waits to return the output that is generated by the foundation model all at one time. Infer text event stream: Returns the output as it is generated by the foundation model. This method is useful in conversational use cases, where you want a chatbot or virtual assistant to respond to a user in a fluid way that mimics a real conversation.  
To stop an inference request that is in progress, close the HTTP connection.  
Tip: If you want help with formatting an inference request in the API, you can submit the same request from the Prompt Lab. Click the View code () icon to see the cURL command that is used to submit your prompt input to the model. You can use the same formatting when you specify the prompt input and parameters for prompting in your API request."
473	Using the API to work with foundation models	"Inferencing a foundation model
Removing inappropriate language and personally identifiable information
When you prompt a foundation model by using the API, you can use the moderations field to apply filters to foundation model input and output. The following types of filters are available:  
HAP filter: Identifies and flags hate speech, abuse, and profanity.  
A threshold value from 0.0 to 1.0 controls the sensitivity of the HAP filter. When the threshold is lower, the classifier is likely to flag harmful content more often. However, the classifier might flag as harmful content that is not harmful. When the threshold is higher, the content that is flagged as harmful is more likely to be harmful, but other occurrences of harmful content can be missed.  
The AI guardrails feature in the Prompt Lab uses the same HAP filter with a threshold score of 0.5 that cannot be changed. For more information about AI guardrails, see Removing harmful content.  
PII filter: Uses a natural language processing AI model to identify and flag mentions of personally identifiable information (PII) information, such as phone numbers and email addresses.  
You can enable the PII filter to prevent personally identifiable information from being shown in content. The filter threshold value is 0.8 and cannot be changed currently.  
For the full list of entity types that are flagged, see Rule-based extraction for general entities.  
Sample notebooks are available that show you the steps to follow to inference a foundation model.  
Use watsonx to generate advertising Use watsonx and Google to extract entities"
474	Using the API to work with foundation models	"Prompt-tuning a foundation model
To prompt-tune a foundation model, you run an experiment that uses training data provided by you. The experiment is a machine learning process that shows the foundation model the output you expect the model to return for your prompt input. The tuning process is complex and involves a data asset, a training asset, and a deployment asset.  
The python library has methods and helper classes for tuning foundation models. For more information about the library, see Prompt tuning.  
Sample notebooks are also available that show you the steps to follow to use the Python library to tune a foundation model:  
Use watsonx to tune Meta llama-2-13b-chat model with Consumer Financial Protection Bureau document Tune a model to classify CFPB documents in watsonx  
Prompt-tuning a foundation model by using the REST API involves the following steps:  
Create a training data file to use for tuning the foundation model.  
For more information about the training data file requirements, see Data formats for tuning foundation models.  
Upload your training data file.  
You can choose to add the file by creating one of the following asset types:  
Connection asset  
Note: Only a Cloud Object Storage connection type is supported for prompt tuning training currently.  
Use the Watson Data API to define a connection to your data asset.  
You will use the connection ID and training data file details when you add the training_data_references section to the request.json file that you create in the next step.  
Data asset  
To create a data asset, use the Watson Data API to define a data asset.  
You will use the asset ID and training data file details when you add the training_data_references section to the request.json file that you create in the next step.  
For more information about the supported ways to reference a training data file, see Data references.  
Use the watsonx.ai API to create a training experiment.  
See create a training.  
You can specify parameters for the experiment in the TrainingResource payload. For more information about available parameters, see Parameters for tuning foundation models.  
For the task_id, specify one of the tasks that are listed as being supported for the foundation model in the response to the List the available foundation models method.  
Save the tuned model to the repository service to generate an asset_id that points to the tuned model."
475	Using the API to work with foundation models	"Prompt-tuning a foundation model
Save the tuned model to the repository service to generate an asset_id that points to the tuned model.  
To save the tuned model, use the Watson Machine Learning API to create a new model.  
Use the watsonx.ai API to create a deployment for the tuned model.  
See create a deployment  
To inference a tuned model, you must use the inference endpoint that includes the unique ID or serving name of the deployment that hosts the tuned model. For more information, see the inference methods in the Deployments section."
476	Using the API to work with foundation models	"Foundation model IDs for APIs
The following list shows the values to use in the {model_id} parameter when you reference a foundation model from the API.  
Alternatively, you can use the List the available foundation models method to get the {model_id} for a foundation model.  
codellama/codellama-34b-instruct-hf elyza/elyza-japanese-llama-2-7b-instruct google/flan-t5-xxl google/flan-ul2 ibm/granite-8b-japanese ibm/granite-13b-chat-v2 ibm/granite-13b-instruct-v2 ibm/granite-20b-multilingual core42/jais-13b-chat meta-llama/llama-3-8b-instruct meta-llama/llama-3-70b-instruct meta-llama/llama-2-13b-chat meta-llama/llama-2-70b-chat mnci/llama2-13b-dpo-v7 mistralai/mixtral-8x7b-instruct-v01 ibm-mistralai/mixtral-8x7b-instruct-v01-q bigscience/mt0-xxl bigcode/starcoder  
codellama-34b-instruct-hf  
elyza-japanese-llama-2-7b-instruct  
flan-t5-xxl-11b  
flan-ul2-20b  
granite-8b-japanese  
granite-13b-chat-v2  
granite-13b-instruct-v2  
granite-20b-multilingual  
jais-13b-chat  
llama-3-8b-instruct  
llama-3-70b-instruct  
llama-2-13b-chat  
llama-2-70b-chat  
llama2-13b-dpo-v7  
mixtral-8x7b-instruct-v01  
mixtral-8x7b-instruct-v01-q  
mt0-xxl-13b  
starcoder-15.5b  
Parent topic: Coding generative AI solutions"
477	Planning your notebooks and scripts experience	"To make a plan for using Jupyter notebooks and scripts, first understand the choices that you have, the implications of those choices, and how those choices affect the order of implementation tasks.  
You can perform most tasks related to notebooks and scripts with Editor or Admin role in an analytics project.  
Before you start working with notebooks and scripts, consider the following questions as most tasks need to be completed in a particular order:  
Which programming language do you want to work in? What will your notebooks be doing? What libraries do you want to work with? How can you use the notebook or script in IBM watsonx?  
To create a plan for using Jupyter notebooks or scripts, determine which of the following tasks you must complete."
478	Planning your notebooks and scripts experience	"Creating a project
You need to create a project before you can start working in notebooks.  
Projects You can create an empty project, one from file, or from URL. In this project:  
You can use the Jupyter Notebook and RStudio. Notebooks are assets in the project. Notebook collaboration is based on locking by user at the project level. R scripts and Shiny apps are not assets in the project. There is no collaboration on R scripts or Shiny apps."
479	Planning your notebooks and scripts experience	"Picking a programming language
You can choose to work in the following languages:"
480	Planning your notebooks and scripts experience	"Selecting a tool
In IBM watsonx, you can work with notebook and scripts in the following tool:"
481	Planning your notebooks and scripts experience	"Checking the library packages
When you open a notebook in a runtime environment, you have access to a large selection of preinstalled data science library packages. Many environments also include libraries provided by IBM at no extra charge, such as:  
The Watson Natural Language Processing library in Python environments Libraries to help you access project assets Libraries for time series or geo-spatial analysis in Spark environments  
For a list of the library packages and the versions included in an environment template, select the template on the Templates page from the Manage tab on the project's Environments page.  
If libraries are missing in a template, you can add them:"
482	Planning your notebooks and scripts experience	"Choosing a runtime environment
Choosing the compute environment for your notebook depends on the amount of data you want to process and the complexity of the data analysis processes.  
Watson Studio offers many default environment templates with different hardware sizes and software configurations to help you quickly get started, without having to create your own templates. These included templates are listed on the Templates page from the Manage tab on the project's Environments page. For more information about the included environments, see Environments.  
If the available templates don't suit your needs, you can create custom templates and determine the hardware size and software configuration. For details, see Customizing environment templates.  
Important: Make sure that the environment has enough memory to store the data that you load to the notebook. Oftentimes this means that the environment must have significantly more memory than the total size of the data loaded to the notebook because some data frameworks, like pandas, can hold multiple copies of the data in memory."
483	Planning your notebooks and scripts experience	"Working with data
To work with data in a notebook, you need to:  
Add the data to your project, which turns the data into a project asset. See Adding data to a project for the different methods for adding data to a project. Use generated code that loads data from the asset to a data structure in your notebook. For a list of the supported data types, see Data load support. Write your own code to load data if the data source isn't added as a project asset or support for adding generated code isn't available for the project asset."
484	Planning your notebooks and scripts experience	"Managing the notebooks and scripts lifecycle
After you create and test a notebook in your tool, you can:  
Publish it to a catalog so that other catalog members can use the notebook in their projects. See Publishing assets from a project into a catalog. Share a read-only copy outside of Watson Studio so that people who aren't collaborators in your projects can see and use it. See Sharing notebooks with a URL. Publish it to a GitHub repository. See Publishing notebooks on GitHub. Publish it as a gist. See Publishing a notebook as a gist.  
R scripts and Shiny apps can't be published or shared using functionality in a project."
485	Planning your notebooks and scripts experience	"Uses for notebooks and scripts after creation
The options for a notebook that is created and ready to use in IBM watsonx include:  
Running it as a job in a project. See Creating and managing jobs in a project.  
Running it as part of a Watson Pipeline. See Configuring pipeline nodes.  
To ensure that a notebook can be run as a job or in a pipeline:  
Ensure that no cells require interactive input by a user. Ensure that the notebook logs enough detailed information to enable understanding the progress and any failures by looking at the log. Use environment variables in the code to access configurations if a notebook or script requires them, for example the input data file or the number of training runs.  
Using the Watson Machine Learning Python client to build, train and then deploy your models. See Watson Machine Learning Python client samples and examples.  
Using the Watson Machine Learning REST API to build, train and then deploy your models.  
R scripts and Shiny apps can only be created and used in the RStudio IDE in IBM watsonx. You can't create jobs for R scripts or R Shiny deployments.  
Parent topic: Notebooks and scripts"
486	Connecting to the aggregator (Party)	Each party follows these steps to connect to a started aggregator.
487	Connecting to the aggregator (Party)	"from ibm_watson_machine_learning import APIClient wml_credentials = { ""url"": ""https://us-south.ml.cloud.ibm.com"", ""apikey"": ""<API KEY>"" } wml_client = APIClient(wml_credentials) wml_client.set.default_project(""XXX-XXX-XXX-XXX-XXX"") party_metadata = { wml_client.remote_training_systems.ConfigurationMetaNames.DATA_HANDLER: { ""name"": ""MnistSklearnDataHandler"", ""path"": ""example.mnist_sklearn_data_handler"", ""info"": { ""npz_file"":""./example_data/example_data.npz"" } party = wml_client.remote_training_systems.create_party(""XXX-XXX-XXX-XXX-XXX"", party_metadata) party.monitor_logs() party.run(aggregator_id=""XXX-XXX-XXX-XXX-XXX"", asynchronous=False) # ----------------------------------------------------------------------------------------- # (C) Copyright IBM Corp. 2023. # https://opensource.org/licenses/BSD-3-Clause # ----------------------------------------------------------------------------------------- # # # Script to create a conda environment and install ibm-watson-machine-learning with # the dependencies required for Federated Learning on MacOS. # The name of the conda environment to be created is passed as the first argument. # # Note: This script requires miniforge to be installed for conda. # usage="". install_fl_rt22.2_macos.sh conda_env_name"" arch=$(uname -m) os=$(uname -s) if (($# < 1)) then echo $usage exit fi ENAME=$1 conda create -y -n ${ENAME} python=3.10 conda activate ${ENAME} pip install ibm-watson-machine-learning if [ ""$os"" == ""Darwin"" -a ""$arch"" == ""arm64"" ] then conda install -y -c apple tensorflow-deps fi python - <<EOF import pkg_resources import platform import subprocess package = 'ibm-watson-machine-learning' extra = 'fl-rt22.2-py3.10' extra_ = extra.replace('.','-') extra_s = '; extra == ""{}""' remove = None add = [] if platform.system() == ""Darwin"" and platform.processor() == ""arm"": remove = 'tensorflow' add = ['tensorflow-macos==2.9.2'] pkgs = pkg_resources.working_set.by_key[package].requires(extras=[extra]) pkgs = ["
488	Connecting to the aggregator (Party)	"and platform.processor() == ""arm"": remove = 'tensorflow' add = ['tensorflow-macos==2.9.2'] pkgs = pkg_resources.working_set.by_key[package].requires(extras=[extra]) pkgs = [ p.__str__().removesuffix(extra_s.format(extra)).removesuffix(extra_s.format(extra_)) for p in pkgs if ( extra in p.__str__() or extra_ in p.__str__() ) and ( not remove or remove not in p.__str__() )] print( ""Installing standard packages for {}[{}]:{}"".format(package,extra,pkgs) ) print( ""Installing additional packages:{}"".format(add) ) cmd = [ 'pip', 'install'] + add + pkgs subprocess.run( cmd ) EOF python3 rts_<RTS Name>_<RTS ID>.py"
489	Connecting to the aggregator (Party)	"Open the project and click the Federated Learning experiment.  
Click View setup information and click the download icon to download the party connector script.  
Each party must configure the party connector script and provide valid credentials to run the script. This is what a sample completed party connector script looks like:  
Parameters:  
api_key: Your IAM API key. To create a new API key, go to the IBM Cloud website, and click Create an IBM Cloud Pak for Data API key under Manage > Access(IAM) > API keys.  
Optional: If you're reusing a script from a different project, you can copy the updated project_id, aggregator_id and experiment_id from the setup information window and copy them into the script.  
Install Watson Machine Learning with the latest Federated Learning package if you have not yet done so:  
If you are using M-series on a Mac, install the latest package with the following script:  
Otherwise install with the following command:pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ 'ibm-watson-machine-learning[fl-rt22.2-py3.10]'  
When your configuration is complete and you save the party connector script, enter this command in a command line to run the script:"
490	Connecting to the aggregator (Party)	"More resources
Federated Learning library functions  
Parent topic: Creating a Federated Learning experiment"
491	Accessing project assets with ibm-watson-studio-lib	The ibm-watson-studio-lib library for Python and R contains a set of functions that help you to interact with IBM Watson Studio projects and project assets. You can think of the library as a programmatical interface to a project. Using the ibm-watson-studio-lib library, you can access project metadata and assets, including files and connections. The library also contains functions that simplify fetching files associated with the project.
492	Accessing project assets with ibm-watson-studio-lib	"Next steps
Start using ibm-watson-studio-lib in new notebooks:  
ibm-watson-studio-lib for Python ibm-watson-studio-lib for R  
Parent topic: Loading and accessing data in a notebook"
493	Data sources for scoring batch deployments	"You can supply input data for a batch deployment job in several ways, including directly uploading a file or providing a link to database tables. The types of allowable input data vary according to the type of deployment job that you are creating.  
For supported input types by framework, refer to Batch deployment input details by framework.  
Input data can be supplied to a batch job as inline data or data reference."
494	Data sources for scoring batch deployments	"Available input types for batch deployments by framework and asset type
Inline data description
Inline type input data for batch processing is specified in the batch deployment job's payload. For example, you can pass a CSV file as the deployment input in the UI or as a value for the scoring.input_data parameter in a notebook. When the batch deployment job is completed, the output is written to the corresponding job's scoring.predictions metadata parameter."
495	Data sources for scoring batch deployments	"Available input types for batch deployments by framework and asset type
Data reference description
Input and output data of type data reference that is used for batch processing can be stored:  
In a remote data source, like a Cloud Object Storage bucket or an SQL or no-SQL database. As a local or managed data asset in a deployment space.  
Details for data references include:  
Data source reference type depends on the asset type. Refer to Data source reference types section in Adding data assets to a deployment space.  
For data_asset type, the reference to input data must be specified as a /v2/assets href in the input_data_references.location.href parameter in the deployment job's payload. The data asset that is specified is a reference to a local or a connected data asset. Also, if the batch deployment job's output data must be persisted in a remote data source, the references to output data must be specified as a /v2/assets href in output_data_reference.location.href parameter in the deployment job's payload.  
Any input and output data_asset references must be in the same space ID as the batch deployment.  
If the batch deployment job's output data must be persisted in a deployment space as a local asset, output_data_reference.location.name must be specified. When the batch deployment job is completed successfully, the asset with the specified name is created in the space.  
Output data can contain information on where in a remote database the data asset is located. In this situation, you can specify whether to append the batch output to the table or truncate the table and update the output data. Use the output_data_references.location.write_mode parameter to specify the values truncate or append.  
Specifying truncate as value truncates the table and inserts the batch output data. Specifying append as value appends the batch output data to the remote database table. write_mode is applicable only for the output_data_references parameter. write_mode is applicable only for remote database-related data assets. This parameter is not applicable for a local data asset or a Cloud Object Storage based data asset."
496	Data sources for scoring batch deployments	"Structuring the input data
How you structure the input data, also known as the payload, for the batch job depends on the framework for the asset you are deploying.  
A .csv input file or other structured data formats must be formatted to match the schema of the asset. List the column names (fields) in the first row and values to be scored in subsequent rows. For example, see the following code snippet:  
A JSON input file must provide the same information on fields and values, by using this format:  
For example:"
497	Data sources for scoring batch deployments	"Structuring the input data
Preparing a payload that matches the schema of an existing model
Refer to this sample code:  
Parent topic: Creating a batch deployment"
498	Using text embeddings to ground prompts in factual information	"Use embedding models to create text embeddings that capture the meaning of a sentence or passage to help with retrieval-augmented generation tasks.  
Retrieval-augmented generation (RAG) is a technique in which a foundation model is augmented with knowledge from external sources to generate text. You can use text embeddings to find higher-quality relevant information to include with the prompt to help the foundation model answer factually.  
The following diagram illustrates the retrieval-augmented generation pattern with embedding support.  
The retrieval-augmented generation pattern with embedding support involves the following steps:  
Convert your content into text embeddings and store them in a vector data store. Use the same embedding model to convert the user input into text embeddings. Search in your knowledge base for content that is related to the user's input. Pull the most relevant search results into your prompt as context and add an instruction, such as “Answer the following question by using only information from the following passages.” Send the combined prompt text to the model to generate output."
499	Using text embeddings to ground prompts in factual information	"Learn more
Supported embedding models Text embeddings API reference Retreival-augmented generation Techniques for overcoming context length limitations  
Parent topic: Coding generative AI solutions"
500	Applying homomorphic encryption for security and privacy	"Federated learning supports homomorphic encryption as an added measure of security for federated training data. Homomorphic encryption is a form of public key cryptography that enables computations on the encrypted data without first decrypting it, meaning the data can be used in modeling without exposing it to the risk of discovery.  
With homomorphic encryption, the results of the computations remain in encrypted form and when decrypted, result in an output that is the same as the output produced with computations performed on unencrypted data. It uses a public key for encryption and a private key for decryption."
501	Applying homomorphic encryption for security and privacy	"How it works with Federated Learning
Homomorphic encryption is an optional encryption method to add additional security and privacy to a Federated Learning experiment. When homomorphic encryption is applied in a Federated Learning experiment, the parties send their homomorphically encrypted model updates to the aggregator. The aggregator does not have the private key and can only see the homomorphically encrypted model updates. For example, the aggregator cannot reverse engineer the model updates to discover information on the parties' training data. The aggregator fuses the model updates in their encrypted form which results in an encrypted aggregated model. Then the aggregator sends the encrypted aggregated model to the participating parties who can use their private key for decryption and continue with the next round of training. Only the participating parties can decrypt model data."
502	Applying homomorphic encryption for security and privacy	"Supported frameworks and fusion methods
Fully Homomorphic Encryption (FHE) supports the simple average fusion method for these model frameworks:  
Tensorflow Pytorch Scikit-learn classification Scikit-learn regression"
503	Applying homomorphic encryption for security and privacy	"Before you begin
To get started with using homomorphic encryption, ensure that your experiment meets the following requirements:  
The hardware spec must be minimum small. Depending on the level of encryption that you apply, you might need a larger hardware spec to accommodate the resource consumption caused by more powerful data encryption. See the encryption level table in Configuring the aggregator.- The software spec is fl-rt22.2-py3.10.  
FHE is supported in Python client version 1.0.263 or later. All parties must use the same Python client version."
504	Applying homomorphic encryption for security and privacy	"Before you begin
Requirements for the parties
Each party must:  
Run on a Linux x86 system. Configure with a root certificate that identifies a certificate authority that is uniform to all parties. Configure an RSA public and private key pair with attributes described in the following table. Configure with a certificate of the party issued by the certificate authority. The RSA public key must be included in the party's certificate.  
Note: You can also choose to use self-signed certificates.  
Homomorphic public and private encryption keys are generated and distributed automatically and securely among the parties for each experiment. Only the parties participating in an experiment have access to the private key generated for the experiment. To support the automatic generation and distribution mechanism, the parties must be configured with the certificates and RSA keys specified previously."
505	Applying homomorphic encryption for security and privacy	"Configuring the aggregator (admin)
As you create a Federated Learning experiment, follow these steps:  
In the Configure tab, toggle ""Enable homomorphic encryption"". Choose small or above for Hardware specification. Depending on the level of encryption that you apply, you might need a larger hardware spec to accommodate the resource consumption for homomorphic encryption. Ensure that you upload an unencrypted initial model when selecting the model file for Model specification. Select ""Simple average (encrypted)"" for Fusion method. Click Next. Check Show advanced in the Define hyperparameters tab. Select the level of encryption in Encryption level. Higher encryption levels increase security and precision, and require higher resource consumption (e.g. computation, memory, network bandwidth). The default is encryption level 1. See the following table for description of the encryption levels:  
Security is the strength of the encryption, typically measured by the number of operations that an attacker must perform to break the encryption. Precision is the precision of the encryption system's outcomes. Higher precision levels reduce loss of accuracy of the model due to the encryption."
506	Applying homomorphic encryption for security and privacy	"Connecting to the aggregator (party)
The following steps only show the configuration needed for homomorphic encryption. For a step-by-step tutorial of using homomorphic encryption in Federated Learning, see FHE sample.  
To see how to create a general end-to-end party connector script, see Connect to the aggregator (party).  
party_config = { ""local_training"": { ""info"": { ""crypto"": { ""key_manager"": { ""key_mgr_info"": { ""distribution"": { ""ca_cert_file_path"": ""path of the root certificate file identifying the certificate authority"", ""my_cert_file_path"": ""path of the certificate file of the party issued by the certificate authority"", ""asym_key_file_path"": ""path of the RSA key file of the party"" } } } } } } } }  
Install the Python client with FHE with the following command: pip install 'ibm_watson_machine_learning[fl-rt23.1-py3.10,fl-crypto]'  
Configure the party as follows:  
Run the party connector script after configuration."
507	Applying homomorphic encryption for security and privacy	"Connecting to the aggregator (party)
Additional resources
Parent topic: Federated Learning"
508	Syntax analysis	"The Watson Natural Language Processing Syntax block encapsulates syntax analysis functionality.  
Block names  
syntax_izumo_<language>_stock syntax_izumo_<language>_stock-dp (Runtime 23.1 only)  
Supported languages  
The Syntax analysis block is available for the following languages. For a list of the language codes and the corresponding language, see Language codes.  
Language codes to use for model syntax_izumo_<language>_stock: af, ar, bs, ca, cs, da, de, el, en, es, fi, fr, he, hi, hr, it, ja, ko, nb, nl, nn, pl, pt, ro, ru, sk, sr, sv, tr, zh_cn, zh_tw  
Language codes to use for model syntax_izumo_<language>_stock-dp: af, ar, bs, ca, cs, da, de, el, en, es, fi, fr, he, hi, hr, it, ja, ko, nb, nl, nn, pl, pt, ro, ru, sk, sr, sv, tr, zh  
Capabilities  
Use this block to perform tasks like sentence detection, tokenization, part-of-speech tagging, lemmatization and dependency parsing in different languages. For most tasks, you will likely only need sentence detection, tokenization, and part-of-speech tagging. For these use cases use the syntax_model_xx_stock model. If you want to run dependency parsing in Runtime 23.1, use the syntax_model_xx_stock-dp model.  
The analysis for Part-of-speech (POS) tagging and dependencies follows the Universal Parts of Speech tagset (Universal POS tags) and the Universal Dependencies v2 tagset (Universal Dependency Relations).  
The following table shows you the capabilities of each task based on the same example and the outcome to the parse.  
Dependencies on other blocks  
None  
Code sample  
Output of the code sample:  
Parent topic: Watson Natural Language Processing task catalog"
509	Libraries and scripts for notebooks	"Watson Studio includes a large selection of preinstalled open source libraries for Python and R in its runtime environments. You can also use preinstalled IBM libraries or install custom libraries.  
Watson Studio includes the following libraries and the appropriate runtime environments with which you can expand your data analysis:  
The Watson Natural Language Processing library in Python and Python with GPU runtime environments. The gespatio-temporal library in Spark with Python runtime environments The Xskipper library for data skipping uses the open source in Spark with Python runtime environments Parquet encryption in Spark with Python runtime environments The tspy library for time series analysis in Spark with Python runtime environments"
510	Libraries and scripts for notebooks	"Listing installed libraries
Many of your favorite open source libraries are pre-installed on runtime environments. All you have to do is import them. See Import preinstalled libraries and packages.  
If a library is not preinstalled, you can add it:  
Through the notebook  
Some libraries require a kernel restart after a version change. If you need to work with a library version that isn't pre-installed in the environment in which you start the notebook, and you install this library version through the notebook, the notebook only runs successfully after you restart the kernel.  
Note that when you run the notebook non-interactively, for example as a notebook job, it fails because the kernel can't be restarted.  
By adding a customization to the environment in which the notebook runs  
If you add a library with a particular version to the software customization, the library is preinstalled at the time the environment is started and no kernel restart is required. Also, if the notebook is run in a scheduled job, it won't fail.  
The advantage of adding an environment customization is that the library is preinstalled each time the environment runtime is started. Libraries that you add through a notebook are persisted for the lifetime of the runtime only. If the runtime is stopped and later restarted, those libraries are not installed.  
To see the list of installed libraries in your environment runtime:  
From the Manage tab, on the project's Environments page, select the environment template.  
From a notebook, run the appropriate command from a notebook cell:  
Python: !pip list --isolated R: installed.packages()  
Optional: Add custom libraries and packages to the environment. See customizing an environment template."
511	Libraries and scripts for notebooks	"Importing an installed library
To import an installed library into your notebook, run the appropriate command from a notebook cell with the library name:  
Python: import library_name R: library(library_name)  
Alternatively, you can write a script that includes multiple classes and methods and then import the script into your notebook."
512	Libraries and scripts for notebooks	"Learn more
Installing custom libraries Importing scripts into a notebook Watson Natural Language Processing library gespatio-temporal library for location analysis Xskipper library for data skipping Parquet encryption tspy library for time series analysis  
Parent topic: Notebooks and scripts"
513	Setting up an AI use case	Create an AI use case to define a business problem and track the related AI assets through their lifecycle. View details about governed assets or generate reports to help meet governance and compliance goals.
514	Setting up an AI use case	"Creating AI use cases in an inventory
An inventory presents a view of all the AI use cases that you can access that are assigned to that inventory. Use multiple inventories to manage groups of AI use cases. For example, you might create an inventory for governing prompt templates and another for governing machine learning assets. Add collaborators to inventories so they can view or contribute to AI uses cases."
515	Setting up an AI use case	"Creating AI use cases in an inventory
Before you begin
Enable watsonx.governance and provision Watson OpenScale. You must have access to an existing inventory or have sufficient access to create a new inventory.  
For details on watsonx.governance roles and managing access for governance, see Collaboration roles for governance. If you do not have sufficient access to create or contribute to an inventory, contact your administrator."
516	Setting up an AI use case	"Viewing AI use cases
Click AI use cases from the navigation menu to view all existing AI use cases you can access, or click Request a model with an AI use case from the home page. From the primary view, you can search for a specific use case or filter the view to focus on certain use cases. For example, filter the view by Inventory to view all the AI use cases in a particular inventory.  
Click the name of an AI use case to open it and view the details on these tabs:  
Overview shows the essential details for the use case. Lifecycle shows the assets that are tracked in the use case, which is organized by the phases of the AI lifecycle. Access lists collaborators for the use case and assigned roles.  
Click the name of an asset to view the associated factsheet."
517	Setting up an AI use case	"Generating a report from a use case
You can generate reports from use cases or factsheets to share or preserve records. By default, the reports generate these default reports:  
Basic report contains the set of facts visible on the Overview and Lifecycle tabs. Full report contains all facts about the use case and the models, prompt templates, and deployments it contains.  
The inventory admin can customize reports to include custom branding or to change the fields included in reports. For details, see Customizing report templates. To create a report:  
Open a use case in an inventory. Click the Export report icon to generate a PDF record of the use case. Choose a format option and export the report."
518	Setting up an AI use case	"Creating an AI use case
Click AI use cases from the navigation menu. Click New AI use case. Enter a name and choose an inventory for the use case. If you do not have access to an inventory, you must create one before you can define a use case. See Managing an inventory for AI use cases for details. Complete optional fields as needed:"
519	Setting up an AI use case	"Creating an AI use case
Use case status details
Update the status field to provide users of the use case an immediate reflection of the current state."
520	Setting up an AI use case	"Adding collaborators to an AI use case
Add collaborators so they can view or contribute to the AI use case.  
From the Access tab of the AI use case, click Add members. Search for a member by name or email address. Assign an access level and click Add. For details on permissions, see Collaboration roles for governance."
521	Setting up an AI use case	"Next steps
After you create an AI use case, use it to track assets. Depending on your governance strategy, your next step might be to:  
Send a link to the use case to a reviewer for approval. Send a link to a data scientist to create the requested asset. Add an asset for tracking in the use case."
522	Setting up an AI use case	"Learn more
Use a sample project to try out watsonx.governance features with the tutorial Quick start: Evaluate and track a prompt template.  
Parent topic: Governing assets in AI use cases"
523	Sentiment classification	"The Watson Natural Language Processing Sentiment classification models classify the sentiment of the input text.  
Supported languages  
Sentiment classification is available for the following languages. For a list of the language codes and the corresponding language, see Language codes.  
ar, cs, da, de, en, es, fi, fr, he, hi, it, ja, ko, nb, nl, nn, pl, pt, ro, ru, sk, sv, tr, zh-cn"
524	Sentiment classification	"Sentiment
The sentiment of text can be positive, negative or neutral.  
The sentiment model computes the sentiment for each sentence in the input document. The aggregated sentiment for the entire document is also calculated using the sentiment transformer workflow in Runtime 23.1.  
The classifications returned contain a probability. The sentiment score varies from -1 to 1. A score greater than 0 denotes a positive sentiment, a score less than 0 a negative sentiment, and a score of 0 a neutral sentiment."
525	Sentiment classification	"Sentiment
Sentence sentiment workflows in runtime 23.1
Workflow names  
sentiment-aggregated_transformer-workflow_multilingual_slate.153m.distilled sentiment-aggregated_transformer-workflow_multilingual_slate.153m.distilled-cpu  
The sentiment-aggregated_transformer-workflow_multilingual_slate.153m.distilled workflow can be used on both CPUs and GPUs.  
The sentiment-aggregated_transformer-workflow_multilingual_slate.153m.distilled-cpu workflow is optimized for CPU-based runtimes.  
Code sample using the sentiment-aggregated_transformer-workflow_multilingual_slate.153m.distilled workflow  
Output of the code sample"
526	Sentiment classification	"Targets sentiment extraction
Targets sentiment extraction extracts sentiments expressed in text and identifies the targets of those sentiments.  
It can handle multiple targets with different sentiment in one sentence as opposed to the sentiment block described above.  
For example, given the input sentence The served food was delicious, yet the service was slow., the Targets sentiment block identifies that there is a positive sentiment expressed in the target ""food"", and a negative sentiment expressed in ""service"".  
The model has been fine-tuned on English data only. Although you can use the model on the other languages listed under Supported languages, the results might vary."
527	Sentiment classification	"Targets sentiment extraction
Targets sentiment workflows in Runtime 23.1
Workflow names  
targets-sentiment_transformer-workflow_multilingual_slate.153m.distilled targets-sentiment_transformer-workflow_multilingual_slate.153m.distilled-cpu  
The targets-sentiment_transformer-workflow_multilingual_slate.153m.distilled workflow can be used on both CPUs and GPUs.  
The targets-sentiment_transformer-workflow_multilingual_slate.153m.distilled-cpu workflow is optimized for CPU-based runtimes.  
Code sample for the targets-sentiment_transformer-workflow_multilingual_slate.153m.distilled workflow  
Output of the code sample:  
Parent topic: Watson Natural Language Processing task catalog"
528	Managing the AI Lifecycle with ModelOps	Use the ModelOps tools to manage your AI assets from development to production.
529	Managing the AI Lifecycle with ModelOps	"ModelOps explained
MLOps synchronizes cadences between the application and model pipelines. It builds on these practices:  
DevOps for bringing a machine learning model from creation through training, to deployment, and to production. ModelOps for managing the lifecycle of a traditional machine learning model, including evaluation and retraining.  
MLOps includes not just the routine deployment of machine learning models but also the continuous retraining, automated updating, and synchronized development and deployment of more complex machine learning models. Explore these resources for more details on developing an MLOps strategy:  
watsonx.ai use case describes how to develop and deploy machine learning models and generative AI solutions. watsonx.governance use case describes how to monitor, maintain, automate, and govern machine learning and generative AI models in production."
530	Managing the AI Lifecycle with ModelOps	"ModelOps tools
Depending on the platform you are using and the services you have enabled, you can design your ModelOps process using a combination of tools to help you manage assets.  
Pipelines for automating the end-to-end flow of a machine learning model through the AI lifecycle. AI Governance for creating a centralized repository of factsheets that track the lifecycle of a model, including request, building, deployment, and evaluation of aAI assets The cpdctl command-line interface tool for managing and automating your machine learning assets that are hosted on Cloud Pak for Data as a Service by using the cpdctl command-line interface tool. Use automatic configuration from IBM Cloud to easily connect with the cpdctl API commands."
531	Managing the AI Lifecycle with ModelOps	"ModelOps tools
Managing access with deployment spaces
Use deployment spaces to organize and manage access to assets as they move through the AI lifecycle. For example, you can manage access with deployment spaces in the following ways:  
Create a deployment space and assign it to Development as the deployment stage. If you are governing assets, deployments in this type of space display in the Develop stage of a use case. Assign access to the data scientists to create the assets or DevOps users to create deployments. Create a deployment space and assign it to Testing as the deployment stage. If you are governing assets, deployments in this type of space display in the Validate stage of a use case. Assign access to the model validators to test the deployments. Create a deployment space and assign it to Production as the deployment stage. If you are governing assets, deployments in this type of space display in the Operate stage of a use case. Limit access to this space to ModelOps users who manage the assets that are deployed to a production environment."
532	Managing the AI Lifecycle with ModelOps	"ModelOps tools
Sharing data assets in a feature store
If your organization uses IBM Knowledge Catalog, a catalog can serve as a feature store. Data assets that are stored in a feature store contain features that you can use in machine learning models and later share them across your organization. Data assets include metadata about where they are used in models. You can control access for Catalogs at the catalog and the data asset level."
533	Managing the AI Lifecycle with ModelOps	"ModelOps tools
Automating ModelOps by using Pipelines
The IBM Watson Pipelines editor provides a graphical interface for orchestrating an end-to-end flow of assets from creation through deployment. Assemble and configure a pipeline to create, train, deploy, and update machine learning models and Python scripts. Make your ModelOps process simpler and repeatable."
534	Managing the AI Lifecycle with ModelOps	"ModelOps tools
Tracking models with AI Factsheets
AI Factsheets provides the capabilities for you to track data science models across the organization and store the details in a catalog. View at a glance which models are in production and which need development or validation. Use the governance features to establish processes to manage the communication flow from data scientists to ModelOps administrators.  
Note: A model inventory tracks only the models that you explicitly track by associating them with model use cases. You can control which models to track for an organization without tracking samples and other models that are not significant to the organization."
535	Managing the AI Lifecycle with ModelOps	"ModelOps tools
Evaluating model deployments
Use Watson OpenScale to analyze your AI with trust and transparency and understand how your AI models are involved in decision making. Detect and mitigate bias and drift. Increase the quality and accuracy of your predictions. Explain transactions and perform what-if analysis."
536	Managing the AI Lifecycle with ModelOps	"ModelOps tools
Automate managing assets and lifecycle
You can automate the AI Lifecycle in a notebook by using the Watson Machine Learning Python client.  
This sample notebook demonstrates how to:  
Download an externally trained scikit-learn model with data set Persist an external model in the Watson Machine Learning repository Deploy a model for online scoring by using the client library Score sample records by using the client library Update a previously persisted model Redeploy a model in-place Scale a deployment  
Alternatively, you can use IBM Cloud Pak for Data Command-Line Interface (IBM cpdctl) to manage configuration settings and automate an end-to-end flow. This end-to-end flow includes training a model, saving it, creating a deployment space, and deploying the model. For more information, see IBM Cloud Pak for Data Command-Line Interface documentation."
537	Managing the AI Lifecycle with ModelOps	"Typical ModelOps scenario
A typical ModelOps scenario in Cloud Pak for Data might be:  
Organize and curate data assets Train a model by using AutoAI Save and deploy the model Track the model in a use case so that all collaborators can track the progress of the model through the lifecycle and make sure that it complies with organizational standards. Evaluate the deployment for bias Update the deployment with a better-performing model Monitor deployments and jobs across the organization"
538	Managing the AI Lifecycle with ModelOps	"More resources
ModelOps Wikipedia article Read the ModelOps blog post. IBM Blog post on ModelOps about using ModelOps to drive value from your AI investment. See how IBM is addressing ModelOps.  
Parent topic: Deploying and managing assets"
539	Troubleshooting Federated Learning experiments	The following are some of the limitations and troubleshoot methods that apply to Federated learning experiments.
540	Troubleshooting Federated Learning experiments	"Limitations
If you choose to enable homomorphic encryption, intermediate models can no longer be saved. However, the final model of the training experiment can be saved and used normally. The aggregator will not be able to decrypt the model updates and the intermediate global models. The aggregator can see only the final global model."
541	Troubleshooting Federated Learning experiments	"Troubleshooting
If a quorum error occurs during homomorphic keys distribution, restart the experiment. Changing the name of a Federated Learning experiment causes it to lose its current name, including earlier runs. If this is not intended, create a new experiment with the new name. The default software spec is used by every run. If your model type becomes outdated and not compatible with future software specs, re-running an older experiment might run into issues. As Remote Training Systems are meant to run on different servers, you might encounter unexpected behavior when you run with multiple parties that are based in the same server."
542	Troubleshooting Federated Learning experiments	"Federated Learning known issues
Known issues for Federated Learning Known issues for Federated Learning  
Parent topic: IBM Federated Learning"
543	Functions used in Watson Pipelines's Expression Builder	"Use these functions in Pipelines code editors, for example, to define a user variable or build an advanced condition.  
The Experssion Builder uses the categories for coding functions:  
Conversion functions Standard functions Accessing advanced global objects"
544	Functions used in Watson Pipelines's Expression Builder	"Conversion functions
Converts a single data element format to another."
545	Functions used in Watson Pipelines's Expression Builder	"Conversion functions
Table for basic data type conversion
For example, to cast a value to type double:  
When you cast double to int | uint, result rounds toward zero and errors if result is out of range."
546	Functions used in Watson Pipelines's Expression Builder	"Standard functions
Functions that are unique to IBM Watson Pipelines."
547	Functions used in Watson Pipelines's Expression Builder	"Standard functions
sub
Replaces substrings of a string that matches the given regular expression that starts at position offset.  
returns: the string with substrings updated.  
Returns 'aaaREcccRE'."
548	Functions used in Watson Pipelines's Expression Builder	"Standard functions
format
Formats a string or timestamp according to a format specifier and returns the resulting string.  
format as a method of strings  
returns: the string that contains the formatted input values.  
format as a method of timestamps  
returns: the formatted timestamp in string format.  
Returns the string 'number=1, text=str'.  
Returns the string '2020/07/24'."
549	Functions used in Watson Pipelines's Expression Builder	"Standard functions
now
Returns the current timestamp.  
returns: the current timestamp."
550	Functions used in Watson Pipelines's Expression Builder	"Standard functions
parseTimestamp
Returns the current timestamp in string format.  
returns: the current timestamp to a string of type string.  
Returns '2020-07-24T09:07:29.000-00:00'."
551	Functions used in Watson Pipelines's Expression Builder	"Standard functions
min
Returns minimum value in list.  
returns: the minimum value of the list.  
Returns the integer 1."
552	Functions used in Watson Pipelines's Expression Builder	"Standard functions
max
Returns maximum value in list.  
returns: the maximum value of the list.  
Returns the integer 3."
553	Functions used in Watson Pipelines's Expression Builder	"Standard functions
argmin
Returns index of minimum value in list.  
returns: the index of the minimum value of the list.  
Returns the integer 0."
554	Functions used in Watson Pipelines's Expression Builder	"Standard functions
argmax
Returns index of maximum value in list.  
returns: the index of the maximum value of the list.  
Returns the integer 2."
555	Functions used in Watson Pipelines's Expression Builder	"Standard functions
sum
Returns the sum of values in list.  
returns: the index of the maximum value of the list.  
Returns the integer 2."
556	Functions used in Watson Pipelines's Expression Builder	"Standard functions
base64.decode
Decodes base64-encoded string to bytes. This function returns an error if the string input is not base64-encoded.  
returns: the decoded base64-encoded string in byte format.  
Returns 'hello' in bytes."
557	Functions used in Watson Pipelines's Expression Builder	"Standard functions
base64.encode
Encodes bytes to a base64-encoded string.  
returns: the encoded base64-encoded string of the original byte value.  
Returns 'aGVsbG8=' in bytes."
558	Functions used in Watson Pipelines's Expression Builder	"Standard functions
charAt
Returns the character at the given position. If the position is negative, or greater than the length of the string, the function produces an error.  
returns: the character of the specified position in integer format.  
Returns the character 'o'."
559	Functions used in Watson Pipelines's Expression Builder	"Standard functions
indexOf
Returns the integer index of the first occurrence of the search string. If the search string is not found the function returns -1.  
returns: the index of the first character occurrence after the offset.  
Returns the integer 7."
560	Functions used in Watson Pipelines's Expression Builder	"Standard functions
lowerAscii
Returns a new string with ASCII characters turned to lowercase.  
returns: the new lowercase string.  
Returns the string 'tacocat'."
561	Functions used in Watson Pipelines's Expression Builder	"Standard functions
replace
Returns a new string based on the target, which replaces the occurrences of a search string with a replacement string if present. The function accepts an optional limit on the number of substring replacements to be made.  
returns: the new string with occurrences of a search string replaced.  
Returns the string 'wello wello'."
562	Functions used in Watson Pipelines's Expression Builder	"Standard functions
split
Returns a list of strings that are split from the input by the separator. The function accepts an optional argument that specifies a limit on the number of substrings that are produced by the split.  
returns: the split string as a string list.  
Returns the string list ['hello', 'hello', 'hello']."
563	Functions used in Watson Pipelines's Expression Builder	"Standard functions
substring
Returns the substring given a numeric range corresponding to character positions. Optionally you might omit the trailing range for a substring from a character position until the end of a string.  
returns: the substring at the specified index of the string.  
Returns the string 'cat'."
564	Functions used in Watson Pipelines's Expression Builder	"Standard functions
trim
Returns a new string, which removes the leading and trailing white space in the target string. The trim function uses the Unicode definition of white space, which does not include the zero-width spaces.  
returns: the new string with white spaces removed.  
Returns the string 'trim'."
565	Functions used in Watson Pipelines's Expression Builder	"Standard functions
upperAscii
Returns a new string where all ASCII characters are upper-cased.  
returns: the new string with all characters turned to uppercase.  
Returns the string 'TACOCAT'."
566	Functions used in Watson Pipelines's Expression Builder	"Standard functions
size
Returns the length of the string, bytes, list, or map.  
returns: the length of the string, bytes, list, or map array.  
Returns the integer 5.  
Returns the integer 5.  
Returns the integer 3.  
Returns the integer 1."
567	Functions used in Watson Pipelines's Expression Builder	"Standard functions
contains
Tests whether the string operand contains the substring.  
returns: a Boolean value of whether the substring exists in the string operand.  
Returns true."
568	Functions used in Watson Pipelines's Expression Builder	"Standard functions
endsWith
Tests whether the string operand ends with the specified suffix.  
returns: a Boolean value of whether the string ends with specified suffix in the string operand.  
Returns true."
569	Functions used in Watson Pipelines's Expression Builder	"Standard functions
startsWith
Tests whether the string operand starts with the prefix argument.  
returns: a Boolean value of whether the string begins with specified prefix in the string operand.  
Returns true."
570	Functions used in Watson Pipelines's Expression Builder	"Standard functions
matches
Tests whether the string operand matches regular expression.  
returns: a Boolean value of whether the string matches the specified regular expression.  
Returns true."
571	Functions used in Watson Pipelines's Expression Builder	"Standard functions
getDate
Get the day of the month from the date with time zone (default Coordinated Universal Time), one-based indexing.  
returns: the day of the month with one-based indexing.  
Returns 24."
572	Functions used in Watson Pipelines's Expression Builder	"Standard functions
getDayOfMonth
Get the day of the month from the date with time zone (default Coordinated Universal Time), zero-based indexing.  
returns: the day of the month with zero-based indexing.  
Returns 23."
573	Functions used in Watson Pipelines's Expression Builder	"Standard functions
getDayOfWeek
Get day of the week from the date with time zone (default Coordinated Universal Time), zero-based indexing, zero for Sunday.  
returns: the day of the week with zero-based indexing.  
Returns 5."
574	Functions used in Watson Pipelines's Expression Builder	"Standard functions
getDayOfYear
Get the day of the year from the date with time zone (default Coordinated Universal Time), zero-based indexing.  
returns: the day of the year with zero-based indexing.  
Returns 205."
575	Functions used in Watson Pipelines's Expression Builder	"Standard functions
getFullYear
Get the year from the date with time zone (default Coordinated Universal Time).  
returns: the year from the date.  
Returns 2020."
576	Functions used in Watson Pipelines's Expression Builder	"Standard functions
getMonth
Get the month from the date with time zone, 0-11.  
returns: the month from the date.  
Returns 6."
577	Functions used in Watson Pipelines's Expression Builder	"Standard functions
getHours
Get hours from the date with time zone, 0-23.  
returns: the hour from the date.  
Returns 9."
578	Functions used in Watson Pipelines's Expression Builder	"Standard functions
getMinutes
Get minutes from the date with time zone, 0-59.  
returns: the minute from the date.  
Returns 7."
579	Functions used in Watson Pipelines's Expression Builder	"Standard functions
getSeconds
Get seconds from the date with time zone, 0-59.  
returns: the second from the date.  
Returns 29."
580	Functions used in Watson Pipelines's Expression Builder	"Standard functions
getMilliseconds
Get milliseconds from the date with time zone, 0-999.  
returns: the millisecond from the date.  
Returns 21."
581	Functions used in Watson Pipelines's Expression Builder	"Access to advanced global objects
Get node outputs, user variables, and pipeline parameters by using the following Pipelines code."
582	Functions used in Watson Pipelines's Expression Builder	"Access to advanced global objects
Get user variable
Gets the most up-to-date value of a user variable."
583	Functions used in Watson Pipelines's Expression Builder	"Access to advanced global objects
Get parameters
Gets the flow parameters."
584	Functions used in Watson Pipelines's Expression Builder	"Access to advanced global objects
Get parameter sets
Gets the flow parameter sets."
585	Functions used in Watson Pipelines's Expression Builder	"Access to advanced global objects
Get task results
Get a pipeline task's resulting output and other metrics from a pipeline task after it completes its run."
586	Functions used in Watson Pipelines's Expression Builder	"Access to advanced global objects
Get pipeline context objects
Gets values that are evaluated in the context of a pipeline that is run in a scope (project, space, catalog)."
587	Functions used in Watson Pipelines's Expression Builder	"Access to advanced global objects
Get error status
If the exception handler is triggered, an error object is created and becomes accessible only within the exception handler.  
Parent topic: Adding conditions to a Pipelines"
588	Managing deployment jobs	"A job is a way of running a batch deployment, script, or notebook in Watson Machine Learning. You can choose to run a job manually or on a schedule that you specify. After you create one or more jobs, you can view and manage them from the Jobs tab of your deployment space.  
From the Jobs tab of your space, you can:  
See the list of the jobs in your space View the details of each job. You can change the schedule settings of a job and pick a different environment template. Monitor job runs Delete jobs  
See the following sections for various aspects of job management:  
Creating a job for a batch deployment Viewing jobs in a space Managing job metadata retention"
589	Managing deployment jobs	"Creating a job for a batch deployment
Important: You must have an existing batch deployment to create a batch job.  
To learn how to create a job for a batch deployment, see Creating jobs in a batch deployment."
590	Managing deployment jobs	"Viewing jobs in a space
You can view all of the jobs that exist for your deployment space from the Jobs page. You can also delete a job.  
To view the details of a specific job, click the job. From the job's details page, you can do the following:  
View the runs for that job and the status of each run. If a run failed, you can select the run and view the log tail or download the entire log file to help you troubleshoot the run. A failed run might be related to a temporary connection or environment problem. Try running the job again. If the job still fails, you can send the log to Customer Support.  
When a job is running, a progress indicator on the information page displays information about relative progress of the run. You can use the progress indicator to monitor a long run.  
Edit schedule settings or pick another environment template.  
Run the job manually by clicking the run icon from the job action bar. You must deselect the schedule to run the job manually."
591	Managing deployment jobs	"Managing job metadata retention
The Watson Machine Learning plan that is associated with your IBM Cloud account sets limits on the number of running and stored deployments that you can create. If you exceed your limit, you cannot create new deployments until you delete existing deployments or upgrade your plan. For more information, see Watson Machine Learning plans."
592	Managing deployment jobs	"Managing job metadata retention
Managing metadata retention and deletion programmatically
If you are managing a job programmatically by using the Python client or REST API, you can retrieve metadata from the deployment endpoint by using the GET method during the 30 days.  
To keep the metadata for more or less than 30 days, change the query parameter from the default of retention=30 for the POST method to override the default and preserve the metadata.  
Note:  
Changing the value to retention=-1 cancels the auto-delete and preserves the metadata.  
To delete a job programmatically, specify the query parameter hard_delete=true for the Watson Machine Learning DELETE method to completely remove the job metadata.  
The following example shows how to use DELETE method:"
593	Managing deployment jobs	"Learn from samples
Refer to Machine learning samples and examples for links to sample notebooks that demonstrate creating batch deployments and jobs by using the Watson Machine Learning REST API and Watson Machine Learning Python client library.  
Parent topic: Managing predictive deployments"
594	Adding data assets to a deployment space	"Learn about various ways of adding and promoting data assets to a space and data types that are used in deployments.  
Data can be:  
A data file such as a .csv file A connection to data that is located in a repository such as a database. Connected data that is located in a storage bucket. For more information, see Using data from the Cloud Object Storage service.  
Notes:  
For definitions of data-related terms, refer to Asset types and properties.  
You can add data to a space in one of these ways:  
Add data and connections to space by using UI Promote a data source, such as a file or a connection from an associated project Save a data asset to a space programmatically Import a space or a project, including data assets, into an existing space.  
Data added to a space is managed in a similar way to data added to a Watson Studio project. For example:  
Adding data to a space creates a new copy of the asset and its attachments within the space, maintaining a reference back to the project asset. If an asset such as a data connection requires access credentials, they persist and are the same whether you are accessing the data from a project or from a space. Just like with data connection in a project, you can edit data connection details from the space. Data assets are stored in a space in the same way that they are stored in a project. They use the same file structure for the space as the structure used for the project."
595	Adding data assets to a deployment space	"Adding data and connections to space by using UI
To add data or connections to space by using UI:  
From the Assets tab of your deployment space, click Import assets. Choose between adding a connected data asset, local file, or connection to a data source:  
If you want to add a connected data asset, select Connected data. Choose a connection and click Import. If you want to add a local file, select Local file > Data asset. Upload your file and click Done. If you want to add a connection to a data source, select Data access > Connection. Choose a connection and click Import.  
The data asset displays in the space and is available for use as an input data source in a deployment job.  
Note: Some types of connections allow for using your personal platform credentials. If you add a connection or connected data that uses your personal platform credentials, tick the Use my platform login credentials checkbox."
596	Adding data assets to a deployment space	"Adding data to space programmatically
If you are using APIs to create, update, or delete Watson Machine Learning assets, make sure that you are using only Watson Machine Learning API calls.  
For an example of how to add assets programmatically, refer to this sample notebook: Use SPSS and batch deployment with Db2 to predict customer churn"
597	Adding data assets to a deployment space	"Adding data to space programmatically
Data source reference types in Watson Machine Learning
Data source reference types are referenced in Watson Machine Learning requests to represent input data and results locations. Use data_asset and connection_asset for these types of data sources:  
Cloud Object Storage Db2 Database data  
Notes:  
For Decision Optimization, the reference type is url.  
For more information, see:  
Watson Machine Learning REST API"
598	Adding data assets to a deployment space	"Using data from the Cloud Object Storage service
Cloud Object Storage service can be used with deployment jobs through a connected data asset or a connection asset. To use data from the Cloud Object Storage service:  
Create a connection to IBM Cloud Object Storage by adding a Connection to your project or space and selecting Cloud Object Storage (infrastructure) or Cloud Object Storage as the connector. Provide the secret key, access key, and login URL.  
Note:  
When you are creating a connection to Cloud Object Storage or Cloud Object Storage (Infrastructure), you must specify both access_key and secret_key. If access_key and secret_key are not specified, downloading the data from that connection doesn't work in a batch deployment job. For reference, see IBM Cloud Object Storage connection and IBM Cloud Object Storage (infrastructure) connection.  
Add input and output files to the deployment space as connected data by using the Cloud Object Storage connection that you created.  
Parent topic: Assets in deployment spaces"
599	Switching the platform for a space	"You can switch the platform for some spaces between the Cloud Pak for Data as a Service and the watsonx platform. When you switch a space to another platform, you can use the tools that are specific to that platform.  
For example, you might switch an existing space from Cloud Pak for Data as a Service to watsonx to consolidate your collaborative work on one platform. See Comparison between watsonx and Cloud Pak for Data as a Service.  
Note: You cannot promote Prompt Lab assets created with foundation model inferencing to a space.  
Requirements Restrictions What happens when you switch a space Switch the platform for a space"
600	Switching the platform for a space	"Requirements
You can switch a space from one platform to the other if you have the required accounts and permissions.  
Watson Studio  
Watson Machine Learning"
601	Switching the platform for a space	"Restrictions
To switch a space from Cloud Pak for Data as a Service to watsonx, all the assets in the space must be supported by both platforms.  
Spaces that contain any of the following asset types, but no other types of assets, are eligible to switch from Cloud Pak for Data as a Service to watsonx:  
Connected data asset Connection Data asset from a file Deployment Jupyter notebook Model Python function Script  
You can’t switch a space that contains assets that are specific to Cloud Pak for Data as a Service. If you add any assets that you created with services other than Watson Studio and Watson Machine Learning to a project, you can't switch that space to watsonx. Although Pipelines assets are supported in both Cloud Pak for Data as a Service and watsonx spaces, you can't switch a space that contains pipeline assets because pipelines can reference unsupported assets.  
For more information about asset types, see Asset types and properties."
602	Switching the platform for a space	"What happens when you switch the platform for a space
Switching a space between platforms has the following effects:"
603	Switching the platform for a space	"Switch the platform for a space
You can switch the platform for a space from within the space on the original platform. You can switch between Cloud Pak for Data as a Service and watsonx.  
To switch the platform for a space:  
From the space you want to switch, open the Manage tab, select the General page, and in the Controls section, click Switch platform. If you don't see a Switch platform button or the button is not active, you can't switch the space. Select the destination platform and click Switch platform."
604	Switching the platform for a space	"Learn more
Comparison between watsonx and Cloud Pak for Data as a Service Asset types and properties  
Parent topic: Deployment spaces"
605	Coding and running a notebook	"After you created a notebook to use in the notebook editor, you need to add libraries, code, and data so you can do your analysis.  
To develop analytic applications in a notebook, follow these general steps:  
Open the notebook in edit mode: click the edit icon (). If the notebook is locked, you might be able to unlock and edit it.  
If the notebook is marked as being untrusted, tell the Jupyter service to trust your notebook content and allow executing all cells by:  
Clicking Not Trusted in the upper right corner of the notebook. Clicking Trust to execute all cells.  
Determine if the environment template that is associated with the notebook has the correct hardware size for the anticipated analysis processing throughput.  
Check the size of the environment by clicking the View notebook info icon () from the notebook toolbar and selecting the Environments page.  
If you need to change the environment, select another one from the list or, if none fits your needs, create your own environment template. See Creating emvironment template.  
If you create an environment template, you can add your own libraries to the template that are preinstalled at the time the environment is started. See Customize your environment for Python and R.  
Import preinstalled libraries. See Libraries and scripts for notebooks.  
Load and access data. You can access data from project assets by running code that is generated for you when you select the asset or programmatically by using preinstalled library functions. See Load and access data.  
Prepare and analyze the data with the appropriate methods:  
Build Watson Machine Learning models Build Decision Optimization models Use Watson Natural Language Processing Use SPSS predictive analytics algorithms Use geospatial location analysis methods Use Data skipping for Spark SQL Apply Parquet encryption Use Time series analysis methods  
If necessary, schedule the notebook to run at a regular time. See Schedule a notebook.  
Monitor the status of your job runs from the project's Jobs page. Click your job to open the job's details page to view the runs for your job and the status of each run. If a run failed, you can select the run and view the log tail or download the entire log file to troubleshoot the run.  
When you're not actively working on the notebook, click File > Stop Kernel to stop the notebook kernel and free up resources."
606	Coding and running a notebook	"When you're not actively working on the notebook, click File > Stop Kernel to stop the notebook kernel and free up resources.  
Stop the active runtime (and unnecessary capacity unit consumption) if no other notebook kernels are active under Tool runtimes on the Environments page on the Manage tab of your project.  
Video disclaimer: Some minor steps and graphical elements in these videos may differ from your deployment.  
Watch this short video to see how to create a Jupyter notebook and custom environment.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Watch this short video to see how to run basic SQL queries on Db2 Warehouse data in a Python notebook.  
This video provides a visual method to learn the concepts and tasks in this documentation."
607	Coding and running a notebook	"Learn more
Markdown cheatsheet  
Notebook interface  
Stop active runtimes  
Load and access data  
Schedule a notebook  
Parent topic: Jupyter Notebook editor"
608	Techniques for avoiding undesirable output	"Every foundation model has the potential to generate output that includes incorrect or even harmful content. Understand the types of undesirable output that can be generated, the reasons for the undesirable output, and steps that you can take to reduce the risk of harm.  
The foundation models that are available in IBM watsonx.ai can generate output that contains hallucinations, personal information, hate speech, abuse, profanity, and bias. The following techniques can help reduce the risk, but do not guarantee that generated output will be free of undesirable content.  
Find techniques to help you avoid the following types of undesirable content in foundation model output:  
Hallucinations Personal information Hate speech, abuse, and profanity Bias"
609	Techniques for avoiding undesirable output	"Hallucinations
When a foundation model generates off-topic, repetitive, or incorrect content or fabricates details, that behavior is sometimes called hallucination.  
Off-topic hallucinations can happen because of pseudo-randomness in the decoding of the generated output. In the best cases, that randomness can result in wonderfully creative output. But randomness can also result in nonsense output that is not useful.  
The model might return hallucinations in the form of fabricated details when it is prompted to generate text, but is not given enough related text to draw upon. If you include correct details in the prompt, for example, the model is less likely to hallucinate and make up details."
610	Techniques for avoiding undesirable output	"Hallucinations
Techniques for avoiding hallucinations
To avoid hallucinations, test one or more of these techniques:  
Choose a model with pretraining and fine-tuning that matches your domain and the task you are doing.  
Provide context in your prompt.  
If you instruct a foundation model to generate text on a subject that is not common in its pretraining data and you don't add information about the subject to the prompt, the model is more likely to hallucinate.  
Specify conservative values for the Min tokens and Max tokens parameters and specify one or more stop sequences.  
When you specify a high value for the Min tokens parameter, you can force the model to generate a longer response than the model would naturally return for a prompt. The model is more likely to hallucinate as it adds words to the output to reach the required limit.  
For use cases that don't require much creativity in the generated output, use greedy decoding. If you prefer to use sampling decoding, be sure to specify conservative values for the temperature, top-p, and top-k parameters.  
To reduce repetitive text in the generated output, try increasing the repetition penalty parameter.  
If you see repetitive text in the generated output when you use greedy decoding, and if some creativity is acceptable for your use case, then try using sampling decoding instead. Be sure to set moderately low values for the temperature, top-p, and top-k parameters.  
In your prompt, instruct the model what to do when it has no confident or high-probability answer.  
For example, in a question-answering scenario, you can include the instruction: If the answer is not in the article, say “I don't know”."
611	Techniques for avoiding undesirable output	"Personal information
A foundation model's vocabulary is formed from words in its pretraining data. If pretraining data includes web pages that are scraped from the internet, the model's vocabulary might contain the following types of information:  
Names of article authors Contact information from company websites Personal information from questions and comments that are posted in open community forums  
If you use a foundation model to generate text for part of an advertising email, the generated content might include contact information for another company!  
If you ask a foundation model to write a paper with citations, the model might include references that look legitimate but aren't. It might even attribute those made-up references to real authors from the correct field. A foundation model is likely to generate imitation citations, correct in form but not grounded in facts, because the models are good at stringing together words (including names) that have a high probability of appearing together. The fact that the model lends the output a touch of legitimacy, by including the names of real people as authors in citations, makes this form of hallucination compelling and believable. It also makes this form of hallucination dangerous. People can get into trouble if they believe that the citations are real. Not to mention the harm that can come to people who are listed as authors of works they did not write."
612	Techniques for avoiding undesirable output	"Personal information
Techniques for excluding personal information
To exclude personal information, try these techniques:  
In your prompt, instruct the model to refrain from mentioning names, contact details, or personal information.  
For example, when you prompt a model to generate an advertising email, instruct the model to include your company name and phone number. Also, instruct the model to “include no other company or personal information”.  
From the watsonx.ai API, you can enable the PII filter in the moderations field when you submit an inference request. For more information, see API reference documentation.  
In your larger application, pipeline, or solution, post-process the content that is generated by the foundation model to find and remove personal information."
613	Techniques for avoiding undesirable output	"Hate speech, abuse, and profanity
As with personal information, when pretraining data includes hateful or abusive terms or profanity, a foundation model that is trained on that data has those problematic terms in its vocabulary. If inappropriate language is in the model's vocabulary, the foundation model might generate text that includes undesirable content.  
When you use foundation models to generate content for your business, you must do the following things:  
Recognize that this kind of output is always possible. Take steps to reduce the likelihood of triggering the model to produce this kind of harmful output. Build human review and verification processes into your solutions."
614	Techniques for avoiding undesirable output	"Hate speech, abuse, and profanity
Techniques for reducing the risk of hate speech, abuse, and profanity
To avoid hate speech, abuse, and profanity, test one or more of these techniques:  
In the Prompt Lab, set the AI guardrails switch to On. When this feature is enabled, any sentence in the input prompt or generated output that contains harmful language is replaced with a message that says that potentially harmful text was removed.  
Do not include hate speech, abuse, or profanity in your prompt to prevent the model from responding in kind.  
In your prompt, instruct the model to use clean language.  
For example, depending on the tone you need for the output, instruct the model to use “formal”, “professional”, “PG”, or “friendly” language.  
From the watsonx.ai API, you can enable the HAP filter in the moderations field when you submit an inference request. For more information, see API reference documentation.  
In your larger application, pipeline, or solution, post-process the content that is generated by the foundation model to remove undesirable content."
615	Techniques for avoiding undesirable output	"Reducing the risk of bias in model output
During pretraining, a foundation model learns the statistical probability that certain words follow other words based on how those words appear in the training data. Any bias in the training data is trained into the model.  
For example, if the training data more frequently refers to doctors as men and nurses as women, that bias is likely to be reflected in the statistical relationships between those words in the model. As a result, the model is likely to generate output that more frequently refers to doctors as men and nurses as women. Sometimes, people believe that algorithms can be more fair and unbiased than humans because the algorithms are “just using math to decide”. But bias in training data is reflected in content that is generated by foundation models that are trained on that data."
616	Techniques for avoiding undesirable output	"Reducing the risk of bias in model output
Techniques for reducing bias
It is difficult to debias output that is generated by a foundation model that was pretrained on biased data. However, you might improve results by including content in your prompt to counter bias that might apply to your use case.  
For example, instead of instructing a model to “list heart attack symptoms”, you might instruct the model to “list heart attack symptoms, including symptoms that are common for men and symptoms that are common for women”.  
Parent topic: Prompt tips"
617	Managing hardware specifications for deployments	When you deploy certain assets in Watson Machine Learning, you can choose the type, size, and power of the hardware configuration that matches your computing needs.
618	Managing hardware specifications for deployments	"Creating hardware specifications for deployments
You can create hardware specifications for your deployments in one of the following ways:"
619	Managing hardware specifications for deployments	"Deployment types that require hardware specifications
Selecting a hardware specification is available for all batch deployment types. For online deployments, you can select a specific hardware specification if you're deploying:  
Python Functions Tensorflow models Models with custom software specifications"
620	Managing hardware specifications for deployments	"Hardware configurations available for deploying assets
XS: 1x4 = 1 vCPU and 4 GB RAM S: 2x8 = 2 vCPU and 8 GB RAM M: 4x16 = 4 vCPU and 16 GB RAM L: 8x32 = 8 vCPU and 32 GB RAM XL: 16x64 = 16 vCPU and 64 GB RAM  
You can use the XS configuration to deploy:  
Python functions Python scripts R scripts Models based on custom libraries and custom images  
For Decision Optimization deployments, you can use these hardware specifications:  
S M L XL"
621	Managing hardware specifications for deployments	"Hardware specifications for GPU inferencing
Use the following predefined hardware specifications for GPU inferencing:"
622	Managing hardware specifications for deployments	"Learn more
Monitoring account resource usage  
Parent topic: Managing predictive deployments"
623	Authenticating for programmatic access	"To use Watson Machine Learning with the Python client library or the REST API, you must authenticate to secure your work. Learn about the different ways to authenticate and how to apply them to the service of your choosing.  
You use IBM Cloud® Identity and Access Management (IAM) to make authenticated requests to public IBM Watson™ services. With IAM access policies, you can assign access to more than one resource from a single key. In addition, a user, service ID, and service instance can hold multiple API keys."
624	Authenticating for programmatic access	"Security overview
Refer to the section that describes your security needs.  
Authentication credentials Python client Rest API"
625	Authenticating for programmatic access	"Authentication credentials
These terms relate to the security requirements described in this topic.  
API keys allow you to easily authenticate when you are using the Python client or APIs and can be used across multiple services. API Keys are considered confidential because they are used to grant access. Treat all API keys as you would a password because anyone with your API key can access your service. An IAM token is an authentication token that is required to access IBM Cloud services. You can generate a token by using your API key in the token request. For details on using IAM tokens, refer to Authenticating to Watson Machine Learning API.  
To authenticate to a service through its API, pass your credentials to the API. You can pass either a bearer token in an authorization header or an API key."
626	Authenticating for programmatic access	"Authentication credentials
Generating an API key
To generate an API key from your IBM Cloud user account, go to Manage access and users - API Keys and create or select an API key for your user account.  
You can also generate and rotate API keys from Profile and settings > User API key. For more information, see Managing the user API key."
627	Authenticating for programmatic access	"Authentication credentials
Authenticate with an IAM token
IAM tokens are temporary security credentials that are valid for 60 minutes. When a token expires, you generate a new one. Tokens can be useful for temporary access to resources. For more information, see Generating an IBM Cloud IAM token by using an API key."
628	Authenticating for programmatic access	"Authentication credentials
Getting a service-level token
You can also authenticate with a service-level token. To generate a service-level token:  
Refer to the IBM Cloud instructions for creating a Service ID. Generate an API key for that Service ID. Open the space where you plan to keep your deployable assets. On the Access control tab, add the Service ID and assign an access role of Admin or Editor.  
You can use the service-level token with your API scoring requests."
629	Authenticating for programmatic access	"Interfaces
Python client REST API"
630	Authenticating for programmatic access	"Interfaces
Python client
Refer to: Watson Machine Learning Python client  
To create an instance of the Watson Machine Learning Python client object, you need to pass your credentials to Watson Machine Learning API client.  
Note:  
Even though you do not explicitly provide an instance_id, it will be picked up from the associated space or project for billing purposes. For details on plans and billing for Watson Machine Learning services, refer to Watson Machine Learning plans and runtime usage.  
Refer to sample notebooks for examples of how to authenticate and then score a model by using the Python client."
631	Authenticating for programmatic access	"Interfaces
REST API
Refer to: Watson Machine Learning REST API  
To use the Watson Machine Learning REST API, you must obtain an IBM Cloud Identity and Access Management (IAM) token. In this example, you would supply your API key in place of the example key.  
The obtained IAM token needs to be prefixed with the word Bearer, and passed in the Authorization header for API calls.  
Parent topic: Managing predictive deployments"
632	SPSS predictive analytics survival analysis algorithms in notebooks	You can use non-parametric distribution fitting, parametric distribution fitting, or parametric regression modeling SPSS predictive analytics algorithms in notebooks.
633	SPSS predictive analytics survival analysis algorithms in notebooks	"Non-Parametric Distribution Fitting
Survival analysis analyzes data where the outcome variable is the time until the occurrence of an event of interest. The distribution of the event times is typically described by a survival function.  
Non-parametric Distribution Fitting (NPDF) provides an estimate of the survival function without making any assumptions concerning the distribution of the data. NPDF includes Kaplan-Meier estimation, life tables, and specialized extension algorithms to support left censored, interval censored, and recurrent event data.  
Python example code:"
634	SPSS predictive analytics survival analysis algorithms in notebooks	"Parametric Distribution Fitting
Survival analysis analyzes data where the outcome variable is the time until the occurrence of an event of interest. The distribution of the event times is typically described by a survival function.  
Parametric Distribution Fitting (PDF) provides an estimate of the survival function by comparing the functions for several known distributions (exponential, Weibull, log-normal, and log-logistic) to determine which, if any, describes the data best. In addition, the distributions for two or more groups of cases can be compared.  
Python excample code:"
635	SPSS predictive analytics survival analysis algorithms in notebooks	"Parametric regression modeling
Parametric regression modeling (PRM) is a survival analysis technique that incorporates the effects of covariates on the survival times. PRM includes two model types: accelerated failure time and frailty. Accelerated failure time models assume that the relationship of the logarithm of survival time and the covariates is linear. Frailty, or random effects, models are useful for analyzing recurrent events, correlated survival data, or when observations are clustered into groups.  
PRM automatically selects the survival time distribution (exponential, Weibull, log-normal, or log-logistic) that best describes the survival times.  
Python example code:  
Parent topic: SPSS predictive analytics algorithms"
636	Scaling a deployment	"When you create an online deployment for a model or function from a deployment space or programmatically, a single copy of the asset is deployed by default. To increase scalability and availability, you can increase the number of copies (replicas) by editing the configuration of the deployment. More copies allow for a larger volume of scoring requests.  
Deployments can be scaled in the following ways:  
Update the configuration for a deployment in a deployment space. Programmatically, using the Watson Machine Learning Python client library, or the Watson Machine Learning REST APIs."
637	Scaling a deployment	"Changing the number of copies of an online deployment from a space
Click the Deployment tab of your deployment space. From the action menu for your deployment name, click Edit. In the Edit deployment dialog box, change the number of copies and click Save."
638	Scaling a deployment	"Increasing the number of replicas of a deployment programmatically
To view or run a working sample of scaling a deployment programmatically, you can increase the number of replicas in the metadata for a deployment."
639	Scaling a deployment	"Increasing the number of replicas of a deployment programmatically
Python example
This example uses the Python client to set the number of replicas to 3.  
The HARDWARE_SPEC value includes a name because the API requires a name or an ID to be provided."
640	Scaling a deployment	"Increasing the number of replicas of a deployment programmatically
REST API example
You must specify a name for the hardware_spec value, but the argument is not applied for scaling.  
Parent topic: Managing predictive deployments"
641	SPSS predictive analytics clustering algorithms in notebooks	You can use the scalable Two-Step or the Cluster model evaluation algorithm to cluster data in notebooks.
642	SPSS predictive analytics clustering algorithms in notebooks	"Two-Step Cluster
Scalable Two-Step is based on the familiar two-step clustering algorithm, but extends both its functionality and performance in several directions.  
First, it can effectively work with large and distributed data supported by Spark that provides the Map-Reduce computing paradigm.  
Second, the algorithm provides mechanisms for selecting the most relevant features for clustering the given data, as well as detecting rare outlier points. Moreover, it provides an enhanced set of evaluation and diagnostic features for enabling insight.  
The two-step clustering algorithm first performs a pre-clustering step by scanning the entire dataset and storing the dense regions of data cases in terms of summary statistics called cluster features. The cluster features are stored in memory in a data structure called the CF-tree. Finally, an agglomerative hierarchical clustering algorithm is applied to cluster the set of cluster features.  
Python example code:"
643	SPSS predictive analytics clustering algorithms in notebooks	"Cluster model evaluation
Cluster model evaluation (CME) aims to interpret cluster models and discover useful insights based on various evaluation measures.  
It's a post-modeling analysis that's generic and independent from any types of cluster models.  
Python example code:  
Parent topic: SPSS predictive analytics algorithms"
644	Compute resource options for AutoAI experiments in projects	"When you run an AutoAI experiment in a project, the type, size, and power of the hardware configuration available depend on the type of experiment you build.  
Default hardware configurations Compute usage"
645	Compute resource options for AutoAI experiments in projects	"Default hardware configurations
The type of hardware configuration available for your AutoAI experiment depends on the type of experiment you are building. A standard AutoAI experiment, with a single data source, has a single, default hardware configuration. An AutoAI experiment with joined data has options for increasing computational power."
646	Compute resource options for AutoAI experiments in projects	"Default hardware configurations
Capacity units per hour for AutoAI experiments
The runtimes for AutoAI stop automatically when processing is complete."
647	Compute resource options for AutoAI experiments in projects	"Compute usage in projects
AutoAI consumes compute resources as CUH from the Watson Machine Learning service.  
You can monitor the total monthly amount of CUH consumption for the Watson Machine Learning service on the Resource usage page on the Manage tab of your project."
648	Compute resource options for AutoAI experiments in projects	"Learn more
AutoAI Watson Machine Learning service Compute resource options for assets and deployments in spaces Monitoring account resource usage  
Parent topic: Choosing compute resources for tools"
649	SPSS predictive analytics classification and regression algorithms in notebooks	You can use generalized linear model, linear regression, linear support vector machine, random trees, or CHAID SPSS predictive analytics algorithms in notebooks.
650	SPSS predictive analytics classification and regression algorithms in notebooks	"Generalized Linear Model
The Generalized Linear Model (GLE) is a commonly used analytical algorithm for different types of data. It covers not only widely used statistical models, such as linear regression for normally distributed targets, logistic models for binary or multinomial targets, and log linear models for count data, but also covers many useful statistical models via its very general model formulation. In addition to building the model, Generalized Linear Model provides other useful features such as variable selection, automatic selection of distribution and link function, and model evaluation statistics. This model has options for regularization, such as LASSO, ridge regression, elastic net, etc., and is also capable of handling very wide data.  
For more details about how to choose distribution and link function, see Distribution and Link Function Combination.  
Example code 1:  
This example shows a GLE setting with specified distribution and link function, specified effects, intercept, conducting ROC curve, and printing correlation matrix. This scenario builds a model, then scores the model.  
Python example:  
Example code 2:  
This example shows a GLE setting with unspecified distribution and link function, and variable selection using the forward stepwise method. This scenario uses the forward stepwise method to select distribution, link function and effects, then builds and scores the model.  
Python example:  
Example code 3:  
This example shows a GLE setting with unspecified distribution, specified link function, and variable selection using the LASSO method, with two-way interaction detection and automatic penalty parameter selection. This scenario detects two-way interaction for effects, then uses the LASSO method to select distribution and effects using automatic penalty parameter selection, then builds and scores the model.  
Python example:"
651	SPSS predictive analytics classification and regression algorithms in notebooks	"Linear Regression
The linear regression model analyzes the predictive relationship between a continuous target and one or more predictors which can be continuous or categorical.  
Features of the linear regression model include automatic interaction effect detection, forward stepwise model selection, diagnostic checking, and unusual category detection based on Estimated Marginal Means (EMMEANS).  
Example code:  
Python example:"
652	SPSS predictive analytics classification and regression algorithms in notebooks	"Linear Support Vector Machine
The Linear Support Vector Machine (LSVM) provides a supervised learning method that generates input-output mapping functions from a set of labeled training data. The mapping function can be either a classification function or a regression function. LSVM is designed to resolve large-scale problems in terms of the number of records and the number of variables (parameters). Its feature space is the same as the input space of the problem, and it can handle sparse data where the average number of non-zero elements in one record is small.  
Example code:  
Python example:"
653	SPSS predictive analytics classification and regression algorithms in notebooks	"Random Trees
Random Trees is a powerful approach for generating strong (accurate) predictive models. It's comparable and sometimes better than other state-of-the-art methods for classification or regression problems.  
Random Trees is an ensemble model consisting of multiple CART-like trees. Each tree grows on a bootstrap sample which is obtained by sampling the original data cases with replacement. Moreover, during the tree growth, for each node the best split variable is selected from a specified smaller number of variables that are drawn randomly from the full set of variables. Each tree grows to the largest extent possible, and there is no pruning. In scoring, Random Trees combines individual tree scores by majority voting (for classification) or average (for regression).  
Example code:  
Python example:"
654	SPSS predictive analytics classification and regression algorithms in notebooks	"CHAID
CHAID, or Chi-squared Automatic Interaction Detection, is a classification method for building decision trees by using chi-square statistics to identify optimal splits. An extension applicable to regression problems is also available.  
CHAID first examines the crosstabulations between each of the input fields and the target, and tests for significance using a chi-square independence test. If more than one of these relations is statistically significant, CHAID will select the input field that's the most significant (smallest p value). If an input has more than two categories, these are compared, and categories that show no differences in the outcome are collapsed together. This is done by successively joining the pair of categories showing the least significant difference. This category-merging process stops when all remaining categories differ at the specified testing level. For nominal input fields, any categories can be merged; for an ordinal set, only contiguous categories can be merged. Continuous input fields other than the target can't be used directly; they must be binned into ordinal fields first.  
Exhaustive CHAID is a modification of CHAID that does a more thorough job of examining all possible splits for each predictor but takes longer to compute.  
Example code:  
Python example:  
Parent topic: SPSS predictive analytics algorithms"
655	Foundation model parameters: decoding and stopping criteria	You can set parameters in the Prompt Lab to control how the model generates output in response to your prompt. Set decoding parameters to adjust how the output text is generated. Set stopping criteria parameters to specify when the model should stop generating output.
656	Foundation model parameters: decoding and stopping criteria	"Decoding
Decoding is the process that a model uses to choose the tokens in the generated output.  
Choose one of the following decoding options:  
Greedy decoding: Selects the token with the highest probability at each step of the decoding process.  
Greedy decoding produces output that closely matches the most common language in the model's pretraining data and in your prompt text, which is desirable in less creative or fact-based use cases. A weakness of greedy decoding is that it can cause repetitive loops in the generated output.  
Sampling decoding: Offers more variability in how tokens are selected.  
With sampling decoding, the model samples tokens, meaning the model chooses a subset of tokens, and then one token is chosen randomly from this subset to be added to the output text. Sampling adds variability and randomness to the decoding process, which can be desirable in creative use cases. However, with greater variability comes a greater risk of incorrect or nonsensical output."
657	Foundation model parameters: decoding and stopping criteria	"Decoding
More options for sampling decoding
When you choose Sampling decoding, more parameters are available that you can use to adjust how the foundation model chooses tokens to sample. The following parameters work together to influence which tokens are sampled:  
Temperature sampling flattens or sharpens the probability distribution over the tokens to be sampled. Top-k sampling samples tokens with the highest probabilities until the specified number of tokens is reached. Top-p sampling samples tokens with the highest probability scores until the sum of the scores reaches the specified threshold value. (Top-p sampling is also called nucleus sampling.)"
658	Foundation model parameters: decoding and stopping criteria	"Decoding
Example of adjusting sampling decoding settings
In this example, the foundation model already generated the output text I took my dog and now the model is choosing the next token.  
To find the best choice, the model calculates a discrete probability distribution over the possible tokens. With this type of distribution, each token is assigned a decimal point probability score between 0 and 1 where the scores add up to 1.  
In a real scenario, there might be hundreds of possible tokens. In this example, the choices include only five tokens, which are shown here in the context of typical sentences:  
I took my dog...  
for a walk. to the vet. with me. and my cat on vacation. by the collar.  
Top K and Top P represent two different methods for choosing the tokens to sample.  
Both methods begin by ordering the choices from most-to-least probable. The following table lists the tokens and their fictional probability scores in order.  
Top K specifies how many tokens to sample. For example, if you set Top K to 3, then only the first three tokens in the list are sampled: for, to, and with.  
Note: A Greedy decoding setting is equivalent to Top K = 1.  
Top P specifies the cumulative probability score threshold that the tokens must reach.  
For example, if you set Top P to 0.6, then only the first two tokens, for and to, are sampled because their probabilities (0.4 and 0.25) add up to 0.65. (As shown in this example, it is okay for the sum to exceed the threshold.)  
Top P is not used unless you set the Top P parameter value to something other than the default value of 1. Using Top P and Top K together can be a useful way to filter out tokens with extra low probability scores. When both parameters are specified, Top K is applied first.  
For example, you might set Top K to 5 and Top P to 0.8. The Top K setting samples all 5 tokens, and then Top P limits the sampled tokens to for, to, and with because their probabilities reach the probability score threshold of 0.8 (0.4 + 0.25 + 0.17 = 0.82)."
659	Foundation model parameters: decoding and stopping criteria	"Decoding
Example of adjusting sampling decoding settings
When both settings are specified, any tokens below the cutoff that is set by Top K are considered to have a probability of zero when Top P is computed. For example, if Top K is set to 2 and Top P to 0.8, then only for and to are sampled. The token with is not sampled because the probability scores for with, and, and by are reset to 0.  
Temperature is a parameter of the softmax function that converts a vector of numbers into a probability distribution. The temperature setting affects the shape of the probability distribution.  
Low temperatures amplify the probability differences between tokens. More-likely terms have much higher scores relative to less-likely terms. As a result, terms that are similar to terms in the model's training data or your prompt input will probably be sampled. Use a lower temperature value when you want more dependable output.  
High temperatures result in token probabilities that are closer to one another. As a result, unusual terms have a better chance of being sampled. Use a higher temperature value when you want to increase the randomness and variability or the output, such as when you want creative output. Remember, randomness can also lead to inaccurate or nonsensical output.  
For example, when a high temperature value such as 2 is applied, the probability scores of the tokens in this example might be closer to one another, as shown in the Table 3.  
When a high temperature value of 2 is used, it takes more tokens to reach the threshold when Top P is set to 0.8.  
With a high temperature, the for, to, with, and and tokens (0.3 + 0.25 + 0.2 + 0.15 = 90) are sampled. With a low temperature, only the for, to, and with tokens (0.4 + 0.25 + 0.17 = 82) are sampled."
660	Foundation model parameters: decoding and stopping criteria	"Decoding
Random seed
When you submit the same prompt to a model multiple times with sampling decoding, the model usually generates different text each time. This variability is the result of intentional pseudo-randomness that is built into the decoding process.  
Random seed refers to the number that is used to start the random number generator that the model uses to randomize its token choices. If you want to remove this intentional randomness as a variable from your experiments, you can pick a number and specify that same number each time you run the experiment.  
Supported values: Integer in the range 1 to 4,294,967,295 Default: Itself randomly generated Use: To produce repeatable results, set the same random seed value every time."
661	Foundation model parameters: decoding and stopping criteria	"Decoding
Repetition penalty
If the generated output for your chosen prompt, model, and parameters consistently contains repetitive text, you can try adding a repetition penalty. When set, the penalty lowers the probability scores of tokens that were recently used so that the model is less likely to repeat them. A higher value leads to more diverse and varied output.  
Supported values: Floating-point number in the range 1.0 (no penalty) to 2.0 (maximum penalty) Default: 1.0 Use: The higher the penalty, the less likely it is that the result will include repeated text."
662	Foundation model parameters: decoding and stopping criteria	"Stopping criteria
Text generation stops after the model considers the output to be complete, a stop sequence is generated, the maximum token limit is reached, or the model generation time limit is reached.  
Model generation stops when the time limit for the generation request is reached. The default time limit is 10 minutes and 5 minutes for Lite plans. You can specify a shorter time limit when you submit an inference request by using the API.  
You can affect the length of the output that is generated by the model in the following ways: specifying stop sequences and setting Min tokens and Max tokens."
663	Foundation model parameters: decoding and stopping criteria	"Stopping criteria
Stop sequences
A stop sequence is a string of one or more characters. If you specify stop sequences, the model will automatically stop generating output after one of the stop sequences that you specify appears in the generated output.  
For example, one way to cause a model to stop generating output after just one sentence is to specify a period as a stop sequence. That way, after the model generates the first sentence and ends it with a period, output generation stops.  
Choosing effective stop sequences depends on your use case and the nature of the generated output that you expect.  
Supported values: 0 to 6 strings, each no longer than 40 tokens  
Default: No stop sequence  
Use:  
Stop sequences are ignored until after the number of tokens that are specified in the Min tokens parameter are generated. If your prompt includes examples of input-and-output pairs, ensure the sample output in the examples ends with one of the stop sequences."
664	Foundation model parameters: decoding and stopping criteria	"Stopping criteria
Minimum and maximum new tokens
If the output from the model is too short or too long, try adjusting the parameters that control the number of generated tokens:  
The Min tokens parameter controls the minimum number of tokens in the generated output The Max tokens parameter controls the maximum number of tokens in the generated output  
The maximum number of tokens that are allowed in the output differs by model. For more information, see the Maximum tokens information in Supported foundation models.  
Defaults:  
Min tokens: 0 Max tokens: 200  
Use:  
Min tokens must be less than or equal to Max tokens. The cost of using foundation models in IBM watsonx.ai is based on use, which is partly related to the number of tokens that are generated. Specifying the lowest value for Max tokens that works for your use case is a cost-saving strategy. For Lite plans, output stops being generated after a dynamic, model-specific, environment-driven upper limit is reached. Output can stop even if the value specified with the Max tokens parameter is not reached. To determine the upper limit, see the Tokens limits section for the model in Supported foundation models or call the get_details function of the foundation models Python library.  
Parent topic: Prompt Lab"
665	Keyword extraction and ranking	"The Watson Natural Language Processing Keyword extraction with ranking block extracts noun phrases from input text based on their relevance.  
Block name  
keywords_text-rank_<language>_stock  
Supported language  
Keyword extraction with text ranking is available for the following languages. For a list of the language codes and the corresponding language, see Language codes.  
ar, cs, da, de, en, es, fi, fr, he, hi, it, ja, ko, nb, nl, nn, pt, ro, ru, sk, sv, tr, zh-cn  
Capabilities  
The keywords and text rank block ranks noun phrases extracted from an input document based on how relevant they are within the document.  
Dependencies on other blocks  
The following blocks must run before you can run the Keyword extraction with ranking block:  
syntax_izumo_<language>_stock noun-phrases_rbr_<language>_stock  
Code sample  
Output of the code sample:  
Parent topic: Watson Natural Language Processing task catalog"
666	Starting the aggregator (Admin)	"An administrator completes the following steps to start the experiment and train the global model.  
Step 1: Set up the Federated Learning experiment Step 2: Create the remote training system Step 3: Start the experiment"
667	Starting the aggregator (Admin)	"Step 1: Set up the Federated Learning experiment
Set up a Federated Learning experiment from a project.  
From the project, click New asset > Federated Learning. Name the experiment. Optional: Add an optional description and tags. Add new collaborators to the project. In the Configure tab, choose the training framework and model type. See Frameworks, fusion methods, and Python versions for a table listing supported frameworks, fusion methods, and their attributes. Optional: You can choose to enable the homomorphic encryption feature. For more details, see Applying encryption. Click Select under Model specification and upload the .zip file that contains your initial model. In the Define hyperparameters tab, you can choose hyperparameter options available for your framework and fusion method to tune your model."
668	Starting the aggregator (Admin)	"Step 2: Create the Remote Training System
Create Remote Training Systems (RTS) that authenticates the participating parties of the experiment.  
Configuring the RTS Field name Definition Example Name A name to identify this RTS instance. Canada Bank Model: Federated Learning Experiment Description(Optional) Description of the training system. This Remote Training System is for a Federated Learning experiment to train a model for predicting credit card fraud with data from Canadian banks. System administrator(Optional) Specify a user with read-only access to this RTS. They can see system details, logs, and scripts, but not necessarily participate in the experiment. They should be contacted if issues occur when running the experiment. Admin (admin@example.com) Allowed identities List project collaborators who can participate in the Federated Learning experiment training. Multiple collaborators can be registered in this RTS, but only one can participate in the experiment. Multiple RTS's are needed to authenticate all participating collaborators. John Doe (john.doe@example.com)Jane Doe (jane.doe@example.com) Allowed IP addresses(Optional) Restrict individual parties from connecting to Federated Learning outside of a specified IP address. 1. To configure this, click Configure.2. For Allowed identities, select the user to place IP constraints on.3. For Allowed IP addresses for user, enter a comma seperated list of IPs and or CIDRs that can connect to the Remote Training System. Note: Both IPv4 and IPv6 are supported. John 1234:5678:90ab:cdef:1234:5678:90ab:cdef: (John’s office IP), 123.123.123.123 (John’s home IP), 0987.6543.21ab.cdef (Remote VM IP) Jane 123.123.123.0/16 (Jane's home IP), 0987.6543.21ab.cdef (Remote machine IP) Tags (Optional) Associate keywords with the Remote Training System to make it easier to find. CanadaBankModelCreditFraud  
At Select remote training system, click Add new systems.  
Configure the RTS.  
Click Add to save the RTS instance. If you are creating multiple remote training instances, you can repeat these steps.  
Click Add systems to save the RTS as an asset in the project."
669	Starting the aggregator (Admin)	"Step 2: Create the Remote Training System
Configure the RTS.  
Click Add to save the RTS instance. If you are creating multiple remote training instances, you can repeat these steps.  
Click Add systems to save the RTS as an asset in the project.  
Tip: You can use an RTS definition for future experiments. For example, in the __Select remote training system__ tab, you can select any Remote Training System that you previously created.  
Each RTS can only authenticate one of its allowed party identities. Create an RTS for each new participating part(ies)."
670	Starting the aggregator (Admin)	"Step 3: Start the experiment
Start the Federated Learning aggregator to initiate training of the global model.  
Click Review and create to view the settings of your current Federated Learning experiment. Then, click Create. The Federated Learning experiment will be in Pending status while the aggregator is starting. When the aggregator starts, the status will change to Setup – Waiting for remote systems.  
Parent topic: Creating a Federated Learning experiment"
671	Configuring pipeline nodes	Configure the nodes of your pipeline to specify inputs and to create outputs as part of your pipeline.
672	Configuring pipeline nodes	"Specifying the workspace scope
By default, the scope for a pipeline is the project that contains the pipeline. You can explicitly specify a scope other than the default, to locate an asset used in the pipeline. The scope is the project, catalog, or space that contains the asset. From the user interface, you can browse for the scope."
673	Configuring pipeline nodes	"Changing the input mode
When you are configuring a node, you can specify any resources that include data and notebooks in various ways. Such as directly entering a name or ID, browsing for an asset, or by using the output from a prior node in the pipeline to populate a field. To see what options are available for a field, click the input icon for the field. Depending on the context, options can include:  
Select resource: use the asset browser to find an asset such as a data file. Assign pipeline parameter: assign a value by using a variable configured with a pipeline parameter. For more information, see Configuring global objects. Select from another node: use the output from a node earlier in the pipeline as the value for this field. Enter the expression: enter code to assign values or identify resources. For more information, see Coding elements."
674	Configuring pipeline nodes	"Pipeline nodes and parameters
Configure the following types of pipeline nodes:"
675	Configuring pipeline nodes	"Copy nodes
Use Copy nodes to add assets to your pipeline or to export pipeline assets.  
Copy selected assets from a project or space to a nonempty space. You can copy these assets to a space:  
AutoAI experiment  
Code package job  
Connection  
Data Refinery flow  
Data Refinery job  
Data asset  
Deployment job  
Environment  
Function  
Job  
Model  
Notebook  
Notebook job  
Pipelines job  
Script  
Script job  
SPSS Modeler job  
Export selected assets from the scope, for example, a project or deployment space. The operation exports all the assets by default. You can limit asset selection by building a list of resources to export.  
Notes:  
If you export a project that contains a notebook, the latest version of the notebook is included in the export file. If the Pipeline with the Run notebook job node was configured to use a different notebook version other than the latest version, the exported Pipeline is automatically reconfigured to use the latest version when imported. This might produce unexpected results or require some reconfiguration after the import. If assets are self-contained in the exported project, they are retained when you import a new project. Otherwise, some configuration might be required following an import of exported assets.  
Import assets from a ZIP file that contains exported assets.  
Notes: After you import a file, paths and references to the imported assets are updated, following these rules:  
References to assets from the exported project or space are updated in the new project or space after the import. If assets from the exported project refer to external assets (included in a different project), the reference to the external asset will persist after the import. If the external asset no longer exists, the parameter is replaced with an empty value and you must reconfigure the field to point to a valid asset."
676	Configuring pipeline nodes	"Create nodes
Configure the nodes for creating assets in your pipeline.  
Use this node to train an AutoAI classification or regression experiment and generate model-candidate pipelines.  
Use this node to train an AutoAI time series experiment and generate model-candidate pipelines.  
Use this node to create a batch deployment for a machine learning model.  
Use this node to create a data asset.  
Use this node to create and configure a space that you can use to organize and create deployments.  
Use this node to create an online deployment where you can submit test data directly to a web service REST API endpoint."
677	Configuring pipeline nodes	"Wait
Use nodes to pause a pipeline until an asset is available in the location that is specified in the path.  
Use this node to wait until all results from the previous nodes in the pipeline are available so the pipeline can continue.  
This node takes no inputs and produces no output. When the results are all available, the pipeline continues automatically.  
Use this node to wait until any result from the previous nodes in the pipeline is available so the pipeline can continue. Run the downstream nodes as soon as any of the upstream conditions are met.  
This node takes no inputs and produces no output. When any results are available, the pipeline continues automatically.  
Wait for an asset to be created or updated in the location that is specified in the path from a job or process earlier in the pipeline. Specify a timeout length to wait for the condition to be met. If 00:00:00 is the specified timeout length, the flow waits indefinitely."
678	Configuring pipeline nodes	"Control nodes
Control the pipeline by adding error handling and logic.  
Loops are a node in a Pipeline that operates like a coded loop.  
The two types of loops are parallel and sequential.  
You can use loops when the number of iterations for an operation is dynamic. For example, if you don't know the number of notebooks to process, or you want to choose the number of notebooks at run time, you can use a loop to iterate through the list of notebooks.  
You can also use a loop to iterate through the output of a node or through elements in a data array."
679	Configuring pipeline nodes	"Control nodes
Loops in parallel
Add a parallel looping construct to the pipeline. A parallel loop runs the iterating nodes independently and possibly simultaneously.  
For example, to train a machine learning model with a set of hyperparameters to find the best performer, you can use a loop to iterate over a list of hyperparameters to train the notebook variations in parallel. The results can be compared later in the flow to find the best notebook.  
If the input array element type is JSON or any type that is represented as such, this field might decompose it as dictionary. Keys are the original element keys and values are the aliases for output names."
680	Configuring pipeline nodes	"Control nodes
Loops in sequence
Add a sequential loop construct to the pipeline. Loops can iterate over a numeric range, a list, or text with a delimiter.  
A use case for sequential loops is if you want to try an operation 3 time before you determine whether an operation failed.  
After you configure the loop iterative range, define a subpipeline flow inside the loop to run until the loop is complete. For example, it can invoke notebook, script, or other flow per iteration."
681	Configuring pipeline nodes	"Control nodes
Terminate loop
In a parallel or sequential loop process flow, you can add a Terminate pipeline node to end the loop process anytime. You must customize the conditions for terminating.  
Attention: If you use the Terminate loop node, your loop cancels any ongoing tasks and terminates without completing its iteration."
682	Configuring pipeline nodes	"Control nodes
Configure a user variable with a key/value pair, then add the list of dynamic variables for this node.  
For more information on how to create a user variable, see Configuring global objects.  
x  
For expressions, use the built-in Expression Builder to create a variable that results from a custom expression. For pipeline parameters, assign a pipeline parameter and use the parameter value as input for the user variable.  
You can initiate and control the termination of a pipeline with a Terminate pipeline node from the Control category. When the error flow runs, you can optionally specify how to handle notebook or training jobs that were initiated by nodes in the pipeline. You must specify whether to wait for jobs to finish, cancel the jobs then stop the pipeline, or stop everything without canceling. Specify the options for the Terminate pipeline node.  
Terminator mode can be:  
Terminate pipeline run and all running jobs stops all jobs and stops the pipeline. Cancel all running jobs then terminate pipeline cancels any running jobs before stopping the pipeline. Terminate pipeline run after running jobs finish waits for running jobs to finish, then stops the pipeline. Terminate pipeline that is run without stopping jobs stops the pipeline but allows running jobs to continue."
683	Configuring pipeline nodes	"Update nodes
Use update nodes to replace or update assets to improve performance. For example, if you want to standardize your tags, you can update to replace a tag with a new tag.  
Update the training details for an AutoAI experiment.  
Use these parameters to update a batch deployment.  
Update the details for a space.  
Use these parameters to update an online deployment (web service)."
684	Configuring pipeline nodes	"Delete nodes
Configure parameters for delete operations.  
You can delete:  
AutoAI experiment Batch deployment Deployment space Online deployment  
For each item, choose the asset for deletion."
685	Configuring pipeline nodes	"Run nodes
Use these nodes to train an experiment, execute a script, or run a data flow.  
Trains and stores AutoAI experiment pipelines and models.  
Run an inline Bash script to automate a function or process for the pipeline. You can enter the Bash script code manually, or you can import the bash script from a resource, pipeline parameter, or the output of another node.  
You can also use a Bash script to process large output files. For example, you can generate a large, comma-separated list that you can then iterate over using a loop.  
In the following example, the user entered the inline script code manually. The script uses the cpdctl tool to search all notebooks with a set variable tag and aggregates the results in a JSON list. The list can then be used in another node, such as running the notebooks returned from the search.  
The output for a Bash script is often the result of a computed expression and can be large. When you are reviewing the properties for a script with valid large output, you can preview or download the output in a viewer.  
These rules govern what type of large output is valid.  
The output of a list_expression is a calculated expression, so it is valid a large output. String output is treated as a literal value rather than a calculated expression, so it must follow the size limits that govern inline expressions. For example, you are warned when a literal value exceeds 1 KB and values of 2 KB and higher result in an error. 4.8.4 and later You can save standard error (standard_error) messages as a separate output and use it as input for other nodes or use it to conditionalize executing the next node.  
The way that you reference a variable in a script depends on whether the variable was created as an input variable or as an output variable. Output variables are created as a file and require a file path in the reference. Specifically:  
Input variables are available using the assigned name Output variable names require that _PATH be appended to the variable name to indicate that values have to be written to the output file pointed by the {output_name}_PATH variable.  
The following steps describe how to use ssh to run your remote Bash script."
686	Configuring pipeline nodes	"Run nodes
The following steps describe how to use ssh to run your remote Bash script.  
Create a private key and public key.ssh-keygen -t rsa -C ""XXX"" Copy the public key to the remote host.ssh-copy-id USER@REMOTE_HOST On the remote host, check whether the public key contents are added into /root/.ssh/authorized_keys. Copy the public and private keys to a new directory in the Run Bash script node.mkdir -p $HOME/.ssh #copy private key content echo ""-----BEGIN OPENSSH PRIVATE KEY----- ... ... -----END OPENSSH PRIVATE KEY-----"" > $HOME/.ssh/id_rsa #copy public key content echo ""ssh-rsa ...... "" > $HOME/.ssh/id_rsa.pub chmod 400 $HOME/.ssh/id_rsa.pub chmod 400 $HOME/.ssh/id_rsa ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o GlobalKnownHostsFile=/dev/null -i $HOME/.ssh/id_rsa USER@REMOTE_HOST ""cd /opt/scripts; ls -l; sh 1.sh""  
The following steps describe how to use sshpass to run your remote Bash script.  
Put your SSH password file in your system path, such as the mounted storage volume path. Use the SSH password directly in the Run Bash script node:cd /mnts/orchestration ls -l sshpass chmod 777 sshpass ./sshpass -p PASSWORD ssh -o StrictHostKeyChecking=no USER@REMOTE_HOST ""cd /opt/scripts; ls -l; sh 1.sh""  
Configure this node to run selected deployment jobs.  
Restriction: Input for batch deployment jobs is limited to data assets. Deployments that require JSON input or multiple files as input, are not supported. For example, SPSS models and Decision Optimization solutions that require multiple files as input are not supported.  
This node runs a specified Data Refinery job.  
Attention: Leave the environments field as is to use the default runtime. If you choose to override, specify an alternate environment for running the job. Be sure any environment that you specify is compatible with the component language and hardware configuration to avoid a runtime error.  
Use these configuration options to specify how to run a Jupyter Notebook in a pipeline."
687	Configuring pipeline nodes	"Run nodes
Use these configuration options to specify how to run a Jupyter Notebook in a pipeline.  
Attention: Leave the environments field as is to use the default environment. If you choose to override, specify an alternate environment for running the job. Be sure any environment that you specify is compatible with the notebook language and hardware configuration to avoid a runtime error.  
Notes:  
Environment variables that you define in a pipeline cannot be used for notebook jobs you run outside of Watson Pipelines. You can run a notebook from a code package in a regular package.  
Run a reusable pipeline component that is created by using a Python script. For more information, see Creating a custom component.  
If a pipeline component is available, configuring the node presents a list of available components. The component that you choose specifies the input and output for the node. Once you assign a component to a node, you cannot delete or change the component. You must delete the node and create a new one.  
Add a pipeline to run a nested pipeline job as part of a containing pipeline. This is a way of adding reusable processes to multiple pipelines. You can use the output from a nested pipeline that is run as input for a node in the containing pipeline.  
Attention: Leave the environments field as is to use the default runtime. If you choose to override, specify an alternate environment for running the job. Be sure any environment that you specify is compatible with the component language and hardware configuration to avoid a runtime error.  
If you create a pipeline with nested pipelines and run a pipeline job from the top-level, the pipelines are named and saved as project assets that use this convention:  
The top-level pipeline job is named ""Trial job - pipeline guid"". All subsequent jobs are named ""pipeline_ pipeline guid"".  
Use these configuration options to specify how to run an SPSS Modeler in a pipeline.  
Attention: Leave the environments field as is to use the default SPSS Modeler runtime. If you choose to override, specify an alternate environment for running the job. Be sure any environment that you specify is compatible with the hardware configuration to avoid a runtime error."
688	Configuring pipeline nodes	"Learn more
Parent topic: Creating a pipeline"
689	Managing AI lifecycle with CPDCTL	"CPDCTL is a command-line interface (CLI) that you can use to manage the lifecycle of AI assets.  
Using the CPDCTL CLI, you can manage configuration settings and automate the end-to-end flow:  
Manage all aspects of a project, including: creating or deleting projects, changing project hardware and software specifications, updating package extensions, and manage project members. Prepare and manage data assets and connections. Create experiments to create and train models, manage models and model pipelines, promote models to space, create new model revisions. Manage all aspects of a deployment space, including: creating, and deleting deployment spaces, changing space hardware and software specifications, package extensions, and managing space members. Deploy and score models and functions; create, manage, run, update, and delete deployment jobs."
690	Learn more	"For installation, configuration, available commands, supported outputs, and usage scenarios, refer to github.com/IBM/cpdctl. For detailed CPDCTL command reference, refer to CPDCTL command reference. For other CPDCTL usage samples, refer to CPDCTL code samples.  
Other useful links:  
Creating a script asset with CPDCTL.  
Parent topic: Managing AI Lifecycle with ModelOps"
691	Get started	"Federated Learning is appropriate for any situation where different entities from different geographical locations or Cloud providers want to train an analytical model without sharing their data.  
To get started with Federated Learning, choose from these options:  
Familiarize yourself with the key concepts and Terminology. Review the architecture for creating a Federated Learning experiment. Follow a tutorial for step-by-step instructions for creating a Federated Learning experiment or review samples."
692	Get started	"Learn more
Terminology Federated Learning architecture  
Parent topic: IBM Federated Learning"
693	Loading and accessing data in a notebook	"You can integrate data into notebooks by accessing the data from a local file, from free data sets, or from a data source connection. You load that data into a data structure or container in the notebook, for example, a pandas.DataFrame, numpy.array, Spark RDD, or Spark DataFrame.  
To work with data in a notebook, you can choose between the following options:  
Important: Make sure that the environment in which the notebook is started has enough memory to store the data that you load to the notebook. The environment must have significantly more memory than the total size of the data that is loaded to the notebook. Some data frameworks, like pandas, can hold multiple copies of the data in memory."
694	Loading and accessing data in a notebook	"Adding a file from your local system
To add a file from your local system to your project by using the Jupyterlab notebook editor:  
Open your notebook in edit mode. From the toolbar, click the Upload asset to project icon () and add your file.  
Tip: You can also drag the file into your notebook sidebar."
695	Loading and accessing data in a notebook	"Load data sets from the Resource hub
The data sets on the Resource hub contain open data. Watch this short video to see how to work with public data sets in the Resource hub.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
To add a data set from the Resource hub to your project:  
From the IBM watsonx navigation menu, select Resource hub. Find the card for the data set that you want to add. Click Add to project, select the project, and click Add. Clicking View project takes you to the project Overview page. The data asset is added to the list of data assets on the project's Assets page."
696	Loading and accessing data in a notebook	"Loading data from files
Prerequisites The file must exist as an asset in your project. For details, see Adding a file from your local system or Loading a data set from the Resource hub.  
To load data from a project file to your notebook:  
Open your notebook in edit mode. Click the Code snippets icon (), click Read data, and then select the data file from your project. If you want to change your selection, use Edit icon. From the Load as drop-down list, select the load option that you prefer. If you select Credentials, only file access credentials will be generated. For details, see Adding credentials. Click in an empty code cell in your notebook and then click Insert code to cell to insert the generated code. Alternatively, click to copy the generated code to the clipboard and then paste the code into your notebook.  
The generated code serves as a quick start to begin working with a data set. For production systems, carefully review the inserted code to determine whether to write your own code that better meets your needs.  
To learn which data structures are generated for which notebook language and data format, see Data load support."
697	Loading and accessing data in a notebook	"Loading data from data source connections
Prerequisites Before you can load data from an IBM data service or from an external data source, you must create or add a connection to your project. See Adding connections to projects.  
To load data from an existing data source connection into a data structure in your notebook:  
Open your notebook in edit mode. Click the Code snippets icon (), click Read data, and then select the data source connection from your project. Select the schema and choose a table. If you want to change your selection, use Edit icon. Select the load option. If you select Credentials, only metadata will be generated. For details, see Adding credentials. Click in an empty code cell in your notebook and then insert code to the cell. Alternatively, click to copy the generated code to the clipboard and then paste the code into your notebook. If necessary, enter your personal credentials for locked data connections that are marked with a key icon (). This is a one-time step that permanently unlocks the connection for you. After you unlock the connection, the key icon is no longer displayed. For more information, see Adding connections to projects.  
The generated code serves as a quick start to begin working with a connection. For production systems, carefully review the inserted code to determine whether to write your own code that better meets your needs.  
To learn which data structures are generated for which notebook language and data format, see Data load support."
698	Loading and accessing data in a notebook	"Adding credentials
You can generate your own code to access the file located in your IBM Cloud Object Storage or a file accessible through a connection. This is useful when, for example, your file format is not supported by the snippet generation tool. With the credentials, you can write your own code to load the data into a data structure in a notebook cell.  
To add the credentials:  
Click the Code snippets icon () and then click Read data. Click in an empty code cell in your notebook, select Credentials as the load option, and then load the credentials to the cell. You can also click to copy the credentials to the clipboard and then paste them into your notebook. Insert your credentials into the code in your notebook to access the data. For example, see this code in a blog for Python."
699	Loading and accessing data in a notebook	"Use an API function or an operating system command to access the data
You can use API functions or operating system commands in your notebook to access data, for example, the wget command to access data by using the HTTP, HTTPS or FTP protocols. When you use these types of API functions and commands, you must include code that sets the project access token. See Manually add the project access token.  
For reference information about the API, see Watson Data API.  
Parent topic: Notebooks and scripts"
700	Markdown cheatsheet	"You can use Markdown tagging to improve the readability of a project readme or the Markdown cells in Jupyter notebooks. The differences between Markdown in the readme files and in notebooks are noted.  
Headings: Use #s followed by a blank space for notebook titles and section headings:  
# title ## major headings ### subheadings #### 4th level subheadings  
Emphasis: Use this code: Bold: __string__ or **string**, Italic: _string_ or *string*, Strikethrough: ~~string~~  
Mathematical symbols: Use this code: $ mathematical symbols $  
Monospace font: Surround text with a back single quotation mark (`). Use monospace for file path and file names and for text users enter or message text users see.  
Line breaks: Sometimes Markdown doesn’t make line breaks when you want them. Put two spaces at the end of the line, or use this code for a manual line break: <br>  
Indented quoting: Use a greater-than sign (> ) and then a space, then type the text. The text is indented and has a gray horizontal line to the left of it until the next carriage return.  
Bullets: Use the dash sign (- ) with a space after it or a space, a dash, and a space (-), to create a circular bullet. To create a sub bullet, use a tab followed a dash and a space. You can also use an asterisk instead of a dash, and it works the same.  
Numbered lists: Start with 1. followed by a space, then your text. Hit return and numbering is automatic. Start each line with some number and a period, then a space. Tab to indent to get subnumbering.  
Checkboxes in readme files: Use this code for an unchecked box: - [ ] Use this code for a checked box: - [x]  
Tables in readme files: Use this code:  
Graphics in notebooks: Drag and drop images to the Markdown cell to attach it to the notebook. To add images to other cell types, use graphics that are hosted on the web with this code, substituting url/name with the full URL and name of the image: <img src=""url/filename.gif"" alt=""Alt text"" title=""Title text"" />"
701	Markdown cheatsheet	"Graphics in readme files: Use this code: ![Alt text](url/filename.gif ""Title text"")  
Geometric shapes: Use this code with a decimal or hex reference number from here: UTF-8 Geometric shapes &#reference_number;  
Horizontal lines: Use three asterisks: ***  
Internal links: To link to a section, add an anchor above the section title and then create a link.  
Use this code to create an anchor: <a id=""section_ID""></a> Use this code to create the link: [section title](#section-ID) Make sure that the section_ID is unique within the notebook or readme.  
Alternatively, for notebooks you can skip creating anchors and use this code: [section title](#section-title) For the text in the parentheses, replace spaces and special characters with a hyphen and make all characters lowercase.  
Test all links!  
External links: Use this code: [link text](http://url)  
To create a link that opens in a new window or tab, use this code: <a href=""http://url"" target=""_blank"" rel=""noopener noreferrer"">link text</a>  
Test all links!  
Parent topic: Projects"
702	IBM slate-30m-english-rtrvr model card	"Model description
The slate.30m.english.rtrvr model is a standard sentence transformers model based on bi-encoders. The model produces an embedding for a given input e.g. query, passage, document etc. At a high level, our model is trained to maximize the cosine similarity between two input pieces of text e.g. text A (query text) and text B (passage text), which result in the sentence embeddings q and p. These sentence embeddings can then be compared using cosine similarity.  
Figure 1. Bi-encoder Embeddings Model for Retrieval"
703	IBM slate-30m-english-rtrvr model card	"Base language model
The underlying Language Model (LM) for our embeddings is slate.30m.english. It has the same architecture as a small-RoBERTa base transformer model (6 layers) and has ~30 million parameters and an embedding dimension of 384. Specifically, “slate.30m.english” was distilled from “slate.125m.english” (formerly, WatBERT). Our final model is called “slate.30m.english.rtrvr” - notice the suffix at the end denoting that we fine-tune the underlying model architecture for retrieval-based tasks."
704	IBM slate-30m-english-rtrvr model card	"Training algorithm
Most embedding models that are either state-of-the-art or at the top of the MTEB leaderboard are typically trained in 3 stages:  
Task Specific (retrieval-based) pre-training Task specific fine-tuning on mined pairs Fine-tuning on supervised pairs  
We follow a similar approach, combining the final two stages into a single fine-tuning step.  
slate.30m.english.rtrvr is produced by distilling from the “slate.125m.english.rtrvr” model in the fine-tuning step. Knowledge Distillation transfers the knowledge from a high-performing teacher model into a smaller student model by training the student’s output probability distribution to match that of the teacher as closely as possible, improving the student’s performance compared to stand-alone finetuning."
705	IBM slate-30m-english-rtrvr model card	"Training algorithm
Task-specific pre-training
This stage uses the RetroMAE framework, to make our underlying LM more retrieval oriented. We initialize our base LM with slate.30m.english and continue with RetroMAE pre-training, using the data in Table 1. Our hyper-parameters are: learning rate: 2e-5, number of steps: 435000, GPUs: 8 A100 (80GB) GPUs. Note: this is our base LM for the following 2 stages."
706	IBM slate-30m-english-rtrvr model card	"Training algorithm
Distillation using Unsupervised and Supervised Pairs
We use a bi-encoder framework for training an embedding model, as in Figure 1. We initialize with the RetroMAE pre-trained model, and further employ Knowledge Distillation with <query, passage> text pairs using a contrastive loss objective with in-batch negatives. Knowledge Distillation trains the student’s output probability distribution to match that of the teacher as closely as possible. In the context of retriever models, the output distribution is the similarity scores between pairs of text. Specifically, for each pair of sentences <query, passage>, the distribution of the teacher’s scores between the embeddings of query and passage, i.e., the cosine similarity between the embeddings, is distilled into the student.  
The teacher used for distillation is the ""slate.125m.english.rtrvr” model trained on the same data mentioned below. The teacher is formed using model fusion between two models, each of which used RetroMAE pretraining and finetuning, but differing in the finetuning data. For more details, please refer to the model card for slate.125m.english.rtrvr. The flow of knowledge transfer is shown pictorially in Figure 2.  
Figure 2. Knowledge Distillation  
We mine large scale pairs from various domain, as indicated in the Training Data section. Furthermore, we also include high-quality pairs for the retrieval task on the following datasets: SQuAD, Natural Questions, Specter, Stack Exchange (Title, Body) pairs, S2ORC, SearchQA, HotpotQA and Fever. Distillation hyperparameters are: learning rate:7e-4, number of steps: 400000, effective batch size: 2048, GPUs: 2 A100_80GB GPUs."
707	IBM slate-30m-english-rtrvr model card	"Usage
The maximum sequence length of this model is 512 tokens."
708	IBM slate-30m-english-rtrvr model card	"Evaluation
Baselines
For a fair comparison, we compare with the following baselines:  
BM25 (a traditional model based on tf-idf). ELSER (a commercial search algorithm provided by Elastic). all-MiniLM-l6-v2: a popular open-source sentence transformers model. This model shares the same architecture as slate.125m.english.rtvr, with a smaller embedding dimension (384) and has been trained on more data without commercial-friendly licenses. See the Hugging Face model card (https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) for more details. E5-base: a recent open-source transformer model with very good performance on the BEIR benchmark. This is a base-sized model, which has the same architecture as slate.125m.english.rtvr. [Reference: Wang et.al., 2022: Text Embeddings by Weakly-Supervised Contrastive Pre-training]. Huggingface model card (https://huggingface.co/intfloat/e5-base). E5-small: a smaller model within the open source E5 family. The embedding dimension of this model matches that of all-minilm-l6-v2 (384), however it has 12 layers, and thus is larger and slightly slower. [Reference: Wang et.al., 2022: Text Embeddings by Weakly-Supervised Contrastive Pre-training]. Huggingface model card (https://huggingface.co/intfloat/e5-small). BGE-base: a recent open-source transformer model with the best performance on the BEIR benchmark for the 768 embedding size (as of 01.20.2024). Huggingface model card (https://huggingface.co/BAAI/bge-base-en-v1.5)."
709	IBM slate-30m-english-rtrvr model card	"Evaluation
Our Evaluation benchmark: BEIR (MTEB’s retrieval tab)
The BEIR benchmark contains of 15 open-source retrieval tasks evaluated under a zero-shot setting. BEIR focused on Diversity, including nine different retrieval tasks: Fact checking, citation prediction, duplicate question retrieval, argument retrieval, news retrieval, question answering, tweet retrieval, bio-medical IR, and entity retrieval. Further, it includes datasets from diverse text domains, datasets that cover broad topics (like Wikipedia) and specialized topics (like COVID-19 publications), different text types (news articles vs. Tweets), datasets of various sizes (3.6k - 15M documents), and datasets with different query lengths (average query length between 3 and 192 words) and document lengths (average document length between 11 and 635 words). BEIR uses the Normalized Cumulative Discount Gain (specifically, nDCG@10) metric for evaluation."
710	IBM slate-30m-english-rtrvr model card	"Evaluation
Long NQ
Long NQ is an IBM dataset designed for evaluating the full RAG pipeline, based on a subset of the NaturalQuestions dataset. The dev set has 300 answerable questions with a corpus of 178,891 passages from 2,345 Wikipedia documents. Long NQ also provides gold Wikipedia passages that are relevant for each question. During retrieval, the task is to obtain the relevant gold passage from the corpus for every question. The DCT code for Long NQ is 65971bbc1953f0115c1bbd77."
711	IBM slate-30m-english-rtrvr model card	"Evaluation
Results
Figure 3. Performance comparison on the BEIR benchmark (MTEB retrieval tab)  
Figure 4. Performance comparison on the Long NQ dataset"
712	IBM slate-30m-english-rtrvr model card	"Runtime Performance
The performance runtime is measured on a re-ranking task with 466 queries. For each query we re-rank the top-100 passages obtained by BM25 and we report the average time over all queries. The re-ranking was performed on a A100_40GB GPU."
713	Detecting entities with regular expressions	"Similar to detecting entities with dictionaries, you can use regex pattern matches to detect entities.  
Regular expressions are not provided in files like dictionaries but in-memory within a regex configuration. You can use multiple regex configurations during the same extraction.  
Regexes that you define with Watson Natural Language Processing can use token boundaries. This way, you can ensure that your regular expression matches within one or more tokens. This is a clear advantage over simpler regular expression engines, especially when you work with a language that is not separated by whitespace, such as Chinese.  
Regular expressions are processed by a dedicated component called Rule-Based Runtime, or RBR for short."
714	Detecting entities with regular expressions	"Creating regex configurations
Begin by creating a module directory inside your notebook. This is a directory inside the notebook file system that is used temporarily to store the files created by the RBR training. This module directory can be the same directory that you created and used for dictionary-based entity extraction. Dictionaries and regular expressions can be used in the same training run.  
To create the module directory in your notebook, enter the following in a code cell. Note that the module directory can't contain a dash (-).  
A regex configuration is a Python dictionary, with the following attributes:  
The regex configurations can be loaded using the following helper methods:  
To load a single regex configuration, use watson_nlp.toolkit.RegexConfig.load(<regex configuration>) To load multiple regex configurations, use watson_nlp.toolkit.RegexConfig.load_all([<regex configuration>)])  
Code sample  
This sample shows you how to load two different regex configurations. The first configuration detects person names. It uses the groups attribute to allow easy access to the full, first and last name at a later stage.  
The second configuration detects acronyms as a sequence of all-uppercase characters. By using the token_boundary attribute, it prevents matches in words that contain both uppercase and lowercase characters."
715	Detecting entities with regular expressions	"Training a model that contains regular expressions
After you have loaded the regex configurations, create an RBR model using the RBR.train() method. In the method, specify:  
The module directory The language of the text The regex configurations to use  
This is the same method that is used to train RBR with dictionary-based extraction. You can pass the dictionary configuration in the same method call.  
Code sample"
716	Detecting entities with regular expressions	"Applying the model on new data
After you have trained the dictionaries, apply the model on new data using the run() method, as you would use on any of the existing pre-trained blocks.  
Code sample  
Output of the code sample:  
Parent topic: Creating your own models"
717	Evaluating the results of a tuning experiment	"Tuning a foundation model in watsonx.ai is an iterative process. You run a tuning experiment and then evaluate the results. If necessary, you change experiment variables and rerun the experiment repeatedly until you are satisfied with the output from the tuned foundation model.  
Check your progress after each experiment run. Find any limitations in your tuning experiment configuration and address them before you assess your training data for potential problems.  
A sample Python notebook named Use watsonx.aito tune IBM granite-13b-instruct-v2 model with Car Rental Company customer satisfaction document is available that contains code for prompt-tuning foundation models in watsonx.ai. The sample notebook has sections for optimizing the experiment parameters and for inferencing the tuned model. For more information, see Tuning a foundation model by using a Python notebook."
718	Evaluating the results of a tuning experiment	"Workflow for improving tuning experiment results
There is no one right set of tuning parameters or training data examples to use. The best tuning parameter settings and data set sizes vary based on your data, the foundation model you use, and the type of task you want the model to do. Follow these steps to save time and stay on track as you experiment.  
You can use the Tuning Studio to complete these steps or use the sample notebook to do them programmatically.  
Before you begin your experimentation, create or preserve a subset of tuning training data to use as a test data set.  
Run a tuning experiment with the default tuning parameters.  
Check the loss function for the experiment run.  
The tuned model is performing well when your loss function has a downward-sloping curve that levels off near zero.  
If necessary, adjust parameter values and rerun the experiment until the loss function levels off to near zero. For more information, see Adjusting tuning parameters.  
Test the quality of the tuned model by submitting prompts from the test data set.  
You can inference the tuned foundation model from the Prompt Lab or programmatically by using the sample notebook. For more information, see Using the notebook to evaluate the tuned model.  
If necessary, revise or augment the training data. For more information, see Addressing data quality problems in tuned model output.  
When new data is introduced, more tuning parameter optimizations might be possible. Rerun the experiment, and then repeat the steps in this workflow starting from Step 3."
719	Evaluating the results of a tuning experiment	"Adjusting tuning parameters
When a tuning experiment run is finished, a loss function graph is displayed. A loss function measures the difference between predicted and actual results with each training run. A successful tuning experiment results in a loss function that has a downward-sloping curve.  
Where the measure of loss drops and levels off is called the convergence. You want the curve to drop, or converge, and the tail end of the curve to reach as close as possible to 0 because it means that the predicted results are as similar as possible to results from the training data.  
If the loss function for your experiment resembles a mountain range with multiple peaks, the loss never converges, or the loss converges but remains at a number much higher than zero, adjust your tuning parameters.  
You can configure the parameter values in the Tuning Studio or use the sample notebook. The sample notebook has steps that help you find the best values to use for your tuning parameters, which is sometimes called hyperparameter optimization. For more information, see Using the notebook to optimize tuning parameter values.  
The following table describes common tuning experiment outcomes and lists actions that might improve the outcomes.  
For more information about how to change tuning parameters and rerun a tuning experiment, see Tuning a foundation model."
720	Evaluating the results of a tuning experiment	"Addressing data quality problems in tuned model output
You know that you're done tuning a model when you can submit zero-shot prompts to the tuned model and get back outputs you expect.  
The following table describes some common training data quality issues and lists actions that you can take to address them.  
Parent topic: Tuning Studio"
721	Deploying scripts in Watson Machine Learning	"When a script is copied to a deployment space, you can deploy it for use. Supported script types are Python scripts. Batch deployment is the only supported deployment type for a script.  
When the script is promoted from a project, your software specification is included. When you create a deployment job for a script, you must manually override the default environment with the correct environment for your script. For more information, see Creating a deployment job"
722	Deploying scripts in Watson Machine Learning	"Learn more
To learn more about supported input and output types and setting environment variables, see Batch deployment details. To learn more about software specifications, see Software specifications and hardware specifications for deployments.  
Parent topic: Managing predictive deployments"
723	Category types	"The categories that are returned by the the Watson Natural Language Processing Categories block are based on the IAB Tech Lab Content Taxonomy, which provides common language categories that can be used when describing content.  
The following table lists the IAB categories taxonomy returned by the Categories block."
724	Loading data through generated code snippets	"You can add automatically generated code to load data from project data assets to a notebook cell. The asset type can be a file or a database connection.  
By clicking in an empty code cell in your notebook, clicking the Code snippets icon () from the notebook toolbar, selecting Read data and an asset from the project, you can:  
Insert the data source access credentials. This capability is available for all data assets that are added to a project. With the credentials, you can write your own code to access the asset and load the data into data structures of your choice.  
Generate code that is added to the notebook cell. The inserted code serves as a quick start to allow you to easily begin working with a data set or connection. For production systems, you should carefully review the inserted code to determine if you should write your own code that better meets your needs.  
When you run the code cell, the data is accessed and loaded into the data structure you selected.  
Notes:  
The ability to provide generated code is disabled for some connections if: If the file type or database connection that you are using doesn't appear in the following lists, you can select to create generic code. For Python this is a StreamingBody object and for R a textConnection object.  
The connection credentials are personal credentials The connection uses a secure gateway link The connection credentials are stored in vaults  
The following tables show you which data source connections (file types and database connections) support the option to generate code. The options for generating code vary depending on the data source, the notebook coding language, and the notebook runtime compute."
725	Loading data through generated code snippets	"Supported database connections
IBM Cognos Analytics  
Microsoft Azure Cosmos DB  
Parent topic: Loading and accessing data in a notebook"
726	Relations extraction	"Watson Natural Language Processing Relations extraction encapsulates algorithms for extracting relations between two entity mentions. For example, in the text Lionel Messi plays for FC Barcelona. a relation extraction model may decide that the entities Lionel Messi and F.C. Barcelona are in a relationship with each other, and the relationship type is works for.  
Capabilities  
Use this model to detect relations between discovered entities.  
The following table lists common relations types that are available out-of-the-box after you have run the entity models.  
Starting with Runtime 23.1, relation extraction is provided as a workflow, which is directly run on the input text."
727	Relations extraction	"Relation extraction in Runtime 23.1
Workflow name  
relations_transformer-workflow_multilingual_slate.153m.distilled  
Supported languages The Relations Workflow is available for the following languages. For a list of the language codes and the corresponding language, see Language codes.  
ar, de, en, es, fr, it, ja, ko, pt  
Code sample  
Output of the code sample  
Parent topic: Watson Natural Language Processing task catalog"
728	Parameters for tuning foundation models	"Tuning parameters configure the tuning experiments that you use to tune the model.  
During the experiment, the tuning model repeatedly adjusts the structure of the prompt so that its predictions can get better over time.  
The following diagram illustrates the steps that occur during a tuning training experiment run. The parts of the experiment flow that you can configure are highlighted. These decision points correspond with experiment tuning parameters that you control.  
You can click the diagram to expand the image.  
The diagram shows the following steps of the experiment:  
Starts from the initialization method that you choose to use to initialize the prompt.  
If the initialization method parameter is set to text, then you must add the initialization text.  
If specified, tokenizes the initialization text and converts it into a prompt vector.  
Reads the training data, tokenizes it, and converts it into batches.  
The size of the batches is determined by the batch size parameter.  
Sends input from the examples in the batch to the foundation model for the model to process and generate output.  
Compares the model's output to the output from the training data that corresponds to the training data input that was submitted. Then, computes the loss gradient, which is the difference between the predicted output and the actual output from the training data.  
At some point, the experiment adjusts the prompt vector that is added to the input based on the performance of the model. When this adjustment occurs depends on how the Accumulation steps parameter is configured.  
Adjustments are applied to the prompt vector that was initialized in Step 2. The degree to which the vector is changed is controlled by the Learning rate parameter. The edited prompt vector is added as a prefix to the input from the next example in the training data, and is submitted to the model as input.  
The process repeats until all of the examples in all of the batches are processed.  
The entire set of batches are processed again as many times as is specified in the Number of epochs parameter.  
Note: No layer of the base foundation model is changed during this process."
729	Parameters for tuning foundation models	"Parameter details
The parameters that you change when you tune a model are related to the tuning experiment, not to the underlying foundation model.  
The following table captures the default parameters for each open source foundation model.  
The default parameters that are used for tuning the granite-13b-instruct-v2 foundation model are adjusted based on the type of task you want the tuned model to do.  
The following table captures the default parameters per supported task type for the granite-13b-instruct-v2 foundation model."
730	Parameters for tuning foundation models	"Parameter details
Segmenting the training data
When an experiment runs, the experiment first breaks the training data into smaller batches, and then trains on one batch at a time. Each batch must fit in GPU memory to be processed. To reduce the amount of GPU memory that is needed, you can configure the tuning experiment to postpone making adjustments until more than one batch is processed. Tuning runs on a batch and its performance metrics are calculated, but the prompt vector isn't changed. Instead, the performance information is collected over some number of batches before the cumulative performance metrics are evaluated.  
Use the following parameters to control how the training data is segmented:  
Batch size Number of labeled examples (also known as samples) to process at one time.  
For example, for a data set with 1,000 examples and a batch size of 10, the data set is divided into 100 batches of 10 examples each.  
If the training data set is small, specify a smaller batch size to ensure that each batch has enough examples in it.  
Accumulation steps: Number of batches to process before the prompt vector is adjusted.  
For example, if the data set is divided into 100 batches and you set the accumulation steps value to 10, then the prompt vector is adjusted 10 times instead of 100 times."
731	Parameters for tuning foundation models	"Parameter details
Initializing prompt tuning
When you create an experiment, you can choose whether to specify your own text to serve as the initial prompt vector or let the experiment generate it for you. These new tokens start the training process either in random positions, or based on the embedding of a vocabulary or instruction that you specify in text. Studies show that as the size of the underlying model grows beyond 10 billion parameters, the initialization method that is used becomes less important.  
The choice that you make when you create the tuning experiment customizes how the prompt is initialized.  
Initialization method: Choose a method from the following options:  
Text: The Prompt Tuning method is used where you specify the initialization text of the prompt yourself. Random: The Prompt Tuning method is used that allows the experiment to add values that are chosen at random to include with the prompt.  
Initialization text: The text that you want to add. Specify a task description or instructions similar to what you use for zero-shot prompting."
732	Parameters for tuning foundation models	"Parameter details
Managing the learning rate
The learning rate parameter determines how much to change the prompt vector when it is adjusted. The higher the number, the greater the change to the vector."
733	Parameters for tuning foundation models	"Parameter details
Choosing the number of training runs to complete
The Number of epochs parameter specifies the number of times to cycle through the training data.  
For example, with a batch size of 10 and a data set with 1,000 examples, one epoch must process 100 batches and update the prompt vector 100 times. If you set the number of epochs to 20, the model is passed through the data set 20 times, which means it processes a total of 2,000 batches during the tuning process.  
The higher the number of epochs and bigger your training data, the longer it takes to tune a model."
734	Parameters for tuning foundation models	"Parameter details
Learn more
Data formats  
Parent topic: Tuning a model"
735	Tutorial: Create a time series anomaly prediction experiment	"This tutorial guides you through using AutoAI and sample data to train a time series experiment to detect if daily electricity usage values are normal or anomalies (outliers).  
When you set up the sample experiment, you load data that analyzes daily electricity usage from Industry A to determine whether a value is normal or an anomaly. Then, the experiment generates pipelines that use algorithms to label these predicted values as normal or an anomaly. After generating the pipelines, AutoAI chooses the best performers, and presents them in a leaderboard for you to review.  
Tech preview This is a technology preview and is not yet supported for use in production environments."
736	Tutorial: Create a time series anomaly prediction experiment	"Data set overview
This tutorial uses the Electricity usage anomalies sample data set. This data set describes the annual electricity usages for Industry A. The first column indicates the electricity usages and the second column indicates the date, which is in a day-by-day format."
737	Tutorial: Create a time series anomaly prediction experiment	"Tasks overview
In this tutorial, follow these steps to create an anomaly prediction experiment:  
Create an AutoAI experiment View the experiment Review experiment results Deploy the trained model Test the deployed model"
738	Tutorial: Create a time series anomaly prediction experiment	"Create an AutoAI experiment
Create an AutoAI experiment and add sample data to your experiment.  
From the navigation menu , click Projects > View all projects.  
Open an existing project or create a new project to store the anomaly prediction experiment.  
On the Assets tab from within your project, click New asset > Build machine learning models automatically.  
Click Resource hub sample > Electricity usage anomalies sample data, then select Next. The AutoAI experiment name and description are pre-populated by the sample data.  
If prompted, associate a Watson Machine Learning instance with your AutoAI experiment.  
Click Associate a Machine Learning service instance and select an instance of Watson Machine Learning. Click Reload to confirm your configuration.  
Click Create."
739	Tutorial: Create a time series anomaly prediction experiment	"View the experiment details
AutoAI pre-populates the details fields for the sample experiment:  
Type series analysis type: Anomaly prediction predicts whether future values in a series are anomalies (outliers). A prediction of 1 indicates a normal value and a prediction of -1 indicates an anomaly. Feature column: industry_a_usage is the predicted value and indicates how much electricity Industry A consumes. Date/Time column: date indicates the time increments for the experiment. For this experiment, there is one prediction value per day. This experiment is optimized for the model performance metric: Average Precision. Average precision evaluates the performance of object detection and segmentation systems.  
Click Run experiment to train the model. The experiment takes several minutes to complete."
740	Tutorial: Create a time series anomaly prediction experiment	"Review the experiment results
The relationship map shows the transformations that are used to create pipelines. Follow these steps to review experiment results and save the pipeline with the best performance.  
The leaderboard lists and saves the three best performing pipelines. Click the pipeline name with Rank 1 to review the details of the pipeline. For details on anomaly prediction metrics, see Creating a time series anomaly prediction experiment. Select the pipeline with Rank 1 and Save the pipeline as a model. The model name is pre-populated with the default name. Click Create to confirm your pipeline selection."
741	Tutorial: Create a time series anomaly prediction experiment	"Deploy the trained model
Before the trained model can make predictions on external values, you must deploy the model. Follow these steps to promote your trained model to a deployment space.  
Deploy the model from the Model details page. To access the Model details page, choose one of these options:  
From the notification displayed when you save the model, click View in project. From the project's Assets, select the model’s name in Models.  
From the Model details page, click the Promote to deployment space icon. Then, select or create a deployment space to deploy the model.  
Select Go to the model in the space after promoting it and click Promote to promote the model."
742	Tutorial: Create a time series anomaly prediction experiment	"Testing the model
After promoting the model to the deployment space, you are ready to test your trained model with new data values.  
Select New Deployment and create a new deployment with the following fields:  
Deployment type: Online Name: Electricity usage online deployment  
Click Create and wait for the status to update to Deployed.  
After the deployment initializes, click the deployment. Use Test input to manually enter and evaluate values or use JSON input to attach a data set.  
Click Predict to see whether there are any anomalies in the values.  
Note:  
-1 indicates an anomaly; 1 indicates a normal value."
743	Tutorial: Create a time series anomaly prediction experiment	"Next steps
Building a time series forecast experiment"
744	Tokens and tokenization	"A token is a collection of characters that has semantic meaning for a model. Tokenization is the process of converting the words in your prompt into tokens.  
You can monitor foundation model token usage in a project on the Environments page on the Resource usage tab."
745	Tokens and tokenization	"Converting words to tokens and back again
Prompt text is converted to tokens before the prompt is processed by foundation models.  
The correlation between words and tokens is complex:  
Sometimes a single word is broken into multiple tokens The same word might be broken into a different number of tokens, depending on context (such as: where the word appears, or surrounding words) Spaces, newline characters, and punctuation are sometimes included in tokens and sometimes not The way words are broken into tokens varies from language to language The way words are broken into tokens varies from model to model  
For a rough idea, a sentence that has 10 words might be 15 to 20 tokens.  
The raw output from a model is also tokens. In the Prompt Lab in IBM watsonx.ai, the output tokens from the model are converted to words to be displayed in the prompt editor."
746	Tokens and tokenization	"Converting words to tokens and back again
Example
The following image shows how this sample input might be tokenized:  
Tomatoes are one of the most popular plants for vegetable gardens. Tip for success: If you select varieties that are resistant to disease and pests, growing tomatoes can be quite easy. For experienced gardeners looking for a challenge, there are endless heirloom and specialty varieties to cultivate. Tomato plants come in a range of sizes.  
Notice a few interesting points:  
Some words are broken into multiple tokens and some are not The word ""Tomatoes"" is broken into multiple tokens at the beginning, but later ""tomatoes"" is all one token Spaces are sometimes included at the beginning of a word-token and sometimes spaces are a token all by themselves Punctuation marks are tokens"
747	Tokens and tokenization	"Token limits
Every model has an upper limit to the number of tokens in the input prompt plus the number of tokens in the generated output from the model. This limit is sometimes called context window length, context window, context length, or maximum sequence length. In the Prompt Lab, an informational message shows how many tokens are used in a prompt submission and the resulting generated output.  
In the Prompt Lab, you use the Max tokens parameter to specify an upper limit on the number of output tokens for the model to generate. The maximum number of tokens that are allowed in the output differs by model. For more information, see the Maximum tokens information in Supported foundation models.  
If the prompts that you need to submit for your use case regularly exceed the context window limit, explore strategies for decreasing the size of your prompt. For more information, see Techniques for overcoming context length limitations.  
You can use the watsonx.ai API to check how many tokens will be calculated for your prompt by a foundation model before you submit it. For more information, see Text tokenization.  
For information about how tokens are measured for billing purposes, see Watson Machine Learning plans.  
Parent topic: Developing generative AI solutions"
748	Deploying Python functions in Watson Machine Learning	You can deploy Python functions in Watson Machine Learning the same way that you can deploy models. Your tools and apps can use the Watson Machine Learning Python client or REST API to send data to your deployed functions the same way that they send data to deployed models. Deploying Python functions gives you the ability to hide details (such as credentials). You can also preprocess data before you pass it to models. Additionally, you can handle errors and include calls to multiple models, all within the deployed function instead of in your application.
749	Deploying Python functions in Watson Machine Learning	"Sample notebooks for creating and deploying Python functions
For examples of how to create and deploy Python functions by using the Watson Machine Learning Python client library, refer to these sample notebooks:  
The notebooks demonstrate the six steps for creating and deploying a function:  
Define the function. Authenticate and define a space. Store the function in the repository. Get the software specification. Deploy the stored function. Send data to the function for processing.  
For links to other sample notebooks that use the Watson Machine Learning Python client library, refer to Using Watson Machine Learning in a notebook."
750	Deploying Python functions in Watson Machine Learning	"Increasing scalability for a function
When you deploy a function from a deployment space or programmatically, a single copy of the function is deployed by default. To increase scalability, you can increase the number of replicas by editing the configuration of the deployment. More replicas allow for a larger volume of scoring requests.  
The following example uses the Python client API to set the number of replicas to 3."
751	Deploying Python functions in Watson Machine Learning	"Learn more
To learn more about defining a deployable Python function, see General requirements for deployable functions section in Writing and storing deployable Python functions. You can deploy a function from a deployment space through the user interface. For more information, see Deployment spaces.  
Parent topic: Managing predictive deployments"
752	Tutorial: AutoAI multivariate time series experiment with Supporting features	"Use sample data to train a multivariate time series experiment that predicts pollution rate and temperature with the help of supporting features that influence the prediction fields.  
When you set up the experiment, you load sample data that tracks weather conditions in Beijing from 2010 to 2014. The experiment generates a set of pipelines that use algorithms to predict future pollution and temperature with supporting features, including dew, pressure, snow, and rain. After generating the pipelines, AutoAI compares and tests them, chooses the best performers, and presents them in a leaderboard for you to review."
753	Tutorial: AutoAI multivariate time series experiment with Supporting features	"Data set overview
For this tutorial, you use the Beijing PM 2.5 data set from the Resource hub. This data set describes the weather conditions in Beijing from 2010 to 2014, which are measured in 1-day steps, or increments. You use this data set to configure your AutoAI experiment and select Supporting features. Details about the data set are described here:  
Each column, other than the date column, represents a weather condition that impacts pollution index. The Resource hub entry shows the origin of the data. You can preview the file before you download the file. The sample data is structured in rows and columns and saved as a .csv file."
754	Tutorial: AutoAI multivariate time series experiment with Supporting features	"Tasks overview
In this tutorial, you follow steps to create a multivariate time series experiment that uses Supporting features:  
Create a project Create an AutoAI experiment Configure the experiment Review experiment results Deploy the trained model Test the deployed model"
755	Tutorial: AutoAI multivariate time series experiment with Supporting features	"Create a project
Follow these steps to create an empty project and download the Beijing PM 2.5 data set from the IBM watsonx Resource hub:  
From the main navigation pane, click Projects > View all projects, then click New Project. a. Click Create an empty project. b. Enter a name and optional description for your project. c. Click Create. From the main navigation panel, click Resource hub and download a local copy of the Beijing PM 2.5 data set."
756	Tutorial: AutoAI multivariate time series experiment with Supporting features	"Create an AutoAI experiment
Follow these steps to create an AutoAI experiment and add sample data to your experiment:  
On the Assets tab from within your project, click New asset > Build machine learning models automatically.  
Specify a name and optional description for your experiment.  
Associate a machine learning service instance with your experiment.  
Choose an environment definition of 8 vCPU and 32 GB RAM.  
Click Create.  
To add sample data, choose one of the these methods:  
If you downloaded your file locally, upload the training data file, PM25.csv by clicking Browse and then following the prompts. If you already uploaded your file to your project, click Select from project, then select the Data asset tab and choose Beijing PM 25.csv."
757	Tutorial: AutoAI multivariate time series experiment with Supporting features	"Configure the experiment
Follow these steps to configure your multivariate AutoAI time series experiment:  
Click Yes for the option to create a Time Series Forecast.  
Choose as prediction columns: pollution, temp.  
Choose as the date/time column: date.  
Click Experiment settings to configure the experiment: a. In the Prediction page, accept the default selection for Algorithms to include. Algorithms that allow you to use Supporting features are indicated by a checkmark in the column Allows supporting features.  
b. Go to the Data Source page. For this tutorial, you will supply future values of Supporting features while testing. Future values are helpful when values for the supporting features are knowable for the prediction horizon. Accept the default enablement for Leverage future values of supporting features. Additionally, accept the default selection for columns that will be used as Supporting features. c. Click Cancel to exit from Experiment settings.  
Click Run experiment to begin the training."
758	Tutorial: AutoAI multivariate time series experiment with Supporting features	"Review experiment results
The experiment takes several minutes to complete. As the experiment trains, the relationship map shows the transformations that are used to create pipelines. Follow these steps to review experiment results and save the pipeline with the best performance.  
Optional: Hover over any node in the relationship map to get details on the transformation for a particular pipeline.  
Optional: After the pipelines are listed on the leaderboard, click Pipeline comparison to see how they differ. For example:  
When the training completes, the top three best performing pipelines are saved to the leaderboard. Click any pipeline name to review details.  
Note: Pipelines that use Supporting features are indicated by **SUP** enhancement.  
Select the pipeline with Rank 1 and click Save as to create your model. Then, click Create. This action saves the pipeline under the Models section in the Assets tab."
759	Tutorial: AutoAI multivariate time series experiment with Supporting features	"Deploy the trained model
Before you can use your trained model to make predictions on new data, you must deploy the model. Follow these steps to promote your trained model to a deployment space:  
You can deploy the model from the model details page. To access the model details page, choose one of these options: Click the Promote to deployment space icon, then select or create a deployment space where the model will be deployed. Optional: Follow these steps to create a deployment space: a. From the Target space list, select Create a new deployment space. b. Enter a name for your deployment space. c. To associate a machine learning instance, go to Select machine learning service (optional) and select a machine learning instance from the list. d. Click Create. Once you select or create your space, click Promote. Click the deployment space link from the notification. From the Assets tab of the deployment space: a. Hover over the model’s name and click the deployment icon . b. In the page that opens, complete the fields: - Select Online as the Deployment type. - Specify a name for the deployment. - Click Create.  
Click the model’s name in the notification that is displayed when you save the model. Open the Assets page for the project that contains the model and click the model’s name in the Machine Learning Model section.  
After the deployment is complete, click the Deployments tab and select the deployment name to view the details page."
760	Tutorial: AutoAI multivariate time series experiment with Supporting features	"Test the deployed model
Follow these steps to test the deployed model from the deployment details page:  
On the Test tab of the deployment details page, go to New observations (optional) spreadsheet and enter the following values: pollution (double): 80.417 temp (double): -5.5 dew (double): -7.083 press (double): 1020.667 wnd_spd (double): 9.518 snow (double): 0 rain (double): 0  
To add future values of Supporting features, go to Future exogenous features (optional) spreadsheet and enter the following values: dew (double): -12.667 press (double): 1023.708 wnd_spd (double): 9.518 snow (double): 0 rain (double): 0.042  
Note: You must provide the same number of values for future exogenous features as the prediction horizon that you set during experiment configuration stage.  
Click Predict. The resulting prediction indicates values for pollution and temperature.  
Note: Prediction values that are shown in the output might differ when you test your deployment."
761	Tutorial: AutoAI multivariate time series experiment with Supporting features	"Learn more
Parent topic: Building a time series experiment"
762	Jupyter Notebook editor	"The Jupyter Notebook editor is largely used for interactive, exploratory data analysis programming and data visualization. Only one person can edit a notebook at a time. All other users can access opened notebooks in view mode only, while they are locked.  
You can use the preinstalled open source libraries that come with the notebook runtime environments, add your own libraries, and benefit from the IBM libraries provided at no extra cost.  
When your notebooks are ready, you can create jobs to run the notebooks directly from the Jupyter Notebook editor. Your job configurations can use environment variables that are passed to the notebooks with different values when the notebooks run."
763	Jupyter Notebook editor	"Learn more
Quick start: Analyze data in a Jupyter notebook Create notebooks in the Jupyter Notebook editor Runtime environments for notebooks Libraries and scripts Code and run notebooks Schedule a notebook Share and publish notebooks  
Parent topic: Notebooks and scripts"
764	Creating notebooks	"You can add a notebook to your project by using one of these methods: creating a notebook file, copying a sample notebook from the Resource hub, adding a notebook from a local file on your computer, or adding a notebook from a URL.    
Watch this short video to learn the basics of Jupyter notebooks.  
This video provides a visual method to learn the concepts and tasks in this documentation."
765	Creating notebooks	"Creating a notebook file in the notebook editor
To create a notebook file in the notebook editor:  
From your project, click New asset > Work with data and models in Python or R notebooks.  
On the New Notebook page, specify the method to use to create your notebook. You can create a blank notebook, choose a sample notebook from the Resource hub, upload a notebook file from your file system, or upload a notebook file from a URL:  
The notebook file that you select to upload must follow these requirements: The URL must be a public URL that is shareable and doesn't require authentication. You can find sample notebooks under Samples and in the Resource hub.  
The file type must be .ipynb. The file name must not exceed 255 characters. The file name must not contain these characters: < > : ” / | ( ) ?  
Specify the runtime environment for the language that you want to use (Python or R). You can select a provided environment template or an environment template which you created and configured under Templates on the Environments page on the Manage tab of your project. For more information on environments, see Notebook environments.  
Click Create. The notebook opens in edit mode.  
Note that the time that it takes to create a new notebook or to open an existing one for editing might vary. If no runtime container is available, a container needs to be created and only after it is available, the Jupyter notebook user interface can be loaded. The time it takes to create a container depends on the cluster load and size. Once a runtime container exists, subsequent calls to open notebooks will be significantly faster.  
The opened notebook is locked by you. For more information, see Locking and unlocking notebooks.  
Tell the service to trust your notebook content and execute all cells.  
When a new notebook is opened in edit mode, the notebook is considered to be untrusted by the Jupyter service by default. When you run an untrusted notebook, content deemed untrusted will not be executed. Untrusted content includes any Javascript, HTML or Javascript in Markdown cells or in any output cells that you did not generate.  
Click Not Trusted in the upper right corner of the notebook. Click Trust to execute all cells."
766	Creating notebooks	"Locking and unlocking notebooks
If you open a notebook in edit mode, this notebook is locked by you. While you hold the lock, only you can make changes to the notebook. All other projects users will see the lock icon on the notebook. Only project administrators are able to unlock a locked notebook and open it in edit mode.  
When you close the notebook, the lock is released and another user can select to open the notebook in edit mode. Note that you must close the notebook while the runtime environment is still active. The notebook lock can't be released for you if the runtime was stopped or is in idle state. If the notebook lock is not released for you, you can unlock the notebook from the project's Assets page. Locking the file avoids possible merge conflicts that might be caused by competing changes to the file."
767	Creating notebooks	"Finding your notebooks
You can find and open notebooks from the Assets page of the project.  
You can open a notebook in view or edit mode. When you open a notebook in view mode, you can't change or run the notebook. You can only change or run a notebook when it is opened in edit mode and started in an environment.  
You can open a notebook by:  
Clicking the notebook. This opens the notebook in view mode. To then open the notebook in edit mode, click the pencil icon () on the notebook toolbar. This starts the environment associated with the notebook. Expanding the three vertical dots on the right of the notebook entry, and selecting View or Edit."
768	Creating notebooks	"Next step
Code and run notebooks"
769	Creating notebooks	"Learn more
Provided CPU runtime environments Provided Spark runtime environments Change the environment runtime used by a notebook  
Parent topic: Jupyter Notebook editor"
770	Frameworks and software specifications in Watson Machine Learning	You can use popular tools, libraries, and frameworks to train and deploy your machine learning models and functions.
771	Frameworks and software specifications in Watson Machine Learning	"Overview of software specifications
Software specifications define the programming language and version that you use for a building a model or a function. You can use software specifications to configure the software that is used for running your models and functions. You can also define the software version to be used and include your own extensions. For example, you can use conda .yml files or custom libraries."
772	Frameworks and software specifications in Watson Machine Learning	"Supported frameworks and software specifications
You can use predefined tools, libraries, and frameworks to train and deploy your machine learning models and functions. Examples of supported frameworks include Scikit-learn, Tensorflow, and more.  
For more information, see Supported deployment frameworks and software specifications."
773	Frameworks and software specifications in Watson Machine Learning	"Lifecycle of supported frameworks and software specifications
Frameworks and software specifications that are supported when you install or upgrade to the latest version of Cloud Pak for Data go through deprecation, constriction, and discontinuation phases in future releases of Cloud Pak for Data, described as follows:  
Deprecated: The runtime environment corresponding to the software specification is supported when you install or upgrade to the latest version of Cloud Pak for Data, but will be removed in an upcoming release.  
For example, the software specification runtime-22.2-py3.10 is deprectaed in Cloud Pak for Data version 4.8.4. In this case, if you install or upgrade to a new Watson Machine Learning service instance, you can create or deploy machine learning models that use runtime-22.2-py3.10. However, the support for runtime-22.2-py3.10 will be discontinued in an upcoming release, so you must upgrade to the latest software specification to make sure that your deployments are supported.  
Constricted: The runtime environment corresponding to the software specification is supported when you upgrade to the latest version of Cloud Pak for Data. However, the runtime environment is not supported when you install the latest version of Cloud Pak for Data.  
For example, the software specification runtime-22.1-py3.9 is constricted in Cloud Pak for Data version 4.8.0. In this case, if you install a new Watson Machine Learning service instance, you cannot create or deploy machine learning models that use runtime-22.1-py3.9. However, if you upgrade from Cloud Pak for Data version 4.7.0 to version 4.8.0, the existing models that you created and deployed with 4.7.0 are supported. You can also create new deployments based on models that use constricted software specifications during an upgrade.  
Discontinued: The runtime environment corresponding to the software specification is not supported when you install or upgrade to the latest version of Cloud Pak for Data."
774	Frameworks and software specifications in Watson Machine Learning	"Lifecycle of supported frameworks and software specifications
Discontinued: The runtime environment corresponding to the software specification is not supported when you install or upgrade to the latest version of Cloud Pak for Data.  
For example, the software specification tensorflow_rt22.1-py3.9-nnpa is discontinued for Cloud Pak for Data version 4.8.0. In this case, if you install or upgrade to a new Watson Machine Learning service instance, you cannot create or deploy machine learning models that use tensorflow_rt22.1-py3.9-nnpa."
775	Frameworks and software specifications in Watson Machine Learning	"Managing outdated frameworks and software specifications
Update software specifications and frameworks in your models when they become outdated. Sometimes, you can seamlessly update your assets. In other cases, you must retrain or redeploy your assets.  
For more information, see Managing outdated software specifications or frameworks.  
Parent topic: Deploying assets with Watson Machine Learning"
776	Working with pre-trained models	Watson Natural Language Processing provides pre-trained models in over 20 languages. They are curated by a dedicated team of experts, and evaluated for quality on each specific language. These pre-trained models can be used in production environments without you having to worry about license or intellectual property infringements.
777	Working with pre-trained models	"Loading and running a model
To load a model, you first need to know its name. Model names follow a standard convention encoding the type of model (like classification or entity extraction), type of algorithm (like BERT or SVM), language code, and details of the type system.  
To find the model that matches your needs, use the task catalog. See Watson NLP task catalog.  
You can find the expected input for a given block class (for example to the Entity Mentions model) by using help() on the block class run() method:  
Watson Natural Language Processing encapsulates natural language functionality through blocks and workflows. Each block or workflow supports functions to:  
load(): load a model run(): run the model on input arguments train(): train the model on your own data (not all blocks and workflows support training) save(): save the model that has been trained on your own data"
778	Working with pre-trained models	"Loading and running a model
Blocks
Two types of blocks exist:  
Blocks that operate directly on the input document Blocks that depend on other blocks  
Workflows run one more blocks on the input document, in a pipeline.  
An example of a block that operates directly on the input document is the Syntax block, which performs natural language processing operations such as tokenization, lemmatization, part of speech tagging or dependency parsing.  
Example: running syntax analysis on a text snippet:  
Blocks that depend on other blocks cannot be applied on the input document directly. They are applied on the output of one or more preceeding blocks. For example, the Keyword Extraction block depends on the Syntax and Noun Phrases block.  
These blocks can be loaded but can only be run in a particular order on the input document. For example:"
779	Working with pre-trained models	"Loading and running a model
Workflows
Workflows are predefined end-to-end pipelines from a raw document to a final block, where all necessary blocks are chained as part of the workflow pipeline.  
Starting with Runtime 23.1, you can call the Entity Mentions workflow. Refer to this sample:  
Parent topic: Watson Natural Language Processing library"
780	Customizing details for a use case or factsheet	You can programmatically customize the information that is collected in factsheets for AI use cases. Use customized factsheets as part of your AI Governance strategy.
781	Customizing details for a use case or factsheet	"Updating a model or model use case programmatically
You might want to update a model use case or model factsheet with additional information. For example, some companies have a standard set of details they want to accompany a model use case or model facts.  
Currently, you must update the tenant-level asset types by modifying the user attributes that uses the Watson Data REST API to update the asset."
782	Customizing details for a use case or factsheet	"Updating a custom asset type
Follow these steps to update a custom asset type:  
Provide the bss_account_id query parameter for the getcatalogtype method.  
Provide asset_type as model_entry_user if you are updating attributes for model_entry. Provide asset_type as modelfacts_user if you are updating attributes for model facts.  
Retrieve the current asset type definition by using the getcatalogtype method where asset_type is either modelfacts_user or model_entry_user.  
Update the current asset type definition with the custom attributes by adding them to properties JSON object following the schema that is defined in the API documentation. The following types of attributes are supported to view and edit from the user interface of the model use case or model:  
string date integer  
After the JSON is updated with the new properties, start the changes by using the replaceassettype method. Provide the asset_type , bss_account_id, and request payload.  
When the update is complete, you can view the custom attributes in the AI use case details page and model details page."
783	Customizing details for a use case or factsheet	"Example 1: Retrieving and updating the model_entry_user asset type
Note:  
This example updates the use case user data. You can use the same format but substitute modelfacts_user to retrieve and update details for the model factsheet.  
This curl command retrieves the asset type model_entry_user:  
This snippet is a sample response payload for model use case user details:  
This curl command updates the model_entry_user asset type:  
The requestbody.json contents look like this:"
784	Customizing details for a use case or factsheet	"Updating user details by using the Python client
You can also update and replace an asset type with properties by using a Python script. For details, see fact sheet elements description.  
After you update asset type definitions with custom attributes, you can provide values for those attributes from the model use case overview and model details pages. You can also update values to the custom attributes that use these Python API client methods:  
Model Asset Utilities Model Entry Utilities"
785	Customizing details for a use case or factsheet	"Capturing cell facts for a model
When a data scientist develops a model in a notebook, they generate visualizations for key model details, such as ROC curve, confusion matrix, panda profiling report, or the output of any cell execution. To capture those facts as part of a model use case, use the 'capture_cell_facts` function in the AI Factsheets Python client library."
786	Customizing details for a use case or factsheet	"Troubleshooting custom fields
After you customize fields and make them available to users, a user trying to update fields in the Additional details section of model details might get this error:  
If the user already has edit permission on the model and is still getting the error message, follow these steps to resolve it.  
{ ""name"": ""modelfacts_system"", ""entity"": { } }  
Invoke the API command for createassetattributenewv2.  
Use this payload with the command:  
where asset_id is the model_id. Enter either project_id or space_id or catalog_id where the model exists."
787	Customizing details for a use case or factsheet	"Learn more
Find out about working with an inventory programmatically, by using the IBM_AIGOV_FACTS_CLIENT documentation.  
Parent topic: Governing AI assets"
788	Batch deployment input details for Python functions	"Follow these rules when you are specifying input details for batch deployments of Python functions.  
Data type summary table:  
You can deploy Python functions in Watson Machine Learning the same way that you can deploy models. Your tools and apps can use the Watson Machine Learning Python client or REST API to send data to your deployed functions in the same way that they send data to deployed models. Deploying functions gives you the ability to:  
Hide details (such as credentials) Preprocess data before you pass it to models Handle errors Include calls to multiple models All of these actions take place within the deployed function, instead of in your application."
789	Batch deployment input details for Python functions	"Data sources
If you are specifying input/output data references programmatically:  
Data source reference type depends on the asset type. Refer to the Data source reference types section in Adding data assets to a deployment space.  
Notes:  
For connections of type Cloud Object Storage or Cloud Object Storage (infrastructure), you must configure Access key and Secret key, also known as HMAC credentials. The environment variables parameter of deployment jobs is not applicable. Make sure that the output is structured to match the output schema that is described in Execute a synchronous deployment prediction."
790	Batch deployment input details for Python functions	"Learn more
Deploying Python functions.  
Parent topic: Batch deployment input details by framework"
791	Managing deployment of asset versions	Learn how to manage versions of models, scripts, and functions in your deployment space.
792	Managing deployment of asset versions	"Creating versions
Currently, you can create versions of a model, script, or function programmatically, by using the Watson Machine Learning REST API, the Watson Machine Learning Python client library, or the CPDCTL command-line interface tool.  
For example, by using REST API calls:  
POST /v2/assets/<asset_id>/revisions GET /v2/assets/<asset_id> GET /v2/assets/<asset_id>/revision_id=<rev_id>  
To create a version, use:  
To reference the current state of an asset, use:  
To reference a specific version of an asset:"
793	Managing deployment of asset versions	"Using versions in a deployment space
In a space, versions you create programmatically can be used when you:  
Deploy a model, script, or function Update your deployment  
For example, you can create a deployment by using a version that uses a specific hardware or software configuration. Then, you can update the deployment with a version that uses a different configuration seamlessly, without stopping the deployment."
794	Managing deployment of asset versions	"Viewing version information in a deployment space
If an asset has versions, you can view information about the versions from the asset detail page.  
From the Assets tab of the project or space, click the name of the model, script, or function to access the details. Click the Versions() icon to access version-related information."
795	Managing deployment of asset versions	"Promoting an asset version from a project to a deployment space
When you promote a version of an asset to a space, the version information is not preserved. That is, if you promote Version B of Asset A to a space, the space shows Asset A and that is the current and only version available. However, if you have access to the project that contains the promoted asset, you can view information about the versions from the asset information page.  
In space, you can create new versions programmatically, in the same way that you create them in a project."
796	Managing deployment of asset versions	"Viewing version information in a deployment space
You can view your deployment version information by going to the Assets tab in your deployment space. The badge indicates that your deployed asset has versions available."
797	Managing deployment of asset versions	"Deploying a version of an asset
When you create a deployment of an asset, the default asset is the current, most recent version. You can optionally choose a different version to deploy.  
To deploy your asset with a different version, follow these steps:  
Go to the Assets tab in your deployment space. Click the action menu icon and choose Deploy. In the dialog box, choose the version that you want to deploy in the Version field. Enter the details and click Create."
798	Managing deployment of asset versions	"Updating a deployment with a different version
When you update a deployment, you can choose a different version of the model, script, or function to replace the deployed asset.  
To update your deployment with a different version, follow these steps:  
Go to the Deployments tab in your deployment space. Click the action menu icon and choose Edit. In the Edit deployment dialog box, select the version with which you want to update your deployment and click Replace asset. Click Save.  
Parent topic: Assets in deployment spaces"
799	Compute resource options for RStudio in projects	"When you run RStudio in a project, you choose an environment template for the runtime environment. The environment template specifies the type, size, and power of the hardware configuration, plus the software template.  
Types of environments Default environment templates Compute usage Runtime scope Changing the runtime"
800	Compute resource options for RStudio in projects	"Types of environments
You can use this type of environment with RStudio:  
Default RStudio CPU environments for standard workloads"
801	Compute resource options for RStudio in projects	"Default environment templates
You can select any of the following default environment templates for RStudio in a project. These default environment templates are listed under Templates on the Environments page on the Manage tab of your project. All environment templates use RStudio with Runtime 23.1 on the R 4.2 programming language.  
If you don't explicitly select an environment, Default RStudio M is the default. The hardware configuration of the available RStudio environments is preset and cannot be changed.  
For compute-intensive processing on a large data set, consider pushing your data processing to Spark from your RStudio session. See Using Spark in RStudio.  
To prevent consuming extra capacity unit hours (CUHs), stop all active RStudio runtimes when you no longer need them. See RStudio idle timeout."
802	Compute resource options for RStudio in projects	"Compute usage in projects
RStudio consumes compute resources as CUH from the Watson Studio service in projects.  
You can monitor the Watson Studio CUH consumption on the Resource usage page on the Manage tab of your project."
803	Compute resource options for RStudio in projects	"Runtime scope
An RStudio environment runtime is always scoped to a project and a user. Each user can only have one RStudio runtime per project at one time. If you start RStudio in a project in which you already have an active RStudio session, the existing active session is disconnected and you can continue working in the new RStudio session."
804	Compute resource options for RStudio in projects	"Changing the RStudio runtime
If you notice that processing is very slow, you can restart RStudio and select a larger environment runtime.  
To change the RStudio environment runtime:  
Save any data from your current session before switching to another environment. Stop the active RStudio runtime under Tool runtimes on the Environments page on the Manage tab of your project. Restart RStudio from the Launch IDE menu on your project's action bar and select another environment with the compute power and memory capacity that better meets your data processing requirements."
805	Compute resource options for RStudio in projects	"Learn more
RStudio Monitoring account resource usage  
Parent topic: Choosing compute resources for tools"
806	Create the data handler	Each party in a Federated Learning experiment must get a data handler to process their data. You or a data scientist must create the data handler. A data handler is a Python class that loads and transforms data so that all data for the experiment is in a consistent format.
807	Create the data handler	"About the data handler class
The data handler performs the following functions:  
Accesses the data that is required to train the model. For example, reads data from a CSV file into a Pandas data frame. Pre-processes the data so data is in a consistent format across all parties. Some example cases are as follows: The data handler ensures that the data formatting is in agreement.  
The Date column might be stored as a time epoch or timestamp. The Country column might be encoded or abbreviated.  
Optional: feature engineer as needed.  
The following illustration shows how a data handler is used to process data and make it consumable by the experiment:"
808	Create the data handler	"Data handler template
A general data handler template is as follows:  
Parameters  
your_data_file_type: This can be any string field. For example, if your data set is in csv format, your_data_file_type can be ""CSV"", "".csv"", ""csv"", ""csv_file"" and more."
809	Create the data handler	"Data handler template
Return a data generator defined by Keras or Tensorflow
The following is a code example that needs to be included as part of the get_data function to return a data generator defined by Keras or Tensorflow:"
810	Create the data handler	"Data handler examples
MNIST Keras data handler Adult XGBoost data handler  
Parent topic: Creating a Federated Learning experiment"
811	Removing harmful language from model input and output	"AI guardrails removes potentially harmful content, such as hate speech, abuse, and profanity, from foundation model output and input.  
The AI guardrails feature in the Prompt Lab is powered by AI that applies a classification task to foundation model input and output text. The sentence classifier, which is also referred to as a hate, abuse, and profanity (HAP) detector or HAP filter, was created by fine-tuning a large language model from the Slate family of encoder-only NLP models built by IBM Research.  
The classifier breaks the model input and output text into sentences, and then reviews each sentence to find and flag harmful content. The classifier assesses each word, relationships among the words, and the context of the sentence to determine whether a sentence contains harmful language. The classifier then assigns a score that represents the likelihood that inappropriate content is present.  
AI guardrails in the Prompt Lab detects and flags the following types of language:  
Hate speech: Expressions of hatred toward an individual or group based on attributes such as race, religion, ethnic origin, sexual orientation, disability, or gender. Hate speech shows an intent to hurt, humiliate, or insult the members of a group or to promote violence or social disorder.  
Abusive language: Rude or hurtful language that is meant to bully, debase, or demean someone or something.  
Profanity: Toxic words such as expletives, insults, or sexually explicit language.  
The AI guardrails feature is supported when you inference natural-language foundation models and can detect harmful content in English text only. AI guardrails are not applicable to programmatic-language foundation models."
812	Removing harmful language from model input and output	"Removing harmful language from input and output in Prompt Lab
To remove harmful content when you're working with foundation models in the Prompt Lab, set the AI guardrails switcher to On.  
The AI guardrails feature is enabled automatically for all natural language foundation models in English.  
After the feature is enabled, when you click Generate, the filter checks all model input and output text. Inappropriate text is handled in the following ways:  
Input text that is flagged as inappropriate is not submitted to the foundation model. The following message is displayed instead of the model output:  
[The input was rejected as inappropriate]  
Model output text that is flagged as inappropriate is replaced with the following message:  
[Potentially harmful text removed]"
813	Removing harmful language from model input and output	"Programmatic alternative
You have more options for filtering content when you inference foundation models by using the watsonx.ai API. For example, you can apply the HAP filter to only model output and control the sensitivity of the filter. You can also apply a PII filter to flag content that might contain personally identifiable information. For more information, see the moderations field details in Text generation."
814	Removing harmful language from model input and output	"Learn more
Techniques for avoiding undesirable output AI risk atlas Security and privacy  
Parent topic: Foundation models"
815	Tracking a machine learning model	Track machine learning models in an AI use case to meet governance and compliance goals.
816	Tracking a machine learning model	"Tracking machine learning models in an AI use case
Track machine learning models that are trained in a project and saved as a model asset. You can add a machine learning model to an AI use case from a project or space.  
Open the project or space that contains the model asset that you want to govern. From the action menu for the asset, click Track in AI use case. Select an existing AI use case or follow the prompts to create a new one. Choose an existing approach or create a new approach. An approach creates a version set for all assets in the same approach. Choose a version numbering scheme. All of the assets in an approach share a common version. Choose from:  
Experimental if you plan to update frequently. Stable if the assets are not changing rapidly. Custom if you want to start a new version number. Version numbering must follow a schema of major.minor.patch.  
Watch this video to see how to track a machine learning model in an AI use case.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Once tracking is enabled, all collaborators for the use case can review details for the asset.  
For a machine learning model, facts include creation details, training data used, and information from evaluation metrics.  
For details on tracking a machine learning model that is created in a Jupyter Notebook or trained with a third-party machine learning provider, see Tracking external models."
817	Tracking a machine learning model	"Learn more
Parent topic: Tracking assets in use cases"
818	Classifying text with a custom classification model	"You can train your own models for text classification using strong classification algorithms from three different families:  
Classic machine learning using SVM (Support Vector Machines) Deep learning using CNN (Convolutional Neural Networks) A transformer-based algorithm using a pre-trained transformer model:  
Runtime 23.1: Slate IBM Foundation model Runtime 22.x: Google BERT Multilingual model  
The Watson Natural Language Processing library also offers an easy to use Ensemble classifier that combines different classification algorithms and majority voting.  
The algorithms support multi-label and multi-class tasks and special cases, like if the document belongs to one class only (single-label task), or binary classification tasks.  
Note:  
Training classification models is CPU and memory intensive. Depending on the size of your training data, the environment might not be large enough to complete the training. If you run into issues with the notebook kernel during training, create a custom notebook environment with a larger amount of CPU and memory, and use that to run your notebook. Especially for transformer-based algorithms, you should use a GPU-based environment, if it is available to you. See Creating your own environment template.  
Topic sections:  
Input data format for training Input data requirements Stopwords Training SVM algorithms Training the CNN algorithm Training the transformer algorithm by using the Slate IBM Foundation model Training a custom transformer model by using a model provided by Hugging Face Training the multilingual BERT algorithm Training an ensemble model Training best practices Applying the model on new data Choosing the right algorithm for your use case"
819	Classifying text with a custom classification model	"Input data format for training
Classification blocks accept training data in CSV and JSON formats.  
Example 1,label 1 Example 2,label 1,label 2 [ { ""text"": ""Example 1"", ""labels"": [""label 1""] }, { ""text"": ""Example 2"", ""labels"": [""label 1"", ""label 2""] }, { ""text"": ""Example 3"", ""labels"": [] } ]  
The CSV Format  
The CSV file should contain no header. Each row in the CSV file represents an example record. Each record has one or more columns, where the first column represents the text and the subsequent columns represent the labels associated with that text.  
Note:  
The SVM and CNN algorithms do not support training data where an instance has no labels. So, if you are using the SVM algorithm, or the CNN algorithm, or an Ensemble including one of these algorithms, each CSV row must have at least one label, i.e., 2 columns. The BERT-based and Slate-based Transformer algorithms support training data where each instance has 0, 1 or more than one label.  
The JSON Format  
The training data is represented as an array with multiple JSON objects. Each JSON object represents one training instance, and must have a text and a labels field. The text represents the training example, and labels stores the labels associated with the example (0, 1, or more than one label).  
Note:  
""labels"": [] denotes an example with no labels. The SVM and CNN algorithms do not support training data where an instance has no labels. So, if you are using the SVM algorithm, or the CNN algorithm, or an Ensemble including one of these algorithms, each JSON object must have at least one label. The BERT-based and Slate-based Transformer algorithms support training data where each instance has 0, 1 or more than one label."
820	Classifying text with a custom classification model	"Input data requirements
For SVM and CNN algorithms:  
Minimum number of unique labels required: 2 Minimum number of text examples required per label: 5  
For the BERT-based and Slate-based Transformer algorithms:  
Minimum number of unique labels required: 1 Minimum number of text examples required per label: 5  
Note that the training data in CSV or JSON format is converted to a DataStream before training. Instead of training data files, you can also pass data streams directly to the training functions of classification blocks."
821	Classifying text with a custom classification model	"Stopwords
You can provide your own stopwords that will be removed during preprocessing. Stopwords file inputs are expected in a standard format: a single text file with one phrase per line. Stopwords can be provided as a list or as a file in a standard format.  
Stopwords can be used only with the Ensemble classifier."
822	Classifying text with a custom classification model	"Training SVM algorithms
SVM is a support vector machine classifier that can be trained using predictions on any kind of input provided by the embedding or vectorization blocks as feature vectors, for example, by USE (Universal Sentence Encoder) embeddings and TF-IDF vectorizers. It supports multi-class and multi-label text classification and produces confidence scores via Platt Scaling.  
For all options that are available for configuring SVM training, enter:  
To train SVM algorithms:  
import watson_nlp from watson_core.data_model.streams.resolver import DataStreamResolver from watson_nlp.blocks.classification.svm import SVM training_data_file = ""<ADD TRAINING DATA FILE PATH>"" # Create datastream from training data data_stream_resolver = DataStreamResolver(target_stream_type=list, expected_keys={'text': str, 'labels': list}) training_data = data_stream_resolver.as_data_stream(training_data_file) # Load a Syntax model syntax_model = watson_nlp.load('syntax_izumo_en_stock') # Create Syntax stream text_stream, labels_stream = training_data[0], training_data[1] syntax_stream = syntax_model.stream(text_stream) # download embedding use_embedding_model = watson_nlp.load('embedding_use_en_stock') use_train_stream = use_embedding_model.stream(syntax_stream, doc_embed_style='raw_text') # NOTE: doc_embed_style can be changed to `avg_sent` as well. For more information check the documentation for Embeddings # Or the USE run function API docs use_svm_train_stream = watson_nlp.data_model.DataStream.zip(use_train_stream, labels_stream) # Train SVM using Universal Sentence Encoder (USE) training stream classification_model = SVM.train(use_svm_train_stream)  
Begin with these preprocessing steps:  
Train the classification model using USE embeddings. See Pretrained USE embeddings available out-of-the-box for a list of the pretrained blocks that are available."
823	Classifying text with a custom classification model	"Training SVM algorithms
Pretrained USE embeddings available out-of-the-box
USE embeddings are wrappers around Google Universal Sentence Encoder embeddings available in TFHub. These embeddings are used in the document classification SVM algorithm.  
The following table lists the pretrained blocks for USE embeddings that are available and the languages that are supported. For a list of the language codes and the corresponding language, see Language codes.  
When using USE embeddings, consider the following:  
Choose embedding_use_en_stock if your task involves English text.  
Choose one of the multilingual USE embeddings if your task involves text in a non-English language, or you want to train multilingual models.  
The USE embeddings exhibit different trade-offs between quality of the trained model and throughput at inference time, as described below. Try different embeddings to decide the trade-off between quality of result and inference throughput that is appropriate for your use case.  
embedding_use_multi_small has reasonable quality, but it is fast at inference time embedding_use_en_stock is a English-only version of embedding_embedding_use_multi_small, hence it is smaller and exhibits higher inference throughput embedding_use_multi_large is based on Transformer architecture, and therefore it provides higher quality of result, with lower throughput at inference time"
824	Classifying text with a custom classification model	"Training the CNN algorithm
CNN is a simple convolutional network architecture, built for multi-class and multi-label text classification on short texts. It utilizes GloVe embeddings. GloVe embeddings encode word-level semantics into a vector space. The GloVe embeddings for each language are trained on the Wikipedia corpus in that language. For information on using GloVe embeddings, see the open source GloVe embeddings documentation.  
For all the options that are available for configuring CNN training, enter:  
To train CNN algorithms:"
825	Classifying text with a custom classification model	"Training the transformer algorithm by using the IBM Slate model
The transformer algorithm using the pretrained Slate IBM Foundation model can be used for multi-class and multi-label text classification on short texts.  
The pretrained Slate IBM Foundation model is only available in Runtime 23.1.  
For all the options available for configuring Transformer training, enter:  
To train Transformer algorithms:"
826	Classifying text with a custom classification model	"Training a custom transformer model by using a model provided by Hugging Face
Note: This training method is only available in Runtime 23.1.  
You can train your custom transformer-based model by using a pretrained model from Hugging Face.  
To use a Hugging Face model, specify the model name as the pretrained_model_resource parameter in the train method of watson_nlp.blocks.classification.transformer.Transformer. Go to https://huggingface.co/models to copy the model name.  
To get a list of all the options available for configuring a transformer training, type this code:  
For information on how to train transformer algorithms, refer to this code example:"
827	Classifying text with a custom classification model	"Training an ensemble model
The Ensemble model is a weighted ensemble of these three algorithms: CNN, SVM with TF-IDF and SVM with USE. It computes the weighted mean of a set of classification predictions using confidence scores. The ensemble model is very easy to use."
828	Classifying text with a custom classification model	"Training an ensemble model
Using the Runtime 23.1 environment
The GenericEnsemble classifier allows more flexibility for the user to choose from the three base classifiers TFIDF-SVM, USE-SVM and CNN. For texts ranging from 50 to 1000 characters, using the combination of TFIDF-SVM and USE-SVM classifiers often yields a good balance of quality and performance. On some medium or long documents (500-1000+ characters), adding the CNN to the Ensemble could help increase quality, but it usually comes with a significant runtime performance impact (lower throughput and increased model loading time).  
For all of the options available for configuring Ensemble training, enter:  
To train Ensemble algorithms:"
829	Classifying text with a custom classification model	"Training an ensemble model
Pretrained stopword models available out-of-the-box
The text model for identifying stopwords is used in training the document classification ensemble model.  
The following table lists the pretrained stopword models and the language codes that are supported (xx stands for the language code). For a list of the language codes and the corresponding language, see Language codes."
830	Classifying text with a custom classification model	"Training best practices
There are certain constraints on the quality and quantity of data to ensure that the classifications model training can complete in a reasonable amount of time and also meets various performance criteria. These are listed below. Note that none are hard restrictions. However, the further one deviates from these guidelines, the greater the chance that the model fails to train or the model will not be satisfactory.  
Data quantity  
The highest number of classes classification model has been tested on is ~1200. The best suited text size for training and testing data for classification is around 3000 code points. However, larger texts can also be processed, but the runtime performance might be slower. Training time will increase based on the number of examples and number of labels. Inference time will increased based on the number of labels.  
Data quality  
Size of each sample (for example, number of phrases in each training sample) can affect quality. Class separation is important. In other words, classes among the training (and test) data should be semantically distinguishable from each another in order to avoid misclassifications. Since the classifier algorithms in Watson Natural Language Processing rely on word embeddings, training classes that contain text examples with too much semantic overlap may make high-quality classification computationally intractable. While more sophisticated heuristics may exist for assessing the semantic similarity between classes, you should start with a simple ""eye test"" of a few examples from each class to discern whether or not they seem adequately separated. It is recommended to use balanced data for training. Ideally there should be roughly equal numbers of examples from each class in the training data, otherwise the classifiers may be biased towards classes with larger representation in the training data. It is best to avoid circumstances where some classes in the training data are highly under-represented as compared to other classes.  
Limitations and caveats:"
831	Classifying text with a custom classification model	"Training best practices
Limitations and caveats:  
The BERT classification block has a predefined sequence length of 128 code points. However, this can be configured at train time by changing the parameter max_seq_length. The maximum value allowed for this parameter is 512. This means that the BERT classification block can only be used to classify short text. Text longer than max_seq_length is trimmed and discarded during classification training and inference. The CNN classification block has a predefined sequence length of 1000 code points. This limit can be configured at train time by changing the parameter max_phrase_len. There is no maximum limit for this parameter, but increasing the maximum phrase length will affect CPU and memory consumption. SVM blocks do not have such limit on sequence length and can be used with longer texts."
832	Classifying text with a custom classification model	"Applying the model on new data
After you have trained the model on a data set, apply the model on new data using the run() method, as you would use on any of the existing pre-trained blocks.  
Sample code  
# run Ensemble model on new text ensemble_prediction = ensemble_classification_model.run(""new input text"") # run Syntax model first syntax_result = syntax_model.run(""new input text"") # run CNN model on top of syntax result cnn_prediction = cnn_classification_model.run(syntax_result)  
For the Ensemble and BERT models, for example for Ensemble:  
For SVM and CNN models, for example for CNN:"
833	Classifying text with a custom classification model	"Choosing the right algorithm for your use case
You need to choose the model algorithm that best suits your use case.  
When choosing between SVM, CNN, and Transformers, consider the following:  
BERT and Transformer-based Slate  
Choose when high quality is required and higher computing resources are available.  
CNN  
Choose when decent size data is available Choose if GloVe embedding is available for the required language Choose if you have the option between single label versus multi-label CNN fine tunes embeddings, so it could give better performance for unknown terms or newer domains.  
SVM  
Choose if an easier and simpler model is required SVM has the fastest training and inference time Choose if your data set size is small  
If you select SVM, you need to consider the following when choosing between the various implementations of SVM:  
SVMs train multi-label classifiers. The larger the number of classes, the longer the training time. TF-IDF: USE:  
Choose TF-IDF vectorization with SVM if the data set is small, i.e. has a small number of classes, a small number of examples and shorter text size, for example, sentences containing fewer phrases. TF-IDF with SVM can be faster than other algorithms in the classification block. Choose TF-IDF if embeddings for the required language are not available.  
Choose Universal Sentence Encoder (USE) with SVM if the data set has one or more sentences in input text. USE can perform better on data sets where understanding the context of words or sentences is important.  
The Ensemble model combines multiple individual (diverse) models together to deliver superior prediction power. Consider the following key data for this model type:  
The ensemble model combines CNN, SVM with TF-IDF and SVM with USE. It is the easiest model to use. It can give better performance than the individual algorithms. It works for all kinds of data sets. However, training time for large datasets (more than 20000 examples) can be high. An ensemble model allows you to set weights. These weights decides how the ensemble model combines the results of individual classifiers. Currently, the selection of weights is a heuristics and needs to be set by trial and error. The default weights that are provided in the function itself are a good starting point for the exploration.  
Parent topic: Creating your own models"
834	Security and privacy for foundation models	"Your work with foundation models is secure and private, in the same way that all your work on watsonx is secure and private. IBM cannot access your prompts or tuned models.  
The following table summarizes the privacy and security of your foundation model work in watsonx."
835	Security and privacy for foundation models	"Foundation model privacy and security
Foundation models that you interact with through watsonx or tune in Tuning Studio are hosted in IBM Cloud. Your data is not sent to any third-party or open source platforms.  
The foundation model prompts that you create and engineer in the Prompt Lab or send by using the API are accessible only by you. Your prompts are used only by you and are submitted only to models you choose. Your prompt text is not accessible or used by IBM or any other person or organization.  
The tuned foundation models that you create in Tuning Studio or by using the API are accessible only to you. Your tuned foundation models are not accessible or used by IBM or any other person or organization.  
You control whether prompts, model choices, and prompt engineering parameter settings are saved. When saved, your data is stored in a dedicated IBM Cloud Object Storage bucket that is associated with your project.  
The tuning experiment, tuning parameters, and training data that you use to tune a foundation model are accessible only by you. Tuning experiment and training data assets that you create are stored in your project storage bucket.  
Metadata about your data assets is stored in a multi-tenanted database. Metadata includes the asset name, description, and tags. The asset data itself is stored in a single-tenant instance of an IBM Cloud Object Storage bucket that is associated with your account and is not shared. Each bucket in the instance is further isolated by permissions. Two projects that use the same IBM Cloud Object Storage instance cannot view each other’s asset data, for example. Data that is stored in your project storage bucket is encrypted at rest and in motion, optionally with your own key. You can delete your stored data at any time by deleting the associated asset from your project or by deleting the project.  
IBM does measure foundation model usage by account for the purposes of billing, infrastructure planning, and model lifecycle management."
836	Security and privacy for foundation models	"Privacy of text in Prompt Lab during a session
Text that you submit by clicking Generate from the prompt editor in Prompt Lab is reformatted as tokens, and then submitted to the foundation model you choose. The submitted message is encrypted in transit.  
Your prompt text is not saved unless you choose to save your work.  
Unsaved prompt text is kept in the web page until the session expires or the page is refreshed, at which time the prompt text is deleted."
837	Security and privacy for foundation models	"Privacy and security of saved work in Prompt Lab
How work that you save in the Prompt Lab is managed differs based on the asset type that you choose to save:  
Prompt template asset: The current prompt text, model, prompt engineering parameters, and any prompt variables are saved as a prompt template asset and stored in the IBM Cloud Object Storage bucket that is associated with your project. For a prompt that you create in chat mode, the foundation model output is saved also. Prompt template assets are retained until they are deleted or changed by you. When autosave is on, if you open a saved prompt and change the text, the text in the saved prompt template asset is replaced.  
Prompt session asset: A prompt session asset includes the prompt input text, model, prompt engineering parameters, and model output. After you create the prompt session asset, prompt information for up to 500 submitted prompts is stored in the project storage bucket where it is retained for 30 days.  
Notebook asset: Your prompt, model, prompt engineering parameters, and any prompt variables are formatted as Python code and stored as a notebook asset in the project storage bucket.  
Only people with Admin or Editor role access to the project or the project storage bucket can view saved assets. You control who can access your project and its associated Cloud Object Storage bucket.  
For more information about asset security, see Data security. For more information about managing project access, see Project collaborators"
838	Security and privacy for foundation models	"Logging and text in the Prompt Lab
Nothing that you add to the prompt editor or submit to a model from the Prompt Lab or by using the API is logged by IBM. Messages that are generated by foundation models and returned to the Prompt Lab also are not logged."
839	Security and privacy for foundation models	"Ownership of your content and foundation model output
Content that you upload into watsonx is yours.  
IBM does not use the content that you upload to watsonx or the output that is generated by a foundation model to further train or improve any IBM developed models.  
IBM does not claim to have any ownership rights to any foundation model outputs. You remain solely responsible for your content and the output of any tuned or untuned foundation models that you work with in watsonx."
840	Security and privacy for foundation models	"Privacy and security in watsonx.governance
If watsonx.governance is provisioned, you can choose to enable evaluation and monitoring of foundation models to support governance functions such as drift detection and explainability. When enabled, the request payloads of foundation model input and output are stored in a watsonx.governance database that you administer.  
You own the data in the database and you control who is allowed to access the data. To meet added encryption or access control needs, you can choose a setup method that gives you more control over the database and deployment space assets that are used.  
Any stored data is purged when monitoring is disabled or when the monitored prompt template asset or model deployment is deleted."
841	Security and privacy for foundation models	"Learn more
Setting up IBM Cloud Object Storage for use with watsonx Managing assets in projects Watsonx terms IBM Watson Machine Learning terms IBM Watson Studio terms  
Parent topic: Foundation models"
842	Techniques for overcoming context length limitations	"Each foundation model has a maximum context length, which is the maximum number of tokens that are allowed in the input plus the generated output for a prompt. If you frequently meet this limit when you use a foundation model to complete a task, try one of these workarounds.  
The maximum context length varies from 4,096 to 32,768 tokens, depending on the foundation model. The context length limit can be a problem, especially when you use a foundation model for retrieval-augmented generation, summarization, or conversational tasks.  
The following techniques can help you give a foundation model the context it needs to complete a task without exceeding the model's context window limit.  
Send fewer tokens with RAG inputs Divide and conquer complex tasks Summarize dialog context Lower your token count with zero-shot prompts"
843	Techniques for overcoming context length limitations	"Sending fewer tokens with RAG inputs
Retrieval-augmented generation (RAG) is a technique in which a foundation model is augmented with knowledge from external sources to generate text. In the retrieval step, relevant documents from an external source are identified from the user’s query. In the generation step, portions of those documents are included in the foundation model prompt to generate a response that is grounded in the retrieved documents.  
If you send a prompt with too much grounding information, the input might exceed the foundation model's context window limit. Try some of the following techniques to work around this limitation.  
Classify content by relevancy Summarize long documents Generate text embeddings from the content  
For more information about RAG, see Retrieval-augmented generation (RAG)."
844	Techniques for overcoming context length limitations	"Sending fewer tokens with RAG inputs
Classify content by relevancy
Use a foundation model that is good at classifying content to first determine whether a document can effectively answer a question before it is included in a prompt.  
For example, you can use a prompt such as, For each question and sample document, check whether the document contains the information that is required to answer the question. If the sample document appears to contain information that is useful for answering the question, respond 'Yes'. Otherwise, respond 'No'.  
If you set the Max tokens parameter to 1, the model is more likely to return a Yes or No answer.  
Use this classification method to find and remove irrelevant documents or sentences that are retrieved by search. Then, recombine the relevant content to send with the question in the foundation model prompt."
845	Techniques for overcoming context length limitations	"Sending fewer tokens with RAG inputs
Summarize long documents
For long documents, use a foundation model to summarize sections of the document. You can then submit a summary of those summaries instead of feeding the entire document to the foundation model.  
For example, to generate a useful summary from one long meeting transcript, you might break the transcript into many smaller chunks and summarize each one separately. Or you might search and extract segments with contributions from different speakers. Then, you can chain the summaries together as context for the prompt that you submit to the foundation model."
846	Techniques for overcoming context length limitations	"Sending fewer tokens with RAG inputs
Generate text embeddings from the content
A text embedding is a numerical representation of a unit of information, such as a word or a sentence, as a vector of real-valued numbers.  
To leverage text embeddings in a RAG use case, complete the following steps:  
Use a text splitter to chunk the content into meaningful segments.  
For example, you can use a recursive character splitter to chunk the document into segments that meet the syntax and character limit requirements set by the foundation model. See LangChain: Recursively split by character.  
Use an embedding model to convert the document chunks into vectors that capture the meaning of the document segment.  
Store the vectors that represent your document segments in a vector database.  
At run time, search the vector store by using keywords or a search query to retrieve the most relevant document segments to feed to the foundation model.  
Converting text to embeddings decreases the size of your foundation model prompt without removing the grounding information that helps the model respond with factual answers."
847	Techniques for overcoming context length limitations	"Divide and conquer complex tasks
For complex use cases where different types of input are expected and need to be handled in different ways, apply a multiple-prompt approach.  
Create several targeted prompts, each one designed for optimal effectiveness with one type of input.  
For example, you might classify customer questions into the following types: a support issue, sales query, or product detail inquiry.  
Use a foundation model to classify the incoming input as one of the predefined input types.  
Route the input to the appropriate targeted prompt based on the input type.  
For example, for the product detail inquiry, you might search your website or other marketing content and include a summary in the prompt. For support issues, you might search your troubleshooting knowledge base, and include a workaround procedure in the prompt. And for sales inquiries, you might connect the customer with a seller."
848	Techniques for overcoming context length limitations	"Summarize dialog context
For conversational tasks, use a foundation model to summarize the context from previous dialog exchanges instead of retaining and resubmitting the chat history with each input. Using summarization reduces the number of tokens that are submitted to the foundation model with subsequent turns in the conversation."
849	Techniques for overcoming context length limitations	"Lower your prompt token count by prompt-tuning the model
Free up the tokens that might otherwise be used by complex instructions and examples that you include in your prompt by tuning the foundation model.  
When you tune a foundation model, you show the model what to return in output for the types of input that you submit. You can submit zero-shot prompts to a tuned model and still get the output that you expect.  
Tuning a model is useful for classification, summarization, or extraction tasks. Tuning is not the best solution to use when you need the foundation model output to include factual information.  
For more information, see Tuning a foundation model."
850	Techniques for overcoming context length limitations	"Learn more
Tokens and tokenization Prompt Lab Sample prompts  
Parent topic: Prompt tips"
851	Tracking prompt templates	Track a prompt template in an AI use case to capture and share facts about the asset to help you meet governance and compliance goals.
852	Tracking prompt templates	"Tracking prompt templates
A prompt template is the saved prompt input for a foundation model. A prompt template can include variables so that it can be run with different options. For example, if you have a prompt that summarizes meeting notes for project-X, you can define a variable so that the same prompt can run for project-Y.  
You can add a saved prompt template to an AI use case to track the details for the prompt template. In addition to recording details about the prompt template creation information and source model details, the factsheet tracks information from prompt template evaluations to capture performance metrics. You can evaluate prompt templates before or after you start tracking a prompt template.  
Important: Before you start tracking a prompt template in an AI use case, make sure the prompt template is stable. After you enable tracking, the prompt template is locked, and you can no longer update it. This is to preserve the integrity of the prompt template so that all of the facts collected in the factsheet apply to a single version of the prompt template. If you are still experimenting with a prompt template, do not start tracking it in an AI use case."
853	Tracking prompt templates	"Tracking prompt templates
Before you begin
Before you can track a prompt template, these conditions must be met.  
Be an administrator or editor for the project that contains the prompt template. The prompt template must include at least one variable. For more information, see Building reusable prompts.  
Watch this video to see how to track a prompt template in an AI use case.  
This video provides a visual method to learn the concepts and tasks in this documentation."
854	Tracking prompt templates	"Tracking prompt templates
Tracking a prompt template or machine learning model in an AI use case
You can add a prompt template to an AI use case from a project or space.  
Open the project or space that contains the prompt template that you want to govern. From the action menu for the asset, click View AI use case. If this prompt template is not already part of an AI use case, you are prompted to Track in AI use case. When you start tracking a prompt template, it is locked and you can no longer edit it. To make changes, you must create a new prompt template. Select an existing AI use case or follow the prompts to create a new one. Choose an existing approach or create a new approach. An approach represents one facet of a complete solution. Each approach creates a version set for all assets in the same approach. Choose a version numbering scheme. All the assets in an approach share a common version. Choose from:  
Experimental if you plan to update frequently. Stable if the assets are not changing rapidly. Custom if you want to start a new version number. Version numbering must follow a schema of major.minor.patch.  
When tracking is enabled, all collaborators for the use case can review details for the prompt template.  
Details are captured for each lifecycle stage for a prompt template.  
Develop provides information about how the prompt is defined, including the prompt itself, creation date, foundation model that is used, prompt parameters set, and variables defined. Evaluate displays the dimension metrics from evaluating your prompt template. Operate provides details that are related to how the prompt template is deployed for productive use."
855	Tracking prompt templates	"Viewing the factsheet for a tracked prompt template
Click the name of the prompt template in an AI use case to view the associated factsheet.  
The factsheet for a prompt template collects this type of data:  
Governance collects basic information such as the name of the AI use case, the description, and the approach name and version data. Foundation model displays the name of the foundation model, the license ID, and the model publisher. Prompt template shows the prompt name, ID, prompt input, and variables. Prompt parameters collect the configuration options for the prompt template, including the decoding method and stopping criteria. Evaluation displays the data from evaluation, including alerts, and metric data from the evaluation. For example, this prompt template shows the metrics data for quality evaluations on the prompt template. One threshold alert was triggered by the evaluation: Validate shows the data for how the prompt template was evaluated, including the data set used for the validation, alerts triggered, and evaluation metric data. Attachments shows information about attachments that support the use case.  
Note: As the prompt template moves from one stage of the lifecycle to the next, facts are added to the factsheet for the prompt template. The factsheet always represents the latest state of the prompt template. For example, if you validate a prompt template in a pre-production deployment space, and then again in a production deployment space, the details from the production phase are recorded in the factsheet, overwriting previous evaluation results."
856	Tracking prompt templates	"Moving a prompt template through lifecycle stages
When a prompt template is tracked, you can see details from creating the prompt template, and evaluating performance against appropriate metrics. The next stage in the lifecycle is to _validate the prompt template. This involves testing the prompt template with new data. If you are the prompt engineer who is tasked with validating the asset, follow these steps to validate the prompt template and capture the validation data in the associated factsheet.  
From the project containing the prompt template, export the project to a compressed ZIP file. Create a new project and populate it with the exported ZIP file. Upload validation data, evaluate the prompt template, and save the results to the validation project. From the project, promote the prompt template to a new or existing deployment space that is designated as a Production stage. The stage is assigned when the space is created and cannot be updated, so create a new space if you do not have a production space available. After you promote the prompt template to a deployment space, you can configure continuous monitoring. Details from monitoring the prompt template in a production space are displayed in the Operate lifecycle stage of the AI use case."
857	Tracking prompt templates	"Learn more
Follow the tutorial Quick start: Evaluate and track a prompt template to evaluate and track a sample prompt template. See Deploying a prompt template for details on preparing a prompt template for production. See Evaluating prompt templates for details on evaluating a prompt template for dimensions such as accuracy or to test for the presence of hateful or abusive speech.  
Parent topic: Tracking assets in an AI use case"
858	Building reusable prompts	"Prompt engineering to find effective prompts for a model takes time and effort. Stretch the benefits of your work by building prompts that you can reuse and share with others.  
A great way to add flexibility to a prompt is to add prompt variables. A prompt variable is a placeholder keyword that you include in the static text of your prompt at creation time and replace with text dynamically at run time.  
Note: You cannot define prompt variables in chat mode."
859	Building reusable prompts	"Using variables to change prompt text dynamically
Foundation model input variables help you to generalize a prompt so that it can be reused more easily.  
For example, a prompt for a generative task might contain the following static text:  
Write a story about a dog.  
If you replace the text dog with a variable that is named {animal}, you add support for dynamic content to the prompt.  
Write a story about a {animal}.  
With the variable {animal}, the text can still be used to prompt the model for a story about a dog. But now it can be reused to ask for a story about a cat, a mouse, or another animal, simply by swapping the value that is specified for the {animal} variable."
860	Building reusable prompts	"Creating prompt variables
To create a prompt variable, complete the following steps:  
Table 1. Example variable name and value pairs Variable name Default value country Ireland city Boston project Project X company IBM  
From the Prompt Lab, review the text in your prompt for words or phrases that, when converted to a variable, will make the prompt easier to reuse.  
Click the Prompt variables icon at the start of the page.  
The Prompt variables panel is displayed where you can add variable name-and-value pairs.  
Note: The Prompt variables panel is not available in chat mode.  
Click New variable.  
Click to add a variable name, tab to the next field, and then add a default value.  
The variable name can contain alphanumeric characters or an underscore (_), but cannot begin with a number.  
The default value for the variable is a fallback value; it is used every time that the prompt is submitted, unless someone overwrites the default value by specifying a new value for the variable.  
Repeat the previous step to add more variables.  
The following table shows some examples of the types of variables that you might want to add.  
Replace static text in the prompt with your variables.  
Select the word or phrase in the prompt that you want to replace, and then click the Prompt variables icon within the text box to see a list of available variables. Click the variable that you want to use from the list.  
The variable replaces the selected text. It is formatted with the syntax {variable name}, where the variable name is surrounded by braces.  
If your static text already contains variables that are formatted with braces, they are ignored unless prompt variables of the same name exist.  
You can click the text icon to see the full prompt text with variables replaced by values.  
To specify a value for a variable at run time, open the Prompt variables panel, click Preview, and then add a value for the variable.  
You can also change the variable value from the edit view of the Prompt variables panel, but the value you specify will become the new default value."
861	Building reusable prompts	"Creating prompt variables
You can also change the variable value from the edit view of the Prompt variables panel, but the value you specify will become the new default value.  
When you find a set of prompt static text, prompt variables, and prompt engineering parameters that generates the results you want from a model, save the prompt as a prompt template asset. After you save the prompt template asset, you can reuse the prompt or share it with collaborators in the current project. For more information, see Saving prompts."
862	Building reusable prompts	"Examples of reusing prompts
The following examples help illustrate ways that using prompt variables can add versatility to your prompts.  
Thank you note example Devil's advocate example"
863	Building reusable prompts	"Examples of reusing prompts
Thank you note example
Replace static text in the Thank you note generation built-in sample prompt with variables to make the prompt reusable.  
To add versatility to a built-in prompt, complete the following steps:  
Write a thank you note for attending a workshop. Attendees: interns Topic: codefest, AI Tone: energetic Table 2. Variables for the thank you note sample Variable name Value event workshop attendees interns topic1 codefest topic2 AI tone energetic Write a thank you note for attending a {event}. Attendees: {attendees} Topic: {topic1}, {topic2} Tone: {tone} Table 3. New values for the variables Variable name Value event human resources presentation attendees expecting parents topic1 resources for new parents topic2 parental leave tone supportive Write a thank you note for attending a human resources presentation. Attendees: expecting parents Topic: resources for new parents, parental leave Tone: supportive Table 4. Another set of values for the variables Variable name Value event usability test attendees user volunteers topic1 testing out new features topic2 sharing early feedback tone appreciative  
From the Prompt Lab, click Sample prompts to list the built-in sample prompts. From the Generation section, click Thank you note generation.  
The input for the built-in sample prompt is added to the prompt editor and the flan-ul2-20b model is selected.  
Review the text for words or phrases that make good variable candidates.  
In this example, if the following words are replaced, the prompt meaning will change:  
workshop interns codefest AI energetic  
Create a variable to represent each word in the list. Add the current value as the default value for the variable.  
Click Preview to review the variables that you added.  
Update the static prompt text to use variables in place of words.  
The original meaning of the prompt is maintained.  
Now, change the values of the variables to change the meaning of the prompt.  
From the Fill in prompt variables view of the prompt variables panel, add values for the variables.  
You effectively converted the original prompt into the following prompt:  
Click Generate to see how the model responds.  
Swap the values for the variables to reuse the same prompt again to generate thank you notes for usability test attendees.  
Click Generate to see how the model responds."
864	Building reusable prompts	"Examples of reusing prompts
Devil's advocate example
Use prompt variables to reuse effective examples that you devise for a prompt.  
You can guide a foundation model to answer in an expected way by adding a few examples that establish a pattern for the model to follow. This kind of prompt is called a few-shot prompt. Inventing good examples for a prompt requires imagination and testing and can be time-consuming. If you successfully create a few-shot prompt that proves to be effective, you can make it reusable by adding prompt variables.  
Maybe you want to use the granite-13b-instruct-v1 model to help you consider risks or problems that might arise from an action or plan under consideration.  
For example, the prompt might have the following instruction and examples:  
You can reuse the prompt by completing the following steps:  
Table 5. Variable that represents the possible action Variable name Default value plan Refreshing our brand image by creating a new logo. You are playing the role of devil's advocate. Argue against the proposed plans. List 3 detailed, unique, compelling reasons why moving forward with the plan would be a bad choice. Consider all types of risks. Plan we are considering: Extend our store hours. Three problems with this plan are: 1. We'll have to pay more for staffing. 2. Risk of theft increases late at night. 3. Clerks might not want to work later hours. Plan we are considering: Open a second location for our business. Three problems with this plan are: 1. Managing two locations will be more than twice as time-consuming than managed just one. 2. Creating a new location doesn't guarantee twice as many customers. 3. A new location means added real estate, utility, and personnel expenses. Plan we are considering: {plan} Three problems with this plan are:  
Replace the text that describes the action that you are considering with a variable.  
For example, you can add the following variable:  
Replace the static text that defines the plan with the {plan} variable.  
Now you can use the same prompt to prompt the model to brainstorm about other actions.  
Change the text in the {plan} variable to describe a different plan, and then click Generate to send the new input to the model.  
Parent topic: Prompt Lab"
865	Run the built-in sample pipeline	You can view and run a built-in sample pipeline that uses sample data to learn how to automate machine learning flows in Watson Pipelines.
866	Run the built-in sample pipeline	"What's happening in the sample pipeline?
The sample pipeline gets training data, trains a machine learning model by using the AutoAI tool, and selects the best pipeline to save as a model. The model is then copied to a deployment space where it is deployed.  
The sample illustrates how you can automate an end-to-end flow to make the lifecycle easier to run and monitor.  
The sample pipeline looks like this:  
The tutorial steps you through this process:  
Prerequisites Preview creating and running the sample pipeline Creating the sample pipeline Running the sample pipeline Reviewing the results Exploring the sample nodes and configuration"
867	Run the built-in sample pipeline	"Prerequisites
To run this sample, you must first create:  
A project, where you can run the sample pipeline. A deployment space, where you can view and test the results. The deployment space is required to run the sample pipeline."
868	Run the built-in sample pipeline	"Preview creating and running the sample pipeline
Watch this video to see how to create and run a sample pipeline.  
This video provides a visual method to learn the concepts and tasks in this documentation."
869	Run the built-in sample pipeline	"Creating the sample pipeline
Create the sample pipeline in the Pipelines editor.  
Open the project where you want to create the pipeline.  
From the Assets tab, click New asset > Automate model lifecycle.  
Click the Resource hub sample tab, and select the Orchestrate an AutoAI experiment.  
Enter a name for the pipeline. For example, enter Bank marketing sample.  
Click Create to open the canvas."
870	Run the built-in sample pipeline	"Running the sample pipeline
To run the sample pipeline:  
Click Run pipeline on the canvas toolbar, then choose Trial run.  
Select a deployment space when prompted to provide a value for the deployment_space pipeline parameter.  
Click Select Space.  
Expand the Spaces section.  
Select your deployment space.  
Click Choose.  
Provide an API key if it is your first time to run a pipeline. Pipeline assets use your personal IBM Cloud API key to run operations securely without disruption.  
If you have an existing API key, click Use existing API key, paste the API key, and click Save.  
If you don't have an existing API key, click Generate new API key, provide a name, and click Save. Copy the API key, and then save the API key for future use. When you're done, click Close.  
Click Run to start the pipeline."
871	Run the built-in sample pipeline	"Reviewing the results
When the pipeline run completes, you can view the output to see the results.  
Open the deployment space that you specified as part of the pipeline. You see the new deployment in the space:  
If you want to test the deployment, use the deployment space Test page to submit payload data in JSON format and get a score back. For example, click the JSON tab and enter this input data:  
When you click Predict, the model generates output with a confidence score for the prediction of whether a customer subscribes to a term deposit promotion.  
In this case, the prediction of ""no"" is accompanied by a confidence score of close to 95%, predicting that the client will most likely not subscribe to a term deposit."
872	Run the built-in sample pipeline	"Exploring the sample nodes and configuration
Get a deeper understanding of how the sample nodes are configured to work in concert in the pipeline sample with the following steps.  
Viewing the pipeline parameter Loading the training data for the AutoAI experiment Creating the AutoAI experiment Running the AutoAI experiment Deploying the model to a web service"
873	Run the built-in sample pipeline	"Exploring the sample nodes and configuration
Viewing the pipeline parameter
A pipeline parameter specifies a setting for the entire pipeline. In the sample pipeline, a pipeline parameter is used to specify a deployment space where the model that is saved from the AutoAI experiment is stored and deployed. You are prompted to select the deployment space the pipeline parameter links to.  
Click the Global objects icon on the canvas toolbar to view or create pipeline parameters. In the sample pipeline, the pipeline parameter is named deployment_space and is of type Space. Click the name of the pipeline parameter to view the details. In the sample, the pipeline parameter is used with the Create data file node and the Create AutoAI experiment node."
874	Run the built-in sample pipeline	"Exploring the sample nodes and configuration
Loading the training data for the AutoAI experiment
In this step, a Create data file node is configured to access the data set for the experiment. Click the node to view the configuration. The data file is bank-marketing-data.csv, which provides sample data to predict whether a bank customer signs up for a term deposit. The data rests in a Cloud Object Storage bucket and can be refreshed to keep the model training up to date."
875	Run the built-in sample pipeline	"Exploring the sample nodes and configuration
Creating the AutoAI experiment
The node to Create AutoAI experiment is configured with these values:  
Those options define an experiment that uses the bank marketing data to predict whether a customer is likely to enroll in a promotion."
876	Run the built-in sample pipeline	"Exploring the sample nodes and configuration
Running the AutoAI experiment
In this step, the Run AutoAI experiment node runs the AutoAI experiment onboarding-bank-marketing-prediction, trains the pipelines, then saves the best model."
877	Run the built-in sample pipeline	"Exploring the sample nodes and configuration
Deploying the model to a web service
The Create Web deployment node creates an online deployment that is named onboarding-bank-marketing-prediction-deployment so you can deliver data and get predictions back in real time from the REST API endpoint.  
Parent topic: IBM Watson Pipelines"
878	Watson Natural Language Processing task catalog	"Watson Natural Language Processing encapsulates natural language functionality in standardized components called blocks or workflows. Each block or workflow can be loaded and run in a notebook, some directly on input data, others in a given order.  
This topic contains descriptions of the natural language processing tasks supported in the Watson Natural Language Processing library. It lists the task names, the languages that are supported, dependencies to other blocks and includes sample code of how you use the natural language processing functionality in a Python notebook.  
The following natural language processing tasks are supported as blocks or workflows in the Watson Natural Language Processing library:  
Language detection Syntax analysis Noun phrase extraction Keyword extraction and ranking Entity extraction Sentiment classification Tone classification Emotion classification Concepts extraction Relations extraction Hierarchical text categorization"
879	Watson Natural Language Processing task catalog	"Language codes
Many of the pre-trained models are available in many languages. The following table lists the language codes and the corresponding language.  
Parent topic: Watson Natural language Processing library"
880	Managing the lifecycle of notebooks and scripts	"After you have created and tested your notebooks, you can add them to pipelines, publish them to a catalog so that other catalog members can use the notebook in their projects, or share read-only copies outside of Watson Studio so that people who aren't collaborators in your Watson Studio projects can see and use them. R scripts and Shiny apps can't be published or shared using functionality in a project at this time.  
You can use any of these methods for notebooks:  
Add notebooks to a pipeline Share a URL on social media Publish on GitHub Publish as a gist Publish your notebook to a catalog  
Make sure that before you share or publish a notebook, you hide any sensitive code, like credentials, that you don't want others to see! See Hide sensitive cells in a notebook.  
Parent topic: Notebooks and scripts"
881	Publishing notebooks on GitHub	"To collaborate with stakeholders and other data scientists, you can publish your notebooks in GitHub repositories. You can also use GitHub to back up notebooks for source code management.  
Watch this video to see how to enable GitHub integration.  
Video disclaimer: Some minor steps and graphical elements in this video might differ from your platform.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Transcript  
Synchronize transcript with video"
882	Publishing notebooks on GitHub	"Video transcript Time Transcript 00:00 This video shows you how to publish notebooks from your Watson Studio project to your GitHub account. 00:07 Navigate to your profile and settings. 00:11 On the ""Integrations"" tab, visit the link to generate a GitHub personal access token. 00:17 Provide a descriptive name for the token and select the repo and gist scopes, then generate the token. 00:29 Copy the token, return to the GitHub integration settings, and paste the token. 00:36 The token is validated when you save it to your profile settings. 00:42 Now, navigate to your projects. 00:44 You enable GitHub integration at the project level on the ""Settings"" tab. 00:50 Simply scroll to the bottom and paste the existing GitHub repository URL. 00:56 You'll find that on the ""Code"" tab in the repo. 01:01 Click ""Update"" to make the connection. 01:05 Now, go to the ""Assets"" tab and open the notebook you want to publish. 01:14 Notice that this notebook has the credentials replaced with X's. 01:19 It's a best practice to remove or replace credentials before publishing to GitHub. 01:24 So, this notebook is ready for publishing. 01:27 You can provide the target path along with a commit message. 01:31 You also have the option to publish content without hidden code, which means that any cells in the notebook that began with the hidden cell comment will not be published. 01:42 When you're, ready click ""Publish"". 01:45 The message tells you that the notebook was published successfully and provides links to the notebook, the repository, and the commit. 01:54 Let's take a look at the commit. 01:57 So, there's the commit, and you can navigate to the repository to see the published notebook. 02:04 Lastly, you can publish as a gist. 02:07 Gists are another way to share your work on GitHub. 02:10 Every gist is a git repository, so it can be forked and cloned. 02:15 There are two types of gists: public and secret. 02:19 If you start out with a secret gist, you can convert it to a public gist later. 02:24 And again, you have the option"
883	Publishing notebooks on GitHub	02:15 There are two types of gists: public and secret. 02:19 If you start out with a secret gist, you can convert it to a public gist later. 02:24 And again, you have the option to remove hidden cells. 02:29 Follow the link to see the published gist. 02:32 So that's the basics of Watson Studio's GitHub integration. 02:37 Find more videos in the Cloud Pak for Data as a Service documentation.
884	Publishing notebooks on GitHub	"Enabling access to GitHub from your account
Before you can publish notebooks on GitHub, you must enable your IBM watsonx account to access GitHub. You enable access by creating a personal access token with the required access scope in GitHub and linking the token to your IBM watsonx account.  
Follow these steps to create a personal access token:  
Click your avatar in the header, and then click Profile and settings. Go to the Integrations tab and click the GitHub personal access tokens link on the dialog and generate a new token. On the New personal access token page, select repo scope and then click to generate a token. Copy the generated access token and paste it in the GitHub integration dialog window in IBM watsonx."
885	Publishing notebooks on GitHub	"Linking a project to a GitHub repository
After you have saved the access token, your project must be connected to an existing GitHub repository. You can only link to one existing GitHub repository from a project. Private repositories are supported.  
To link a project to an existing GitHub repository, you must have administrator permission to the project. All project collaborators, who have adminstrator or editor permission, can publish files to this GitHub repository. However, these users must have permission to access the repository. Granting user permissions to repositories must be done in GitHub.  
To connect a project to an existing GitHub repository:  
Select the Manage tab and go to the Services and Integrations page. Click the Third-party integrations tab. Click Connect integration. Enter your generated access token from Github.  
Now you can begin publishing notebooks on GitHub.  
Note:  
For information on how to change your Git integration, refer to Managing your integrations."
886	Publishing notebooks on GitHub	"Publishing a notebook on GitHub
To publish a notebook on GitHub:  
Open the notebook in edit mode. Click the GitHub integration icon () and select Publish on GitHub from the opened notebook's action bar.  
When you enter the name of the file you want to publish on GitHub, you can specify a folder path in the GitHub repository. Note that notebook files are always pushed to the master branch.  
If you get this error: An error occurred while publishing the notebook. Invalid access token permissions or repository does not exist. make sure that:  
You generated your personal access token, as described in Enabling access to GitHub from your account and the token was not deleted. The repository that you want to publish your notebook to still exists.  
Parent topic: Managing the lifecycle of notebooks and scripts"
887	Watsonx.governance	Use watsonx.governance to accelerate responsible, transparent, and explainable AI workflows with an AI governance solution that provides end-to-end monitoring for machine learning and generative AI models. Monitor your foundation model and machine learning assets from request to production. Collect facts about models that are built with IBM tools or third-party providers in a single dashboard to aid in meeting compliance and governance goals.
888	Watsonx.governance	"Develop a comprehensive governance solution
Using watsonx.governance, you can extend the best practices of AI governance from predictive machine learning models to generative AI while monitoring and mitigating the risks associated with models, users, and data sets. The benefits of this approach include:  
Responsible AI: extend the practices of responsible AI from governing predictive machine learning models to the use of generative AI with any foundation or model provider. Explainability: Use automation to improve transparency and explainability for tracked models. Use tools for detecting and mitigating risks that are associated with AI. Transparent and regulatory policies: Mitigate AI risks by tracking the end-to-end AI lifecycle to aid compliance with internal policies and external regulations for enterprise-wide AI solutions."
889	Watsonx.governance	"Develop a comprehensive governance solution
Use the AI risk atlas as a guide
Start your governance journey by reviewing the Risk Atlas to learn about the potential risks of working with AI models. The Risk Atlas provides a guide to understanding some of the risks of working with AI models, including generative AI, foundation models, and machine learning models. In addition to describing potential risks, it provides real-world context. It is intended as an educational resource and is not meant as a prescriptive tool."
890	Watsonx.governance	"Develop a comprehensive governance solution
Governance in action
This illustration depicts a typical governance flow, from request to monitoring in production."
891	Watsonx.governance	"Components of watsonx.governance
Watsonx.governance includes these tools for addressing your governance needs in an integrated solution:  
Watson OpenScale provides tools for configuring monitors that evaluate your deployed assets against thresholds you specify. For example, you can configure threshold that alerts you when predictive machine learning models perform below a specified threshold for fairness in monitored outcomes, or drift from accuracy. Alerts for foundation models can warn you when a threshold is breached for the presence of hateful or abusive language or the detection of personal identifiable information. A Model Health monitor provides real-time performance tracking for deployed models. AI Factsheets collects the metadata for machine learning models and prompt templates you explicitly track. Develop AI use cases to gather all of the information for managing a model or prompt template from the request phase through development and into production. Manage multiple versions or a model, or compare different approaches to solving a business problem within a use case. Factsheets display information about the models including creation information, data that is used, and where the asset is in the lifecycle. A common model inventory dashboard gives you a view of all tracked assets, or you can view the details of a particular model, all in service of meeting policy and compliance goals. Optionally integrate with IBM OpenPages Model Risk Governance to collect metadata about foundation models as well as machine learning models to help you achieve your governance goals. You can also use OpenPages to develop workflows that support your governance processes."
892	Watsonx.governance	"Components of watsonx.governance
Extend governance with watsonx.ai
To create an end-to-end experience for developing assets and then adding them to governance, use watsonx.ai with watsonx.governance. Watsonx.ai extends the Watson Studio and Watson Machine Learning services to work with foundation models, including capabilities for saving prompt templates for a curated collection of large language model assets.  
For more information on watsonx.ai, see:  
Overview of IBM watsonx Signing up for IBM watsonx"
893	Watsonx.governance	"Next steps
Develop a governance plan  
To begin governance, follow the steps in Provisioning and launching IBM watsonx.governance to provision Watson OpenScale with AI Factsheets."
894	Watson Natural Language Processing library usage samples	The sample notebooks demonstrate how to use the different Watson Natural Language Processing blocks and how to train your own models.
895	Watson Natural Language Processing library usage samples	"Sample project and notebooks
To help you get started with the Watson Natural Language Processing library, you can download a sample project and notebooks from the Resource hub.  
You can access the Resource hub by selecting Resource hub from the Cloud Pak for Data navigation menu.  
Sample notebooks  
Financial complaint analysis  
This notebook shows you how to analyze financial customer complaints using Watson Natural Language Processing. It uses data from the Consumer Complaint Database published by the Consumer Financial Protection Bureau (CFPB). The notebook teaches you to use the Tone classification and Emotion classification models.  
Car complaint analysis  
This notebook demonstrates how to analyze car complaints using Watson Natural Language Processing. It uses publicly available complaint records from car owners stored by the National Highway and Transit Association (NHTSA) of the US Department of Transportation. This notebook shows you how use syntax analysis to extract the most frequently used nouns, which typically depict the problems that review authors talk about and combine these results with structured data using association rule mining.  
Complaint classification with Watson Natural Language Processing  
This notebook demonstrates how to train different text classifiers using Watson Natural Language Processing. The classifiers predict the product group from the text of a customer complaint. This could be used, for example to route a complaint to the appropriate staff member. The data that is used in this notebook is taken from the Consumer Complaint Database that is published by the Consumer Financial Protection Bureau (CFPB), a U.S. government agency and is publicly available. You will learn how to train a custom CNN model and a VotingEnsemble model and evaluate their quality.  
Entity extraction on Financial Complaints with Watson Natural Language Processing  
This notebook demonstrates how to extract named entities from financial customer complaints using Watson Natural Language Processing. It uses data from the Consumer Complaint Database published by the Consumer Financial Protection Bureau (CFPB). In the notebook you will learn how to do dictionary-based term extraction to train a custom extraction model based on given dictionaries and extract entities using the BERT or a transformer model.  
Sample project  
If you don't want to download the sample notebooks to your project individually, you can download the entire sample project Text Analysis with Watson Natural Language Processing from the IBM watsonx Resource hub.  
The sample project contains the sample notebooks listed in the previous section, including:  
Analyzing hotel reviews using Watson Natural Language Processing"
896	Watson Natural Language Processing library usage samples	"Sample project and notebooks
The sample project contains the sample notebooks listed in the previous section, including:  
Analyzing hotel reviews using Watson Natural Language Processing  
This notebook shows you how to use syntax analysis to extract the most frequently used nouns from the hotel reviews, classify the sentiment of the reviews and use targets sentiment analysis. The data file that is used by this notebook is included in the project as a data asset.  
You can run all of the sample notebooks with the NLP + DO Runtime 23.1 on Python 3.10 XS environment except for the Analyzing hotel reviews using Watson Natural Language Processing notebook. To run this notebook, you need to create an environment template that is large enough to load the CPU-optimized models for sentiment and targets sentiment analysis.  
Parent topic: Watson Natural Language Processing library"
897	Batch deployment input details for Pytorch models	"Follow these rules when you are specifying input details for batch deployments of Pytorch models.  
Data type summary table:"
898	Batch deployment input details for Pytorch models	"Data sources
Input or output data references:  
Local or managed assets from the space Connected (remote) assets: Cloud Object Storage  
If you are specifying input/output data references programmatically:  
Data source reference type depends on the asset type. Refer to the Data source reference types section in Adding data assets to a deployment space. If you deploy Pytorch models with ONNX format, specify the keep_initializers_as_inputs=True flag and set opset_version to 9 (always set opset_version to the most recent version that is supported by the deployment runtime).torch.onnx.export(net, x, 'lin_reg1.onnx', verbose=True, keep_initializers_as_inputs=True, opset_version=9)  
Note: The environment variables parameter of deployment jobs is not applicable.  
Parent topic: Batch deployment input details by framework"
899	Compute resource options for Data Refinery in projects	"When you create or edit a Data Refinery flow in a project, you use the Default Data Refinery XS runtime environment. However, when you run a Data Refinery flow in a job, you choose an environment template for the runtime environment. The environment template specifies the type, size, and power of the hardware configuration, plus the software template.  
Types of environments Default environment templates Compute usage Changing the runtime Runtime logs for jobs"
900	Compute resource options for Data Refinery in projects	"Types of environments
You can use these types of environments with Data Refinery:  
Default Data Refinery XS runtime environment for running jobs on small data sets. Spark environments for running jobs on larger data sets. The Spark environments have default environment templates so you can get started quickly. Otherwise, you can create custom environment templates for Spark environments. You should use a Spark & R environment only if you are working on a large data set. If your data set is small, you should select the Default Data Refinery XS runtime. The reason is that, although the SparkR cluster in a Spark & R environment is fast and powerful, it requires time to create, which is noticeable when you run a Data Refinery job on small data set."
901	Compute resource options for Data Refinery in projects	"Default environment templates
When you work in Data Refinery, the Default Data Refinery XS environment runtime is started and appears as an active runtime under Tool runtimes on the Environments page on the Manage tab of your project. This runtime stops after an hour of inactivity in the Data Refinery interface. However, you can stop it manually under Tool runtimes on the Environments page.  
When you create a job to run a Data Refinery flow in a project, you select an environment template. After a runtime for a job is started, it is listed as an active runtime under Tool runtimes on the Environments page on the Manage tab of your project. The runtime for a job stops when the Data Refinery job stops running.  
Compute usage is tracked by capacity unit hours (CUH).  
All default environment templates for Data Refinery are HIPAA ready.  
The Spark default environment templates are listed under Templates on the Environments page on the Manage tab of your project."
902	Compute resource options for Data Refinery in projects	"Compute usage in projects
You can monitor the Watson Studio CUH consumption on the Resource usage page on the Manage tab of your project."
903	Compute resource options for Data Refinery in projects	"Changing the runtime
You can't change the runtime for working in Data Refinery.  
You can change the runtime for a Data Refinery flow job by editing the job template. See Creating jobs in Data Refinery."
904	Compute resource options for Data Refinery in projects	"Runtime logs for jobs
To view the accumulated logs for a Data Refinery job:  
From the project's Jobs page, click the job that ran the Data Refinery flow for which you want to see logs. Click the job run. You can view the log tail or download the complete log file."
905	Compute resource options for Data Refinery in projects	"Next steps
Creating your own environment template Creating jobs in Data Refinery Stopping active runtimes"
906	Compute resource options for Data Refinery in projects	"Learn more
Monitoring account resource usage  
Parent topic: Choosing compute resources for tools"
907	Compute resource options for Decision Optimization	"When you run a Decision Optimization model, you use the Watson Machine Learning instance that is linked to the deployment space associated with your experiment.  
Default hardware configurations Compute usage"
908	Compute resource options for Decision Optimization	"Default hardware configuration
The following hardware configuration is used by default when running models in an experiment:  
The CUH is consumed only when the model is running and not when you are adding data or editing your model.  
You can also switch to any other experiment environment as required. See the Decision Optimization plans for a list of environments for Decision Optimization experiments.  
For more information on how to configure Decision Optimization experiment environments, see Configuring environments."
909	Compute resource options for Decision Optimization	"Compute usage in projects
Decision Optimization experiments consume compute resources as CUH from the Watson Machine Learning service.  
You can monitor the total monthly amount of CUH consumption for the Watson Machine Learning service on the Resource usage page on the Manage tab of your project."
910	Compute resource options for Decision Optimization	"Learn more
Decision Optimization Watson Machine Learning plans and compute usage Monitoring account resource usage  
Parent topic: Choosing compute resources for tools"
911	Adding conditions to the pipeline	Add conditions to a pipeline to handle various scenarios.
912	Adding conditions to the pipeline	"Configuring conditions for the pipeline
As you create a pipeline, you can specify conditions that must be met before you run the pipeline. For example, you can set a condition that the output from a node must satisfy a particular condition before you proceed with the pipeline execution.  
To define a condition:  
Hover over the link between two nodes.  
Click Add condition.  
Choose the type of condition:  
Condition Response checks a condition on the status of the previous node. Simple condition is a no-code condition in the form of an if-then statement. Advanced condition Advanced condition uses expression code, providing the most features and flexibility.  
Define and save your expression.  
When you define your expression, a summary captures the condition and the expected result. For example:  
If Run AutoAI is Successful, then Create deployment node.  
When you return to the flow, you see an indicator that you defined a condition. Hover over the icon to edit or delete the condition."
913	Adding conditions to the pipeline	"Configuring a condition based on node status
If you select Condition Response as your condition type, the previous node status must satisfy at least one of these conditions to continue with the flow:  
Completed - the node activity is completed without error. Completed with warnings - the node activity is completed but with warnings. Completed with errors - the node activity is completed, but with errors. Failed - the node activity failed to complete. Cancelled - the previous action or activity was canceled."
914	Adding conditions to the pipeline	"Configuring a simple condition
To configure a simple condition, choose the condition that must be satisfied to continue with the flow.  
Optional: edit the default name. Depending on the node, choose a variable from the drop-down options. For example, if you are creating a condition based on a Run AutoAI node, you can choose Model metric as the variable to base your condition on. Based on the variable, choose an operator from: Equal to, Not equal to, Greater than, Less than, Greater than or equal to, Less than or equal to. Specify the required value. For example, if you are basing a condition on an AutoAI metric, specify a list of values that consists of the available metrics. Optional: click the plus icon to add an And (all conditions must be met) or an Or (either condition must be met) to the expression to build a compound conditional statement. Review the summary and save the condition."
915	Adding conditions to the pipeline	"Configuring an advanced condition
Use coding constructs to build a more complex condition. The next node runs when the condition is met. You build the advanced condition by using the expression builder.  
Optional: edit the default name. Add items from the Expression elements panel to the Expression canvas to build your condition. You can also type your conditions and the elements autocomplete. When your expression is complete, review the summary and save the condition."
916	Adding conditions to the pipeline	"Configuring an advanced condition
Learn more
For more information on using the code editor to build an expression, see:  
Functions used in pipelines Expression Builder  
Parent topic: Creating a pipeline"
917	Federated Learning XGBoost samples	These are links to sample files to run Federated Learning by using API calls with an XGBoost framework. To see a step-by-step UI driven approach, go to Federated Learning XGBoost tutorial for UI.
918	Federated Learning XGBoost samples	"Download the Federated Learning sample files
The Federated Learning samples have two parts, both in Jupyter Notebook format that can run in the latest Python environment.  
For single-user demonstrative purposes, the Notebooks are placed in a project. Go to the following link and click Create project to get all the sample files.  
Download the Federated Learning project  
You can also get the Notebook separately. For practical purposes of Federated Learning, one user would run the admin Notebook and multiple users would run the party Notebook. For more details on the admin and party, see Terminology  
Federated Learning XGBoost Demo Part 1 - for Admin  
Federated Learning XGBoost Demo Part 2 - for Party  
Parent topic: Federated Learning tutorial and samples"
919	IBM granite-8b-japanese model card	"Model Version (1.0.0): Released 2/29/2024  
The Granite 8 Billion Japanese (granite-8b-japanese) model is an instruct variant initialized from the pre-trained Granite Base 8 Billion Japanese model. Pre-training went through 1.0T tokens of English, 0.5T tokens of Japanese, and 0.1T tokens of code. This model is designed to work with Japanese text. IBM Generative AI Large Language Foundation Models are Enterprise-level Multilingual models trained with large volumes of data that has been subjected to intensive pre-processing and careful analysis.  
Person or organization developing the model: Model release date and version: Model type: Information about training algorithms, parameters, fairness constraints or other applied approaches, and features: License:  
granite-8b-japanese was developed by IBM Research.  
granite-8b-japanese version 1.0 was released on 2/29/2024.  
granite-8b-japanese is a decoder-only transformer model. The following features were used in the design of the model:  
Decoder-only model Group-Query Attention IBM Japanese/English Trained Tokenizer 4096 context length Rotary Position Embedding(RoPE) SwiGLU Activations Root Mean Square Layer Normalization  
Model was trained using 4x Tensor Parallel + 4x Pipeline Parallel + Megatron distributed optimizer Megatron-LM. GPUs: 448x A100 80GB Interconnect: 1600 gigabit Infiniband  
Available only through IBM products and offerings. Contact IBM for licensing terms."
920	IBM granite-8b-japanese model card	"Intended Use
Primary intended uses: Primary intended users: Out-of-scope use cases:  
granite-8b-japanese is used for text generation, summarization, question and answer, classification, and extraction in Japanese.  
The primary users are IBM Enterprise clients looking to bolster their portfolios with Enterprise-level generative AI models.  
granite-8b-japanese is not designed, tested, or supported, for code use cases of any kind."
921	IBM granite-8b-japanese model card	"Factors
Relevant factors: granite-8b-japanese works with Japanese text. All datasets have been cleansed of any type of tagging (e.g., HTML), and all media has been removed as well."
922	IBM granite-8b-japanese model card	"Metrics
granite-8b-japanese was evaluated using the following eight well known datasets from Stability-AI/lm-evaluation-harness:  
JCommonsenseQA is a Japanese version of CommonsenseQA (Talmor+, 2019), which is a multiple-choice question answering dataset that requires commonsense reasoning ability. It is built using crowdsourcing with seeds extracted from the knowledge base ConceptNet.  
JNLI is a Japanese version of the NLI (Natural Language Inference) dataset. NLI is a task to recognize the inference relation that a premise sentence has to a hypothesis sentence. The inference relations are 含意, 矛盾, and 中立.  
MARC-ja is a dataset of the text classification task. This dataset is based on the Japanese portion of Multilingual Amazon Reviews Corpus (MARC) (Keung+, 2020).  
JSQuAD is a Japanese version of SQuAD (Rajpurkar+, 2016), one of the datasets of reading comprehension. Each instance in the dataset consists of a question regarding a given context (Wikipedia article) and its answer. JSQuAD is based on SQuAD 1.1 (there are no unanswerable questions). We used the Japanese Wikipedia dump as of 20211101.  
Japanese Questions on Knowledge of Entity (JAQKET) is a Japanese open-domain question answering dataset where the answers are Wikipedia article titles.  
XLSum-ja This is a filtered Japanese subset of XLSum based on ROUGE-2, which PaLM 2 uses. It is composed of filtered data based on 15-gram overlap as PaLM 2 did.  
XWinograd - XWinograd is a set of Winograd Schema sentence pairs. For example:  
ボブはトムに尋ねた。トムはお金をいくらか貸してくれるかと。 ボブはトムに尋ねた。ボブはお金をいくらか貸してくれるかと。"
923	IBM granite-8b-japanese model card	"Metrics
In this case the first sentence is correct, because it doesn't make sense for Bob to ask Tom how much money Bob himself will loan. The task is for the model to assign the higher log likelihood to the reasonable sentence. Because of the way the task is defined, it's always zero-shot with no prompt. While XWinograd is a multilingual task, this only uses the Japanese subset, which has 959 pairs.  
Multilingual Grade School Math is a set of 250 math word problems in Japanese, and the task is to get the right integer solution to the problem."
924	IBM granite-8b-japanese model card	"Data, Limitations, and Recommendations
Data selection for training:  
The granite-8b-japanese underwent pre-training using 1.0T tokens of English, 0.5T tokens of Japanese, and 0.1T tokens of code."
925	Manage default settings	"You can manage the global settings of your IBM Watson Pipelines such as a default error policy and default rules for node caching.  
Global settings apply to all nodes in the pipeline unless local node settings overwrite them. To update global settings, click the Manage default settings icon on the toolbar. You can configure:  
Error policy Node caching Pipelines settings"
926	Manage default settings	"Setting global error policy
You can define the behavior of Pipelines when an error occurs.  
Fail pipeline on error stops the flow and initiates an error-handling flow. Continue pipeline on error tries to continue running the pipeline."
927	Manage default settings	"Setting global error policy
Error handling
You can configure the behavior of Pipelines for error handling.  
Create custom-error handling response: Customize an error-handling response. Add an error handling node to the canvas so you can configure a custom error response. The response applies to all configured nodes to fail when an error occurs.  
Show icon on nodes linked to error handling pipeline: An icon flags a node with an error to help debug the flow.  
To learn more about error handling, see Managing pipeline errors"
928	Manage default settings	"Setting node caches
Manual caching for nodes sets the default for how the pipeline caches and stores information. You can override these settings for individual nodes."
929	Manage default settings	"Setting node caches
Default cache usage frequency
You can change the following cache settings:  
Choose whether to enable automatic caching for all nodes or choose to manually set cache conditions for specific nodes.  
Enable automatic caching for all nodes (recommended) All nodes that support caching enable it by default. Setting Creation Mode or Copy Mode in your node's settings to Overwrite automatically disables cache, if the node supports these setting parameters. Enable caching for specific nodes in the node properties panelIn individual nodes, you can select Create data cache at this node in Output to allow caching for individual nodes. A save icon appears on nodes that uses this feature.  
Choose the conditions for using cached data.  
Do not use cache Always use cache Use cache when all selected conditions are met, options include:  
Retrying from a previous failed run Input values for the current pipeline are unchanged from previous run Pipeline version is unchanged from previous run  
To view and download your cache data, see Run tracker in your flow. You can download the results by opening a preview of the node's cache and clicking the download icon."
930	Manage default settings	"Setting node caches
Resetting the cache
If your cache was enabled, you can choose to reset your cache when you run a Pipelines job. When you click Run again, you can select Clear pipeline cache in Define run settings. By choosing this option, you are overriding the default cache settings to reset the cache for the current run. However, the pipeline still creates cache for subsequent runs while cache is enabled."
931	Manage default settings	"Managing your Pipelines settings
Configure other global settings for your Pipelines asset."
932	Manage default settings	"Managing your Pipelines settings
Autosave
Choose to automatically save your current Pipelines canvas at a selected frequency. Only changes that impact core pipeline flow are saved.  
Parent topic: IBM Watson Pipelines"
933	Supported embedding models available with watsonx.ai	"Use IBM embedding models that are deployed in IBM watsonx.ai to help with semantic search and document comparison tasks.  
Embedding models are encoder-only foundation models that create text embeddings. A text embedding encodes the meaning of a sentence or passage in an array of numbers known as a vector. For more information, see Converting sentences into text embeddings.  
The following embedding models from the Slate family of foundation models by IBM are available in watsonx.ai:  
slate-30m-english-rtrvr slate-125m-english-rtrvr  
   
You can use the watsonx.ai Python library or REST API to submit text to one of the supported embedding models.  
For more information about generative foundation models, see Supported foundation models."
934	Supported embedding models available with watsonx.ai	"IBM embedding models
The following table lists the supported embedding models that IBM provides.  
   
For a list of which models are provided in each regional data center, see Regional availability of foundation models. For information about billing classes, see Watson Machine Learning plans."
935	Supported embedding models available with watsonx.ai	"Embedding model details
Use the watsonx.ai API or Python library to pass text to one of the supported embedding models."
936	Supported embedding models available with watsonx.ai	"Embedding model details
slate-125m-english-rtrvr
The slate-125m-english-rtrvr foundation model is provided by IBM. The slate-125m-english-rtrvr foundation model generates embeddings for various inputs such as queries, passages, or documents. The training objective is to maximize cosine similarity between a query and a passage. This process yields two sentence embeddings, one that represents the question and one that represents the passage, allowing for comparison of the two through cosine similarity.  
Usage: Two to three times slower but performs slightly better than the slate-30m-english-rtrvr model.  
Cost: Class C1. For pricing details, see Watson Machine Learning plans.  
Try it out  
Number of dimensions: 384  
Input token limits: 512  
Supported natural languages: English  
Fine-tuning information: This version of the model was fine-tuned to be better at retrieval-based tasks.  
Model architecture: Encoder-only  
License: Terms of use  
Learn more  
Model card"
937	Supported embedding models available with watsonx.ai	"Embedding model details
slate-30m-english-rtrvr
The slate-30m-english-rtrvr foundation model is a distilled version of the slate-125m-english-rtrvr, which are both provided by IBM. The slate-30m-english-rtrvr embedding model is trained to maximize the cosine similarity between two text inputs so that embeddings can be evaluated based on similarity later.  
The embedding model architecture has 6 layers that are used sequentially to process data.  
Usage: Two to three times faster and has slightly lower performance scores than the slate-125m-english-rtrvr model.  
Cost: Class C1. For pricing details, see Watson Machine Learning plans.  
Try it out  
Number of dimensions: 384  
Input token limits: 512  
Supported natural languages: English  
Fine-tuning information: This version of the model was fine-tuned to be better at retrieval-based tasks.  
Model architecture: Encoder-only  
License: Terms of use  
Learn more  
Model card"
938	Customizing environment templates	"You can change the name, the description, and the hardware configuration of an environment template that you created. You can customize the software configuration of Jupyter notebook environment templates through conda channels or by using pip. You can provide a list of conda packages, a list of pip packages, or a combination of both. When using conda packages, you can provide a list of additional conda channel locations through which the packages can be obtained.  
To customize an environment template that you created:  
# Modify the following content to add a software customization to an environment. # To remove an existing customization, delete the entire content and click Apply. # Add conda channels below defaults, indented by two spaces and a hyphen. channels: - defaults # To add packages through conda or pip, remove the comment on the following line. # dependencies: # Add conda packages here, indented by two spaces and a hyphen. # Remove the comment on the following line and replace sample package name with your package name: # - a_conda_package=1.0 # Add pip packages here, indented by four spaces and a hyphen. # Remove the comments on the following lines and replace sample package name with your package name. # - pip: # - a_pip_package==1.0  
Under your project's Manage tab, click the Environments page.  
In the Active Runtimes section, check that no runtime is active for the environment template you want to change.  
In the Environment Templates section, click the environment template you want to customize.  
Make your changes.  
For a Juypter notebook environment template, select to create a customization and specify the libraries to add to the standard packages that are available by default. You can also use the customization to upgrade or downgrade packages that are part of the standard software configuration.  
The libraries that are added to an environment template through the customization aren't persisted; however, they are automatically installed each time the environment runtime is started. Note that if you add a library using pip install through a notebook cell and not through the customization, only you will be able to use this library; the library is not available to someone else using the same environment template.  
If you want you can use the provided template to add the custom libraries. There is a different template for Python and for R. The following example shows you how to add Python packages:"
939	Customizing environment templates	"If you want you can use the provided template to add the custom libraries. There is a different template for Python and for R. The following example shows you how to add Python packages:  
Important when customizing:  
Before you customize a package, verify that the changes you are planning have the intended effect. If you can get a package through conda from the default channels and through pip from PyPI, the preferred method is through conda from the default channels. Conda does dependency checking when installing packages which can be memory intensive if you add many packages to the customization. Ensure that you select an environment with sufficient RAM to enable dependency checking at the time the runtime is started. To prevent unnecessary dependency checking if you only want packages from one Conda channel, exclude the default channels by removing defaults from the channels list in the template and adding nodefaults. In addition to the Anaconda main channel, many packages for R can be found in Anaconda's R channel. In R environments, this channel is already part of the default channels, hence it does not need to be added separately. If you add packages only through pip or only through conda to the customization template, you must make sure that dependencies is not commented out in the template. When you specify a package version, use a single = for conda packages and == for pip packages. Wherever possible, specify a version number as this reduces the installation time and memory consumption significantly. If you don't specify a version, the package manager might pick the latest version available, or keep the version that is available in the package. You cannot add arbitrary notebook extensions as a customization because notebook extensions must be pre-installed.  
conda can report the changes required for installing a given package, without actually installing it. You can verify the changes from your notebook. For example, for the library Plotly: pip does install the package. However, restarting the runtime again after verification will remove the package. Here too you verify the changes from your notebook. For example, for the library Plotly:  
In a Python notebook, enter: !conda install --dry-run plotly In an R notebook, enter: print(system2(""conda"", args=c(""install"",""--dry-run"",""r-plotly""), stdout=TRUE))"
940	Customizing environment templates	"In a Python notebook, enter: !conda install --dry-run plotly In an R notebook, enter: print(system2(""conda"", args=c(""install"",""--dry-run"",""r-plotly""), stdout=TRUE))  
In a Python notebook, enter: !pip install plotly In an R notebook, enter: print(system2(""pip"", args=""install plotly"", stdout=TRUE))  
Apply your changes."
941	Customizing environment templates	"Learn more
Examples of customizations Installing custom packages through a notebook  
Parent topic: Managing compute resources"
942	Retrieval-augmented generation (RAG)	"You can use foundation models in IBM watsonx.ai to generate factually accurate output that is grounded in information in a knowledge base by applying the retrieval-augmented generation pattern.  
   
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video chapters [ 0:08 ] Scenario description [ 0:27 ] Overview of pattern [ 1:03 ] Knowledge base [ 1:22 ] Search component [ 1:41 ] Prompt augmented with context [ 2:13 ] Generating output [ 2:31 ] Full solution [ 2:55 ] Considerations for search [ 3:58 ] Considerations for prompt text [ 5:01 ] Considerations for explainability"
943	Retrieval-augmented generation (RAG)	"Providing context in your prompt improves accuracy
Foundation models can generate output that is factually inaccurate for various reasons. One way to improve the accuracy of generated output is to provide the necessary facts as context in your prompt text."
944	Retrieval-augmented generation (RAG)	"Providing context in your prompt improves accuracy
Example
The following prompt includes context to establish some facts:  
Unless Aisha is a famous person whose favorite color was mentioned in many online articles that are included in common pretraining data sets, without the context at the beginning of the prompt, no foundation model can reliably generate the correct completion of the sentence at the end of the prompt.  
If you prompt a model with text that includes fact-filled context, then the output the model generates is more likely to be accurate. For more details, see Generating factually accurate output."
945	Retrieval-augmented generation (RAG)	"The retrieval-augmented generation pattern
You can scale out the technique of including context in your prompts by using information in a knowledge base.  
The following diagram illustrates the retrieval-augmented generation pattern. Although the diagram shows a question-answering example, the same workflow supports other use cases.  
The retrieval-augmented generation pattern involves the following steps:  
Search in your knowledge base for content that is related to the user's input. Pull the most relevant search results into your prompt as context and add an instruction, such as “Answer the following question by using only information from the following passages.” Only if the foundation model that you're using is not instruction-tuned: Add a few examples that demonstrate the expected input and output format. Send the combined prompt text to the model to generate output."
946	Retrieval-augmented generation (RAG)	"The retrieval-augmented generation pattern
The origin of retrieval-augmented generation
The term retrieval-augmented generation (RAG) was introduced in this paper: Retrieval-augmented generation for knowledge-intensive NLP tasks.  
“We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.”  
In that paper, the term RAG models refers to a specific implementation of a retriever (a specific query encoder and vector-based document search index) and a generator (a specific pre-trained, generative language model). However, the basic search-and-generate approach can be generalized to use different retriever components and foundation models."
947	Retrieval-augmented generation (RAG)	"The retrieval-augmented generation pattern
Knowledge base
The knowledge base can be any collection of information-containing artifacts, such as:  
Process information in internal company wiki pages Files in GitHub (in any format: Markdown, plain text, JSON, code) Messages in a collaboration tool Topics in product documentation, which can include long text blocks Text passages in a database that supports structured query language (SQL) queries, such as Db2 A document store with a collection of files, such as legal contracts that are stored as PDF files Customer support tickets in a content management system"
948	Retrieval-augmented generation (RAG)	"The retrieval-augmented generation pattern
Retriever
The retriever can be any combination of search and content tools that reliably returns relevant content from the knowledge base, including search tools like IBM Watson Discovery or search and content APIs like those provided by GitHub.  
Vector databases are especially effective retrievers. A vector database stores not only the data, but also a vector embedding of the data, which is a numerical representation of the data that captures its semantic meaning. At query time, a vector embedding of the query text is used to find relevant matches.  
IBM watsonx.ai offers an embedding API and embedding models that you can use to convert sentences and passaged into text embeddings. For more information, see Text embedding generation.  
Watsonx.ai does not include a vector database, but you can use the foundation models in watsonx.ai with any vector database on the market. The example notebooks illustrate the steps for connecting to popular vector databases, such as Chroma and Elasticsearch."
949	Retrieval-augmented generation (RAG)	"The retrieval-augmented generation pattern
Generator
The generator component can use any model in watsonx.ai, whichever one suits your use case, prompt format, and content you are pulling in for context."
950	Retrieval-augmented generation (RAG)	"Sample project
Import a sample project with notebooks and other assets that implement a question and answer solution by using retrieval-augmented generation. The project uses HTML and PDF files as the knowledge base and an Elasticsearch vector index as the retriever. A Python function queries the index to search for information related to a question, and then inferences a foundation model to generate an answer. The function checks the answer for hallucinated content before the answer is returned. Try the Q&A with RAG Accelerator sample project."
951	Retrieval-augmented generation (RAG)	"Examples
The following examples demonstrate how to apply the retrieval-augmented generation pattern."
952	Retrieval-augmented generation (RAG)	"Learn more
Text embedding generation  
Try these tutorials:  
Prompt a foundation model by using Prompt Lab Prompt a foundation model with the retrieval-augmented generation pattern  
Parent topic: Developing generative AI solutions"
953	ibm-watson-studio-lib for Python	"The ibm-watson-studio-lib library for Python provides access to assets. It can be used in notebooks that are created in the notebook editor. ibm-watson-studio-lib provides support for working with data assets and connections, as well as browsing functionality for all other asset types.  
There are two kinds of data assets:  
Stored data assets refer to files in the storage associated with the current project. The library can load and save these files. For data larger than one megabyte, this is not recommended. The library requires that the data is kept in memory in its entirety, which might be inefficient when processing huge data sets. Connected data assets represent data that must be accessed through a connection. Using the library, you can retrieve the properties (metadata) of the connected data asset and its connection. The functions do not return the data of a connected data asset. You can either use the code that is generated for you when you click Read data on the Code snippets pane to access the data or you must write your own code.  
Note: The ibm-watson-studio-lib functions do not encode or decode data when saving data to or getting data from a file. Additionally, the ibm-watson-studio-lib functions can't be used to access connected folder assets (files on a path to the project storage)."
954	ibm-watson-studio-lib for Python	"Setting up the ibm-watson-studio-lib library
The ibm-watson-studio-lib library for Python is pre-installed and can be imported directly in a notebook in the notebook editor. To use the ibm-watson-studio-lib library in your notebook, you need the ID of the project and the project token.  
To insert the project token to your notebook:  
from ibm_watson_studio_lib import access_project_or_space wslib = access_project_or_space({""token"":""<ProjectToken>""})  
Click the More icon on your notebook toolbar and then click Insert project token.  
If a project token exists, a cell is added to your notebook with the following information:  
<ProjectToken> is the value of the project token.  
If you are told in a message that no project token exists, click the link in the message to be redirected to the project's Access Control page where you can create a project token. You must be eligible to create a project token. For details, see Manually adding the project token.  
To create a project token:  
From the Manage tab, select the Access Control page, and click New access token under Access tokens. Enter a name, select Editor role for the project, and create a token. Go back to your notebook, click the More icon on the notebook toolbar and then click Insert project token."
955	ibm-watson-studio-lib for Python	"Helper functions
You can get information about the supported functions in the ibm-watson-studio-lib library programmatically by using help(wslib), or for an individual function by using help(wslib.<function_name>, for example help(wslib.get_connection).  
You can use the helper function wslib.show(...) for formatted printing of Python dictionaries and lists of dictionaries, which are the common result output type of the ibm-watson-studio-lib functions."
956	ibm-watson-studio-lib for Python	"The ibm-watson-studio-lib functions
The ibm-watson-studio-lib library exposes a set of functions that are grouped in the following way:  
Get project information  
Get authentication token  
Fetch data  
Save data  
Get connection information  
Get connected data information  
Access assets by ID instead of name  
Access project storage directly  
Spark support  
Browse project assets"
957	ibm-watson-studio-lib for Python	"The ibm-watson-studio-lib functions
Get project information
While developing code, you might not know the exact names of data assets or connections. The following functions provide lists of assets, from which you can pick the relevant ones. In all examples, you can use wslib.show(assets) to pretty-print the list. The index of each item is printed in front of the item.  
# Import the lib from ibm_watson_studio_lib import access_project_or_space wslib = access_project_or_space({""token"":""<ProjectToken>""}) assets = wslib.list_connections() wslib.show(assets) connprops = wslib.get_connection(assets[0]) wslib.show(connprops)  
list_connections()  
This function returns a list of the connections. The list of returned connections is not sorted by any criterion and can change when you call the function again. You can pass a dictionary item instead of a name to the get_connection function.  
For example:  
list_connected_data()  
This function returns the connected data assets. The list of returned connected data assets is not sorted by any criterion and can change when you call the function again. You can pass a dictionary item instead of a name to the get_connected_data function.  
list_stored_data()  
This function returns a list of the stored data assets (data files). The list of returned data assets is not sorted by any criterion and can change when you call the function again. You can pass a dictionary item instead of a name to the load_data and save_datafunctions.  
Note: A heuristic is applied to distinguish between connected data assets and stored data assets. However, there may be cases where a data asset of the wrong kind appears in the returned lists.  
wslib.here  
By using this entry point, you can retrieve metadata about the project that the lib is working with. The entry point wslib.here provides the following functions:  
get_name()  
This function returns the name of the project.  
get_description()  
This function returns the description of the project.  
get_ID()  
This function returns the ID of the project.  
get_storage()  
This function returns storage information for the project."
958	ibm-watson-studio-lib for Python	"The ibm-watson-studio-lib functions
Get authentication token
Some tasks require an authentication token. For example, if you want to run your own requests against the Watson Data API, you need an authentication token.  
You can use the following function to get the bearer token:  
get_current_token()  
For example:  
This function returns the bearer token that is currently used by the ibm-watson-studio-lib library."
959	ibm-watson-studio-lib for Python	"The ibm-watson-studio-lib functions
Fetch data
You can use the following functions to fetch data from a stored data asset (a file) in your project.  
load_data(asset_name_or_item, attachment_type_or_item=None)  
This function loads the data of a stored data asset into a BytesIO buffer. The function is not recommended for very large files.  
The function takes the following parameters:  
# Import the lib from ibm_watson_studio_lib import access_project_or_space wslib = access_project_or_space({""token"":""<ProjectToken>""}) # Fetch the data from a file my_file = wslib.load_data(""MyFile.csv"") # Read the CSV data file into a pandas DataFrame my_file.seek(0) import pandas as pd pd.read_csv(my_file, nrows=10)  
asset_name_or_item: (Required) Either a string with the name of a stored data asset or an item like those returned by list_stored_data().  
attachment_type_or_item: (Optional) Attachment type to load. A data asset can have more than one attachment with data. Without this parameter, the default attachment type, namely data_asset is loaded. Specify this parameter if the attachment type is not data_asset. For example, if a plain text data asset has an attached profile from Natural Language Analysis, this can be loaded as attachment type data_profile_nlu.  
Here is an example that shows you how to load the data of a data asset:  
download_file(asset_name_or_item, file_name=None, attachment_type_or_item=None)  
This function downloads the data of a stored data asset and stores it in the specified file in the file system of your runtime. The file is overwritten if it already exists.  
The function takes the following parameters:  
# Import the lib from ibm_watson_studio_lib import access_project_or_space wslib = access_project_or_space({""token"":""<ProjectToken>""}) # Let's assume you have a Python script ""helpers.py"" with helper functions on your local machine. # Upload the script to your project using the Data Panel on the right of the opened notebook. # Download the script to the file system of your runtime wslib.download_file(""helpers.py"") # import the required functions to use them in your notebook from helpers import my_func my_func()"
960	ibm-watson-studio-lib for Python	"The ibm-watson-studio-lib functions
Fetch data
asset_name_or_item: (Required) Either a string with the name of a stored data asset or an item like those returned by list_stored_data().  
file_name: (Optional) The name of the file that the downloaded data is stored to. It defaults to the asset's attachment name.  
attachment_type_or_item: (Optional) The attachment type to download. A data asset can have more than one attachment with data. Without this parameter, the default attachment type, namely data_asset is downloaded. Specify this parameter if the attachment type is not data_asset. For example, if a plain text data asset has an attached profile from Natural Language Analysis, this can be downlaoded loaded as attachment type data_profile_nlu.  
Here is an example that shows you how to you can use download_file to make your custom Python script available in your notebook:"
961	ibm-watson-studio-lib for Python	"The ibm-watson-studio-lib functions
Save data
The functions to save data in your project storage do multiple things:  
Store the data in project storage Add the data as a data asset (by creating an asset or overwriting an existing asset) to your project so you can see the data in the data assets list in your project. Associate the asset with the file in the storage.  
You can use the following functions to save data:  
save_data(asset_name_or_item, data, overwrite=None, mime_type=None, file_name=None)  
This function saves data in memory to the project storage.  
The function takes the following parameters:  
# Import the lib from ibm_watson_studio_lib import access_project_or_space wslib = access_project_or_space({""token"":""<ProjectToken>""}) # let's assume you have the pandas DataFrame pandas_df which contains the data # you want to save as a csv file wslib.save_data(""my_asset_name.csv"", pandas_df.to_csv(index=False).encode()) # the function returns a dict which contains the asset_name, asset_id, file_name and additional information upon successful saving of the data  
asset_name_or_item: (Required) The name of the created asset or list item that is returned by list_stored_data(). You can use the item if you like to overwrite an existing file.  
data: (Required) The data to upload. This can be any object of type bytes-like-object, for example a byte buffer.  
overwrite: (Optional) Overwrites the data of a stored data asset if it already exists. By default, this is set to false. If an asset item is passed instead of a name, the behavior is to overwrite the asset.  
mime_type: (Optional) The MIME type for the created asset. By default the MIME type is determined from the asset name suffix. If you use asset names without a suffix, specify the MIME type here. For example mime_type=application/text for plain text data. This parameter is ignored when overwriting an asset."
962	ibm-watson-studio-lib for Python	"The ibm-watson-studio-lib functions
Save data
file_name: (Optional) The file name to be used in the project storage. The data is saved in the storage associated with the project. When creating a new asset, the file name is derived from the asset name, but might be different. If you want to access the file directly, you can specify a file name. This parameter is ignored when overwriting an asset.  
Here is an example that shows you how to save data to a file:  
upload_file(file_path, asset_name=None, file_name=None, overwrite=False, mime_type=None) This function saves data in the file system in the runtime to a file associated with your project. The function takes the following parameters:  
# Import the lib from ibm_watson_studio_lib import access_project_or_space wslib = access_project_or_space({""token"":""<ProjectToken>""}) # Let's assume you have downloaded a file and want to save it # in your project. import urllib.request urllib.request.urlretrieve(""https://some/url/data_file.csv"", ""data_file.csv"") wslib.upload_file(""data_file.csv"") # The function returns a dictionary which contains the asset_name, asset_id, file_name and additional information upon successful saving of the data.  
file_path: (Required) The path to the file in the file system.  
asset_name: (Optional) The name of the data asset that is created. It defaults to the name of the file to be uploaded.  
file_name: (Optional) The name of the file that is created in the storage associated with the project. It defaults to the name of the file to be uploaded.  
overwrite: (Optional) Overwrites an existing file in storage. Defaults to false.  
mime_type: (Optional) The MIME type for the created asset. By default the MIME type is determined from the asset name suffix. If you use asset names without a suffix, specify the MIME type here. For example mime_type='application/text' for plain text data. This parameter is ignored when overwriting an asset.  
Here is an example that shows you how you can upload a file to the project:"
963	ibm-watson-studio-lib for Python	"The ibm-watson-studio-lib functions
Get connection information
You can use the following function to access the connection metadata of a given connection.  
get_connection(name_or_item)  
This function returns the properties (metadata) of a connection which you can use to fetch data from the connection data source. Use wslib.show(connprops) to view the properties. The special key ""."" in the returned dictionary provides information about the connection asset.  
The function takes the following required parameter:  
name_or_item: Either a string with the name of a connection or an item like those returned by list_connections().  
Note that when you work with notebooks, you can click Read data on the Code snippets pane to generate code to load data from a connection into a pandas DataFrame for example."
964	ibm-watson-studio-lib for Python	"The ibm-watson-studio-lib functions
Get connected data information
You can use the following function to access the metadata of a connected data asset.  
get_connected_data(name_or_item)  
This function returns the properties of a connected data asset, including the properties of the underlying connection. Use wslib.show() to view the properties. The special key ""."" in the returned dictionary provides information about the data and the connection assets.  
The function takes the following required parameter:  
name_or_item: Either a string with the name of a connected data asset or an item like those returned by list_connected_data().  
Note that when you work with notebooks, you can click Read data on the Code snippets pane to generate code to load data from a connected data asset into a pandas DataFrame for example."
965	ibm-watson-studio-lib for Python	"The ibm-watson-studio-lib functions
Access asset by ID instead of name
You should preferably always access data assets and connections by a unique name. Asset names are not necessarily always unique and the ibm-watson-studio-lib functions will raise an exception when a name is ambiguous. You can rename data assets in the UI to resolve the conflict.  
Accessing assets by a unique ID is possible but is discouraged as IDs are valid only in the current project and will break code when transferred to a different project. This can happen for example, when projects are exported and re-imported. You can get the ID of a connection, connected or stored data asset by using the corresponding list function, for example list_connections().  
The entry point wslib.by_id provides the following functions:  
get_connection(asset_id)  
This function accesses a connection by the connection asset ID.  
get_connected_data(asset_id)  
This function accesses a connected data asset by the connected data asset ID.  
load_data(asset_id, attachment_type_or_item=None)  
This function loads the data of a stored data asset by passing the asset ID. See load_data() for a description of the other parameters you can pass.  
save_data(asset_id, data, overwrite=None, mime_type=None, file_name=None)  
This function saves data to a stored data asset by passing the asset ID. This implies overwrite=True. See save_data() for a description of the other parameters you can pass.  
download_file(asset_id, file_name=None, attachment_type_or_item=None)  
This function downloads the data of a stored data asset by passing the asset ID. See download_file() for a description of the other parameters you can pass."
966	ibm-watson-studio-lib for Python	"The ibm-watson-studio-lib functions
Access project storage directly
You can fetch data from project storage and store data in project storage without synchronizing the project assets using the entry point wslib.storage.  
The entry point wslib.storage provides the following functions:  
fetch_data(filename)  
This function returns the data in a file as a BytesIO buffer. The file does not need to be registered as a data asset.  
The function takes the following required parameter:  
filename: The name of the file in the projectstorage.  
store_data(filename, data, overwrite=False)  
This function saves data in memory to storage, but does not create a new data asset. The function returns a dictionary which contains the file name, file path and additional information. Use wslib.show() to print the information.  
The function takes the following parameters:  
filename: (Required) The name of the file in the project storage. data: (Required) The data to save as a bytes-like object. overwrite: (Optional) Overwrites the data of a file in storage if it already exists. By default, this is set to false.  
download_file(storage_filename, local_filename=None)  
This function downloads the data in a file in storage and stores it in the specified local file. The local file is overwritten if it already existed.  
The function takes the following parameters:  
storage_filename: (Required) The name of the file in storage to download. local_filename: (Optional) The name of the file in the local file system of your runtime to downloaded the file to. Omit this parameter to use the storage file name.  
register_asset(storage_path, asset_name=None, mime_type=None)  
This function registers the file in storage as a data asset in your project. This operation fails if a data asset with the same name already exists.  
You can use this function if you have very large files that you cannot upload via save_data(). You can upload large files directly to the IBM Cloud Object Storage bucket of your project, for example via the UI, and then register them as data assets using register_asset().  
The function takes the following parameters:"
967	ibm-watson-studio-lib for Python	"The ibm-watson-studio-lib functions
Access project storage directly
The function takes the following parameters:  
storage_path: (Required) The path of the file in storage. asset_name: (Optional) The name of the created asset. It defaults to the file name. mime_type: (Optional) The MIME type for the created asset. By default the MIME type is determined from the asset name suffix. Use this parameter to specify a MIME type if your file name does not have a file extension or if you want to set a different MIME type.  
Note: You can register a file several times as a different data asset. Deleting one of those assets in the project also deletes the file in storage, which means that other asset references to the file might be broken."
968	ibm-watson-studio-lib for Python	"The ibm-watson-studio-lib functions
Spark support
The entry point wslib.spark provides functions to access files in storage with Spark. To get help information about the available functions, use help(wslib.spark.API).  
The entry point wslib.spark provides the following functions:  
from ibm_watson_studio_lib import access_project_or_space wslib = access_project_or_space({""token"":""<ProjectToken>""}) wslib.spark.provide_spark_context(sc)  
provide_spark_context(sc)  
Use this function to enable Spark support.  
The function takes the following required parameter:  
sc: The SparkContext. It is provided in the notebook runtime.  
The following example shows you how to set up Spark support:  
get_data_url(asset_name)  
This function returns a URL to access a file in storage from Spark via Hadoop.  
The function takes the following required parameter:  
asset_name: The name of the asset.  
storage.get_data_url(file_name)  
This function returns a URL to access a file in storage from Spark via Hadoop. The function expects the file name and not the asset name.  
The function takes the following required parameter:  
file_name: The name of a file in the project storage."
969	ibm-watson-studio-lib for Python	"The ibm-watson-studio-lib functions
Browse project assets
The entry point wslib.assets provides generic, read-only access to assets of any type. For selected asset types, there are dedicated functions that provide additional data. To get help on the available functions, use help(wslib.assets.API).  
The following naming conventions apply:  
Functions named list_<something> return a list of Python dictionaries. Each dictionary represents one asset and includes a small set of properties (metadata) that identifies the asset. Functions named get_<something> return a single Python dictionary with the properties for the asset.  
To pretty-print a dictionary or list of dictionaries, use wslib.show().  
The functions expect either the name of an asset, or an item from a list as the parameter. By default, the functions return only a subset of the available asset properties. By setting the parameter raw=True, you can get the full set of asset properties.  
The entry point wslib.assets provides the following functions:  
# Import the lib from ibm_watson_studio_lib import access_project_or_space wslib = access_project_or_space({""token"":""<ProjectToken>""}) # List all assets in the project all_assets = wslib.assets.list_assets(""asset"") wslib.show(all_assets) # List all data assets with name 'MyFile.csv' assets_by_name = wslib.assets.list_assets(""data_asset"", name=""MyFile.csv"") # List all data assets whose name starts with ""MyF"" assets_by_query = wslib.assets.list_assets(""data_asset"", query=""asset.name:(MyF*)"") # List all data assets which are larger than 1MB sizeFilter = lambda x: x['metadata']['size'] > 1000000 large_assets = wslib.assets.list_assets(""data_asset"", selector=sizeFilter, raw=True) # List all notebooks notebooks = wslib.assets.list_assets(""notebook"") notebooks = wslib.assets.list_assets('notebook') wslib.show(notebooks) notebook = wslib.assets.get_asset(notebooks[0]) wslib.show(notebook) assets = wslib.list_stored_data() wslib.show(assets) asset = assets[0] attachments = wslib.assets.list_attachments(asset) wslib.show(attachments) buffer = wslib.load_data(asset, attachments[0])  
list_assets(asset_type, name=None, query=None, selector=None, raw=False)"
970	ibm-watson-studio-lib for Python	"The ibm-watson-studio-lib functions
Browse project assets
list_assets(asset_type, name=None, query=None, selector=None, raw=False)  
This function lists all assets for the given type with respect to the given constraints.  
The function takes the following parameters:  
asset_type: (Required) The type of the assets to list, for example data_asset. See list_asset_types() for a list of the available asset types. Use asset type asset for the list of all available assets in the project. name: (Optional) The name of the asset to list. Use this parameter if more than one asset with the same name exists. You can only specify either name and query. query: (Optional) A query string that is passed to the Watson Data API to search for assets. You can only specify either name and query. selector: (Optional) A custom filter function on the candidate asset dictionary items. If the selector function returns True, the asset is included in the returned asset list. raw: (Optional) Returns all of the available metadata. By default, the parameter is set to False and only a subset of the properties is returned.  
Examples of using the list_assets function:  
list_asset_types(raw=False)  
This function lists all available asset types.  
The function can take the following parameter:  
raw: (Optional) Returns the full set of metadata. By default, the parameter is False and only a subset of the properties is returned.  
list_datasource_types(raw=False)  
This function lists all available data source types.  
The function can take the following parameter:  
raw: (Optional) Returns the full set of metadata. By default, the parameter is False and only a subset of the properties is returned.  
get_asset(name_or_item, asset_type=None, raw=False)  
The function returns the metadata of an asset.  
The function takes the following parameters:  
name_or_item: (Required) The name of the asset or an item like those returned by list_assets() asset_type: (Optional) The type of the asset. If the parameter name_or_item contains a string for the name of the asset, setting asset_type is required. raw: (Optional) Returns the full set of metadata. By default, the parameter is False and only a subset of the properties is returned."
971	ibm-watson-studio-lib for Python	"The ibm-watson-studio-lib functions
Browse project assets
Example of using the list_assets and get_asset functions:  
get_connection(name_or_item, with_datasourcetype=False, raw=False)  
This function returns the metadata of a connection.  
The function takes the following parameters:  
name_or_item: (Required) The name of the connection or an item like those returned by list_connections() with_datasourcetype: (Optional) Returns additional information about the data source type of the connection. raw: (Optional) Returns the full set of metadata. By default, the parameter is False and only a subset of the properties is returned.  
get_connected_data(name_or_item, with_datasourcetype=False, raw=False)  
This function returns the metadata of a connected data asset.  
The function takes the following parameters:  
name_or_item: (Required) The name of the connected data asset or an item like those returned by list_connected_data() with_datasourcetype: (Optional) Returns additional information about the data source type of the associated connected data asset. raw: (Optional) Returns the full set of metadata. By default, the parameter is False and only a subset of the properties is returned.  
get_stored_data(name_or_item, raw=False)  
This function returns the metadata of a stored data asset.  
The function takes the following parameters:  
name_or_item: (Required) The name of the stored data asset or an item like those returned by list_stored_data() raw: (Optional) Returns the full set of metadata. By default, the parameter is False and only a subset of the properties is returned.  
list_attachments(name_or_item_or_asset, asset_type=None, raw=False)  
This function returns a list of the attachments of an asset.  
The function takes the following parameters:  
name_or_item_or_asset: (Required) The name of the asset or an item like those returned by list_stored_data() or get_asset(). asset_type: (Optional) The type of the asset. It defaults to type data_asset. raw: (Optional) Returns the full set of metadata. By default, the parameter is False and only a subset of the properties is returned.  
Example of using the list_attachments function to read an attachment of a stored data asset:"
972	ibm-watson-studio-lib for Python	"The ibm-watson-studio-lib functions
Browse project assets
Example of using the list_attachments function to read an attachment of a stored data asset:  
Parent topic: Using ibm-watson-studio-lib"
973	Time series implementation details	These implementation details describe the stages and processing that are specific to an AutoAI time series experiment.
974	Time series implementation details	"Implementation details
Refer to these implementation and configuration details for your time series experiment.  
Time series stages for processing an experiment. Time series optimizing metrics for tuning your pipelines. Time series algorithms for building the pipelines. Supported date and time formats."
975	Time series implementation details	"Time series stages
An AutoAI time series experiment includes these stages when an experiment runs:  
Initialization Pipeline selection Model evaluation Final pipeline generation Backtest"
976	Time series implementation details	"Time series stages
Stage 1: Initialization
The initialization stage processes the training data, in this sequence:  
Load the data Split the data set L into training data T and holdout data H Set the validation, timestamp column handling, and lookback window generation. Notes:  
The training data (T) is equal to the data set (L) minus the holdout (H). When you configure the experiment, you can adjust the size of the holdout data. By default, the size of the holdout data is 20 steps. You can optionally specify the timestamp column. By default, a lookback window is generated automatically by detecting the seasonal period by using signal processing method. However, if you have an idea of an appropriate lookback window, you can specify the value directly."
977	Time series implementation details	"Time series stages
Stage 2: Pipeline selection
The pipeline selection step uses an efficient method called T-Daub (Time Series Data Allocation Using Upper Bounds). The method selects pipelines by allocating more training data to the most promising pipelines, while allocating less training data to unpromising pipelines. In this way, not all pipelines see the complete set of data, and the selection process is typically faster. The following steps describe the process overview:  
All pipelines are sequentially allocated several small subsets of training data. The latest data is allocated first. Each pipeline is trained on every allocated subset of training data and evaluated with testing data (holdout data). A linear regression model is applied to each pipeline by using the data set described in the previous step. The accuracy score of the pipeline is projected on the entire training data set. This method results in a data set containing the accuracy and size of allocated data for each pipeline. The best pipeline is selected according to the projected accuracy and allotted rank 1. More data is allocated to the best pipeline. Then, the projected accuracy is updated for the other pipelines. The prior two steps are repeated until the top N pipelines are trained on all the data."
978	Time series implementation details	"Time series stages
Stage 3: Model evaluation
In this step, the winning pipelines N are retrained on the entire training data set T. Further, they are evaluated with the holdout data H."
979	Time series implementation details	"Time series stages
Stage 4: Final pipeline generation
In this step, the winning pipelines are retrained on the entire data set (L) and generated as the final pipelines.  
As the retraining of each pipeline completes, the pipeline is posted to the leaderboard. You can select to inspect the pipeline details or save the pipeline as a model."
980	Time series implementation details	"Time series stages
Stage 5: Backtest
In the final step, the winning pipelines are retrained and evaluated by using the backtest method. The following steps describe the backtest method:  
The training data length is determined based on the number of backtests, gap length, and holdout size. To learn more about these parameters, see Building a time series experiment. Starting from the oldest data, the experiment is trained by using the training data. Further, the experiment is evaluated on the first validation data set. If the gap length is non-zero, any data in the gap is skipped over. The training data window is advanced by increasing the holdout size and gap length to form a new training set. A fresh experiment is trained with this new data and evaluated with the next validation data set. The prior two steps are repeated for the remaining backtesting periods."
981	Time series implementation details	"Time series optimization metrics
Accept the default metric, or choose a metric to optimize for your experiment."
982	Time series implementation details	"Time series optimization metrics
Reviewing the metrics for an experiment
When you view the results for a time series experiment, you see the values for metrics used to train the experiment in the pipeline leaderboard:  
You can see that the accuracy measures for time-series experiments may vary widely, depending on the experiment data evaluated.  
Validation is the score calculated on training data. Holdout is the score calculated on the reserved holdout data. Backtest is the mean score from all backtests scores."
983	Time series implementation details	"Time series algorithms
These algorithms are available for your time series experiment. You can use the algorithms that are selected by default, or you can configure your experiment to include or exclude specific algorithms."
984	Time series implementation details	"Supported date and time formats
The date/time formats supported in time series experiments are based on the definitions that are provided by dateutil.  
Supported date formats are:  
Common:  
Uncommon:  
Numberng for the ISO week and day values follows the same logic as datetime.date.isocalendar().  
Supported time formats are:  
Notes:  
Midnight can be represented as 00:00 or 24:00. The decimal separator can be either a period or a comma. Dates can be submitted as strings, with double quotation marks, such as ""1958-01-16""."
985	Time series implementation details	"Supporting features
Supporting features, also known as exogenous features, are input features that can influence the prediction target. You can use supporting features to include additional columns from your data set to improve the prediction and increase your model’s accuracy. For example, in a time series experiment to predict prices over time, a supporting feature might be data on sales and promotions. Or, in a model that forecasts energy consumption, including daily temperature makes the forecast more accurate."
986	Time series implementation details	"Supporting features
Algorithms and pipelines that use Supporting features
Only a subset of algorithms allow supporting features. For example, Holt-winters and BATS do not support the use of supporting features. Algorithms that do not support supporting features ignore your selection for supporting features when you run the experiment.  
Some algorithms use supporting features for certain variations of the algorithm, but not for others. For example, you can generate two different pipelines with the Random Forest algorithm, RandomForestRegressor and ExogenousRandomForestRegressor. The ExogenousRandomForestRegressor variation provides support for supporting features, whereas RandomForestRegressor does not.  
This table details whether an algorithm provides support for Supporting features in a time series experiment:"
987	Time series implementation details	"Learn more
Scoring a time series model  
Parent topic: Building a time series experiment"
988	The parts of a notebook	"You can see some information about a notebook before you open it on the Assets page of a project. When you open a notebook in edit mode, you can do much more with the notebook by using multiple menu options, toolbars, an information pane, and by editing and running the notebook cells.  
You can view the following information about a notebook by clicking the Notebooks asset type in the Assets page of your project:  
The name of the notebook The date when the notebook was last modified and the person who made the change The programming language of the notebook Whether the notebook is currently locked  
When you open a notebook in edit mode, the notebook editor includes the following features:  
Menu bar and toolbar  
Notebook action bar  
The cells in a Jupyter notebook  
Jupyter Code cells Jupyter markdown cells Raw Jupyter NBConvert cells  
Spark job progress bar  
Project token for authorization"
989	The parts of a notebook	"Menu bar and toolbar
You can select notebook features that affect the way the notebook functions and perform the most-used operations within the notebook by clicking an icon."
990	The parts of a notebook	"Notebook action bar
You can select features that enhance notebook collaboration. From the action bar, you can:  
Publish your notebook as a gist or on GitHub. Create a permanent URL so that anyone with the link can view your notebook. Create jobs in which to run your notebook. See Schedule a notebook. Download your notebook. Add a project token so that code can access the project resources. See Add code to set the project token. Generate code snippets to add data from a data asset or a connection to a notebook cell. View your notebook information. You can: Save versions of your notebook. Upload assets to the project.  
Change the name of your notebook by editing it in the Name field. Edit the description of your notebook in the Description field. View the date when the notebook was created. View the environment details and runtime status; you can change the notebook runtime from here. See Notebook environments."
991	The parts of a notebook	"The cells in a Jupyter notebook
A Jupyter notebook consists of a sequence of cells. The flow of a notebook is sequential. You enter code into an input cell, and when you run the cell, the notebook executes the code and prints the output of the computation to an output cell.  
You can change the code in an input cell and re-run the cell as often as you like. In this way, the notebook follows a read-evaluate-print loop paradigm. You can choose to use tags to describe cells in a notebook.  
The behavior of a cell is determined by a cell’s type. The different types of cells include:"
992	The parts of a notebook	"The cells in a Jupyter notebook
Jupyter code cells
Where you can edit and write new code."
993	The parts of a notebook	"The cells in a Jupyter notebook
Jupyter markdown cells
Where you can document the computational process. You can input headings to structure your notebook hierarchically.  
You can also add and edit image files as attachments to the notebook. The markdown code and images are rendered when the cell is run.  
See Markdown for Jupyter notebooks cheatsheet."
994	The parts of a notebook	"The cells in a Jupyter notebook
Raw Jupyter NBConvert cells
Where you can write output directly or put code that you don’t want to run. Raw cells are not evaluated by the notebook."
995	The parts of a notebook	"Spark job progress bar
When you run code in a notebook that triggers Spark jobs, it is often challenging to determine why your code is not running efficiently.  
To help you better understand what your code is doing and assist you in code debugging, you can monitor the execution of the Spark jobs for a code cell.  
To enable Spark monitoring for a cell in a notebook:  
Select the code cell you want to monitor. Click the Enable Spark Monitoring icon () on the notebook toolbar.  
The progress bars you see display the real time runtime progress of your jobs on the Spark cluster. Each Spark job runs on the cluster in one or more stages, where each stage is a list of tasks that can be run in parallel. The monitoring pane can become very large is the Spark job has many stages.  
The job monitoring pane also displays the duration of each job and the status of the job stages. A stage can have one of the following statuses:  
Running: Stage active and started. Completed: Stage completed. Skipped: The results of this stage were cached from a earlier operation and so the task doesn't have to run again. Pending: Stage hasn't started yet.  
Click the icon again to disable monitoring in a cell.  
Note: Spark monitoring is currently only supported in notebooks that run on Python.  
Parent topic: Creating notebooks"
996	Analyzing data and working with models	"You can analyze data and work with models with tools in projects that provide various levels of automation. The methods that you choose for working with data or models help you determine which tools best fit your needs.  
Each tool has a specific, primary task. Some tools have capabilities for multiple types of tasks.  
You can choose a tool in a project based on how much automation you want:  
Code editor tools: Use to write code in Python or R, all also with Spark. Graphical builder tools: Use menus and drag-and-drop functionality on a builder to visually program. Automated builder tools: Use to configure automated tasks that require limited user input."
997	Analyzing data and working with models	"Learn more
Notebooks and scripts"
998	Developing generative AI solutions with foundation models	"You can develop generative AI solutions with foundation models in IBM watsonx.ai. You can generate prompts to generate, classify, summarize, or extract content from your input text. Choose from IBM models or open source models from Hugging Face. You can tune foundation models to customize your prompt output or optimize inferencing performance.  
Foundation models are large AI models that have billions of parameters and are trained on terabytes of data. Foundation models can do various tasks, including text, code, or image generation, classification, conversation, and more. Large language models are a subset of foundation models that can do tasks related to text and code. Watsonx.ai has a range of deployed large language models for you to try. For details, see Supported foundation models."
999	Developing generative AI solutions with foundation models	"Foundation model architecture
Foundation models represent a fundamentally different model architecture and purpose for AI systems. The following diagram illustrates the difference between traditional AI models and foundation models.  
As shown in the diagram, traditional AI models specialize in specific tasks. Most traditional AI models are built by using machine learning, which requires a large, structured, well-labeled data set that encompasses a specific task that you want to tackle. Often these data sets must be sourced, curated, and labeled by hand, a job that requires people with domain knowledge and takes time. After it is trained, a traditional AI model can do a single task well. The traditional AI model uses what it learns from patterns in the training data to predict outcomes in unknown data. You can create machine learning models for your specific use cases with tools like AutoAI and Jupyter notebooks, and then deploy them.  
In contrast, foundation models are trained on large, diverse, unlabeled data sets and can be used for many different tasks. Foundation models were first used to generate text by calculating the most-probable next word in natural language translation tasks. However, model providers are learning that, when prompted with the right input, foundation models can do various other tasks well. Instead of creating your own foundation models, you use existing deployed models and engineer prompts to generate the results that you need."
1000	Developing generative AI solutions with foundation models	"Methods of working with foundation models
The possibilities and applications of foundation models are just starting to be discovered. Explore and validate use cases with foundation models in watsonx.ai to automate, simplify, and speed up existing processes or provide value in a new way.  
You can interact with foundation models in the following ways:  
Engineer prompts and inference deployed foundation models directly by using the Prompt Lab Inference deployed foundation models programmatically by using the Python library Tune foundation models to return output in a certain style or format by using the Tuning Studio"
1001	Developing generative AI solutions with foundation models	"Learn more
Prompt Lab Tuning Studio Coding generative AI solutions Security and privacy Model terms of use Tokens Retrieval-augmented generation AI risk atlas Quick start tutorials: Working with generative AI"
1002	Choosing compute resources for running tools in projects	"You use compute resources in projects when you run jobs and most tools. Depending on the tool, you might have a choice of compute resources for the runtime for the tool.  
Compute resources are known as either environment templates or hardware and software specifications. In general, compute resources with larger hardware configurations incur larger usage costs.  
These tools have multiple choices for configuring runtimes that you can choose from:  
Notebook editor Data Refinery SPSS Modeler AutoAI Decision Optimization experiment RStudio IDE Synthetic Data Generator Tuning Studio  
Prompt Lab does not consume compute resources. Prompt Lab usage is measured by the number of processed tokens."
1003	Choosing compute resources for running tools in projects	"Learn more
Monitoring account resource usage  
Parent topic: Projects"
1004	Supported foundation models available with watsonx.ai	"A collection of open source and IBM foundation models are deployed in IBM watsonx.ai.  
The following models are available in watsonx.ai:  
granite-13b-chat-v2 granite-13b-instruct-v2 granite-7b-lab granite-8b-japanese granite-20b-multilingual codellama-34b-instruct elyza-japanese-llama-2-7b-instruct flan-t5-xl-3b flan-t5-xxl-11b flan-ul2-20b jais-13b-chat llama-3-8b-instruct llama-3-70b-instruct llama-2-13b-chat llama-2-70b-chat llama2-13b-dpo-v7 merlinite-7b mixtral-8x7b-instruct-v01 mixtral-8x7b-instruct-v01-q mt0-xxl-13b starcoder-15.5b  
You can prompt these models in the Prompt Lab or programmatically.  
To understand how the model provider, instruction tuning, token limits, and other factors can affect which model you choose, see Choosing a model."
1005	Supported foundation models available with watsonx.ai	"IBM foundation models
The following table lists the supported foundation models that IBM provides for inferencing. All IBM models are instruction-tuned. For more information about contractual protections that are related to IBM indemnification, see the IBM Client Relationship Agreement and IBM watsonx.ai service description.  
   
For more information about the supported foundation models that IBM provides for embedding text, see Supported embedding models."
1006	Supported foundation models available with watsonx.ai	"Third-party foundation models
The following table lists the supported foundation models that third parties provide through Hugging Face. All third-party models are instruction-tuned. IBM indemnification does not apply to any third-party models.  
   
For a list of which models are provided in each regional data center, see Regional availability of foundation model. For information about the billing classes and rate limiting, see Watson Machine Learning plans."
1007	Supported foundation models available with watsonx.ai	"Foundation model details
The available foundation models support a range of use cases for both natural languages and programming languages. To see the types of tasks that these models can do, review and try the sample prompts."
1008	Supported foundation models available with watsonx.ai	"Foundation model details
codellama-34b-instruct
A programmatic code generation model that is based on Llama 2 from Meta. Code Llama is fine-tuned for generating and discussing code.  
When you inference this model from the Prompt Lab, disable AI guardrails.  
Usage: Use Code Llama to create prompts that generate code based on natural language inputs, explain code, or that complete and debug code.  
Cost: Class 2. For pricing details, see Watson Machine Learning plans.  
Size: 34 billion parameters  
Token limits  
Context window length (input + output): 16,384 Note: The maximum new tokens, which means the tokens generated by the foundation model, is limited to 4096.  
Supported natural languages: English  
Supported programming languages: The codellama-34b-instruct-hf foundation model supports many programming languages, including Python, C++, Java, PHP, Typescript (Javascript), C#, Bash, and more.  
Instruction tuning information: The instruction fine-tuned version was fed natural language instruction input and the expected output to guide the model to generate helpful and safe answers in natural language.  
Model architecture: Decoder  
License: License  
Learn more  
Model card Research paper Meta AI Blog"
1009	Supported foundation models available with watsonx.ai	"Foundation model details
elyza-japanese-llama-2-7b-instruct
The elyza-japanese-llama-2-7b-instruct model is provided by ELYZA, Inc on Hugging Face. The elyza-japanese-llama-2-7b-instruct foundation model is a version of the Llama 2 model from Meta that is trained to understand and generate Japanese text. The model is fine-tuned for solving various tasks that follow user instructions and for participating in a dialog.  
Note: This foundation model is available only in the Tokyo data center. When you inference this model from the Prompt Lab, disable AI guardrails.  
Usage: General use with zero- or few-shot prompts. Works well for classification and extraction in Japanese and for translation between English and Japanese. Performs best when prompted in Japanese.  
Cost: Class 2. For pricing details, see Watson Machine Learning plans.  
Try it out  
Sample prompt: Classification Sample prompt: Translation  
Size: 7 billion parameters  
Token limits: Context window length (input + output): 4096  
Supported natural languages: Japanese, English  
Instruction tuning information: For Japanese language training, Japanese text from many sources were used, including Wikipedia and the Open Super-large Crawled ALMAnaCH coRpus (a multilingual corpus that is generated by classifying and filtering language in the Common Crawl corpus). The model was fine-tuned on a dataset that was created by ELYZA. The ELYZA Tasks 100 dataset contains 100 diverse and complex tasks that were created manually and evaluated by humans. The ELYZA Tasks 100 dataset is publicly available from HuggingFace.  
Model architecture: Decoder  
License: License  
Learn more  
Blog post on note.com Model card (Tokyo data center) Model card"
1010	Supported foundation models available with watsonx.ai	"Foundation model details
flan-t5-xl-3b
The flan-t5-xl-3b model is provided by Google on Hugging Face. This model is based on the pretrained text-to-text transfer transformer (T5) model and uses instruction fine-tuning methods to achieve better zero- and few-shot performance. The model is also fine-tuned with chain-of-thought data to improve its ability to perform reasoning tasks.  
Note: This foundation model can be tuned by using the Tuning Studio.  
Usage: General use with zero- or few-shot prompts.  
Cost: Class 1. For pricing details, see Watson Machine Learning plans.  
Try it out: Sample prompts  
Size: 3 billion parameters  
Token limits  
Context window length (input + output): 4096  
Note: Lite plan output is limited to 700  
Supported natural languages: Multilingual  
Instruction tuning information: The model was fine-tuned on tasks that involve multiple-step reasoning from chain-of-thought data in addition to traditional natural language processing tasks. Details about the training data sets used are published.  
Model architecture: Encoder-decoder  
License: Apache 2.0 license  
Learn more  
Research paper Model card Sample notebook: Tune a model to classify CFPB documents in watsonx Sample notebook: Prompt Tuning for Multi-class Classification with watsonx"
1011	Supported foundation models available with watsonx.ai	"Foundation model details
flan-t5-xxl-11b
The flan-t5-xxl-11b model is provided by Google on Hugging Face. This model is based on the pretrained text-to-text transfer transformer (T5) model and uses instruction fine-tuning methods to achieve better zero- and few-shot performance. The model is also fine-tuned with chain-of-thought data to improve its ability to perform reasoning tasks.  
Usage: General use with zero- or few-shot prompts.  
Cost: Class 2. For pricing details, see Watson Machine Learning plans.  
Try it out  
Sample prompts  
Sample prompt: Marketing email generation  
Sample notebook: Use watsonx and Google flan-t5-xxl to generate advertising copy  
Sample notebook: Use watsonx and LangChain to make a series of calls to a language model  
Size: 11 billion parameters  
Token limits  
Context window length (input + output): 4096  
Note: Lite plan output is limited to 700  
Supported natural languages: English, German, French  
Instruction tuning information: The model was fine-tuned on tasks that involve multiple-step reasoning from chain-of-thought data in addition to traditional natural language processing tasks. Details about the training data sets used are published.  
Model architecture: Encoder-decoder  
License: Apache 2.0 license  
Learn more  
Research paper Model card"
1012	Supported foundation models available with watsonx.ai	"Foundation model details
flan-ul2-20b
The flan-ul2-20b model is provided by Google on Hugging Face. This model was trained by using the Unifying Language Learning Paradigms (UL2). The model is optimized for language generation, language understanding, text classification, question answering, common sense reasoning, long text reasoning, structured-knowledge grounding, and information retrieval, in-context learning, zero-shot prompting, and one-shot prompting.  
Usage: General use with zero- or few-shot prompts.  
Cost: Class 3. For pricing details, see Watson Machine Learning plans.  
Try it out  
Sample prompts  
Sample prompt: Earnings call summary  
Sample prompt: Meeting transcript summary  
Sample prompt: Scenario classification  
Sample prompt: Sentiment classification  
Sample prompt: Thank you note generation  
Sample prompt: Named entity extraction  
Sample prompt: Fact extraction  
Sample notebook: Use watsonx to summarize cybersecurity documents  
Sample notebook: Use watsonx and LangChain to answer questions by using retrieval-augmented generation (RAG)  
Sample notebook: Use watsonx, Elasticsearch, and LangChain to answer questions (RAG)  
Sample notebook: Use watsonx, and Elasticsearch Python SDK to answer questions (RAG)  
Size: 20 billion parameters  
Token limits  
Context window length (input + output): 4096  
Note: Lite plan output is limited to 700  
Supported natural languages: English  
Instruction tuning information: The flan-ul2-20b model is pretrained on the colossal, cleaned version of Common Crawl's web crawl corpus. The model is fine-tuned with multiple pretraining objectives to optimize it for various natural language processing tasks. Details about the training data sets used are published.  
Model architecture: Encoder-decoder  
License: Apache 2.0 license  
Learn more  
Unifying Language Learning (UL2) research paper  
Fine-tuned Language Model (Flan) research paper  
Model card"
1013	Supported foundation models available with watsonx.ai	"Foundation model details
granite-13b-chat-v2
The granite-13b-chat-v2 model is provided by IBM. This model is optimized for dialog use cases and works well with virtual agent and chat applications.  
Usage: Generates dialog output like a chatbot. Uses a model-specific prompt format. Includes a keyword in its output that can be used as a stop sequence to produce succinct answers.  
Cost: Class 1. For pricing details, see Watson Machine Learning plans.  
Try it out: Sample prompt  
Size: 13 billion parameters  
Token limits: Context window length (input + output): 8192  
Supported natural languages: English  
Instruction tuning information: The Granite family of models is trained on enterprise-relevant data sets from five domains: internet, academic, code, legal, and finance. Data used to train the models first undergoes IBM data governance reviews and is filtered of text that is flagged for hate, abuse, or profanity by the IBM-developed HAP filter. IBM shares information about the training methods and data sets used.  
Model architecture: Decoder  
License  
Terms of use For more information about contractual protections related to IBM indemnification, see the IBM Client Relationship Agreement and IBM watsonx.ai service description.  
Learn more  
Model information Research paper Model card Prompting the granite-13b-chat-v2 foundation model from IBM"
1014	Supported foundation models available with watsonx.ai	"Foundation model details
granite-13b-instruct-v2
The granite-13b-instruct-v2 model is provided by IBM. This model was trained with high-quality finance data, and is a top-performing model on finance tasks. Financial tasks evaluated include: providing sentiment scores for stock and earnings call transcripts, classifying news headlines, extracting credit risk assessments, summarizing financial long-form text, and answering financial or insurance-related questions.  
Note: This foundation model can be tuned by using the Tuning Studio.  
Usage: Supports extraction, summarization, and classification tasks. Generates useful output for finance-related tasks. Uses a model-specific prompt format. Accepts special characters, which can be used for generating structured output.  
Cost: Class 1. For pricing details, see Watson Machine Learning plans.  
Try it out  
Sample 3b: Generate a numbered list on a particular theme  
Sample 4c: Answer a question based on a document  
Sample 4d: Answer general knowledge questions  
Sample prompt: Finance Q&A  
Sample notebook: Use watsonx and ibm/granite-13b-instruct to analyze car rental customer satisfaction from text  
Size: 13 billion parameters  
Token limits: Context window length (input + output): 8192  
Supported natural languages: English  
Instruction tuning information: The Granite family of models is trained on enterprise-relevant data sets from five domains: internet, academic, code, legal, and finance. Data used to train the models first undergoes IBM data governance reviews and is filtered of text that is flagged for hate, abuse, or profanity by the IBM-developed HAP filter. IBM shares information about the training methods and data sets used.  
Model architecture: Decoder  
License  
Terms of use For more information about contractual protections related to IBM indemnification, see the IBM Client Relationship Agreement and IBM watsonx.ai service description.  
Learn more  
Model information  
Research paper  
Model card"
1015	Supported foundation models available with watsonx.ai	"Foundation model details
granite-7b-lab
The granite-7b-lab foundation model is provided by IBM. The granite-7b-lab foundation model uses a novel alignment tuning method from IBM Research. Large-scale Alignment for chatBots, or LAB is a method for adding new skills to existing foundation models by generating synthetic data for the skills, and then using that data to tune the foundation model.  
Usage: Supports general purpose tasks, including extraction, summarization, classification, and more.  
Cost: Class 1. For pricing details, see Watson Machine Learning plans.  
Size: 7 billion parameters  
Token limits:  
Context window length (input + output): 8192 Note: The maximum new tokens, which means the tokens generated by the foundation model, is limited to 4096.  
Supported natural languages: English  
Instruction tuning information: The granite-7b-lab foundation model is trained iteratively by using the large-scale alignment for chatbots (LAB) methodology.  
Model architecture: Decoder  
License  
Terms of use For more information about contractual protections related to IBM indemnification, see the IBM Client Relationship Agreement and IBM watsonx.ai service description.  
Learn more  
Model card Research paper (LAB)"
1016	Supported foundation models available with watsonx.ai	"Foundation model details
granite-8b-japanese
The granite-8b-japanese model is provided by IBM. The granite-8b-japanese foundation model is based on the IBM Granite Instruct foundation model and is trained to understand and generate Japanese text.  
Note: This foundation model is available only in the Tokyo data center. When you inference this model from the Prompt Lab, disable AI guardrails.  
Usage: Useful for general purpose tasks in the Japanese language, such as classification, extraction, question-answering, and for language translation between Japanese and English.  
Cost: Class 1. For pricing details, see Watson Machine Learning plans.  
Try it out  
Sample 4e: Answer a question based on a document Sample 7d: Converse in a dialog Sample 8c: Translate text  
Size: 8 billion parameters  
Token limits: Context window length (input + output): 8192  
Supported natural languages: English, Japanese  
Instruction tuning information: The Granite family of models is trained on enterprise-relevant data sets from five domains: internet, academic, code, legal, and finance. The granite-8b-japanese model was pretrained on 1 trillion tokens of English and 0.5 trillion tokens of Japanese text.  
Model architecture: Decoder  
License  
Terms of use For more information about contractual protections related to IBM indemnification, see the IBM Client Relationship Agreement and IBM watsonx.ai service description.  
Learn more  
Model information Research paper Model card"
1017	Supported foundation models available with watsonx.ai	"Foundation model details
granite-20b-multilingual
A foundation model from the IBM Granite family. The granite-20b-multilingual foundation model is based on the IBM Granite Instruct foundation model and is trained to understand and generate text in English, German, Spanish, French, and Portuguese.  
Usage: English, German, Spanish, French, and Portuguese closed-domain question answering, summarization, generation, extraction, and classification.  
Cost: Class 1. For pricing details, see Watson Machine Learning plans.  
Size: 13 billion parameters  
Token limits: Context window length (input + output): 8192  
Supported natural languages: English, German, Spanish, French, and Portuguese  
Instruction tuning information: The Granite family of models is trained on enterprise-relevant data sets from five domains: internet, academic, code, legal, and finance. Data used to train the models first undergoes IBM data governance reviews and is filtered of text that is flagged for hate, abuse, or profanity by the IBM-developed HAP filter. IBM shares information about the training methods and data sets used.  
Model architecture: Decoder  
License: Terms of use  
Learn more  
Model information  
Research paper  
Model card"
1018	Supported foundation models available with watsonx.ai	"Foundation model details
jais-13b-chat
The jais-13b-chat foundation model is a bilingual large language model for Arabic and English that is fine-tuned to support conversational tasks.  
Note: This foundation model is available only in the Frankfurt data center. When you inference this model from the Prompt Lab, disable AI guardrails.  
Usage: Supports Q&A, summarization, classification, generation, extraction, and translation in Arabic.  
Cost: Class 2. For pricing details, see Watson Machine Learning plans.  
Try it out  
Sample prompt  
Size: 13 billion parameters  
Token limits: Context window length (input + output): 2048  
Supported natural languages: Arabic (Modern Standard Arabic) and English  
Instruction tuning information: Jais-13b-chat is based on the Jais-13b model, which is a foundation model that is trained on 116 billion Arabic tokens and 279 billion English tokens. Jais-13b-chat is fine-tuned with a curated set of 4 million Arabic and 6 million English prompt-and-response pairs.  
Model architecture: Decoder  
License: Apache 2.0  
Learn more  
Research paper Model card (Frankfurt data center) Model card"
1019	Supported foundation models available with watsonx.ai	"Foundation model details
Llama 3 Chat
Meta Llama 3 foundation models are accessible, open large language model that are built with Meta Llama 3 and provided by Meta on Hugging Face. The Llama 3 foundation models are instruction fine-tuned language models that can support various use cases.  
Usage: Generates dialog output like a chatbot.  
Cost  
8b: Class 1 70b: Class 2 For pricing details, see Watson Machine Learning plans.  
Try it out  
Sample prompt  
Available sizes  
8 billion parameters 70 billion parameters  
Token limits  
Context window length (input + output): 8192 Note: The maximum new tokens, which means the tokens generated by the foundation model, is limited to 4096.  
Supported natural languages: English  
Instruction tuning information: Llama 3 features improvements in post-training procedures that reduce false refusal rates, improve alignment, and increase diversity in the foundation model output. The result is better reasoning, code generation, and instruction-following capabilities. Llama 3 has more training tokens (15T) that result in better language comprehension.  
Model architecture: Decoder-only  
License: META LLAMA 3 Community License  
Learn more  
Meta AI website 8b Model card 70b Model card"
1020	Supported foundation models available with watsonx.ai	"Foundation model details
Llama 2 Chat
The Llama 2 Chat model is provided by Meta on Hugging Face. The fine-tuned model is useful for chat generation. The model is pretrained with publicly available online data and fine-tuned using reinforcement learning from human feedback.  
You can choose to use the 13 billion parameter or 70 billion parameter version of the model.  
Note: The 13 billion parameter version of this foundation model can be tuned by using the Tuning Studio.  
Usage: Generates dialog output like a chatbot. Uses a model-specific prompt format.  
Cost  
13b: Class 1 70b: Class 2 For pricing details, see Watson Machine Learning plans.  
Try it out  
Sample prompt  
Sample prompt: Questions about an article  
Sample notebook: Use watsonx and Meta llama-2-70b-chat to answer questions about an article  
Sample notebook: Use watsonx and Meta llama-2-70b-chat to answer questions about an article  
Sample notebook: Use watsonx to tune Meta llama-2-13b-chat model with CFPB documents  
Available sizes  
13 billion parameters 70 billion parameters  
Token limits  
Context window length (input + output): 4096  
Lite plan output is limited as follows:  
70b version: 900 13b version: 2048  
Supported natural languages: English  
Instruction tuning information: Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction data sets and more than one million new examples that were annotated by humans.  
Model architecture: Decoder-only  
License: License  
Learn more  
Research paper  
13b Model card  
70b Model card"
1021	Supported foundation models available with watsonx.ai	"Foundation model details
llama2-13b-dpo-v7
The llama2-13b-dpo-v7 foundation model is provided by Minds & Company. The llama2-13b-dpo-v7 foundation model is a version of llama2-13b foundation model from Meta that is instruction-tuned and fine-tuned by using the direct preference optimzation method to handle Korean.  
Note: This foundation model is available only in the Tokyo data center. When you inference this model from the Prompt Lab, disable AI guardrails.  
Usage: Suitable for many tasks, including classification, extraction, summarization, code creation and conversion, question-answering, generation, and retreival-augmented generation in Korean.  
Cost: Class 2. For pricing details, see Watson Machine Learning plans.  
Try it out  
Sample summarization prompt Sample chat prompt  
Size: 13.2 billion parameters  
Token limits: Context window length (input + output): 4096  
Supported natural languages: English, Korean  
Instruction tuning information: Direct preference optimzation (DPO) is an alternative to reinforcement learning from human feedback. With reinforcement learning from human feedback, responses must be sampled from a language model and an intermediate step of training a reward model is required. The direct preference optimzation uses a binary method of reinforcement learning where the model chooses the best of two answers based on preference data.  
Model architecture: Decoder-only  
License: License  
Learn more  
Research paper (DPO) Model card Model card (Tokyo data center)"
1022	Supported foundation models available with watsonx.ai	"Foundation model details
merlinite-7b
The merlinite-7b foundation model is provided by Mistral AI and tuned by IBM. The merlinite-7b foundation model is a derivative of the Mistral-7B-v0.1 model that is tuned with a novel alignment tuning method from IBM Research. Large-scale Alignment for chatBots, or LAB is a method for adding new skills to existing foundation models by generating synthetic data for the skills, and then using that data to tune the foundation model.  
Usage: Supports general purpose tasks, including extraction, summarization, classification, and more.  
Cost: Class 1. For pricing details, see Watson Machine Learning plans.  
Size: 7 billion parameters  
Token limits  
Context window length (input + output): 32,768 Note: The maximum new tokens, which means the tokens generated by the foundation model, is limited to 8192.  
Supported natural languages:  
Instruction tuning information: The merlinite-7b foundation model is trained iteratively by using the large-scale alignment for chatbots (LAB) methodology.  
Model architecture: Decoder  
License: Apache 2.0 license  
Learn more  
Research paper (LAB) Model card"
1023	Supported foundation models available with watsonx.ai	"Foundation model details
mixtral-8x7b-instruct-v01
The mixtral-8x7b-instruct-v01 foundation model is provided by Mistral AI. The mixtral-8x7b-instruct-v01 foundation model is a pretrained generative sparse mixture-of-experts network that groups the model parameters, and then for each token chooses a subset of groups (referred to as experts) to process the token. As a result, each token has access to 47 billion parameters, but only uses 13 billion active parameters for inferencing, which reduces costs and latency.  
Usage: Suitable for many tasks, including classification, summarization, generation, code creation and conversion, and language translation. Due to the model's unusually large context window, use the max tokens parameter to specify a token limit when prompting the model.  
Cost: Class 1. For pricing details, see Watson Machine Learning plans.  
Size: 46.7 billion parameters  
Token limits  
Context window length (input + output): 32,768 Note: The maximum new tokens, which means the tokens generated by the foundation model, is limited to 16,384.  
Supported natural languages: English, French, German, Italian, Spanish  
Instruction tuning information: The Mixtral foundation model is pretrained on internet data. The Mixtral 8x7B Instruct foundation model is fine-tuned to follow instructions.  
Model architecture: Decoder-only  
License: Apache 2.0 license  
Learn more  
Research paper Blog post for Mixtral 8x7B Model card"
1024	Supported foundation models available with watsonx.ai	"Foundation model details
mixtral-8x7b-instruct-v01-q (Deprecated)
This model is deprecated. For more information, see Foundation model lifecycle.  
The mixtral-8x7b-instruct-v01-q model is provided by IBM. The mixtral-8x7b-instruct-v01-q foundation model is a quantized version of the Mixtral 8x7B Instruct foundation model from Mistral AI.  
The underlying Mixtral 8x7B foundation model is a sparse mixture-of-experts network that groups the model parameters, and then for each token chooses a subset of groups (referred to as experts) to process the token. As a result, each token has access to 47 billion parameters, but only uses 13 billion active parameters for inferencing, which reduces costs and latency.  
Usage: Suitable for many tasks, including classification, summarization, generation, code creation and conversion, and language translation. Due to the model's unusually large context window, use the max tokens parameter to specify a token limit when prompting the model.  
Cost: Class 1. For pricing details, see Watson Machine Learning plans.  
Try it out: Sample prompts  
Size: 8 x 7 billion parameters  
Token limits  
Context window length (input + output): 32,768 Note: The maximum new tokens, which means the tokens generated by the foundation model, is limited to 4096.  
Supported natural languages: English, French, German, Italian, Spanish  
Instruction tuning information: The Mixtral foundation model is pretrained on internet data. The Mixtral 8x7B Instruct foundation model is fine-tuned to follow instructions.  
The IBM-tuned model uses the AutoGPTQ (Post-Training Quantization for Generative Pre-Trained Transformers) method to compress the model weight values from 16-bit floating point data types to 4-bit integer data types during data transfer. The weights decompress at computation time. Compressing the weights to transfer data reduces the GPU memory and GPU compute engine size requirements of the model.  
Model architecture: Decoder-only  
License: Apache 2.0 license  
Learn more  
Research paper Research paper on the quantization method Blog post for Mixtral 8x7B Model card"
1025	Supported foundation models available with watsonx.ai	"Foundation model details
mt0-xxl-13b
The mt0-xxl-13b model is provided by BigScience on Hugging Face. The model is optimized to support language generation and translation tasks with English, languages other than English, and multilingual prompts.  
Usage: General use with zero- or few-shot prompts. For translation tasks, include a period to indicate the end of the text you want translated or the model might continue the sentence rather than translate it.  
Cost: Class 2. For pricing details, see Watson Machine Learning plans.  
Try it out  
Sample prompts  
Sample notebook: Simple introduction to retrieval-augmented generation with watsonx.ai  
Size: 13 billion parameters  
Supported natural languages: Multilingual  
Token limits  
Context window length (input + output): 4096  
Note: Lite plan output is limited to 700  
Supported natural languages: The model is pretrained on multilingual data in 108 languages and fine-tuned with multilingual data in 46 languages to perform multilingual tasks.  
Instruction tuning information: BigScience publishes details about its code and data sets.  
Model architecture: Encoder-decoder  
License: Apache 2.0 license  
Learn more  
Research paper  
Model card"
1026	Supported foundation models available with watsonx.ai	"Foundation model details
starcoder-15.5b (Deprecated)
This model is deprecated. For more information, see Foundation model lifecycle.  
The starcoder-15.5b model is provided by BigCode on Hugging Face. This model can generate code and convert code from one programming language to another. The model is meant to be used by developers to boost their productivity.  
Usage: Code generation and code conversion  
Note: The model output might include code that is taken directly from its training data, which can be licensed code that requires attribution.  
Cost: Class 2. For pricing details, see Watson Machine Learning plans.  
Try it out  
Sample prompts Sample notebook: Use watsonx and BigCode starcoder-15.5b to generate code based on instruction  
Size: 15.5 billion parameters  
Token limits: Context window length (input + output): 8192  
Supported programming languages: Over 80 programming languages, with an emphasis on Python.  
Data used during training: This model was trained on over 80 programming languages from GitHub. A filter was applied to exclude from the training data any licensed code or code that is marked with opt-out requests. Nevertheless, the model's output might include code from its training data that requires attribution. The model was not instruction-tuned. Submitting input with only an instruction and no examples might result in poor model output.  
Model architecture: Decoder  
License: License  
Learn more  
Research paper Model card  
Any deprecated foundation models are highlighted with a warning icon (). For more information about deprecation, including foundation model withdrawal dates, see Foundation model lifecycle."
1027	Supported foundation models available with watsonx.ai	"Learn more
Choosing a model Foundation model lifecycle  
Parent topic: Developing generative AI solutions"
1028	Enabling model tracking with AI factsheets	"If your organization is using AI Factsheets as part of an AI governance strategy, you can track models after adding them to a space.  
Tracking a model populates a factsheet in an associated model use case. The model use cases are maintained in a model inventory in a catalog, providing a way for all stakeholders to view the lifecyle details for a machine learning model. From the inventory, collaborators can view the details for a model as it moves through the model lifecycle, including the request, development, deployment, and evaluation of the model.  
To enable model tracking by using AI Factsheets:  
From the asset list in your space, click a model name and then click the Model details tab. Click Track this model. Associate the model with an existing model use case in the inventory or create a new use case. Specify the details for the new use case, including specifying a catalog if you have access to more than one, and save to register the model. A link to the model inventory is added to the model details page. Click the link to open the model use case in the inventory. Optional: update the model use case. For example, add tags, supporting documentation, or other details."
1029	Tone classification	"The Tone model in the Watson Natural Language Processing classification workflow classifies the tone in the input text.  
Workflow name  
ensemble_classification-workflow_en_tone-stock  
Supported languages  
English and French  
Capabilities  
The Tone classification model is a pre-trained document classification model for the task of classifying the tone in the input document. The model identifies the tone of the input document and classifies it as:  
Excited Frustrated Impolite Polite Sad Satisfied Sympathetic  
Unlike the Sentiment model, which classifies each individual sentence, the Tone model classifies the entire input document. As such, the Tone model works optimally when the input text to classify is no longer than 1000 characters. If you would like to classify texts longer than 1000 characters, split the text into sentences or paragraphs for example and apply the Tone model on each sentence or paragraph.  
A document may be classified into multiple categories or into no category.  
Dependencies on other blocks  
None  
Code sample  
Output of the code sample:  
Parent topic: Watson Natural Language Processing task catalog"
1030	Data formats for tuning foundation models	"Prepare a set of prompt examples to use to tune the model. The examples must contain the type of input that the model will need to process at run time and the appropriate output for the model to generate in response.  
You can add one file as training data. The maximum file size that is allowed is 200 MB.  
Prompt input-and-output example pairs are sometimes also referred to as samples or records.  
Follow these guidelines when you create your training data:  
Add 100 to 1,000 labeled prompt examples to a file. Between 50 to 10,000 examples are allowed.  
Use one of the following formats:  
JavaScript Object Notation (JSON) JSON Lines (JSONL) format  
Each example must include one input and output pair.  
The language of the training data must be English.  
If the input or output text includes quotation marks, escape each quotation mark with a backslash(\). For example, He said, \""Yes.\"".  
To represent a carriage return or line break, you can use a backslash followed by n (\n) to represent the new line. For example, ...end of paragraph.\nStart of new paragraph.  
You can control the number of tokens from the input and output that are used during training. If an input or output example from the training data is longer than the specified limit, it will be truncated. Only the allowed maximum number of tokens will be used by the experiment. For more information, see Controlling the number of tokens used.  
How tokens are counted differs by model, which makes the number of tokens difficult to estimate. For language-based foundation models, you can think of 256 tokens as about 130—170 words and 128 tokens as about 65—85 words. To learn more about tokens, see Tokens and tokenization.  
If you are using the model to classify data, follow these extra guidelines:  
Try to limit the number of class labels to 10 or fewer. Include an equal number of examples of each class type.  
You can use the Prompt Lab to craft examples for the training data. For more information, see Prompt Lab.  
After you collect a representative set of examples, group the examples into a set to use for training and a separate, smaller set to use for testing purposes."
1031	Data formats for tuning foundation models	"JSON example
The following example shows an excerpt from a training data file with labeled prompts for a classification task in JSON format."
1032	Data formats for tuning foundation models	"JSONL example
The following example shows an excerpt from a training data file with labeled prompts for a classification task in JSONL format.  
Parent topic: Tuning a model"
1033	Foundation model terms of use in watsonx.ai	"Review the following model terms of use to understand your responsibilities and risks with foundation models.  
By using any foundation model provided with this IBM offering, you acknowledge and understand that:  
Some models that are included in this IBM offering are Non-IBM Products. Review the applicable model information for details on the third-party provider and license terms that apply. Foundation models that are not IBM products are listed in the Supported third-party foundation models in watsonx.ai table. For more information, see Supported foundation models. Third Party models have been trained with data that may contain biases and inaccuracies and could generate outputs containing misinformation, obscene or offensive language, or discriminatory content. Users should review and validate the outputs that are generated. The output that is generated by all models is provided to augment, not replace, human decision-making by the Client.  
Parent topic: Developing generative AI solutions"
1034	Prompt Lab	"In the Prompt Lab in IBM watsonx.ai, you can experiment with prompting different foundation models, explore sample prompts, and save and share your best prompts.  
You use the Prompt Lab to engineer effective prompts that you submit to deployed foundation models for inferencing. You do not use the Prompt Lab to create new foundation models.  
This video provides a visual method to learn the concepts and tasks in this documentation."
1035	Prompt Lab	"Requirements
If you signed up for watsonx.ai and you have a sandbox project, all requirements are met and you're ready to use the Prompt Lab.  
You must meet these requirements to use the Prompt Lab:  
You must have a project. You must have the Editor or Admin role in the project. The project must have an associated Watson Machine Learning service instance. Otherwise, you might be prompted to associate the service when you open the Prompt Lab."
1036	Prompt Lab	"Creating and running a prompt
To create and run a new prompt, complete the following steps:  
From the watsonx.ai home page, choose a project, and then click Experiment with foundation models and build prompts.  
Select a model.  
Optional: Choose a different edit mode to work in, such as Freeform.  
If necessary, update model parameters or add prompt variables.  
Enter a prompt.  
Click the Send icon .  
In Structured or Freeform mode, click Generate.  
You can cancel an inference request at any time by clicking Stop icon .  
Tokens in your input are counted as tokens used. Any tokens that were generated by the model as output before the request was canceled are also counted.  
To preserve your work so that you can reuse or share a prompt with collaborators in the current project, save your work as a project asset. For more information, see Saving prompts.  
To run a sample prompt, complete the following steps:  
From the Sample prompts menu in the Prompt Lab, select a sample prompt.  
The prompt is opened in the editor and an appropriate model is selected.  
Click Generate."
1037	Prompt Lab	"Prompt editing options
You type your prompt in the prompt editor. The prompt editor has the following edit modes:  
Chat Structured Freeform"
1038	Prompt Lab	"Prompt editing options
Chat mode
You chat with the foundation model.  
You start the chat by submitting a query or request for the foundation model to answer. Each subsequent turn in the conversation builds on information that was exchanged previously.  
Note: You cannot make changes while a chat is in progress. Click Clear chat to stop and make changes.  
Before starting a chat, review and adjust the model choice and parameter settings. To support long dialog exchanges, the Max tokens parameter is set to a high default value. You might want to add a stop sequence to prevent the model from generating wordy outputs, for example.  
Predefined text called a system prompt is included at the start of the chat to establish ground rules for the conversation. To review and customize the text, click Edit system prompt .  
Some foundation models recommend specific templates that identify different segments of the prompt, such as the prompt instruction and user input. Chat mode adjusts the syntax of your prompt input to conform to each foundation model's recommended format. You can click the text icon to see the full prompt text that will be submitted to the foundation model.  
The following features are omitted from chat mode:  
The token usage count is not shown in chat mode.  
Keep in mind that the chat history is sent with each new prompt that you submit which contributes to the overall token count.  
You can check the token count yourself by using the API. Click the text icon to open and copy the full prompt text, and then use the Text tokenization method to count the tokens.  
You cannot define prompt variables in chat mode. As a consequence, you cannot govern saved chat prompt templates.  
Watch this video showing Chat mode in the Prompt Lab.  
This video provides a visual method to learn the concepts and tasks in this documentation."
1039	Prompt Lab	"Prompt editing options
Structured mode
You add parts of your prompt into the appropriate fields:  
Instruction: Add an instruction if it makes sense for your use case. An instruction is an imperative statement, such as Summarize the following article. Examples: Add one or more pairs of examples that contain the input and the corresponding output that you want. Providing a few example input-and-output pairs in your prompt is called few-shot prompting. If you need a specific prefix to the input or the output, you can replace the default labels, ""Input:"" or ""Output:"", with the labels you want to use. A space is added between the example label and the example text. Test your input: In the Try area, enter the final input of your prompt.  
Structured mode is designed to help new users create effective prompts. Text from the fields is sent to the model in a template format."
1040	Prompt Lab	"Prompt editing options
Freeform mode
You add your prompt in plain text. Your prompt text is sent to the model exactly as you typed it.  
Quotation marks in your text are escaped with a backslash (\""). Newline characters are represented by \n. Apostrophes are escaped (it'\''s) so that they can be handled properly in the cURL command."
1041	Prompt Lab	"Model and prompt configuration options
You must specify which model to prompt and can optionally set parameters that control the generated result."
1042	Prompt Lab	"Model and prompt configuration options
Model choices
In the Prompt Lab, you can submit your prompt to any of the models that are supported by watsonx.ai. You can choose recently-used models from the drop-down list. Or you can click View all foundation models to view all the supported models, filter them by task, and read high-level information about the models.  
If you tuned a foundation model by using the Tuning Studio and deployed the tuned model, your tuned model is also available for prompting from the Prompt Lab."
1043	Prompt Lab	"Model and prompt configuration options
Model parameters
To control how the model generates output in response to your prompt, you can specify decoding parameters and stopping criteria. For more information, see Model parameters for prompting."
1044	Prompt Lab	"Model and prompt configuration options
Prompt variables
To add flexibility to your prompts, you can define prompt variables. A prompt variable is a placeholder keyword that you include in the static text of your prompt at creation time and replace with text dynamically at run time. For more information, see Building reusable prompts."
1045	Prompt Lab	"Model and prompt configuration options
View full prompt text
You might want to see the full prompt text that will be submitted to the foundation model in the following situations:  
When prompt variables are in use, to see resolved variable values in context. In chat mode, where the recommended prompt formats for different foundation models are applied automatically. In structured mode, where you add parts of the prompt into separate fields."
1046	Prompt Lab	"Model and prompt configuration options
AI guardrails
When you set the AI guardrails switcher to On, harmful language is automatically removed from the input prompt text and from the output that is generated by the model. Specifically, any sentence in the input or output that contains harmful language is replaced with a message that says that potentially harmful text was removed.  
Note: This feature is supported for English-language models only. If you're working with a non-English foundation model, disable AI guardrails.  
For more information, see Removing harmful content."
1047	Prompt Lab	"Prompt code
If you want to run the prompt programmatically, you can view and copy the prompt code or use the Python library."
1048	Prompt Lab	"Prompt code
View code
When you click the View code icon ), a cURL command is displayed that you can call from outside the Prompt Lab to submit the current prompt and parameters to the selected model and get a generated response.  
In the command, there is a placeholder for an IBM Cloud IAM token. For information about generating the access token, see Generating an IBM Cloud IAM token."
1049	Prompt Lab	"Prompt code
Programmatic alternative to the Prompt Lab
The Prompt Lab graphical interface is a great place to experiment and iterate with your prompts. However, you can also prompt foundation models in watsonx.ai programmatically by using the Python library or REST API. For details, see Coding generative AI solutions."
1050	Prompt Lab	"Available prompts
In the side panel, you can access sample prompts, your session history, and saved prompts."
1051	Prompt Lab	"Available prompts
Samples
A collection of sample prompts are available in the Prompt Lab. The samples demonstrate effective prompt text and model parameters for different tasks, including classification, extraction, content generation, question answering, and summarization.  
When you click a sample, the prompt text loads in the editor, an appropriate model is selected, and optimal parameters are configured automatically."
1052	Prompt Lab	"Available prompts
History
As you experiment with different prompt text, model choices, and parameters, the details are captured in the session history each time you submit your prompt. To load a previous prompt, click the entry in the history and then click Restore."
1053	Prompt Lab	"Available prompts
Saved
From the Saved prompt templates menu, you can load any prompts that you saved to the current project as a prompt template asset.  
When watsonx.governance is provisioned, if your prompt template includes at least one prompt variable, you can evaluate the effectiveness of model responses. For more information, see Evaluating prompt templates in projects."
1054	Prompt Lab	"Learn more
Building reusable prompts  
Saving prompts  
Model parameters for prompting  
Sample prompts  
Prompt tips  
Evaluating prompt templates in projects  
Security and privacy for foundation models  
Try these tutorials:  
Prompt a foundation model using Prompt Lab Prompt a foundation model with the retrieval-augmented generation pattern  
Watch these other prompt lab videos:  
Parent topic: Developing generative AI solutions"
1055	Frameworks, fusion methods, and Python versions	These are the available machine learning model frameworks and model fusion methods for the Federated Learning model. The software spec and frameworks are also compatible with specific Python versions.
1056	Frameworks, fusion methods, and Python versions	"Frameworks and fusion methods
This table lists supported software frameworks for building Federated Learning models. For each framework you can see the supported model types, fusion methods, and hyperparameter options.  
Rounds  
Rounds"
1057	Frameworks, fusion methods, and Python versions	"Software specifications and Python version by framework
This table lists the software spec and Python versions available for each framework."
1058	Frameworks, fusion methods, and Python versions	"Learn more
Hyperparameter definitions  
Parent topic: IBM Federated Learning"
1059	Compute resource options for Tuning Studio experiments in projects	"A Tuning Studio experiment has a single hardware configuration.  
The following table shows the hardware configuration that is used when tuning foundation models in a tuning experiment."
1060	Compute resource options for Tuning Studio experiments in projects	"Compute usage in projects
Tuning Studio consumes compute resources as CUH from the Watson Machine Learning service.  
You can monitor the total monthly amount of CUH consumption for the Watson Machine Learning service on the Resource usage page on the Manage tab of your project."
1061	Compute resource options for Tuning Studio experiments in projects	"Learn more
Tuning Studio Watson Machine Learning plans Compute resource options for assets and deployments in spaces Monitoring account resource usage  
Parent topic: Choosing compute resources for tools"
1062	RStudio	R is a popular statistical analysis and machine-learning package that enables data management and includes tests, models, analyses and graphics. RStudio, included in IBM Watson Studio, provides an integrated development environment for working with R scripts.
1063	RStudio	"Accessing RStudio
RStudio is integrated in IBM Watson Studio projects and can be launched after you create a project. With RStudio integration in projects, you can access and use the data files that are stored in the IBM Cloud Object Storage bucket associated with your project in RStudio.  
To start RStudio in your project:  
Click RStudio from the Launch IDE menu on your project's action bar.  
Select an environment.  
Click Launch.  
The environment runtime is initiated and the development environment opens.  
Sometimes, when you start an RStudio session, you might experience a corrupted RStudio state from a previous session and your session will not start. If this happens, select to reset the workspace at the time you select the RStudio environment and then start the RStudio IDE again. By resetting the workspace, RStudio is started using the default settings with a clean RStudio workspace."
1064	RStudio	"Working with data files
In RStudio, you can work with data files from different sources:  
Files in the RStudio server file structure, which you can view by clicking Files in the bottom right section of RStudio. This is where you can create folders, upload files from your local system, and delete files.  
To access these files in R, you need to set the working directory to the directory with the files. You can do this by navigating to the directory with the files and clicking More > Set as Working Directory.  
Be aware that files stored in the Home directory of your RStudio instance are persistent within your instance only and cannot be shared across environments nor within your project.  
Video disclaimer: Some minor steps and graphical elements in the videos on this page may differ from your deployment.  
Watch this video to see how to load data to RStudio.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Project data assets that are stored in the IBM Cloud Object Storage bucket associated with your project. When RStudio is launched, the IBM Cloud Object Storage bucket content is mounted to the project-objectstorage directory in your RStudio Home directory.  
If you want data files to appear in the project-objectstorage directory, you must add them as assets to your project. See Adding files as project assets.  
If new data assets are added to the project while you are in RStudio and you want to access them, you need to refresh the project-objectstorage folder.  
See how to read and write data to and from Cloud Object Storage.  
Data stored in a database system.  
Watch this video to see how to connect to external data sources in RStudio.  
This video provides a visual method to learn the concepts and tasks in this documentation."
1065	RStudio	"Working with data files
Data stored in a database system.  
Watch this video to see how to connect to external data sources in RStudio.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Files stored in local storage that are mounted to /home/rstudio. The home directory has a storage limitation of 2 GB and is used to store the RStudio session workspace. Note that you are allocated 2 GB for your home directory storage across all of your projects, irrespective of whether you use RStudio in each project. As a consequence, you should only store R script files and small data files in the home directory. It is not intended for large data files or large generated output. All large data files should be uploaded as project assets, which are mounted to the project-objectstorage directory from where you can access them."
1066	RStudio	"Adding files as project assets
If you worked with data files and want them appear in the project-objectstorage directory, you must add them to your project as data assets. To add these files as data assets to the project:  
On the Assets page of the project, click the Upload asset to project icon () and select the Files tab. Select the files you want to add to the project as assets. From the Actions list, select Add as data asset and apply your changes."
1067	RStudio	"Capacity consumption and runtime scope
An RStudio environment runtime is always scoped to an environment template and an RStudio session user. Only one RStudio session can be active per Watson Studio user at one time. If you started RStudio in another project, you are asked if you want to stop that session and start a new RStudio session in the context of the current project you're working in.  
Runtime usage is calculated by the number of capacity unit hours (CUHs) consumed by the active environment runtime. The CUHs consumed by an active RStudio runtime in a project are billed to the account of the project creator. See Capacity units per hour billing for RStudio.  
You can see which RStudio environment runtimes are active on the project's Environments page. You can stop your runtime from this page.  
Remember: The CUH counter continues to increase while the runtime is active so stop the runtime if you aren't using RStudio. If you don't explicitly stop the runtime, it is stopped for you after an idle time of 2 hour. During this idle time, you will continue to consume CUHs for which you are billed. Long compute-intensive jobs are hard stopped after 24 hours.  
Watch this video to see an overview of the RStudio IDE.  
Video disclaimer: Some minor steps and graphical elements in this video might differ from your platform.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Transcript  
Synchronize transcript with video"
1068	RStudio	"Capacity consumption and runtime scope
Video transcript Time Transcript 00:00 This video is a quick tour of the RStudio integrated development environment inside a Watson Studio project. 00:07 From any project, you can launch the RStudio IDE. 00:12 RStudio is a free and open-source integrated development environment for R, a programming language for statistical computing and graphics. 00:22 In RStudio, there are four panes: the source pane, the console pane, the environment pane, and the files pane. 00:32 The panes help you organize your work and separate the different tasks you'll do with R. 00:39 You can drag to resize the panes or use the icons to minimize and maximize a pane. 00:47 You can also rearrange the panes in global options. 00:53 The console pane is your interface to R. 00:56 It's exactly what you would see in terminal window or user interfaces bundled with R. 01:01 The console pane does have some added features that you'll find helpful. 01:06 To run code from the console, just type the command. 01:11 Start typing a command to see a list of commands that begin with the letters you started typing. 01:17 Highlight a command in the list and press ""Enter"" to insert it. 01:24 Use the up arrow to scroll through the commands you've previously entered. 01:31 As you issue more commands, you can scroll through the results. 01:36 Use the menu option to clear the console. 01:39 You can also use tab completion to see a list of the functions, objects, and data sets beginning with that text. 01:47 And use the arrows to highlight a command to see help for that command. 01:51 When you're ready, just press ""Enter"" to insert it. 01:55 Next, you'll see a list of the options for that command in the current context. 01:59 For example, the first argument for the read.csv function is the file. 02:05 RStudio will display a list of the folders and files in your working directory, so you can easily locate the file to include with the argument. 02:16 Lastly, if you use the tab completion with a function that expects a package name, such as a library, you'll see"
1069	RStudio	"Capacity consumption and runtime scope
folders and files in your working directory, so you can easily locate the file to include with the argument. 02:16 Lastly, if you use the tab completion with a function that expects a package name, such as a library, you'll see a list of all the installed packages. 02:28 Next, let's look at the source pane, which is simply a text editor for you to write your R code. 02:34 The text editor supports R command files and plain text, as well as several other languages, and includes language-specific highlighting in context. 02:47 And you'll notice the tab completion is also available in the text editor. 02:53 From the text editor, you can run a single line of code, or select several lines of code to run, and you'll see the results in the console pane. 03:08 You can save your code as an R script to share or run again later. 03:15 The view function opens a new tab that shows the dataframe in spreadsheet format. 03:22 Or you can display it in its own window. 03:25 Now, you can scroll through the data, sort the columns, search for specific values, or filter the rows using the sliders and drop-down menus. 03:41 The environment pane contains an ""Environment"" tab, a ""History"" tab, and a ""Connections"" tab, and keeps track of what's been happening in this R session. 03:51 The ""Environment"" tab contains the R objects that exist in your global environment, created during the session. 03:58 So, when you create a new object in the console pane, it automatically displays in the environment pane. 04:04 You can also view the objects related to a specific package, and even see the source code for a specific function. 04:12 You can also see a list of the data sets, expand a data set to inspect its individual elements, and view them in the source pane. 04:22 You can save the contents of an environment as an .RData file, so you can load that .RData file at a later date. 04:29 From here, you can also clear the objects from the workspace. 04:33 If you want to delete specific items, use the grid view. 04:38 For example, you can easily find large items"
1070	RStudio	"Capacity consumption and runtime scope
later date. 04:29 From here, you can also clear the objects from the workspace. 04:33 If you want to delete specific items, use the grid view. 04:38 For example, you can easily find large items to delete to free up memory in your R session. 04:45 The ""Environment"" tab also allows you to import a data set. 04:50 You can see a preview of the data set and change options before completing the import. 04:55 The imported data will display in the source pane. 05:00 The ""History"" tab displays a history of each of the commands that you run at the command line. 05:05 Just like the ""Environment"" tab, you can save the history as an .Rhistory file, so you can open it at a later date. 05:11 And this tab has the same options to clear all of the history and individual entries in the history. 05:17 Select a command and send it to the console to rerun the command. 05:23 You can also copy a command to the source pane to include it in a script. 05:31 On the ""Connections"" tab, you can create a new connection to a data source. 05:36 The choices in this dialog box are dependent upon which packages you have installed. 05:41 For example, a ""BLUDB"" connection allows you to connect to a Db2 Warehouse on Cloud service. 05:49 The files pane contains the ""Files"", ""Plots"", ""Packages"", ""Help"", and ""Viewer"" tabs. 05:55 The ""Files"" tab displays the contents of your working directory. 05:59 RStudio will load files from this directory and save files to this directory. 06:04 Navigate to a file and click the file to view it in the source pane. 06:09 From here, you can create new folders and upload files, either by selecting individual files to upload or selecting a .zip file containing all of the files to upload. 06:25 From here, you can also delete and rename files and folders. 06:30 In order to access the file in R, you need to set the data folder as a working directory. 06:36 You'll see that the setwd command was executed in the console. 06:43"
1071	RStudio	"Capacity consumption and runtime scope
and folders. 06:30 In order to access the file in R, you need to set the data folder as a working directory. 06:36 You'll see that the setwd command was executed in the console. 06:43 You can access the data assets in your project by opening the project folder. 06:50 The ""Plots"" tab displays the results of R's plot functions, such as: plot, hist, ggplot, and xyplot 07:00 You can navigate through different plots using the arrows or zoom to see a graph full screen. 07:09 You can also delete individual plots or all plots from here. 07:13 Use the ""Export"" option to save the plot as a graphic or print file at the specified resolution. 07:21 The ""Packages"" tab displays the packages you currently have installed in your system library. 07:26 The search bar lets you quickly find a specific package. 07:30 The checked packages are the packages that were already loaded, using the library command, in the current session. 07:38 You can check additional packages from here to load them or uncheck packages to detach them from the current session. 07:45 The console pane displays the results. 07:48 Use the ""X"" next to a package name to remove it from the system library. 07:54 You can also find new packages to install or update to the latest version of any package. 08:03 Clicking any of the packages opens the ""Help"" tab with additional information for that package. 08:09 From here, you can search for functions to get more help. 08:13 And from the console, you can use the help command, or simply type a question mark followed by the function, to get help with that function. 08:21 The ""Viewer"" tab displays HTML output. 08:25 Some R functions generate HTML to display reports and interactive graphs. 08:31 The R Markdown package creates reports that you can view in the ""Viewer"" tab. 08:38 The Shiny package creates web apps that you can view in the ""Viewer"" tab. 08:44 And other packages build on the htmlwidgets framework and include Java-based, interactive visualizations. 08:54 You can also publish the visualization to the free site, called"
1072	RStudio	"Capacity consumption and runtime scope
apps that you can view in the ""Viewer"" tab. 08:44 And other packages build on the htmlwidgets framework and include Java-based, interactive visualizations. 08:54 You can also publish the visualization to the free site, called ""RPubs.com"". 09:01 This is been a brief overview of the RStudio IDE. 09:05 Find more videos on RStudio in the Cloud Pak for Data as a Service documentation."
1073	RStudio	"Learn more
RStudio environments Using Spark in RStudio  
Parent topic: Notebooks and scripts"
1074	Saving your work	"Prompt engineering involves trial and error. Keep track of your experimentation and save model-and-prompt combinations that generate the output you want.  
When you save your work, you can choose to save it as different asset types. Saving your work as an asset makes it possible to share your work with collaborators in the current project.  
Each of these asset types is available from the project's Assets page. Project collaborators with the Admin or Editor role can open and work with them. Your prompt template and prompt session assets are locked automatically, but you can unlock them by clicking the lock icon ().  
Editors and Admins can use locked prompt template assets to generate inferences. Only the lock owner can use a locked prompt session asset to generate inferences."
1075	Saving your work	"Saving your work
To save your prompt engineering work, complete the following steps:  
From the header of the prompt editor, click the Save work icon , and then click Save as.  
Choose an asset type.  
Name the asset, and then optionally add a description.  
Watsonx.governance only: Choose the task type that best matches your goal.  
If you save the prompt as a notebook asset only: Select View in project after saving.  
Click Save."
1076	Saving your work	"Working with prompts saved in a notebook
When you save your work as a notebook asset, a Python notebook is built.  
To work with a prompt notebook asset, complete the following steps:  
Open the notebook asset from the Assets tab of your project.  
Click the Edit icon to instantiate the notebook so you can step through the code.  
The notebook contains runnable code that manages the following steps for you:  
Authenticates with the service. Defines a Python class. Defines the input text for the model and declares any prompt variables. You can edit the static prompt text and assign values to prompt variables. Uses the defined class to call the watsonx.ai inferencing API and pass your input to the foundation model. Shows the output that is generated by the foundation model.  
Use the notebook as is, or change it to meet the needs of your use case.  
The Python code that is generated by using the Prompt Lab executes successfully. You must test and validate any changes that you make to the code."
1077	Saving your work	"Working with saved prompt templates
To continue working with a saved prompt, open it from the Saved prompt templates tab of the Prompt Lab.  
When you open a saved prompt template, Autosave is on, which means that any changes you make to the prompt will be reflected in the saved prompt template asset. If you want the prompt template that you saved to remain unchanged, click New prompt to start a new prompt.  
You cannot change edit modes from a saved prompt template.  
Unlike other edit modes, a prompt template that is saved in chat mode also stores the model output. You cannot switch to a different mode from a prompt template that was saved in chat mode.  
When watsonx.governance is provisioned, if your prompt template includes at least one prompt variable, you can evaluate the effectiveness of model responses.  
For more information about prompt variables, see Building reusable prompts. For more information about evaluating model responses, see Evaluating prompt templates in projects."
1078	Saving your work	"Working with saved prompt sessions
To continue working with a saved prompt session, open it from the History tab of the Prompt Lab.  
To review previous prompt submissions, you can click a prompt entry from the history to open it in the prompt editor. If you prefer the results from the earlier prompt, you can reset it as your current prompt by clicking Restore. When you restore an earlier prompt, your current prompt session is replaced by the earlier version of the prompt session."
1079	Saving your work	"Learn more
Security and privacy for foundation models  
Parent topic: Prompt Lab"
1080		"Learn how to run pipelines in real time or on a schedule, and how to preserve your pipeline versions.  
You can do the following tasks:  
Running a pipeline Creating a pipeline job Saving a version of a pipeline Exporting pipeline assets for deployment"
1081		"Running a pipeline
You can run a pipeline in real time to test a flow as you work. When you are satisfied with a pipeline, you can then define a job to run a pipeline with parameters or to run on a schedule.  
To run a pipeline:  
Click Run pipeline on the toolbar. Choose an option:  
Trial run runs the pipeline without creating a job. Use this to test a pipeline. Create a job presents you with an interface for configuring and scheduling a job to run the pipeline. You can save and reuse run details, such as pipeline parameters, for a version of your pipeline. View history compares all of your runs over time.  
You must make sure requirements are met when you run a pipeline. For example, you might need a deployment space or an API key to run some of your nodes before you can begin."
1082		"Running a pipeline
Setting a job run name
You can optionally specify a job run name when running a pipeline flow or a pipeline job and see the different job runs in the Job details dashboard. Otherwise, you can also assign a local parameter DSJobInvocationId to either a Run pipeline job node or Run DataStage job node (the latter is not available for watsonx).  
If both the parameter DSJobInvocationId and job run name of the node are set, DSJobInvocationId will be used. If neither are set, the default value ""job run"" is used."
1083		"Creating a pipeline job
The following are all the configuration options for defining a job to run the pipeline.  
Name your pipeline job and choose a version. Input your IBM API key. (Optional) Schedule your job by toggling the Schedule button. (Optional) Enter the pipeline parameters needed for your job, for example assigning a space to a deployment node. To see how to create a pipeline parameter, see Defining pipeline parameters in Creating a pipeline. (Optional) Choose if you want to be notified of pipeline job status after running.  
Choose the start date and fine tune your schedule to repeat by any minute, hour, day, week, month. Add exception days to prevent the job from running on certain days. Add a time for terminating the job."
1084		"Viewing pipeline results
After you run a pipeline from a trial run or a job, you can view the status and other details of the run such as parameter results in the Run tracker.  
Open each tab to view details of the pipeline run."
1085		"Viewing pipeline results
Notes on running a pipeline
Errors in the pipeline are flagged with an error badge. Open the node or condition with an error to change or complete the configuration. View the consolidated logs to review operations or identify issues with the pipeline."
1086		"Saving a version of a pipeline
You can save a version of a pipeline and revert to it at a later time. For example, if you want to preserve a particular configuration before you make changes, save a version. You can revert the pipeline to a previous version. When you share a pipeline, the latest version is used.  
To save a version:  
Click the Versions icon on the toolbar. In the Versions pane, click Save version to create a new version with a version number incremented by 1.  
When you run the pipeline, you can choose from available saved versions.  
Note: You cannot delete a saved version."
1087		"Exporting pipeline assets for deployment spaces
Watson Pipelines does not support quick deployment or promotion of pipelines like other assets. Instead, you can export a project's or space's assets and import them into a deployment space. When you export, include pipelines in the list of assets that is exported to a zip file and then import into a project or space.  
Importing a pipeline into a space extends your MLOps capabilities to run jobs for various assets from a space, or to move all jobs from a pre-production to a production space. Note these considerations for working with pipelines in a space:  
Pipelines in a space are read-only. You cannot edit the pipeline. You must edit the pipeline from the project, then export the updated pipeline and import it into the space. Although you cannot edit the pipeline in a space, you can create new jobs to run the pipeline. You can also use parameters to assign values for jobs so you can have different values for each job you configure. If there is already a pipeline in the space with the same name, the pipeline import will fail. If there is no pipeline in the space with the same name, a pipeline with version 1 is created in the space. Any supporting assets or references required to run a pipeline job must also be part of the import package or the job will fail. If your pipeline contains assets or tools not supported in a space, such as an SPSS modeler job, the pipeline job will fail. You can automate export import with a CLI tool such as CPDCTL.  
Attention: When you import a pipeline into a deployment space, dependencies cannot be identified and automatically deployed. You must ensure that all dependencies are deployed for the pipeline.  
Parent topic: IBM Watson Pipelines"
1088	Batch deployment input details by framework	"Various data types are supported as input for batch deployments, depending on your specific model type.  
For details, follow these links:  
AutoAI models Decision optimization models Python functions Python scripts Pytorch models Scikit-Learn and XGBoost models Spark models SPSS models Tensorflow models  
For more information, see Using multiple inputs for an SPSS job.  
Parent topic: Creating a batch deployment"
1089	Assets in deployment spaces	"Learn about various ways of adding and promoting assets to a space. Find the list of asset types that you can add to a space.  
Note these considerations for importing assets into a space:  
Upon import, some assets are automatically assigned a version number, starting with version 1. This version numbering prevents overwriting existing assets if you import their updated versions later. Assets or references that are required to run jobs in the space must be part of the import package, or must be added separately. If you don't add these supporting assets or references, jobs fail.  
The way to add an asset to a space depends on the asset type. You can add some assets directly to a space (for example a model that was created outside of watsonx). Other asset types originate in a project and must be transferred from a project to a space. The third class includes asset types that you can add to a space only as a dependency of another asset. These asset types do not display in the Assets tab in the UI.  
For more information, see:  
Asset types that you can directly add to a space Asset types that are created in projects and can be transferred into a space Asset types that can be added to a space only as a dependency  
For more information about working with space assets, see:  
Accessing asset details Managing asset versions Managing AI lifecycle with CPDCTL"
1090	Assets in deployment spaces	"Asset types that you can directly add to a space
Connection Data asset (from a connection or an uploaded file) Model  
For more information, see:  
For data assets and connections: Adding data assets to a deployment space For models: Importing models into a deployment space"
1091	Assets in deployment spaces	"Assets types that are created in projects and can be transferred into a space
Connection Data Refinery flow Environment Function Job Model Script  
If your asset is located in a standard Watson Studio project, you can transfer the asset to the deployment space by promoting it.  
For more information, see Promoting assets to a deployment space.  
Alternatively, you can export the project and then import it into the deployment space. For more information, see:  
Exporting a project Importing spaces and projects  
If you export the whole project, any matching custom environments are exported as well.  
If your use case requires it, you can create a code package from some of the assets in your project by using cpdcli or cpdctl and then importing manually into the space."
1092	Assets in deployment spaces	"Assets types that are created in projects and can be transferred into a space
Transferring assets from a deprecated Git-based project
The available scenarios are the same as the ones that apply to transferring assets from a standard Watson Studio project.  
Note:  
Before you move assets to a deployment space, you must synchronize your changes with the Git repository. If your asset depends on any other assets, like custom environments, you must synchronize them, too."
1093	Assets in deployment spaces	"Assets types that are created in projects and can be transferred into a space
Transferring assets from a standard Git-based project
Before you move assets to a deployment space, you must synchronize your changes with the Git repository. If your asset depends on any other assets, like custom environments, you must synchronize those assets as well. To transfer your assets from a standard Git-based project, export the project and then import it into the deployment space. For more information, see:  
Exporting a project Importing spaces and projects  
If you export the whole project, any matching custom environments are exported as well.  
If your specific use case requires it, you can also create a code package from some of the assets in your project by using cpdcli or cpdctl and then import it manually into the space.  
Note: When you work with a Git-based project, Notebooks, Scripts, and Shiny Apps are represented as files within your Git repository. When you import your Git archive file into a deployment space, all of these file types are accessible within the Code Package asset that is created as part of the import."
1094	Assets in deployment spaces	"Asset types that can be added to a space only as a dependency
Hardware Specification Package Extension Software Specification Watson Machine Learning Experiment Watson Machine Learning Model Definition"
1095	Assets in deployment spaces	"Learn more
Deploying assets Managing asset versions Training and deploying machine learning models in notebooks  
Parent topic: Deployment spaces"
1096	Extracting targets sentiment with a custom transformer model	"You can train your own models for targets sentiment extraction based on the Slate IBM Foundation model. This pretrained model can be find-tuned for your use case by training it on your specific input data.  
The Slate IBM Foundation model is available only in Runtime 23.1.  
Note: Training transformer models is CPU and memory intensive. Depending on the size of your training data, the environment might not be large enough to complete the training. If you run into issues with the notebook kernel during training, create a custom notebook environment with a larger amount of CPU and memory, and use that to run your notebook. Use a GPU-based environment for training and also inference time, if it is available to you. See Creating your own environment template.  
Input data format for training Loading the pretrained model resources Training the model Applying the model on new data Storing and loading the model"
1097	Extracting targets sentiment with a custom transformer model	"Input data format for training
You must provide a training and development data set to the training function. The development data is usually around 10% of the training data. Each training or development sample is represented as a JSON object. It must have a text and a target_mentions field. The text represents the training example text, and the target_mentions field is an array, which contains an entry for each target mention with its text, location, and sentiment.  
Consider using Watson Knowledge Studio to enable your domain subject matter experts to easily annotate text and create training data.  
The following is an example of an array with sample training data:  
The training and development data sets are created as data streams from arrays of JSON objects. To create the data streams, you may use the utility method read_json_to_stream. It requires the syntax analysis model for the language of your input data.  
Sample code:"
1098	Extracting targets sentiment with a custom transformer model	"Loading the pretrained model resources
The pretrained Slate IBM Foundation model needs to be loaded before passing it to the training algorithm.  
To load the model:"
1099	Extracting targets sentiment with a custom transformer model	"Training the model
For all options that are available for configuring sentiment transformer training, enter:  
The train method will create a new targets sentiment block model.  
The following is a sample call that uses the input data and pretrained model from the previous section (Training the model):"
1100	Extracting targets sentiment with a custom transformer model	"Applying the model on new data
After you train the model on a data set, apply the model on new data by using the run() method, as you would use on any of the existing pre-trained blocks. Because the created custom model is a block model, you need to run syntax analysis on the input text and pass the results to the run() methods.  
Sample code:"
1101	Extracting targets sentiment with a custom transformer model	"Storing and loading the model
The custom targets sentiment model can be stored as any other model as described in Saving and loading custom models, using ibm_watson_studio_lib.  
To load the custom targets sentiment model, additional steps are required:  
import zipfile model_zip = 'custom_TSA_model_file' model_folder = 'custom_TSA' wslib.download_file('custom_TSA_model', file_name=model_zip) with zipfile.ZipFile(model_zip, 'r') as zip_ref: zip_ref.extractall(model_folder) custom_TSA_model = watson_nlp.load(model_folder)  
Ensure that you have an access token on the Access control page on the Manage tab of your project. Only project admins can create access tokens. The access token can have Viewer or Editor access permissions. Only editors can inject the token into a notebook.  
Add the project token to the notebook by clicking More > Insert project token from the notebook action bar. Then run the cell.  
By running the inserted hidden code cell, a wslib object is created that you can use for functions in the ibm-watson-studio-lib library. For information on the available ibm-watson-studio-lib functions, see Using ibm-watson-studio-lib for Python.  
Download and extract the model to your local runtime environment:  
Load the model from the extracted folder:  
Parent topic: Creating your own models"
1102	Hiding sensitive code cells in a notebook	"If your notebook includes code cells with sensitive data, such as credentials for data sources, you can hide those code cells from anyone you share your notebook with. Any collaborators in the same project can see the cells, but when you share a notebook with a link, those cells will be hidden from anyone who uses the link.  
To hide code cells:  
Open the notebook and select the code cell to hide.  
Insert a comment with the hide tag on the first line of the code cell.  
For the Python and R languages, enter the following syntax: # @hidden_cell  
Parent topic: Sharing notebooks"
1103	Hierarchical text categorization	"The Watson Natural Language Processing Categories block assigns individual nodes within a hierarchical taxonomy to an input document. For example, in the text IBM announces new advances in quantum computing, examples of extracted categories are technology and computing/hardware/computer and technology and computing/operating systems. These categories represent level 3 and level 2 nodes in a hierarchical taxonomy.  
This block differs from the Classification block in that training starts from a set of seed phrases associated with each node in the taxonomy, and does not require labeled documents.  
Note that the Hierarchical text categorization block can only be used in a notebook that is started in an environment based on Runtime 23.1 that includes the Watson Natural Language Processing library.  
Block name  
categories_esa_en_stock  
Supported languages  
The Categories block is available for the following languages. For a list of the language codes and the corresponding language, see Language codes.  
de, en  
Capabilities  
Use this block to determine the topics of documents on the web by categorizing web pages into a taxonomy of general domain topics, for ad placement and content recommendation. The model was tested on data from news reports and general web pages.  
For a list of the categories that can be returned, see Category types.  
Dependencies on other blocks  
The following block must run before you can run the hierarchical categorization block:  
syntax_izumo_<language>_stock  
Code sample  
Output of the code sample:  
Parent topic: Watson Natural Language Processing task catalog"
1104	Managing predictive deployments	"For proper deployment, you must set up a deployment space and then select and configure a specific deployment type. After you deploy assets, you can manage and update them to make sure they perform well and to monitor their accuracy.  
To be able to deploy assets from a space, you must have a machine learning service instance that is provisioned and associated with that space.  
Online and batch deployments provide simple ways to create an online scoring endpoint or do batch scoring with your models.  
If you want to implement a custom logic:  
Create a Python function to use for creating your online endpoint Write a notebook or script for batch scoring  
Note: If you create a notebook or a script to perform batch scoring such an asset runs as a platform job, not as a batch deployment."
1105	Managing predictive deployments	"Deployable assets
Following is the list of assets that you can deploy from a Watson Machine Learning space, with information on applicable deployment types:  
An R Shiny app is the only asset type that is supported for web app deployments.  
Notes:  
A deployment job is a way of running a batch deployment, or a self-contained asset like a flow in Watson Machine Learning. You can select the input and output for your job and choose to run it manually or on a schedule. For more information, see Creating a deployment job. Notebooks and flows use notebook environments. You can run them in a deployment space, but they are not deployable.  
For more information, see:  
Creating online deployments Creating batch deployments Deploying Python functions Deploying scripts  
After you deploy assets, you can manage and update them to make sure they perform well and to monitor their accuracy. Some ways to manage or update a deployment are as follows:  
Manage deployment jobs. After you create one or more jobs, you can view and manage them from the Jobs tab of your deployment space.  
Update a deployment. For example, you can replace a model with a better-performing version without having to create a new deployment.  
Scale a deployment to increase availability and throughput by creating replicas of the deployment.  
Delete a deployment to remove a deployment and free up resources."
1106	Managing predictive deployments	"Configuring API gateways to provide stable endpoints
Watson Machine Learning provides stable endpoints to prevent downtime. However, you might experience downtime if you move to a new Cloud Pak for Data instance or add a new instance for increased availability.  
API gateways provide a stable URL that can be used with your Watson Machine Learning REST API endpoint. You can use an API gateway (available in Cloud Pak for Integration) with your deployment endpoints in the following cases:  
If you have more than one instance of Cloud Pak for Data in a high-availability configuration, and one of the available instances fail. In this case, you can use an API gateway to switch automatically to another instance and prevent complete failure. If you have more than one application that uses the same endpoint, and the deployment is accidentally deleted. In this case, you can update the endpoint in the API gateway to ensure that applications continue to use it."
1107	Managing predictive deployments	"Learn more
Full list of asset types that can be added to a deployment space  
Parent topic: Deploying assets with Watson Machine Learning"
1108	Inferencing a foundation model with a notebook	"You can prompt foundation models in IBM watsonx.ai programmatically by using the Python library.  
After you create a prompt in the Prompt Lab, you can save the prompt as a notebook, and then edit the notebook. Using the generated notebook as a starting point is useful because it handles the initial setup steps, such as getting credentials and the project ID information for you.  
Alternatively, you can work with foundation models directly from a notebook by using the watsonx.ai Python library. For more information about the library and prerequisite steps, see Foundation models Python library."
1109	Inferencing a foundation model with a notebook	"Sample notebook
The Use watsonx to analyze car rentals reviews is a sample notebook that you can run to learn the steps involved in inferencing a foundation model in watsonx.ai.  
The following examples show you how to use the library to perform a few basic tasks in a notebook."
1110	Inferencing a foundation model with a notebook	"Example: List available foundation models
You can view ModelTypes to see available foundation models.  
Python code  
Sample output"
1111	Inferencing a foundation model with a notebook	"Example: View details of a foundation model
You can view details, such as a short description and foundation model limits, by using get_details().  
Python code  
Note:  
Replace {my-IBM-Cloud-API-key} and {my-project-ID} with your API key and project ID.  
Sample output"
1112	Inferencing a foundation model with a notebook	"Example: Prompt a foundation model
Prompt a foundation model to generate a response.  
Python code  
Note:  
Replace {my-IBM-Cloud-API-key} and {my-project-ID} with your API key and project ID.  
Sample output"
1113	Inferencing a foundation model with a notebook	"Example: Inferencing a foundation model by using a prompt template
You can use a prompt template in the following ways:  
Deploy the prompt template, and then use the deployed template in an inference request. Add text to a prompt based on a prompt template, and then inference a foundation model (without deploying the prompt template).  
For more information about the steps to follow, see Sample notebook: Use watsonx to manage prompt template assets and create a deployment.  
Inferencing a foundation model by using a deployed prompt template involves the following steps. For the complete steps and more options, see the sample notebook.  
prompt_template = PromptTemplate(name=""New prompt"", model_id=ModelTypes.FLAN_T5_XXL, model_params = {GenParams.DECODING_METHOD: ""sample""}, description=""My example"", task_ids=[""generation""], input_variables=[""object""], instruction=""Answer the following question"", input_prefix=""Human"", output_prefix=""Assistant"", input_text=""What is {object} and how does it work?"", examples=[[""What is a loan and how does it work?"", ""A loan is a debt that is repaid with interest over time.""]] ) stored_prompt_template = prompt_mgr.store_prompt(prompt_template=prompt_template) print(f""Asset id: {stored_prompt_template.prompt_id}"") prompt_input_text = prompt_mgr.load_prompt(prompt_id=stored_prompt_template.prompt_id, astype=PromptTemplateFormats.STRING) print(prompt_input_text) meta_props = { client.deployments.ConfigurationMetaNames.NAME: ""SAMPLE DEPLOYMENT PROMPT TEMPLATE"", client.deployments.ConfigurationMetaNames.ONLINE: {}, client.deployments.ConfigurationMetaNames.BASE_MODEL_ID: ""ibm/granite-13b-chat-v2""} deployment_details = client.deployments.create(artifact_uid=stored_prompt_template.prompt_id, meta_props=meta_props) model_inference = ModelInference( deployment_id=deployment_id, api_client=client) model_inference.generate_text(params={""prompt_variables"": {""object"": ""a mortgage""}, GenParams.DECODING_METHOD: DecodingMethods.GREEDY, GenParams.STOP_SEQUENCES: ['\n\n'], GenParams.MAX_NEW_TOKENS: 50})  
Import and instantiate the PromptTemplateManager object.  
Define a prompt template.  
Store the prompt template in your project to generate a prompt template ID."
1114	Inferencing a foundation model with a notebook	"Example: Inferencing a foundation model by using a prompt template
Import and instantiate the PromptTemplateManager object.  
Define a prompt template.  
Store the prompt template in your project to generate a prompt template ID.  
Load the text in the prompt template.  
Create a prompt template deployment and generate a deployment ID.  
Import and instantiate the ModelInference object to use for inferencing the foundation model by using the deployed prompt template.  
Inference the foundation model. Be sure to specify values for any prompt variables that are defined in the prompt template.  
   
Inferencing a foundation model by using a prompt template that is not deployed involves the following steps. For the complete steps and more options, see the sample notebook.  
prompt_text = prompt_mgr.load_prompt(prompt_id=stored_prompt_template.prompt_id, astype=PromptTemplateFormats.STRING) print(prompt_text) filled_prompt_text = prompt_text.format(object='credit card')  
List all of the available prompt templates.  
For more information, see List all prompt templates.  
Load the prompt template that you want to use. The prompt template does not need to be deployed. Convert the template to prompt text.  
For more information, see Load prompt.  
Substitute any prompt variables with the values that you want to use.  
For more information about prompt variables, see Building reusable prompts.  
Send the filled prompt text to a foundation model for inferencing.  
For example, generated_response = model.generate_text(prompt=filled_prompt_input, ...  
Parent topic: Python library"
1115	Coding generative AI solutions	Use the watsonx.ai Python library from a notebook or the watsonx.ai REST API directly to leverage foundation models in your generative AI applications.
1116	Coding generative AI solutions	"Tasks that you can do programmatically
The following table describes the tasks that you can perform by using a notebook, the REST API, or both. A checkmark (✓) indicates that the task is supported for the coding method."
1117	Coding generative AI solutions	"Learn more
Authenticating for programmatic access to a project or space Python library REST API Text embedding generation  
Parent topic: Developing generative AI solutions"
1118	Emotion classification	"The Emotion model in the Watson Natural Language Processing classification workflow classifies the emotion in the input text.  
Workflow name ensemble_classification-workflow_en_emotion-stock  
Supported languages  
English and French  
Capabilities  
The Emotion classification model is a pre-trained document classification model for the task of classifying the emotion in the input document. The model identifies the emotion of a document, and classifies it as:  
Anger Disgust Fear Joy Sadness  
Unlike the Sentiment model, which classifies each individual sentence, the Emotion model classifies the entire input document. As such, the Emotion model works optimally when the input text to classify is no longer than 1000 characters. If you would like to classify texts longer than 1000 characters, split the text into sentences or paragraphs for example and apply the Emotion model on each sentence or paragraph.  
A document may be classified into multiple categories or into no category.  
Dependencies on other blocks  
None  
Code sample  
Output of the code sample:  
Parent topic: Watson Natural Language Processing task catalog"
1119	Customizing Watson Machine Learning deployment runtimes	"Create custom Watson Machine Learning deployment runtimes with libraries and packages that are required for your deployments. You can build custom images based on deployment runtime images available in IBM Watson Machine Learning. The images contain preselected open source libraries and selected IBM libraries.  
For a list of requirements for creating private Python packages, refer to Requirements for using custom components in ML models.  
You can customize your deployment runtimes by customizing Python runtimes with third-party libraries and user-created Python packages  
Parent topic: Deploying and managing assets"
1120	IBM Watson Pipelines	"The Watson Pipelines editor provides a graphical interface for orchestrating an end-to-end flow of assets from creation through deployment. Assemble and configure a pipeline to create, train, deploy, and update machine learning models and Python scripts.  
To design a pipeline that you drag nodes onto the canvas, specify objects and parameters, then run and monitor the pipeline."
1121	IBM Watson Pipelines	"Automating the path to production
Putting a model into a product is a multi-step process. Data must be loaded and processed, models must be trained and tuned before they are deployed and tested. Machine learning models require more observation, evaluation, and updating over time to avoid bias or drift.  
Automating the pipeline makes it simpler to build, run, and evaluate a model in a cohesive way, to shorten the time from conception to production. You can assemble the pipeline, then rapidly update and test modifications. The Pipelines canvas provides tools to visualize the pipeline, customize it at run time with pipeline parameter variables, and then run it as a trial job or on a schedule.  
The Pipelines editor also allows for more cohesive collaboration between a data scientist and a ModelOps engineer. A data scientist can create and train a model. A ModelOps engineer can then automate the process of training, deploying, and evaluating the model after it is published to a production environment."
1122	IBM Watson Pipelines	"Next steps
Add a pipeline to your project and get to know the canvas tools."
1123	IBM Watson Pipelines	"Additional resources
For more information, see this blog post about automating the AI lifecycle with a pipeline flow."
1124	Foundation models built by IBM	"In IBM watsonx.ai, you can use IBM foundation models that are built with integrity and designed for business.  
The following families of IBM foundation models are available in watsonx.ai:  
Granite Slate"
1125	Foundation models built by IBM	"Granite foundation models
The Granite family of IBM foundation models includes decoder-only models that can efficiently predict and generate language.  
The models were built with trusted data that has the following characteristics:  
Sourced from quality data sets in domains such as finance (SEC Filings), law (Free Law), technology (Stack Exchange), science (arXiv, DeepMind Mathematics), literature (Project Gutenberg (PG-19)), and more. Compliant with rigorous IBM data clearance and governance standards. Scrubbed of hate, abuse, and profanity, data duplication, and blocklisted URLs, among other things.  
IBM is committed to building AI that is open, trusted, targeted, and empowering. For more information about contractual protections related to IBM indemnification, see the IBM Client Relationship Agreement and IBM watsonx.ai service description.  
The following Granite models are available in watsonx.ai today:"
1126	Foundation models built by IBM	"Granite foundation models
Prompt samples
To get started with the models, try these samples:  
Generative task sample: Generate a numbered list on a particular theme  
Question-answering task sample: Answer a question based on a document  
Chat sample: Converse in a dialog  
Question-answering sample: Answer general knowledge questions in Japanese  
Sample: Converse with granite-8b-japanese  
Sample Python notebook: Use watsonx and a Granite model to analyze car rental customer satisfaction from text"
1127	Foundation models built by IBM	"Granite foundation models
Related resources
To learn more about the models, read the following resources:  
Model information  
Research paper  
granite-13b-instruct-v2 model card  
granite-13b-chat-v2 model card  
granite-8b-japanese model card  
granite-20b-multilingual model card"
1128	Foundation models built by IBM	"Slate foundation models
The Slate family of IBM foundation models includes encoder-only models that specialize in natural language processing and text embedding tasks.  
The following Slate embedding models are available in watsonx.ai today:  
For more information about using Slate models to convert sentences and passages into text embeddings, see Text embedding generation.  
IBM Slate models also power a set of libraries that you can use for common natural language processing (NLP) tasks, such as classification, entity extraction, sentiment analysis, and more.  
For more information about how to use the NLP capabilities of the Slate models, see Watson NLP library.  
Parent topic: Supported foundation models"
1129	Tuning a foundation model	To tune a foundation model, create a tuning experiment that guides the foundation model to return the output you want in the form you want.
1130	Tuning a foundation model	"Requirements
The Tuning Studio is not available with all plans or in all data centers. The foundation models that are available for tuning in the Tuning Studio can also differ by data center. For more information, see Watson Machine Learning plans and Regional availability for services and features.  
Typically, the Tuning Studio is available from a project that is created for you automatically when you sign up for watsonx.ai. The project is named sandbox and you can use it to get started with testing and customizing foundation models.  
If you don't have a project, create one. From the main menu, expand Projects, and then click All projects.  
Click New project.  
Name the project, and then optionally add a description.  
For more information about project options, such as reporting or logging, see Creating a project.  
Click Create."
1131	Tuning a foundation model	"Before you begin
Find the foundation model that works best for your use case. For more information, see Choosing a foundation model to tune.  
Create a set of example prompts that follow the pattern that generates the best results based on your prompt engineering work. For more information, see Data formats."
1132	Tuning a foundation model	"Tune a model
From the watsonx.ai home page, choose your project, and then click Tune a foundation model with labeled data.  
Text Uses text that you specify. Random Uses values that are generated for you as part of the tuning experiment. Classification Predicts categorical labels from features. For example, given a set of customer comments, you might want to label each statement as a question or a problem. By separating out customer problems, you can find and address them more quickly. Generation Generates text. For example, writes a promotional email. Summarization Generates text that describes the main ideas that are expressed in a body of text. For example, summarizes a research paper.  
Name the tuning experiment.  
Optional: Add a description and tags. Add a description as a reminder to yourself and to help collaborators understand the goal of the tuned model. Assigning a tag gives you a way to filter your tuning assets later to show only the assets associated with a tag.  
Click Create.  
Click Select a foundation model to choose the foundation model that you want to tune.  
Click a tile to see a model card with details about the foundation model. When you find the foundation model that you want to use, click Select.  
For more information, see Choosing a foundation model to tune.  
Choose how to initialize the prompt from the following options:  
These options are related to the prompt tuning method for tuning models. For more information about how each option affects the tuning experiment, see How prompt-tuning works.  
Required for the Text initialization method only: Add the initialization text that you want to include with the prompt.  
For a classification task, give an instruction that describes what you want to classify and lists the class labels to be used. For example, Classify whether the sentiment of each comment is Positive or Negative. For a generative task, describe what you want the model to provide in the output. For example, Make the case for allowing employees to work from home a few days a week. For a summarization task, give an instruction such as, Summarize the main points from a meeting transcript.  
Choose a task type.  
Choose the task type that most closely matches what you want the model to do:"
1133	Tuning a foundation model	"Tune a model
Choose a task type.  
Choose the task type that most closely matches what you want the model to do:  
Whichever task you choose, the input is submitted to the underlying foundation model as a generative request type during the experiment. For classification tasks, class names are taken into account in the prompts that are used to tune the model. As models and tuning methods evolve, task-specific enhancements are likely to be added that you can leverage if tasks are represented accurately.  
Required for classification tasks only: In the Classification output (verbalizer) field, add the class labels that you want the model to use one at a time.  
Important: Specify the same labels that are used in your training data.  
During the tuning experiment, class label information is submitted along with the input examples from the training data.  
Add the training data that will be used to tune the model. You can upload a file or use an asset from your project.  
To see examples of how to format your file, expand What should your data look like?, and then click Preview template. For more information, see Data formats.  
Optional: If you want to limit the size of the input or output examples that are used during training, adjust the maximum number of tokens that are allowed.  
Expand What should your data look like?, and then scroll to see the Maximum input tokens and Maximum output tokens fields. Drag the sliders to change the values. Limiting the size can reduce the time that it takes to run the tuning experiment. For more information, see Controlling the number of tokens used.  
Optional: Click Configure parameters to edit the parameters that are used by the tuning experiment.  
The tuning run is configured with parameter values that represent a good starting point for tuning a model. You can adjust them if you want.  
For more information about the available parameters and what they do, see Tuning parameters.  
After you change parameter values, click Save.  
Click Start tuning.  
The tuning experiment begins. It might take a few minutes to a few hours depending on the size of your training data and the availability of compute resources. When the experiment is finished, the status shows as completed.  
A tuned model asset is not created until after you create a deployment from a completed tuning experiment. For more information, see Deploying a tuned model."
1134	Tuning a foundation model	"Controlling the number of tokens used
You can change the number of tokens that are allowed in the model input and output during a tuning experiment.  
The larger the number of allowed input and output tokens, the longer it takes to tune the model. Use the smallest number of tokens in your examples that is possible to use but still represent your use case properly.  
You already have some control over the input size. The input text that is used during a tuning experiment comes from your training data. So, you can manage the input size by keeping your example inputs to a set length. However, you might be getting uncurated training data from another team or process. In that case, you can use the Maximum input tokens slider to manage the input size. If you set the parameter to 200 and the training data has an example input with 1,000 tokens, for example, the example is truncated. Only the first 200 tokens of the example input are used.  
The Max output tokens value is important because it controls the number of tokens that the model is allowed to generate as output at training time. You can use the slider to limit the output size, which helps the model to generate concise output.  
Tip: For classification tasks, minimizing the size of the output is a good way to force a generative model to return the class label only, without repeating the classification pattern in the output.  
For natural language models, words are converted to tokens. 256 tokens is equal to approximately 130—170 words. 128 tokens is equal to approximately 65—85 words. However, token numbers are difficult to estimate and can differ by model. For more information, see Tokens and tokenization."
1135	Tuning a foundation model	"Evaluating the tuning experiment
When the experiment is finished, a loss function graph is displayed that illustrates the improvement in the model output over time. The epochs are shown on the x-axis and a measure of the difference between predicted and actual results per epoch is shown on the y-axis. The value that is shown per epoch is calculated from the average gradient value from all of the accumulation steps in the epoch.  
For more information about how to evaluate the results, see Evaluating the results of a tuning experiment.  
When you are satisfied with the tuning experiment results, deploy the tuned foundation model. For more information, see Deploying a tuned model."
1136	Tuning a foundation model	"Running a tuning experiment again
To rerun a tuning experiment, complete the following steps:  
From the project's Assets page, filter your assets by the Tuning experiments asset type. Find and open your tuning experiment asset, and then click New tuned model.  
The loss function for the tuning experiment is shown in the same graph with loss functions from previous runs so that you can compare them."
1137	Tuning a foundation model	"Learn more
Data formats  
Tuning parameters  
Deploying a tuned model  
Quick start: Tune a foundation model  
Sample notebook: Tune a model to classify CFPB documents in watsonx  
Sample notebook: Use watsonx.ai to tune Meta llama-2-13b-chat model with CFPB documents  
Sample notebook: Prompt tuning for multi-class classification with watsonx  
Parent topic: Tuning Studio"
1138	Deploying AI assets	"Using IBM Watson Machine Learning, you can deploy machine learning models, scripts, and functions, and prompt templates for generative AI models. After you create deployments, you can test and manage them, and prepare your assets to deploy into pre-production and production environments to generate predictions and insights.  
Service The administrator must install the Watson Machine Learning service on Cloud Pak for Data platform to use its capabilities."
1139	Deploying AI assets	"Deployments overview
You can use Watson Machine Learning to create an online, batch, or application deployment for machine learning, foundation model, and Decison Optimization assets.  
The following graphic shows the typical activities to deploy AI assets:"
1140	Deploying AI assets	"Deployments overview
Types of deployments
The most common types of deployments are as follows:  
Online deployment: Create an online deployment to process input data in real-time. To test the online deployment in real-time, you can submit new customer data to the deployment endpoint to get a prediction in real-time.  
Batch deployment: Create a batch deployment to process a large batch of input data from a data source and write the output to a selected destination. You can configure the batch deployment job and run the job on a schedule or on demand.  
Application deployment: Create an application deployment to deploy your application assets, such as R Shiny applications."
1141	Deploying AI assets	"Deployments overview
Types of deployable assets
The type of asset that you deploy dictates the type of deployment that you can create. For example, Python functions, scripts, and models, such as AutoAI or Decision Optimization models support online and batch deployments. However, you can create online deployments only for models that are imported from a file. The different types of deployable assets are as follows:  
Foundation model assets: You can deploy foundation model assets such as tuned model, prompt template assets, or custom foundation models with watsonx.ai.  
Machine Learning assets: You can deploy machine learning Machine Learning assets such as Python functions, R Shiny applications, NLP models, scripts, and more with Watson Machine Learning.  
Decision Optimization models: You can deploy Decision Optimization model with Watson Machine Learning."
1142	Deploying AI assets	"Ways to deploy assets
You can deploy and manage your assets in the following ways:  
Deploy and manage assets in a deployment space by using a no-code approach.  
Deploy and manage assets programmatically by using a custom-code approach."
1143	Deploying AI assets	"Deploying and managing assets in deployment spaces
Create a deployment space to collaborate with stakeholders and deploy and manage assets in a deployment space.  
To manage your assets within a deployment space, you must promote your assets from a project to your deployment space. You can also import or export assets from your deployment space. You can manage assets and deployments in your deployment space by updating details, scaling the number of copies, monitoring performance, or deleting the deployment to free up resources.  
To use you deployed asset in applications for making predictions, retrieve the endpoint URL for online and batch deployments. The model endpoint provides access to an interface to invoke and manage model deployments.  
Use the deployments dashboard to get an aggregate view of your deployments and monitor deployment activity. You can use the dashboard to monitor the status of your batch deployment jobs, such as active runs and finished runs based on job schedule that you defined when you created the job. You can also get information about the number of successful and failed online deployments."
1144	Deploying AI assets	"Deploying and managing assets programmatically
You can deploy and manage assets programmatically by using Python client or Watson Machine Learning API. ."
1145	Deploying AI assets	"Deploying and managing assets programmatically
Managing frameworks and software specifications for deployments
Software specifications and frameworks contain bundles of packages with corresponding versions of the packages.  
You can use predefined software specifications or create custom software specifications by adding new packages to existing frameworks, create new packages, or updating package versions in software specifications."
1146	Deploying AI assets	"Deploying and managing assets programmatically
Managing runtime environments for deployments
Runtime envionments provide the necessary functions that are required to run your deployment.  
Important: You must use the same runtime environment to build and deploy your model.  
You can use predefined runtime environments or create custom runtime environments to include more components, depending on your use case. To create a custom runtime environment for your deployment, you must create a Dockerfile and add a base image. Further, you can add the docker commands to build the runtime environment for your deployment."
1147	Deploying AI assets	"Learn more
Deployment spaces  
Parent topic: Deploying and managing AI assets"
1148	Creating deployment spaces	"Create a deployment space to store your assets, deploy assets, and manage your deployments.  
Required permissions: All users in your IBM Cloud account with the Editor IAM platform access role for all IAM enabled services or for Cloud Pak for Data can manage to create deployment spaces. For more information, see IAM Platform access roles.  
A deployment space is not associated with a project. You can publish assets from multiple projects to a space. For example, you might have a test space for evaluating deployments, and a production space for deployments you want to deploy in business applications.  
Follow these steps to create a deployment space:  
From the navigation menu, select Deployments > New deployment space. Enter a name for your deployment space. Optional: Add a description and tags. Select a storage service to store your space assets. Optional: If you want to deploy assets from your space, select a machine learning service instance to associate with your deployment space. To associate a machine learning instance to a space, you must:  
If you have a Cloud Object Storage repository that is associated with your IBM Cloud account, choose a repository from the list to store your space assets. If you do not have a Cloud Object Storage repository that is associated with your IBM Cloud account, you are prompted to create one.  
Be a space administrator. Have admin access to the machine learning service instance that you want to associate with the space.  
Tip: If you want to evaluate assets in the space, switch to the **Manage** tab and associate a Watson OpenScale instance.  
Tip: If you get an import error, clear your browser cookies and then try again.  
Click Create."
1149	Creating deployment spaces	"Viewing and managing deployment spaces
To view all deployment spaces that you can access, click Deployments on the navigation menu. To view any of the details about the space after you create it, such as the associated service instance or storage ID, open your deployment space and then click the Manage tab. Your space assets are stored in a Cloud Object Storage repository. You can access this repository from IBM Cloud. To find the bucket ID, open your deployment space, and click the Manage tab."
1150	Creating deployment spaces	"Learn more
To learn more about adding assets to a space and managing them, see Assets in deployment spaces.  
To learn more about creating a space and accessing its details programmatically, see Notebook on managing spaces.  
To learn more about handling spaces programmatically, see Python client or REST API.  
Parent topic: Deployment spaces"
1151	Promoting assets to a deployment space	Learn about how to promote assets from a project to a deployment space and the requirements for promoting specific asset types.
1152	Promoting assets to a deployment space	"Promoting assets to your deployment space
You can promote assets from your project to a deployment space. For a list of assets that can be promoted from a project to a deployment space, refer to Adding assets to a deployment space. When you are promoting assets, you can:  
Choose an existing space or create a new one. Add tags to help identify the promoted asset. Choose dependent assets to promote them at the same time.  
Follow these steps to promote your assets to your deployment space:  
From your project, go to the Assets tab. Select the Options () icon and click Promote to space.  
Tip: If the asset that you want to promote is a model, you can also click the model name to open the model details page, and then click the Promote to deployment space icon.  
Notes:  
Promoting assets and their dependencies from a project to a space by using the Watson Studio user interface is the recommended method to guarantee that the promotion flow results in a complete asset definition. For example, relying on the Watson Data API to manage the promotion flow of an asset, together with its dependencies, can result in the promoted asset from being inaccessible from the space. Promoting assets from default Git-based projects is not supported. Depending on your configuration and the type of asset that you promote, large asset attachments, typically more than 2 GB, can cause the promotion action to time out.  
For more information, see:  
Promoting connections and connected data Promoting models"
1153	Promoting assets to a deployment space	"Promoting connections and connected data
When you promote a connection that uses personal credentials or Cloud Pak for Data authentication to a deployment space, the credentials are not promoted. You must provide the credentials information again or allow Cloud Pak for Data authentication. Because Storage Volume connections support only personal credentials, to be able to use this type of asset after it is promoted to a space, you must provide the credentials again. Some types of connections allow for using your personal platform credentials. If you promote a connection or connected data that uses your personal platform credentials, tick the Use my platform login credentials checkbox. Although you can promote any kind of data connection to a space, where you can use the connection is governed by factors such as model and deployment type. For example, you can access any of the connected data by using a script. However, in batch deployments you are limited to particular types of data, as listed in Creating a batch deployment."
1154	Promoting assets to a deployment space	"Promoting models
When you promote a model to a space:  
Components that are required for a successful deployment, such as a custom software specification or model definition are automatically promoted as well. The data assets that were used to train the model are not promoted with it. Information on data assets used to train the model is included in model metadata.  
Parent topic: Assets in deployment spaces"
1155	Compute resource options for Synthetic Data Generator in projects	To create data with the Synthetic Data Generator, you must have the Watson Studio and Watson Machine Learning services provisioned. Running a synthetic data flow consumes compute resources from the Watson Studio service.
1156	Compute resource options for Synthetic Data Generator in projects	"Compute usage in projects
Running a synthetic data flow consumes compute resources from the Watson Studio service.  
You can monitor the total monthly amount of CUH consumption for Watson Studio on the Resource usage page on the Manage tab of your project."
1157	Compute resource options for Synthetic Data Generator in projects	"Learn more
Synthetic data Watson Machine Learning service Watson Studio service Monitoring account resource usage  
Parent topic: Choosing compute resources for tools"
1158	Deployment space collaborator roles and permissions	When you add collaborators to a deployment space, you can specify which actions they can do by assigning them access levels. Learn how to add collaborators to your deployment spaces and the differences between access levels.
1159	Deployment space collaborator roles and permissions	"User roles and permissions in deployment spaces
You can assign the following roles to collaborators based on the access level that you want to provide:  
Admin: Administrators can control your deployment space assets, users, and settings. Editor: Editors can control your space assets. Viewer: Viewers can view your deployment space.  
The following table provides details on permissions based on user access level:"
1160	Deployment space collaborator roles and permissions	"User roles and permissions in deployment spaces
Service IDs
You can create service IDs in IBM Cloud to enable an application outside of IBM Cloud access to your IBM Cloud services. Service IDs are not tied to a specific user. Therefore, if a user leaves an organization and is deleted from the account, the service ID remains. Thus, your application or service stays up and running. For more information, see Creating and working with service IDs.  
To learn more about assigning space access by using a service ID, see Adding collaborators to your deployment space."
1161	Deployment space collaborator roles and permissions	"Adding collaborators to your deployment space
Prerequisites: All users in your IBM Cloud account with the Admin IAM platform access role for all IAM enabled services can manage space collaborators. For more information, see IAM Platform access roles.  
Restriction: You can add collaborators to your deployment space only if they are a part of your organization and if they provisioned Watson Studio.  
To add one or more collaborators to a deployment space:  
From your deployment space, go to the Manage tab and click Access Control. Click Add collaborators and choose one of the following options: Add the user or user groups that you want to have the same access level and click Add.  
If you want to add a user, click Add users. Assign a role that applies to the user. If you want to add pre-defined user groups, click . Assign a role that applies to all members of the group.  
Parent topic: Deployment spaces"
1162	Managing outdated software specifications or frameworks	"Use these guidelines when you are updating assets that refer to outdated software specifications or frameworks.  
In some cases, asset update is seamless. In other cases, you must retrain or redeploy the assets. For general guidelines, refer to Migrating assets that refer to discontinued software specifications or Migrating assets that refer to discontinued framework versions.  
For more information, see the following sections:  
Updating software specifications  
Updating a machine learning model  
Updating a Python function  
Retraining an SPSS Modeler flow"
1163	Managing outdated software specifications or frameworks	"Managing assets that refer to discontinued software specifications
During migration, assets that refer to the discontinued software specification are mapped to a comparable-supported default software specification (only in cases where the model type is still supported). When you create new deployments of the migrated assets, the updated software specification in the asset metadata is used. Existing deployments of the migrated assets are updated to use the new software specification. If deployment or scoring fails due to framework or library version incompatibilities, follow the instructions in Updating software specifications. If the problem persists, follow the steps that are listed in Updating a machine learning model."
1164	Managing outdated software specifications or frameworks	"Migrating assets that refer to discontinued framework versions
During migration, model types are not be updated. You must manually update this data. For more information, see Updating a machine learning model. After migration, the existing deployments are removed and new deployments for the deprecated framework are not allowed."
1165	Managing outdated software specifications or frameworks	"Updating software specifications
You can update software specifications from the UI or by using the API. For more information, see the following sections:  
Updating software specifications from the UI Updating software specifications by using the API"
1166	Managing outdated software specifications or frameworks	"Updating software specifications
Updating software specifications from the UI
From the deployment space, click the model (make sure it does not have any active deployments.) Click the i symbol to check model details. Use the dropdown list to update the software specification.  
Refer to the example image:"
1167	Managing outdated software specifications or frameworks	"Updating software specifications
Updating software specifications by using the API
You can update a software specification by using the API Patch command:  
For software_spec field, type /software_spec. For value field, use either the ID or the name of the new software specification.  
Refer to this example:  
For more information, see Updating an asset by using the Patch API command."
1168	Managing outdated software specifications or frameworks	"Updating a machine learning model
Follow these steps to update a model built with a deprecated framework."
1169	Managing outdated software specifications or frameworks	"Updating a machine learning model
Option 1: Save the model with a compatible framework
client.repository.download(<model-id>, filename=""xyz.tar.gz"") model_metadata = { client.repository.ModelMetaNames.NAME: ""example model"", client.repository.ModelMetaNames.DESCRIPTION: ""example description"", client.repository.ModelMetaNames.TYPE: ""<new model type>"", client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: client.software_specifications.get_uid_by_name(""<new software specification name>"") } model_details = client.repository.store_model(model=""xyz.tar.gz"", meta_props=model_metadata)  
Download the model by using either the Watson Machine Learning REST API or the Watson Machine Learning Python client library.  
The following example shows how to download your model:  
Edit model metadata with the model type and version that is supported in the current release. For more information, see Software specifications and hardware specifications for deployments.  
The following example shows how to edit model metadata:  
Save the model to the Watson Machine Learning repository. The following example shows how to save the model to the repository:  
Deploy the model.  
Score the model to generate predictions.  
If deployment or scoring fails, the model is not compatible with the new version that was used for saving the model. In this case, use Option 2."
1170	Managing outdated software specifications or frameworks	"Updating a machine learning model
Option 2: Retrain the model with a compatible framework
Retrain the model with a model type and version that is supported in the current version. Save the model with the supported model type and version. Deploy and score the model.  
It is also possible to update a model by using the API. For more information, see Updating an asset by using the Patch API command."
1171	Managing outdated software specifications or frameworks	"Updating a Python function
Follow these steps to update a Python function built with a deprecated framework."
1172	Managing outdated software specifications or frameworks	"Updating a Python function
Option 1: Save the Python function with a compatible runtime or software specification
Download the Python function by using either the Watson Machine Learning REST API or the Watson Machine Learning Python client library. Save the Python function with a supported runtime or software specification version. For more information, see Software specifications and hardware specifications for deployments. Deploy the Python function. Score the Python function to generate predictions.  
If your Python function fails during scoring, the function is not compatible with the new runtime or software specification version that was used for saving the Python function. In this case, use Option 2."
1173	Managing outdated software specifications or frameworks	"Updating a Python function
Option 2: Modify the function code and save it with a compatible runtime or software specification
Modify the Python function code to make it compatible with the new runtime or software specification version. In some cases, you must update dependent libraries that are installed within the Python function code. Save the Python function with the new runtime or software specification version. Deploy and score the Python function.  
It is also possible to update a function by using the API. For more information, see Updating an asset by using the Patch API command."
1174	Managing outdated software specifications or frameworks	"Retraining an SPSS Modeler flow
Some models that were built with SPSS Modeler in IBM Watson Studio Cloud before 1 September 2020 can no longer be deployed by using Watson Machine Learning. This problem is caused by an upgrade of the Python version in supported SPSS Modeler runtimes. If you're using one of the following six nodes in your SPSS Modeler flow, you must rebuild and redeploy your models with SPSS Modeler and Watson Machine Learning:  
XGBoost Tree XGBoost Linear One-Class SVM HDBSCAN KDE Modeling Gaussian Mixture  
To retrain your SPSS Modeler flow, follow these steps:  
If you're using the Watson Studio user interface, open the SPSS Modeler flow in Watson Studio, retrain, and save the model to Watson Machine Learning. After you save the model to the project, you can promote it to a deployment space and create a new deployment. If you're using REST API or Python client, retrain the model by using SPSS Modeler and save the model to the Watson Machine Learning repository with the model type spss-modeler-18.2.  
Parent topic: Frameworks and software specifications"
1175	Creating a pipeline	"Create a pipeline to run an end-to-end scenario to automate all or part of the AI lifecycle. For example, create a pipeline that creates and trains an asset, promotes it to a space, creates a deployment, then scores the model.  
Watch this video to see how to create and run a sample pipeline.  
This video provides a visual method to learn the concepts and tasks in this documentation."
1176	Creating a pipeline	"Overview: Adding a pipeline to a project
Follow these steps to add a pipeline to a project:  
Open a project.  
Click New task > Automate model lifecycle.  
Enter a name and an optional description.  
Click Create to open the canvas."
1177	Creating a pipeline	"Overview: Adding a pipeline to a project
Pipeline access
When you use a pipeline to automate a flow, you must have access to all of the elements in the pipeline. Make sure that you create and run pipelines with the proper access to all assets, projects, and spaces used in the pipeline."
1178	Creating a pipeline	"Overview: Adding a pipeline to a project
Related services
In addition to access to all elements in a pipeline, you must have the services available to run all assets you add to a pipeline. For example, if you automate a pipeline that trains and deploys a model, you must have the Watson Studio and Watson Machine Learning services. If a required service is missing, the pipeline will not run. This table lists assets that require services in addition to Watson Studio:"
1179	Creating a pipeline	"Overview: Building a pipeline
Follow these high-level steps to build and run a pipeline.  
Drag any node objects onto the canvas. For example, drag a Run notebook job node onto the canvas. Use the action menu for each node to view and select options. Configure a node as required. You are prompted to supply the required input options. For some nodes, you can view or configure output options as well. For examples of configuring nodes, see Configuring pipeline components. Drag from one node to another to connect and order the pipeline. Optional: Click the Global objects icon in the toolbar to configure runtime options for the pipeline. When the pipeline is complete, click the Run icon on the toolbar to run the pipeline. You can run a trial to test the pipeline, or you can schedule a job when you are confident in the pipeline."
1180	Creating a pipeline	"Overview: Building a pipeline
Configuring nodes
As you add nodes to a pipeline, you must configure them to provide all of the required details. For example, if you add a node to run an AutoAI experiment, you must configure the node to specify the experiment, load the training data, and specify the output file:"
1181	Creating a pipeline	"Overview: Building a pipeline
Connecting nodes
When you build a complete pipeline, the nodes must be connected in the order in which they run in the pipeline. To connect nodes, hover over a node and drag a connection to the target node. Disconnected nodes are run in parallel.  
You can re-link nodes by dragging the links on canvas from one node to another. General conditions attached to the links will refer to the new node, including conditions that refer to node status.  
Change the connection between nodes can result in errors. Error messages will notify if you must update the node configuration. For example, if you refer to an inaccessible node, this will result in an error."
1182	Creating a pipeline	"Overview: Building a pipeline
Defining pipeline parameters
A pipeline parameter defines a global variable for the whole pipeline. Use pipeline parameters to specify data from one of these categories:  
To specify a pipeline parameter:  
Click the global objects icon in the toolbar to open the Manage global objects window. Select the Pipeline parameters tab to configure parameters. Click Add pipeline parameter. Specify a name and an optional description. Select a type and provide any required information. Click Add when the definition is complete, and repeat the previous steps until you finish defining the parameters. Close the Manage global objects dialog.  
The parameters are now available to the pipeline."
1183	Creating a pipeline	"Next steps
Configure pipeline components  
Parent topic: IBM Watson Pipelines"
1184	AutoAI tutorial: Build a Binary Classification Model	"This tutorial guides you through training a model to predict if a customer is likely to buy a tent from an outdoor equipment store.  
Create an AutoAI experiment to build a model that analyzes your data and selects the best model type and algorithms to produce, train, and optimize pipelines. After you review the pipelines, save one as a model, deploy it, and then test it to get a prediction.  
Watch this video to see a preview of the steps in this tutorial.  
Video disclaimer: Some minor steps and graphical elements in this video might differ from your platform.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Transcript  
Synchronize transcript with video"
1185	AutoAI tutorial: Build a Binary Classification Model	"Video transcript Time Transcript 00:00 In this video, you will see how to build a binary classification model that assesses the likelihood that a customer of an outdoor equipment company will buy a tent. 00:11 This video uses a data set called ""GoSales"", which you'll find in the Resource Hub. 00:16 View the data set. 00:20 The feature columns are ""GENDER"", ""AGE"", ""MARITAL_STATUS"", and ""PROFESSION"" and contain the attributes on which the machine learning model will base predictions. 00:31 The label columns are ""IS_TENT"", ""PRODUCT_LINE"", and ""PURCHASE_AMOUNT"" and contain historical outcomes that the models could be trained to predict. 00:44 Add this data set to the ""Machine Learning"" project and then go to the project. 00:56 You'll find the GoSales.csv file with your other data assets. 01:02 Add to the project an ""AutoAI experiment"". 01:08 This project already has the Watson Machine Learning service associated. 01:13 If you haven't done that yet, first, watch the video showing how to run an AutoAI experiment based on a sample. 01:22 Just provide a name for the experiment and then click ""Create"". 01:30 The AutoAI experiment builder displays. 01:33 You first need to load the training data. 01:36 In this case, the data set will be from the project. 01:40 Select the GoSales.csv file from the list. 01:45 AutoAI reads the data set and lists the columns found in the data set. 01:50 Since you want the model to predict the likelihood that a given customer will purchase a tent, select ""IS_TENT"" as the column to predict. 01:59 Now, edit the experiment settings. 02:03 First, look at the settings for the data source. 02:06 If you have a large data set, you can run the experiment on a subsample of rows and you can configure how much of the data will be used for training and how much will be used for evaluation. 02:19 The default is a 90%/10% split, where 10% of the data is reserved for evaluation. 02:27 You can also select which columns from the data set to include when running the experiment. 02:35 On"
1186	AutoAI tutorial: Build a Binary Classification Model	"The default is a 90%/10% split, where 10% of the data is reserved for evaluation. 02:27 You can also select which columns from the data set to include when running the experiment. 02:35 On the ""Prediction"" panel, you can select a prediction type. 02:39 In this case, AutoAI analyzed your data and determined that the ""IS_TENT"" column contains true-false information, making this data suitable for a ""Binary classification"" model. 02:52 The positive class is ""TRUE"" and the recommended metric is ""Accuracy"". 03:01 If you'd like, you can choose specific algorithms to consider for this experiment and the number of top algorithms for AutoAI to test, which determines the number of pipelines generated. 03:16 On the ""Runtime"" panel, you can review other details about the experiment. 03:21 In this case, accepting the default settings makes the most sense. 03:25 Now, run the experiment. 03:28 AutoAI first loads the data set, then splits the data into training data and holdout data. 03:37 Then wait, as the ""Pipeline leaderboard"" fills in to show the generated pipelines using different estimators, such as XGBoost classifier, or enhancements such as hyperparameter optimization and feature engineering, with the pipelines ranked based on the accuracy metric. 03:58 Hyperparameter optimization is a mechanism for automatically exploring a search space for potential hyperparameters, building a series of models and comparing the models using metrics of interest. 04:10 Feature engineering attempts to transform the raw data into the combination of features that best represents the problem to achieve the most accurate prediction. 04:21 Okay, the run has completed. 04:24 By default, you'll see the ""Relationship map"". 04:28 But you can swap views to see the ""Progress map"". 04:32 You may want to start with comparing the pipelines. 04:36 This chart provides metrics for the eight pipelines, viewed by cross validation score or by holdout score. 04:46 You can see the pipelines ranked based on other metrics, such as average precision. 04:55 Back on the ""Experiment summary"" tab, expand a pipeline to view the model evaluation measures and ROC curve. 05:03 During AutoAI training, your data set is split into two parts: training data and holdout data."
1187	AutoAI tutorial: Build a Binary Classification Model	"Back on the ""Experiment summary"" tab, expand a pipeline to view the model evaluation measures and ROC curve. 05:03 During AutoAI training, your data set is split into two parts: training data and holdout data. 05:11 The training data is used by the AutoAI training stages to generate the model pipelines, and cross validation scores are used to rank them. 05:21 After training, the holdout data is used for the resulting pipeline model evaluation and computation of performance information, such as ROC curves and confusion matrices. 05:33 You can view an individual pipeline to see more details in addition to the confusion matrix, precision recall curve, model information, and feature importance. 05:46 This pipeline had the highest ranking, so you can save this as a machine learning model. 05:52 Just accept the defaults and save the model. 05:56 Now that you've trained the model, you're ready to view the model and deploy it. 06:04 The ""Overview"" tab shows a model summary and the input schema. 06:09 To deploy the model, you'll need to promote it to a deployment space. 06:15 Select the deployment space from the list, add a description for the model, and click ""Promote"". 06:24 Use the link to go to the deployment space. 06:28 Here's the model you just created, which you can now deploy. 06:33 In this case, it will be an online deployment. 06:37 Just provide a name for the deployment and click ""Create"". 06:41 Then wait, while the model is deployed. 06:44 When the model deployment is complete, view the deployment. 06:49 On the ""API reference"" tab, you'll find the scoring endpoint for future reference. 06:56 You'll also find code snippets for various programming languages to utilize this deployment from your application. 07:05 On the ""Test"" tab, you can test the model prediction. 07:09 You can either enter test input data or paste JSON input data, and click ""Predict"". 07:20 This shows that there's a very high probability that the first customer will buy a tent and a very high probability that the second customer will not buy a tent. 07:33 And back in the project, you'll find the AutoAI experiment and the model on the ""Assets"""
1188	AutoAI tutorial: Build a Binary Classification Model	"high probability that the first customer will buy a tent and a very high probability that the second customer will not buy a tent. 07:33 And back in the project, you'll find the AutoAI experiment and the model on the ""Assets"" tab. 07:44 Find more videos in the Cloud Pak for Data as a Service documentation."
1189	AutoAI tutorial: Build a Binary Classification Model	"Overview of the data sets
The sample data is structured (in rows and columns) and saved in a .csv file format.  
You can view the sample data file in a text editor or spreadsheet program:  
Choose the column whose values that your model predicts.  
In this tutorial, the model predicts the values of the IS_TENT column:  
IS_TENT: Whether the customer bought a tent  
The model that is built in this tutorial predicts whether a customer is likely to purchase a tent."
1190	AutoAI tutorial: Build a Binary Classification Model	"Tasks overview
This tutorial presents the basic steps for building and training a machine learning model with AutoAI:  
Create a project Create an AutoAI experiment Training the experiment Deploy the trained model Test the deployed model Creating a batch to score the model"
1191	AutoAI tutorial: Build a Binary Classification Model	"Task 1: Create a project
From the Resource hub, download the GoSales data set file to your local computer. From the Projects page, to create a new project, select New Project. b. Type your project name. c. Click Create."
1192	AutoAI tutorial: Build a Binary Classification Model	"Task 2: Create an AutoAI experiment
On the Assets tab from within your project, click New asset > Build machine learning models automatically.  
Specify a name and optional description for your new experiment.  
Select the Associate a Machine Learning service instance link to associate the Watson Machine Learning Server instance with your project. Click Reload to confirm your configuration.  
To add a data source, you can choose one of these options: a. If you downloaded your file locally, upload the training data file, GoSales.csv, from your local computer. Drag the file onto the data panel or click browse and follow the prompts. b. If you already uploaded your file to your project, click select from project, then select the data asset tab and choose GoSales.csv."
1193	AutoAI tutorial: Build a Binary Classification Model	"Task 3: Training the experiment
In Configuration details, select No for the option to create a Time Series Forecast.  
Choose IS_TENT as the column to predict. AutoAI analyzes your data and determines that the IS_TENT column contains True and False information, making this data suitable for a binary classification model. The default metric for a binary classification is ROC/AUC.  
Click Run experiment. As the model trains, an infographic shows the process of building the pipelines.  
Note:  
You might see slight differences in results based on the Cloud Pak for Data platform and version you use.  
For a list of algorithms or estimators that are available with each machine learning technique in AutoAI, see AutoAI implementation detail.  
When all the pipelines are created, you can compare their accuracy on the Pipeline leaderboard.  
Select the pipeline with Rank 1 and click Save as to create your model. Then, select Create. This option saves the pipeline under the Models section in the Assets tab."
1194	AutoAI tutorial: Build a Binary Classification Model	"Task 4: Deploy the trained model
You can deploy the model from the model details page. You can access the model details page in one of these ways:  
Clicking the model’s name in the notification displayed when you save the model. Open the Assets tab for the project, select the Models section and select the model’s name.  
Click the Promote to deployment space icon, and then select an existing space, or or create a new space where the model will be deployed.  
Type a name for the deployment space. Associate it with a Machine Learning Service. Click Create.  
After you create your deployment space or select an existing one, select Promote.  
Click the deployment space link from the notification.  
From the Assets tab of the deployment space:  
Hover over the model’s name and click the deployment icon .  
In the page that opens, complete the fields:  
Select Online as the Deployment type. Specify a name for the deployment. Click Create.  
After the deployment is complete, click Deployments and select the deployment name to view the details page."
1195	AutoAI tutorial: Build a Binary Classification Model	"Task 5: Test the deployed model
You can test the deployed model from the deployment details page:  
{""input_data"":[{ ""fields"": [""GENDER"",""AGE"",""MARITAL_STATUS"",""PROFESSION"",""PRODUCT_LINE"",""PURCHASE_AMOUNT""], ""values"": [[""M"",27,""Single"", ""Professional"",""Camping Equipment"",144.78]] }]}  
On the Test tab of the deployment details page, complete the form with test values or enter JSON test data by clicking the terminal icon to provide the following JSON input data.  
Note: The test data replicates the data fields for the model, except for the prediction field.  
Click Predict to predict whether a customer with the entered attributes is likely to buy a tent. The resulting prediction indicates that a customer with the attributes entered has a high probability of purchasing a tent."
1196	AutoAI tutorial: Build a Binary Classification Model	"Task 6: Creating a batch job to score the model
For a batch deployment, you provide input data, also known as the model payload, in a CSV file. The data must be structured like the training data, with the same column headers. The batch job processes each row of data and creates a corresponding prediction.  
In a real scenario, you would submit new data to the model to get a score. However, this tutorial uses the same training data GoSales-updated.csv that you downloaded as part of the tutorial setup. Ensure that you delete the IS_TENT column and save the file before you upload it to the batch job. When deploying a model, you can add the payload data to a project, upload it to a space, or link to it in a storage repository such as a Cloud Object Storage bucket. For this tutorial, upload the file directly to the deployment space."
1197	AutoAI tutorial: Build a Binary Classification Model	"Task 6: Creating a batch job to score the model
Step 1: Add data to space
From the Assets page of the deployment space:  
Click Add to space then choose Data. Upload the file GoSales-updated.csv file that you saved locally."
1198	AutoAI tutorial: Build a Binary Classification Model	"Task 6: Creating a batch job to score the model
Step 2: Create the batch deployment
Now you can define the batch deployment.  
Click the deployment icon next to the model’s name. Enter a name a name for the deployment.  
Select Batch as the Deployment type. Choose the smallest hardware specification. Click Create."
1199	AutoAI tutorial: Build a Binary Classification Model	"Task 6: Creating a batch job to score the model
Step 3: Create the batch job
The batch job runs the deployment. To create the job, you must specify the input data and the name for the output file. You can set up a job to run on a schedule or run immediately.  
Click New job. Specify a name for the job Configure to the smallest hardware specification (Optional): To set a schedule and receive notifications. Upload the input file: GoSales-updated.csv Name the output file: GoSales-output.csv Review and click Create to run the job."
1200	AutoAI tutorial: Build a Binary Classification Model	"Task 6: Creating a batch job to score the model
Step 4: View the output
When the deployment status changes to Deployed, return to the Assets page for the deployment space. The file GoSales-output.csv was created and added to your assets list.  
Click the download icon next to the output file and open the file in an editor. You can review the prediction results for the customer information that is submitted for batch processing.  
For each case, the prediction that is returned indicates the confidence score of whether a customer will buy a tent."
1201	AutoAI tutorial: Build a Binary Classification Model	"Next steps
Building an AutoAI experiment  
Parent topic: AutoAI overview"
1202	Noun phrase extraction	"The Watson Natural Language Processing Noun phrase extraction block extracts noun phrases from input text.  
Block name  
noun-phrases_rbr_<language>_stock  
Note: The ""rbr"" abbreviation in model name means rule-based reasoning. RBR models handle syntactically regular entity types such as number, email and phone.  
Supported languages  
Noun phrase extraction is available for the following languages. For a list of the language codes and the corresponding language, see Language codes.  
ar, cs, da, de, es, en, fi, fr, he, hi, it, ja, ko, nb, nl, nn, pt, ro, ru, sk, sv, tr, zh_cn, zh_tw  
Capabilities  
The Noun phrase extraction block extracts non-overlapping noun phrases from the input text.  
Dependencies on other blocks  
None  
Code sample  
Output of the code sample:  
Parent topic: Watson Natural Language Processing task catalog"
1203	Supported machine learning tools, libraries, frameworks, and software specifications	"In IBM Watson Machine Learning, you can use popular tools, libraries, and frameworks to train and deploy machine learning models and functions. The environment for these models and functions is made up of specific hardware and software specifications.  
Software specifications define the language and version that you use for a model or function. You can use software specifications to configure the software that is used for running your models and functions. By using software specifications, you can precisely define the software version to be used and include your own extensions (for example, by using conda .yml files or custom libraries).  
You can get a list of available software and hardware specifications and then use their names and IDs for use with your deployment. For more information, see Python client or REST API."
1204	Supported machine learning tools, libraries, frameworks, and software specifications	"Predefined software specifications
You can use popular tools, libraries, and frameworks to train and deploy machine learning models and functions.  
This table lists the predefined (base) model types and software specifications.  
When you have assets that rely on discontinued software specifications or frameworks, in some cases the migration is seamless. In other cases, your action is required to retrain or redeploy assets.  
Existing deployments of models that are built with discontinued framework versions or software specifications are removed on the date of discontinuation. No new deployments of models that are built with discontinued framework versions or software specifications are allowed."
1205	Supported machine learning tools, libraries, frameworks, and software specifications	"Learn more
To learn more about how to customize software specifications, see Customizing with third-party and private Python libraries.  
To learn more about how to use and customize environments, see Environments.  
To learn more about how to use software specifications for deployments, see the following Jupyter notebooks:  
Using REST API and cURL Using the Python client  
Parent topic: Frameworks and software specifications"
1206	Tips for writing foundation model prompts: prompt engineering	"Part art, part science, prompt engineering is the process of crafting prompt text to best effect for a specific foundation model with specific prompt parameter settings. When it comes to prompting foundation models, there isn't just one right answer. There are usually multiple ways to prompt a foundation model for a successful result.  
Use the Prompt Lab to experiment with crafting prompts.  
For help using the prompt editor, see Prompt Lab. Try the samples that are available from the Sample prompts tab of the Prompt Lab. Learn from documented samples. See Sample prompts.  
As you experiment, remember these tips. They will help you successfully prompt most text-generating foundation models."
1207	Tips for writing foundation model prompts: prompt engineering	"Tip 1: Remember that everything is text completion
Your prompt is the text you submit for processing by a foundation model.  
For most models, simply asking a question or typing an instruction won't yield the best results. That's because the model isn't answering your prompt, the model is appending text to it.  
This image demonstrates prompt text and generated output:  
Prompt text: ""I took my dog "" Generated output: ""to the park."""
1208	Tips for writing foundation model prompts: prompt engineering	"Tip 2: Include all the needed prompt components
Effective prompts usually have one or more of the following components: instruction, context, examples, and cue."
1209	Tips for writing foundation model prompts: prompt engineering	"Tip 2: Include all the needed prompt components
Instruction
An instruction is an imperative statement that tells the model what to do. For example, if you want the model to list ideas for a dog-walking business, your instruction could be: ""List ideas for starting a dog-walking business:"""
1210	Tips for writing foundation model prompts: prompt engineering	"Tip 2: Include all the needed prompt components
Context
Including background or contextual information in your prompt can nudge the model output in the right direction. Specifically, words that appear in your prompt text are more likely to be included in the generated output."
1211	Tips for writing foundation model prompts: prompt engineering	"Tip 2: Include all the needed prompt components
Examples
To indicate the format or shape that you want the model response to be, include one or more pairs of example input and corresponding output that illustrate the pattern you want the generated text to follow.  
Including one example in your prompt is called one-shot prompting, including two or more examples in your prompt is called few-shot prompting, and when your prompt has no examples, that's called zero-shot prompting.  
When you prompt models that have been tuned, you might not need to include examples."
1212	Tips for writing foundation model prompts: prompt engineering	"Tip 2: Include all the needed prompt components
Cue
A cue is text at the end of the prompt that is likely to start the generated output on the right path. Remember, as much as it seems like the model is responding to your prompt, the model is really appending text to your prompt or continuing your prompt."
1213	Tips for writing foundation model prompts: prompt engineering	"Tip 3: Include descriptive details
The more guidance, the better. Experiment with including descriptive phrases related to aspects of your ideal result: content, style, and length. Including these details in your prompt can cause a more creative or more complete result to be generated.  
For example, you can improve the sample instruction by adding descriptive information as follows:  
Original: ""List ideas for starting a dog-walking business"" Improved: ""List ideas for starting a large, wildly successful dog-walking business"""
1214	Tips for writing foundation model prompts: prompt engineering	"Example
Before
A prompt with a simple instruction:  
List ideas for starting a dog-walking business.  
The resulting foundation model output isn't great:  
Take care of the dog and walk it in your neighborhood. You can create flyers and put them around the neighborhood, you can also put up ads on websites where dog lovers hang out."
1215	Tips for writing foundation model prompts: prompt engineering	"Example
After
A prompt with all of the recommended prompt components:  
Instruction:  
Context:  
Example:  
Cue:  
The resulting foundation model output is much more useful:  
Start by doing some basic market research. Interview dog owners in your area and ask them what they would want from their dog walker. Offer a free sample or trial period - like several weeks - so you can see if they are really interested. Give personalized service. Offer to train the owner's dog to get in a proper sit or down for the dog walker. Train the human too! Be friendly and outgoing. Social media is huge and people love to post photos of themselves with their pets. Use technology to help keep costs low, such as using web-enabled interactions. For example, you could offer a coupon on a popular pet food store website.  
You can experiment with this prompt yourself in the Prompt Lab. Depending on the foundation model you choose and the prompt parameter settings you use, the output might differ."
1216	Tips for writing foundation model prompts: prompt engineering	"Learn more
Sample prompts Avoiding hallucinations Generating accurate output  
Parent topic: Prompt Lab"
1217	Foundation models augmented with task-specific knowledge and skills	"Explore IBM foundation models that are designed to support alignment tuning.  
IBM watsonx.ai supports foundation models that use a novel alignment tuning method from IBM Research. Large-scale Alignment for chatBots, or LAB is a method for adding new skills to existing foundation models by generating synthetic data for the skills, and then using that data to tune the foundation model.  
Foundation models that are trained with skills built by using the LAB tuning method have a new model taxonomy page. From the model card for the foundation model in Prompt Lab, click Training taxonomy to explore the model's skills.  
The following IBM foundation models support alignment tuning:  
granite-7b-lab granite-13b-chat-v2 granite-20b-multilingual merlinite-7b  
For more information about these foundation models, see Supported foundation models."
1218	Foundation models augmented with task-specific knowledge and skills	"Learn more
IBM Research: A faster, systematic way to train large language models for enterprise IBM Research: What is AI alignment? Research paper on LAB  
Parent topic: Foundation models"
1219	Promoting an environment template to a space	"If you created an environment template and associated it with an asset that you promoted to a deployment space, you can also promote the environment template to the same space. Promoting the environment template to the same space enables running the asset in the same environment that was used in the project.  
You can only promote environment templates that you created.  
To promote an environment template associated with an asset that you promoted to a deployment space:  
From the Manage tab of your project on the Environments page under Templates, select the custom environment template and click Actions > Promote. Select the space that you promoted your asset to as the target deployment space and optionally provide a description and tags.  
Parent topic: Environments"
1220	Concepts extraction block	"The Watson Natural Language Processing Concepts block extracts general DBPedia concepts (concepts drawn from language-specific Wikipedia versions) that are directly referenced or alluded to, but not directly referenced, in the input text.  
Block name  
concepts_alchemy_<language>_stock  
Supported languages  
The Concepts block is available for the following languages. For a list of the language codes and the corresponding language, see Language codes.  
de, en, es, fr, it, ja, ko, pt  
Capabilities  
Use this block to assign concepts from DBPedia (2016 edition). The output types are based on DBPedia.  
Dependencies on other blocks  
The following block must run before you can run the Concepts extraction block:  
syntax_izumo_<language>_stock  
Code sample  
Output of the code sample:  
Parent topic: Watson Natural Language Processing block catalog"
1221	Creating a Federated Learning experiment	"Learn how to create a Federated Learning experiment to train a machine learning model.  
Watch this short overview video of how to create a Federated Learning experiment.  
Video disclaimer: Some minor steps and graphical elements in this video might differ from your platform.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Follow these steps to create a Federated Learning experiment:  
Set up your system Creating the initial model Create the data handler Starting the aggregator (Admin) Connecting to the aggregator (Party) Monitoring and saving the model  
Parent topic: IBM Federated Learning"
1222	Federated Learning Tensorflow samples	"Download and review sample files that show how to run a Federated Learning experiment by using API calls with a Tensorflow Keras model framework.  
To see a step-by-step UI driven approach rather than sample files, see the Federated Learning Tensorflow tutorial for UI."
1223	Federated Learning Tensorflow samples	"Download the Federated Learning sample files
The Federated Learning sample has two parts, both in Jupyter Notebook format that can run in the latest Python environment.  
For single-user demonstrative purposes, the Notebooks are placed in a project. Access the Federated Learning project, and click Create project to get all the sample files at once.  
You can also get the Notebook separately. Since, for practical purposes of Federated Learning, one user would run the admin Notebook and multiple users would run the party Notebook. For more details on the admin and party, see Terminology.  
Federated Learning Tensorflow Demo Part 1 - for Admin  
Federated Learning Tensorflow Demo Part 2 - for Party  
Parent topic: Federated Learning tutorial and samples"
1224	Watson Studio environments compute usage	Compute usage is calculated by the number of capacity unit hours (CUH) consumed by an active environment runtime in Watson Studio. Watson Studio plans govern how you are billed monthly for the resources you consume.
1225	Watson Studio environments compute usage	"Capacity units per hour for notebooks
The rate of capacity units per hour consumed is determined for:  
Default Python or R environments by the hardware size and the number of users in a project using one or more runtimes  
For example: The IBM Runtime 23.1 on Python 3.10 XS with 2 vCPUs will consume 1 CUH if it runs for one hour. If you have a project with 7 users working on notebooks 8 hours a day, 5 days a week, all using the IBM Runtime 23.1 on Python 3.10 XS environment, and everyone shuts down their runtimes when they leave in the evening, runtime consumption is 5 x 7 x 8 = 280 CUH per week.  
The CUH calculation becomes more complex when different environments are used to run notebooks in the same project and if users have multiple active runtimes, all consuming their own CUHs. Additionally, there might be notebooks, which are scheduled to run during off-hours, and long-running jobs, likewise consuming CUHs.  
Default Spark environments by the hardware configuration size of the driver, and the number of executors and their size."
1226	Watson Studio environments compute usage	"Capacity units per hour for notebooks with Decision Optimization
The rate of capacity units per hour consumed is determined by the hardware size and the price for Decision Optimization."
1227	Watson Studio environments compute usage	"Capacity units per hour for notebooks with Watson Natural Language Processing
The rate of capacity units per hour consumed is determined by the hardware size and the price for Watson Natural Language Processing."
1228	Watson Studio environments compute usage	"Runtime capacity limit
You are notified when you're about to reach the monthly runtime capacity limit for your Watson Studio service plan. When this happens, you can:  
Stop active runtimes you don't need. Upgrade your service plan. For up-to-date information, see theServices catalog page for Watson Studio.  
Remember: The CUH counter continues to increase while a runtime is active so stop the runtimes you aren't using. If you don't explicitly stop a runtime, the runtime is stopped after an idle timeout. During the idle time, you will continue to consume CUHs for which you are billed."
1229	Watson Studio environments compute usage	"Track runtime usage for a project
You can view the environment runtimes that are currently active in a project, and monitor usage for the project from the project's Environments page."
1230	Watson Studio environments compute usage	"Track runtime usage for an account
The CUH consumed by the active runtimes in a project are billed to the account that the project creator has selected in his or her profile settings at the time the project is created. This account can be the account of the project creator, or another account that the project creator has access to. If other users are added to the project and use runtimes, their usage is also billed against the account that the project creator chose at the time of project creation.  
You can track the runtime usage for an account on the Environment Runtimes page if you are the IBM Cloud account owner or administrator.  
To view the total runtime usage across all of the projects and see how much of your plan you have currently used, choose Administration > Environment runtimes.  
A list of the active runtimes billed to your account is displayed. You can see who created the runtimes, when, and for which projects, as well as the capacity units that were consumed by the active runtimes at the time you view the list."
1231	Watson Studio environments compute usage	"Learn more
Idle runtime timeouts Monitor account resource usage Upgrade your service  
Parent topic: Managing compute resources"
1232	Building an AutoAI model	"AutoAI automatically prepares data, applies algorithms, and builds model pipelines that are best suited for your data and use case. Learn how to generate the model pipelines that you can save as machine learning models.  
Follow these steps to upload data and have AutoAI create the best model for your data and use case.  
Collect your input data Open the AutoAI tool Specify details of your model and training data and start AutoAI View the results"
1233	Building an AutoAI model	"Collect your input data
Collect and prepare your training data. For details on allowable data sources, see AutoAI overview.  
Note:  
If you are creating an experiment with a single training data source, you have the option of using a second data source specifically as testing, or holdout, data for validating the pipelines."
1234	Building an AutoAI model	"Open the AutoAI tool
For your convenience, your AutoAI model creation uses the default storage that is associated with your project to store your data and to save model results.  
Open your project.  
Click the Assets tab.  
Click New asset > Build machine learning models automatically.  
Note: After you create an AutoAI asset it displays on the Assets page for your project in the **AutoAI experiments** section, so you can return to it."
1235	Building an AutoAI model	"Specify details of your experiment
Specify a name and description for your experiment.  
Select a machine learning service instance and click Create.  
Choose data from your project or upload it from your file system or from the asset browser, then press Continue. Click the preview icon to review your data. (Optional) Add a second file as holdout data for testing the trained pipelines.  
Choose the Column to predict for the data you want the experiment to predict.  
Based on analyzing a subset of the data set, AutoAI selects a default model type: binary classification, multiclass classification, or regression. Binary is selected if the target column has two possible values. Multiclass has a discrete set of 3 or more values. Regression has a continuous numeric variable in the target column. You can optionally override this selection.  
Note: The limit on values to classify is 200. Creating a classification experiment with many unique values in the prediction column is resource-intensive and affects the experiment's performance and training time. To maintain the quality of the experiment:- AutoAI chooses a default metric for optimizing. For example, the default metric for a binary classification model is *Accuracy*.- By default, 10% of the training data is held out to test the performance of the model.  
(Optional): Click Experiment settings to view or customize options for your AutoAI run. For details on experiment settings, see Configuring a classification or regression experiment.  
Click Run Experiment to begin model pipeline creation.  
An infographic shows you the creation of pipelines for your data. The duration of this phase depends on the size of your data set. A notification message informs you if the processing time will be brief or require more time. You can work in other parts of the product while the pipelines build.  
Hover over nodes in the infographic to explore the factors that pipelines share and their unique properties. You can see the factors that pipelines share and the properties that make a pipeline unique. For a guide to the data in the infographic, click the Legend tab in the information panel. Or, to see a different view of the pipeline creation, click the Experiment details tab of the notification pane, then click Switch views to view the progress map. In either view, click a pipeline node to view the associated pipeline in the leaderboard."
1236	Building an AutoAI model	"View the results
When the pipeline generation process completes, you can view the ranked model candidates and evaluate them before you save a pipeline as a model."
1237	Building an AutoAI model	"View the results
Next steps
Build an experiment from sample data  
Configuring experiment settings  
Configure a text analysis experiment  
Video disclaimer: Some minor steps and graphical elements in this video might differ from your platform.  
Watch this video to see how to build a binary classification model  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Watch this video to see how to build a multiclass classification model  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Parent topic: AutoAI overview"
1238	Evaluating deployments in spaces	"Configure watsonx.governance evaluations in your deployment spaces to gain insights about your model performance. When you configure evaluations, you can analyze evaluation results and model transaction records directly in your spaces.  
watsonx.governance evaluates your model deployments to help you measure performance and understand your model predictions. When you configure model evaluations, watsonx.governance generates metrics for each evaluation that provide different insights that you can review. watsonx.governance also logs the transactions that are processed during evaluations to help you understand how your model predictions are determined. For more information, see Evaluating AI models with Watson OpenScale.  
If you have an instance of watsonx.governance provisioned, you can seamlessly create an online deployment, then monitor the deployment results for fairness, quality, drift, and explainability.  
A typical scenario follows this sequence:"
1239	Evaluating deployments in spaces	"If you have an instance of watsonx.governance provisioned, you can seamlessly create an online deployment, then monitor the deployment results for fairness, quality, drift, and explainability.  
A typical scenario follows this sequence:  
Create a deployment space and associate a watsonx.governance instance with the space to enable all the monitoring capabilities. You can choose the type of space, for example production, or pre-production, depending on your requirements. Promote a trained machine learning model and input (payload) data to the deployment space and create an online deployment for the model. From the deployment Test tab, provide the input data and get predictions back. From the Evaluations tab, configure the evaluation to monitor your deployment for quality, fairness, and explainability. Provide all the required model details so that Watson OpenScale can connect to the model, the training and payload data, and to a repository for storing evaluation results. Configure a monitor for fairness to make sure that your model is producing unbiased results. Select fields to monitor for fairness, then set thresholds to measure predictions for a monitored group compared to a reference group. For example, you can evaluate your model to make sure that it is providing unbiased predictions based on gender. Configure a monitor for quality to determine model performance based on the number of correct outcomes that are produced by the model based on labeled test data that is called Feedback data. Set quality thresholds to track when a metric value is outside an acceptable range. Configure a monitor for drift to make sure that your deployments are up-to-date and consistent. Use feature importance to determine the impact of feature drift on your model. For example, a small amount of drift in an important feature can have a bigger impact on your model than a moderate amount of drift in a less important feature. You can monitor your deployment results for explainability to understand the factors that led the model to determine a prediction. Choose the explanation method that is best suited to your needs. For example, you can choose SHAP (Shapley Additive EXplanations) method for thorough explanations or LIME (Local Interpretable Model-Agnostic Explanations) method for faster explanations. Finally, you can inspect your model evaluations to find areas where small changes to a few inputs would result in a different decision. Test scenarios to determine whether changing inputs can improve model performance."
1240	Evaluating deployments in spaces	"Watch the following video for a demonstration of how to use watsonx.governance to monitor a deployment in a space.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
The following sections describe how to configure watsonx.governance evaluations and review model insights in your deployment spaces:"
1241	Evaluating deployments in spaces	"Preparing to evaluate models in spaces
To configure Watson OpenScale evaluations from your deployment spaces, you must select Associate a service instance in the Associate a service instance dialog box before you can run evaluations. In the Associate instance for evaluation window, you must choose the watsonx.governance instance that you want to use and select Associate a service instance to associate an instance with your project. You must be assigned the Admin role for your project to associate instances.  
If you don't have a database that is associated with your watsonx.governance instance, you must also associate a database before you can run evaluations. To associate a database, you must also click Associate database in the Database required dialog box to connect to a database. You must be assigned the Admin role for your project and watsonx.governance instance to associate databases."
1242	Evaluating deployments in spaces	"Configuring Watson OpenScale evaluations in spaces
After you associate your Watson OpenScale instance, you can select deployments to view the Evaluations and Transactions tabs that you can use to configure evaluations and review model insights. To start configuring model evaluations in your space, you can select Configure OpenScale evaluation settings to open a wizard that provides a guided series of steps.  
You can evaluate online deployments only in your deployment space."
1243	Evaluating deployments in spaces	"Configuring Watson OpenScale evaluations in spaces
Providing model details
To configure model evaluations, you must provide model details to enable watsonx.governance to understand how your model is set up. You must provide details about your training data and your model output.  
For more information, see Providing model details."
1244	Evaluating deployments in spaces	"Configuring Watson OpenScale evaluations in spaces
Configuring explainability
You can configure explainability in watsonx.governance to reveal which features contribute to the outcome predicted by your model for a transaction and predict what changes would result in a different outcome. You can choose to configure local explanations to analyze the impact of factors for specific model transactions and configure global explanations to analyze general factors that impact model outcomes.  
For more information, see Configuring explainability."
1245	Evaluating deployments in spaces	"Configuring Watson OpenScale evaluations in spaces
Configuring fairness evaluations
You can configure fairness evaluations to determine whether your model produces biased outcomes for different groups. To configure fairness evaluations, you can specify the reference group that you expect to represent favorable outcomes and the fairness metrics that you want to use. You can also select features that are compared to the reference group to evaluate them for bias.  
For more information, see Configuring fairness evaluations."
1246	Evaluating deployments in spaces	"Configuring Watson OpenScale evaluations in spaces
Configuring quality evaluations
You can configure quality evaluations to understand how well your model predicts accurate outcomes. To configure quality evaluations, you must specify thresholds for each metric to enable watsonx.governance to identify when model quality declines.  
For more information, see Configuring quality evaluations."
1247	Evaluating deployments in spaces	"Configuring Watson OpenScale evaluations in spaces
Configuring drift v2 evaluations
You can configure drift v2 evaluations to measure changes in your data over time to make sure that you get consistent outcomes for your model. To configure drift v2 evaluations, you must set thresholds that enable watsonx.governance to identify changes in your model output, the accuracy of your predictions, and the distribution of your input data. You must also select important features to enable watsonx.governance to measure the change in value distribution.  
For more information, see Configuring drift v2 evaluations."
1248	Evaluating deployments in spaces	"Configuring Watson OpenScale evaluations in spaces
Configuring drift evaluations
You can configure drift evaluations to enable watsonx.governance to detect drops in accuracy and data consistency in your model. To configure drift evaluations, you must set thresholds to enable watsonx.governance to establish an accuracy and consistency baseline for your model.  
For more information, see Configuring drift evaluations."
1249	Evaluating deployments in spaces	"Running evaluations
After you configure evaluations, you can close the wizard to run the evaluations. To run the evaluations, you must select Evaluate now in the Actions menu on the Evaluations tab to send model transactions."
1250	Evaluating deployments in spaces	"Reviewing evaluation results
You can analyze evaluation results on the Evaluations tab to gain insights about your model performance. To analyze evaluation results, you can click the arrow in an evaluation section or use the Actions menu to view details about your model.  
For more information, see Reviewing evaluation results."
1251	Evaluating deployments in spaces	"Reviewing model transactions
You can analyze model transactions on the Transactions tab to understand how your model predicts outcomes and predict what changes might cause different outcomes. To analyze transactions, you can choose to view explanations that provide details about how your model predictions are determined.  
For more information, see Explaining model transactions.  
Parent topic: Managing predictive deployments"
1252	Creating environment templates	"You can create custom environment templates if you do not want to use the default environments provided by Watson Studio.  
You can create environment templates for the following types of assets:  
Notebooks in the Notebook editor Notebooks in RStudio Modeler flows in the SPSS Modeler Data Refinery flows Jobs that run operational assets, such as Data Refinery flows, or Notebooks in a project  
Note:  
If you want additional packages to be automatically installed in your environment template (software customization), you can configure this after the custom environment template is created. For details, refer to Adding customizations.  
To create an environment template:  
On the Manage tab of your project, select the Environments page and click New template under Templates. Enter a name and a description. Select one of the following engine types: Select the hardware configuration from the Hardware configuration drop-down menu. Select the software version if you selected a runtime of ""Default,"" ""Spark,"" or ""GPU.""  
Default: Select for Python, R, and RStudio runtimes for Watson Studio. Spark: Select for Spark with Python or R runtimes for Watson Studio. GPU: Select for more computing power to improve model training performance for Watson Studio."
1253	Creating environment templates	"Where to find your custom environment template
Your new environment template is listed under Templates on the Environments page in the Manage tab of your project. From this page, you can:  
Check which runtimes are active Update custom environment templates Track the number of capacity units per hour that your runtimes have consumed so far Stop active runtimes."
1254	Creating environment templates	"Limitations
The default environments provided by Watson Studio cannot be edited or modified.  
You can't add a software customization to the default Python and R environment templates included in Watson Studio. You can only add a customization to an environment template that you create.  
If you add a software customization using conda, your environment must have at least 2 GB RAM.  
You can't customize an R environment for a notebook by installing R packages directly from CRAN or GitHub. You can check if the CRAN package you want is available only from conda channels and, if the package is available, add that package name in the customization list as r-<package-name>.  
After you have started a notebook in an Watson Studio environment, you can't create another conda environment from inside that notebook and use it. Watson Studio environments do not behave like a Conda environment manager.  
You can't customize the software configuration of a Spark environment template."
1255	Creating environment templates	"Next steps
Customize environment templates for Python or R"
1256	Creating environment templates	"Learn more
Parent topic: Managing compute resources"
1257	Deploying foundation model assets	Deploy foundation model assets to test the assets, put them into production, and monitor them.
1258	Deploying foundation model assets	"Deploying a prompt template asset
After you save a prompt template as a project asset, you can promote it to a deployment space. From the space, you can test the prompt template and get the endpoint for putting the asset to productive use.  
If you have the watsonx.governance service, you can also capture and track the deployment details for a prompt template to meet governance requirements.  
For details, see Deploying a prompt template."
1259	Deploying foundation model assets	"Learn more
Tracking prompt templates Evaluating a prompt template in a deployment space  
Parent topic: Deploying assets with Watson Machine Learning"
1260	Watson Natural Language Processing library	"The Watson Natural Language Processing library provides natural language processing functions for syntax analysis and pre-trained models for a wide variety of text processing tasks, such as sentiment analysis, keyword extraction, and classification. The Watson Natural Language Processing library is available for Python only.  
With Watson Natural Language Processing, you can turn unstructured data into structured data, making the data easier to understand and transferable, in particular if you are working with a mix of unstructured and structured data. Examples of such data are call center records, customer complaints, social media posts, or problem reports. The unstructured data is often part of a larger data record that includes columns with structured data. Extracting meaning and structure from the unstructured data and combining this information with the data in the columns of structured data:  
Gives you a deeper understanding of the input data Can help you to make better decisions.  
Watson Natural Language Processing provides pre-trained models in over 20 languages. They are curated by a dedicated team of experts, and evaluated for quality on each specific language. These pre-trained models can be used in production environments without you having to worry about license or intellectual property infringements.  
Although you can create your own models, the easiest way to get started with Watson Natural Language Processing is to run the pre-trained models on unstructured text to perform language processing tasks.  
Some examples of language processing tasks available in Watson Natural Language Processing pre-trained models:  
Language detection: detect the language of the input text Syntax: tokenization, lemmatization, part of speech tagging, and dependency parsing Entity extraction: find mentions of entities (like person, organization, or date) Noun phrase extraction: extract noun phrases from the input text Text classification: analyze text and then assign a set of pre-defined tags or categories based on its content Sentiment classification: is the input document positive, negative or neutral? Tone classification: classify the tone in the input document (like excited, frustrated, or sad) Emotion classification: classify the emotion of the input document (like anger or disgust) Keywords extraction: extract noun phrases that are relevant in the input text Concepts: find concepts from DBPedia in the input text Relations: detect relations between two entities Hierarchical categories: assign individual nodes within a hierarchical taxonomy to the input document Embeddings: map individual words or larger text snippets into a vector space"
1261	Watson Natural Language Processing library	"Watson Natural Language Processing encapsulates natural language functionality through blocks and workflows. Blocks and workflows support functions to load, run, train, and save a model.  
For more information, refer to Working with pre-trained models.  
Some examples of how you can use the Watson Natural Language Processing library:  
Running syntax analysis on a text snippet:  
Extracting entities from a text snippet:  
For examples of how to use the Watson Natural Language Processing library, refer to Watson Natural Language Processing library usage samples."
1262	Watson Natural Language Processing library	"Using Watson Natural Language Processing in a notebook
You can run your Python notebooks that use the Watson Natural Language Processing library in any of the environments that listed here. The GPU environment templates include the Watson Natural Language Processing library.  
DO + NLP: Indicates that the environment template includes both the CPLEX and the DOcplex libraries to model and solve decision optimization problems and the Watson Natural Language Processing library.  
~ : Indicates that the environment template requires the Watson Studio Professional plan. See Offering plans.  
Normally these environments are sufficient to run notebooks that use prebuilt models. If you need a larger environment, for example to train your own models, you can create a custom template that includes the Watson Natural Language Processing library. Refer to Creating your own environment template.  
Create a custom template without GPU by selecting the engine type Default, the hardware configuration size that you need, and choosing NLP + DO Runtime 23.1 on Python 3.10 as the software version. Create a custom template with GPU by selecting the engine type GPU, the hardware configuration size that you need, and choosing GPU Runtime 23.1 on Python 3.10 as the software version."
1263	Watson Natural Language Processing library	"Learn more
Creating your own environment template  
Parent topic: Notebooks and scripts"
1264	Batch deployment input details for SPSS models	"Follow these rules when you are specifying input details for batch deployments of SPSS models.  
Data type summary table:"
1265	Batch deployment input details for SPSS models	"Data sources
Input or output data references:  
Local or managed assets from the space Connected (remote) assets from these sources:  
Cloud Object Storage Db2 Warehouse Db2 Google Big-Query (googlebq) MySQL Microsoft SQL Server Teradata (teradata) PostgreSQL Oracle Snowflake Informix Netezza Performance Server  
Notes:  
For connections of type Cloud Object Storage or Cloud Object Storage (infrastructure), you must configure Access key and Secret key, also known as HMAC credentials. For SPSS deployments, these data sources are not compliant with Federal Information Processing Standard (FIPS):  
Cloud Object Storage Cloud Object Storage (infrastructure) Storage volumes  
If you are specifying input/output data references programmatically:  
Data source reference type depends on the asset type. Refer to the Data source reference types section in Adding data assets to a deployment space. SPSS jobs support multiple data source inputs and a single output. If the schema is not provided in the model metadata at the time of saving the model, you must enter id manually and select a data asset for each connection. If the schema is provided in model metadata, id names are populated automatically by using metadata. You select the data asset for the corresponding ids in Watson Studio. For more information, see Using multiple data sources for an SPSS job. To create a local or managed asset as an output data reference, the name field must be specified for output_data_reference so that a data asset is created with the specified name. Specifying an href that refers to an existing local data asset is not supported.  
Note:  
Connected data assets that refer to supported databases can be created in the output_data_references only when the input_data_references also refers to one of these sources."
1266	Batch deployment input details for SPSS models	"Data sources
Note:  
Connected data assets that refer to supported databases can be created in the output_data_references only when the input_data_references also refers to one of these sources.  
job_payload_ref = { client.deployments.ScoringMetaNames.INPUT_DATA_REFERENCES: [{ ""id"": ""DB2Connection"", ""name"": ""drug_ref_input1"", ""type"": ""data_asset"", ""connection"": {}, ""location"": { ""href"": <input_asset_href1> } },{ ""id"": ""Db2 WarehouseConn"", ""name"": ""drug_ref_input2"", ""type"": ""data_asset"", ""connection"": {}, ""location"": { ""href"": <input_asset_href2> } }], client.deployments.ScoringMetaNames.OUTPUT_DATA_REFERENCE: { ""type"": ""data_asset"", ""connection"": {}, ""location"": { ""href"": <output_asset_href> } } }  
Table names that are provided in input and output data references are ignored. Table names that are referred in the SPSS model stream are used during the batch deployment.  
Use SQL PushBack to generate SQL statements for IBM SPSS Modeler operations that can be “pushed back” to or run in the database to improve performance. SQL Pushback is only supported by:  
Db2 SQL Server Netezza Performance Server  
If you are creating a job by using the Python client, you must provide the connection name that is referred in the data nodes of the SPSS model stream in the id field, and the data asset href in location.href for input/output data references of the deployment jobs payload. For example, you can construct the job payload like this:  
An SPSS Modeler flow can have a number of input and output data nodes. When you connect to a supported database as an input and output data source, the connection details are selected from the input and output data reference, but the input and output table names are selected from the SPSS model stream file."
1267	Batch deployment input details for SPSS models	"Data sources
For batch deployment of an SPSS model that uses a database connection, make sure that the modeler stream Input and Output nodes are Data Asset nodes. In SPSS Modeler, the Data Asset nodes must be configured with the table names that are used later for job predictions. Set the nodes and table names before you save the model to Watson Machine Learning. When you are configuring the Data Asset nodes, choose the table name from the Connections; choosing a Data Asset that is created in your project is not supported.  
When you are creating the deployment job for an SPSS model, make sure that the types of data sources are the same for input and output. The configured table names from the model stream are passed to the batch deployment and the input/output table names that are provided in the connected data are ignored.  
For batch deployment of an SPSS model that uses a Cloud Object Storage connection, make sure that the SPSS model stream has single input and output data asset nodes.  
You must specify compatible sources for the SPSS Modeler flow input, the batch job input, and the output. If you specify an incompatible combination of types of data sources, you get an error when you try to run the batch job.  
These combinations are supported for batch jobs:  
If you are specifying multiple inputs for an SPSS model stream deployment with no schema, specify an ID for each element in input_data_references.  
For more information, see Using multiple data sources for an SPSS job.  
In this example, when you create the job, provide three input entries with IDs: sample_db2_conn, sample_teradata_conn, and sample_googlequery_conn and select the required connected data for each input.  
Note: The environment variables parameter of deployment jobs is not applicable.  
Parent topic: Batch deployment input details by framework"
1268	Prompting the granite-13b-chat-v2 foundation model from IBM	"Experiment with inferencing the IBM granite-13b-chat-v2 foundation model to see how this versatile foundation model can help you accomplish various tasks.  
The granite-13b-chat-v2 foundation model is a 13 billion parameter large language model that is designed to help business users get work done. The IBM Granite series of foundation models are trained on enterprise-specialized datasets, which means the models are familiar with the specialized language and jargon from various industries and can more readily generate content that is grounded in relevant industry knowledge.  
The granite-13b-chat-v2 foundation model is optimized for the following use cases:  
Chat Returning factual answers Natural language processing tasks  
Because the model can be used for different use cases, to get the best results you need to adjust the model parameters and system prompt."
1269	Prompting the granite-13b-chat-v2 foundation model from IBM	"Conversing with Granite Chat
To get the best results when chatting with the granite-13b-chat-v2 foundation model, first follow these recommendations and then experiment to get the results that you want.  
You are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. You always respond to greetings (for example, hi, hello, g'day, morning, afternoon, evening, night, what's up, nice to meet you, sup) with ""Hello! I am Granite Chat, created by IBM. How can I help you today?"". Please do not say anything else and do not start a conversation.  
Work with the granite-13b-chat-v2 foundation model from the Prompt Lab in chat mode. Chat mode has default prompt parameter values that are optimized for conversational exchanges, including a higher Max tokens value.  
From the Model parameters panel of the Prompt Lab, set the switcher to Sampling to use the sampling decoding method.  
For more information about sampling decoding and other parameters, see Model parameters for prompting.  
Add instructions for chatting to the system prompt.  
For example, the following instruction text was used to train the model, and therefore is familiar to the model.  
You are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. You always respond to greetings (for example, hi, hello, g'day, morning, afternoon, evening, night, what's up, nice to meet you, sup) with ""Hello! I am Granite Chat, created by IBM. How can I help you today?"". Please do not say anything else and do not start a conversation.  
Click the Copy to clipboard icon from the following code snippet to copy the recommended system prompt text.  
If you want the foundation model to generate a response in table format, explicitly ask in the prompt for the model to return a table that is generated in markdown.  
You can ask follow-up questions that continue the conversation."
1270	Prompting the granite-13b-chat-v2 foundation model from IBM	"Conversing with Granite Chat
If you want the foundation model to generate a response in table format, explicitly ask in the prompt for the model to return a table that is generated in markdown.  
You can ask follow-up questions that continue the conversation.  
With each follow-up question, the conversation history is included as part of the model prompt. The granite-13b-chat-v2 foundation model can typically sustain a conversation for up to 5 turns or until the input reaches 4,000 tokens in length.  
Although chat mode has a high initial Max tokens parameter value, you might need to increase the parameter value for longer conversations.  
In follow-up questions, avoid pronouns. For example, ask “What does the variable represent?” instead of “What does it represent?”  
For more sample prompts that illustrate the chat use case, see Converse in a dialog.  
For more information about using chat mode in Prompt Lab, see Prompt Lab."
1271	Prompting the granite-13b-chat-v2 foundation model from IBM	"Returning factual answers
To guide the granite-13b-chat-v2 foundation model to return factual answers, use the retrieval-augmented generation pattern. Retrieval-augmented generation grounds the input that you submit to the model with factual information about the topic to be discussed. For more information, see Retrieval-augmented generation (RAG).  
When you want to return factual answers from the granite-13b-chat-v2 foundation model, follow these recommendations."
1272	Prompting the granite-13b-chat-v2 foundation model from IBM	"Returning factual answers
<|system|> You are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. <|user|> You are an AI language model designed to function as a specialized Retrieval Augmented Generation (RAG) assistant. When generating responses, prioritize correctness, meaning ensure that your response is correct given the context and user query, and that it is grounded in the context. Furthermore, make sure that the response is supported by the given document or context. When the question cannot be answered using the context or document, output the following response: '' Always make sure that your response is relevant to the question. If an explanation is needed, first provide the explanation or reasoning, and then give the final answer. Answer length: {{ANSWER_LENGTH}} [Document] {{DOCUMENT}} [End] {{QUERY}} <|assistant|> Table 1: RAG template placeholder variables Placeholder variable Description Examples {{RESPONSE_MESSAGE}} Sentence (or a short variable that can be replaced with a sentence by a post-processing step) that indicates that there is no valid answer to the question. “I don't know.” {{ANSWER_LENGTH}} Optional. Defines the expected response length for the answer. Options include (from shortest to longest answers): single word, concise, narrative {{DOCUMENT}} Text excerpt with the factual information that you want the model to be able to discuss knowledgeably. Text from a marketing brochure, product documentation, company website, or other trusted resource. {{QUERY}} Question to be answered factually. A question about the topic that is discussed in the document. <|system|> You are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. <|user|> You are an AI language model designed to function as a specialized Retrieval Augmented Generation (RAG) assistant. When generating responses, prioritize correctness, meaning ensure that your response is correct given the context and user query, and that it is grounded in the context. Furthermore, make sure that the response is supported by the given document or context. When the question cannot be answered using the context or document, output the following response: 'I don't know.' Always make sure that your response is relevant to the question. If an explanation"
1273	Prompting the granite-13b-chat-v2 foundation model from IBM	"Returning factual answers
that the response is supported by the given document or context. When the question cannot be answered using the context or document, output the following response: 'I don't know.' Always make sure that your response is relevant to the question. If an explanation is needed, first provide the explanation or reasoning, and then give the final answer. Answer length: concise [Document] How prompt-tuning works Foundation models are sensitive to the input that you give them. Your input, or how you prompt the model, can introduce context that the model will use to tailor its generated output. Prompt engineering to find the right prompt often works well. However, it can be time-consuming, error-prone, and its effectiveness can be restricted by the context window length that is allowed by the underlying model. Prompt-tuning a model in the Tuning Studio applies machine learning to the task of prompt engineering. Instead of adding words to the input itself, prompt-tuning is a method for finding a sequence of values that, when added as a prefix to the input text, improve the model's ability to generate the output you want. This sequence of values is called a prompt vector. Normally, words in the prompt are vectorized by the model. Vectorization is the process of converting text to tokens, and then to numbers defined by the model's tokenizer to identify the tokens. Lastly, the token IDs are encoded, meaning they are converted into a vector representation, which is the input format that is expected by the embedding layer of the model. Prompt-tuning bypasses the model's text-vectorization process and instead crafts a prompt vector directly. This changeable prompt vector is concatenated to the vectorized input text and the two are passed as one input to the embedding layer of the model. Values from this crafted prompt vector affect the word embedding weights that are set by the model and influence the words that the model chooses to add to the output. To find the best values for the prompt vector, you run a tuning experiment. You demonstrate the type of output that you want for a corresponding input by providing the model with input and output example pairs in training data. With each training run of the experiment, the generated output is compared to the training data output. Based on what it learns from differences between the two, the experiment adjusts the values in the prompt vector. After many runs through the training data, the model finds the prompt vector that works best. [End] How does prompt tuning"
1274	Prompting the granite-13b-chat-v2 foundation model from IBM	"Returning factual answers
training data output. Based on what it learns from differences between the two, the experiment adjusts the values in the prompt vector. After many runs through the training data, the model finds the prompt vector that works best. [End] How does prompt tuning work? <|assistant|>"
1275	Prompting the granite-13b-chat-v2 foundation model from IBM	"Returning factual answers
Find a reliable resource with factual information about the topic that you want the model to discuss and that you have permission to use. Copy an excerpt of the document to a text editor or other tool where you can access it later.  
For example, the resource might be product information from your own company website or product documentation.  
Work with the granite-13b-chat-v2 foundation model from the Prompt Lab in freeform mode so that you can structure your prompts.  
From the Model parameters panel, change the Max tokens parameter to 500 and set the Repetition penalty parameter to 1.05.  
For more information, see Model parameters for prompting.  
In your prompt, clearly define the system prompt, user input, and where the model's output should go.  
For example, the following prompt structure and text was used to train the model, and therefore is familiar to the model.  
Note: The start and end of the document content is denoted by the special tags [Document] and [End]. Use a similar syntax if you want to add special tags that identify content types or subsection headers in your prompts. When the granite-13b-chat-v2 foundation model was created, it was trained to handle the following special tags:  
<|system|> <|user|> <|assistant|>  
Do not use the same <|tagname|> syntax for your custom tags or you might confuse the model.  
If you do copy this prompt template, after you paste it into the Prompt Lab editor, replace the placeholder variables.  
Tip: Alternatively, you can define and use a prompt variable for the document so that the prompt can be reused and the content can be replaced dynamically each time. For more information, see Building reusable prompts.  
For example, the following prompt uses the granite-13b-chat-v2 foundation model to answer questions about prompt tuning.  
Note: The document content is taken from the Methods for tuning foundation models topic.  
The following screenshot shows the output that is generated by the granite-13b-chat-v2 foundation model when this prompt is submitted by using the Prompt Lab. The output is an accurate summary of how prompt tuning works that is based on information from the document content that was provided with the input:"
1276	Prompting the granite-13b-chat-v2 foundation model from IBM	"Returning factual answers
“Prompt tuning is a method for improving a model's ability to generate output by finding a sequence of values, called a prompt vector, that, when added as a prefix to the input text, influences the model's word embedding weights. This is done by running a tuning experiment, where input and output example pairs are provided to the model, and the model's generated output is compared to the training data output. The experiment then adjusts the values in the prompt vector based on the differences between the two. Over many runs through the training data, the model finds the prompt vector that works best.”"
1277	Prompting the granite-13b-chat-v2 foundation model from IBM	"NLP tasks
You can use the granite-13b-chat-v2 foundation model for natural language processing tasks such as classification and extraction. When the granite-13b-chat-v2 model was created, it was trained to perform these types of natural language processing tasks.  
During the model's training, the prompts that were used to train the model followed a specific format per task type. When you use the model to perform one of these tasks, mimic the established format in the prompts that you submit."
1278	Prompting the granite-13b-chat-v2 foundation model from IBM	"NLP tasks
Classification
To use the granite-13b-chat-v2 foundation model to classify information, follow these recommendations."
1279	Prompting the granite-13b-chat-v2 foundation model from IBM	"NLP tasks
Classification
<|system|> You are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. <|user|> {{INSTRUCTION}} Your response should only include the answer. Do not provide any further explanation. Here are some examples, complete the last one: {{INPUT_LABEL}}: {{ICL_INPUT_1}} {{OUTPUT_LABEL}}: {{ICL_OUTPUT_1}} {{INPUT_LABEL}}: {{ICL_INPUT_2}} {{OUTPUT_LABEL}}: {{ICL_OUTPUT_2}} {{INPUT_LABEL}}: {{TEST_INPUT}} {{OUTPUT_LABEL}}: <|assistant|> Table 2: Classification template placeholder variables Placeholder variable Description Examples {{INSTRUCTION}} Description of the task. Include a list of the classes that you want the model to assign to the input. For each product review, indicate whether the review is Positive or Negative. {{INPUT_LABEL}} Short label for the text to be classified. Input, Customer review, Feedback, Comment {{OUTPUT_LABEL}} Short label that represents the classification value. Class {{ICL_INPUT_N}} Optional. Examples of input text to be classified. Add examples when you want to use a few-shot prompt to support in-context learning. The service representative did not listen to a word I said. It was a waste of my time. {{ICL_OUTPUT_N}} Example outputs with class labels assigned to the corresponding input text examples. Positive, Negative <|system|> You are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. <|user|> For each feedback, specify whether the content is Positive or Negative. Your response should only include the answer. Do not provide any further explanation. Here are some examples, complete the last one: Feedback: Carol, the service rep was so helpful. She answered all of my questions and explained things beautifully. Class: Positive Feedback: The service representative did not listen to a word I said. It was a waste of my time. Class: Negative Feedback: Carlo was so helpful and pleasant. He was able to solve a problem that I've been having with my software for weeks now. Class: <|assistant|>"
1280	Prompting the granite-13b-chat-v2 foundation model from IBM	"NLP tasks
Classification
Identify the classes or classification labels that you want the model to assign to the input. Be sure to list these class labels in the instruction segment of your prompt.  
For example, if you want to classify customer product reviews as positive or negative, you might define two class labels: Postitive and Negative.  
Collect 2 or 3 representative examples of the type of input text that you want the model to classify.  
Work with the granite-13b-chat-v2 foundation model from the Prompt Lab in freeform mode so that you can structure your prompts.  
From the Model parameters panel, change the Max tokens parameter to a value that covers the number of tokens in your longest class label. For example, 5 or 10.  
Add each supported class label as a stop sequence.  
Adding the classes as stop sequences forces the model to stop generating text after a class is assigned to the input.  
For more information about model parameter options, see Model parameters for prompting.  
In your prompt, clearly identify the system prompt, user input, and where the model's output should go.  
For example, the following prompt structure was used when the granite-13b-chat-v2 foundation model was trained to classify text:  
You can use a similar structure to leverage the model's training. Simply replace the placeholder variables in the prompt template.  
For example, the following prompt classifies feedback that customers share about support center personnel.  
The following screenshot shows the output that is generated by the granite-13b-chat-v2 foundation model when this prompt is submitted by using the Prompt Lab. The output is Positive."
1281	Prompting the granite-13b-chat-v2 foundation model from IBM	"NLP tasks
Extraction
To use the granite-13b-chat-v2 foundation model to extract information, follow these recommendations."
1282	Prompting the granite-13b-chat-v2 foundation model from IBM	"NLP tasks
Extraction
<|system|> You are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. <|user|> {{INSTRUCTION}} Your response should only include the answer. Do not provide any further explanation. Here are some examples, complete the last one: {{INPUT_LABEL}}: {{ICL_INPUT_1}} {{OUTPUT_LABEL}}: {{ICL_OUTPUT_1}} {{INPUT_LABEL}}: {{ICL_INPUT_2}} {{OUTPUT_LABEL}}: {{ICL_OUTPUT_2}} {{INPUT_LABEL}}: {{TEST_INPUT}} {{OUTPUT_LABEL}}: <|assistant|> Table 3: Extraction template placeholder variables Placeholder variable Description {{INSTRUCTION}} Description of the task. Include a list of the information types that you want the model to extract from the input. {{INPUT_LABEL}} Short label for the text to be classified. {{OUTPUT_LABEL}} Short label that represents the extracted value. {{ICL_INPUT_N}} Optional. Examples of input text with information types to be extracted. Add examples when you want to use a few-shot prompt to support in-context learning. {{ICL_OUTPUT_N}} Example outputs with information types extracted from the corresponding inputs. <|system|> You are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. <|user|> Extract the Line Of Credit Facility Maximum Borrowing Capacity from the 10K sentences. Your response should only include the answer. Do not provide any further explanation. Here are some examples, complete the last one: 10K Sentence: The credit agreement also provides that up to $500 million in commitments may be used for letters of credit. Line Of Credit Facility Maximum Borrowing Capacity: $500M 10K Sentence: In March 2020, we upsized the Credit Agreement by $100 million, which matures July 2023, to $2.525 billion. Line Of Credit Facility Maximum Borrowing Capacity: $2.525B 10K Sentence: We prepared our impairment test as of October 1, 2022 and determined that the fair values of each of our reporting units exceeded net book value by more than 50%. Among our reporting units, the narrowest difference between the calculated fair value and net"
1283	Prompting the granite-13b-chat-v2 foundation model from IBM	"NLP tasks
Extraction
our impairment test as of October 1, 2022 and determined that the fair values of each of our reporting units exceeded net book value by more than 50%. Among our reporting units, the narrowest difference between the calculated fair value and net book value was in our Principal Markets segment's Canada reporting unit, whose calculated fair value exceeded its net book value by 53%. Future developments related to macroeconomic factors, including increases to the discount rate used, or changes to other inputs and assumptions, including revenue growth, could reduce the fair value of this and/or other reporting units and lead to impairment. There were no goodwill impairment losses recorded for the nine months ended December 31, 2022. Cumulatively, the Company has recorded $469 million in goodwill impairment charges within its former EMEA ($293 million) and current United States ($176 million) reporting units. Revolving Credit Agreement In October 2021, we entered into a $3.15 billion multi-currency revolving credit agreement (the ""Revolving Credit Agreement"") for our future liquidity needs. The Revolving Credit Agreement expires, unless extended, in October 2026. Interest rates on borrowings under the Revolving Credit Agreement are based on prevailing market interest rates, plus a margin, as further described in the Revolving Credit Agreement. The total expense recorded by the Company for the Revolving Credit Agreement was not material in any of the periods presented. We may voluntarily prepay borrowings under the Revolving Credit Agreement without premium or penalty, subject to customary ""breakage"" costs. The Revolving Credit Agreement includes certain customary mandatory prepayment provisions. Interest on Debt Interest expense for the three and nine months ended December 31, 2022 was $27 million and $65 million, compared to $18 million and $50 million for the three and nine months ended December 31, 2021. Most of the interest for the pre-Separation period presented in the historical Consolidated Income Statement reflects the allocation of interest expense associated with debt issued by IBM from which a portion of the proceeds benefited Kyndryl. Line Of Credit Facility Maximum Borrowing Capacity: <|assistant|>"
1284	Prompting the granite-13b-chat-v2 foundation model from IBM	"NLP tasks
Extraction
Identify the information types that you want the model to extract from the input. Be sure to list these information type labels in the instruction segment of your prompt.  
For example, if you want to extract key pieces of information from a company's US Securities and Exchange Commission 10-K form, you might identify an information type such as a Line Of Credit Facility Maximum Borrowing Capacity.  
Collect 2 or 3 representative examples of input text with the type of information that you want the model to extract.  
Work with the granite-13b-chat-v2 foundation model from the Prompt Lab in freeform mode so that you can structure your prompts.  
From the Model parameters panel, change the Max tokens parameter to a value that covers the number of tokens in the longest mention of the information type that you want to extract. Set Repetition penalty to 1.2.  
Clearly identify the system prompt, user input, and where the model's output should go.  
For example, the following prompt structure was used when the granite-13b-chat-v2 foundation model was trained to extract information from text:  
You can use a similar structure to leverage the model's training. Simply replace the placeholder variables in the prompt template.  
For example, the following prompt extracts the Line Of Credit Facility Maximum Borrowing Capacity value from a company's SEC 10-K form.  
The following screenshot shows the output that is generated by the granite-13b-chat-v2 foundation model when this prompt is submitted by using the Prompt Lab. The output is $3.15B."
1285	Prompting the granite-13b-chat-v2 foundation model from IBM	"Learn more
To learn more about the granite-13b-chat-v2 foundation model, read the following resources:  
Model information  
Research paper  
Model card  
Parent topic: IBM foundation models"
1286	Batch deployment input details for Python scripts	"Follow these rules when you specify input details for batch deployments of Python scripts.  
Data type summary table:"
1287	Batch deployment input details for Python scripts	"Data sources
Input or output data references:  
Local or managed assets from the space Connected (remote) assets: Cloud Object Storage  
Notes:  
For connections of type Cloud Object Storage or Cloud Object Storage(infrastructure), you must configure Access key and Secret key, also known as HMAC credentials.  
If you are specifying input/output data references programmatically:"
1288	Batch deployment input details for Python scripts	"Data sources
Data source reference type depends on the asset type. For more information, see Data source reference types section in Adding data assets to a deployment space. You can specify the environment variables that are required for running the Python Script as 'key': 'value' pairs in scoring.environment_variables. The key must be the name of an environment variable and the value must be the corresponding value of the environment variable. The deployment job's payload is saved as a JSON file in the deployment container where you run the Python script. The Python script can access the full path file name of the JSON file that uses the JOBS_PAYLOAD_FILE environment variable. If input data is referenced as a local or managed data asset, deployment service downloads the input data and places it in the deployment container where you run the Python script. You can access the location (path) of the downloaded input data through the BATCH_INPUT_DIR environment variable. For input data references (data asset or connection asset), downloading of the data must be handled by the Python script. If a connected data asset or a connection asset is present in the deployment jobs payload, you can access it using the JOBS_PAYLOAD_FILE environment variable that contains the full path to the deployment job's payload that is saved as a JSON file. If output data must be persisted as a local or managed data asset in a space, you can specify the name of the asset to be created in scoring.output_data_reference.location.name. As part of a Python script, output data can be placed in the path that is specified by the BATCH_OUTPUT_DIR environment variable. The deployment service compresses the data to compressed file format and upload it in the location that is specified in BATCH_OUTPUT_DIR. These environment variables are set internally. If you try to set them manually, your values are overridden: If output data must be saved in a remote data store, you must specify the reference of the output data reference (for example, a data asset or a connected data asset) in output_data_reference.location.href. The Python script must take care of uploading the output data to the remote data source. If a connected data asset or a connection asset reference is present in the deployment jobs payload, you can access it using the JOBS_PAYLOAD_FILE environment variable, which contains the full path to the deployment job's payload that is saved as a JSON file. If the Python script does not require any input or output data references to be specified in the deployment job payload, then do not provide the"
1289	Batch deployment input details for Python scripts	"Data sources
environment variable, which contains the full path to the deployment job's payload that is saved as a JSON file. If the Python script does not require any input or output data references to be specified in the deployment job payload, then do not provide the scoring.input_data_references and scoring.output_data_references objects in the payload."
1290	Batch deployment input details for Python scripts	"Data sources
BATCH_INPUT_DIR BATCH_OUTPUT_DIR JOBS_PAYLOAD_FILE"
1291	Batch deployment input details for Python scripts	"Learn more
Deploying scripts in Watson Machine Learning.  
Parent topic: Batch deployment input details by framework"
1292	Accessing asset details	"Display details about an asset and preview data assets in a deployment space.  
To display details about the asset, click the asset name. For example, click a model name to view details such as the associated software and hardware specifications, the model creation date, and more. Some details, such as the model name, description, and tags, are editable.  
For data assets, you can also preview the data."
1293	Accessing asset details	"Previewing data assets
To preview a data asset, click the data asset name.  
User's access to the data is based on the API layer. This means that if user's bearer token allows for viewing data, the data preview is displayed. For tabular data, only a subset of the data is displayed. Also, column names are displayed but their data types are not inferred. For data in XLS files, only the first worksheet is displayed for preview. All data from Cloud Object Storage connectors is assumed to be tabular data.  
MIME types supported for preview:  
Parent topic: Assets in deployment spaces"
1294	Deleting a deployment	Delete your deployment when you no longer need it to free up resources. You can delete a deployment from a deployment space, or programmatically, by using the Python client or Watson Machine Learning APIs.
1295	Deleting a deployment	"Deleting a deployment from a space
To remove a deployment:  
Open the Deployments page of your deployment space. Choose Delete from the action menu for the deployment name."
1296	Deleting a deployment	"Deleting a deployment by using the Python client
Use the following method to delete the deployment.  
Returns a SUCCESS message. To check that the deployment was removed, you can list deployments and make sure that the deleted deployment is no longer listed.  
Returns:"
1297	Deleting a deployment	"Deleting a deployment by using the REST API
Use the DELETE method for deleting a deployment.  
For more information, see Delete.  
For example, see the following code snippet:  
Parent topic: Managing predictive deployments"
1298	Choosing a foundation model to tune	"Find the right foundation model to customize for your task.  
The following models are available for prompt tuning from the Tuning Studio in watsonx.ai:  
flan-t5-xl-3b granite-13b-instruct-v2 llama-2-13b-chat  
Note: The foundation models that are available for tuning in the Tuning Studio can differ by data center. For more information, see Regional availability for services and features.  
To help you choose the best model, follow these steps:  
Consider whether any measures were taken to curate the data that was used to train the foundation model to improve the integrity of the foundation model output.  
Review other general considerations for choosing a model.  
For more information, see Choosing a foundation model.  
Consider the costs that are associated with the foundation model, both at inference time and at tuning time. A smaller model, such as a 3 billion parameter model, costs less to tune and is a good place to start.  
Tuning incurs compute resource consumption costs that are measured in capacity unit hours (CUH). The larger the model, the longer it takes to tune the model. A foundation model that is four times the size takes four times as long to tune.  
For example, on the same data set (10,000 examples and 1.25 MB in size), the time it takes to tune the supported foundation models is as follows:  
flan-t5-xl-3b: 3 hours 25 minutes llama-2-13b-chat: 11 hours 45 minutes  
For more information about CUH costs, see Watson Machine Learning plans and compute usage.  
Experiment with the models in the Prompt Lab.  
Use the largest version (meaning the version with the most parameters) of the model in the same model family for testing purposes. By testing with a larger, more powerful model you can establish the best prompt pattern for getting the output you want. Then, you can tune a smaller version of the same model type to save costs. A prompt-tuned version of a smaller model can generate similar, if not better results and costs less to inference.  
Craft and try prompts until you find the input pattern that generates the best results from the large foundation model.  
For more information, see Prompt Lab."
1299	Choosing a foundation model to tune	"Craft and try prompts until you find the input pattern that generates the best results from the large foundation model.  
For more information, see Prompt Lab.  
The following table shows the foundation models to experiment with before you choose a foundation model to tune.  
Parent topic: Tuning Studio"
1300	Methods for tuning foundation models	"Learn more about different tuning methods and how they work.  
Models can be tuned in the following ways:  
Fine-tuning: Changes the parameters of the underlying foundation model to guide the model to generate output that is optimized for a task.  
Note: You currently cannot fine-tune models in Tuning Studio.  
Prompt-tuning: Adjusts the content of the prompt that is passed to the model to guide the model to generate output that matches a pattern you specify. The underlying foundation model and its parameters are not edited. Only the prompt input is altered.  
When you prompt-tune a model, the underlying foundation model can be used to address different business needs without being retrained each time. As a result, you reduce computational needs and inference costs."
1301	Methods for tuning foundation models	"How prompt-tuning works
Foundation models are sensitive to the input that you give them. Your input, or how you prompt the model, can introduce context that the model will use to tailor its generated output. Prompt engineering to find the right prompt often works well. However, it can be time-consuming, error-prone, and its effectiveness can be restricted by the context window length that is allowed by the underlying model.  
Prompt-tuning a model in the Tuning Studio applies machine learning to the task of prompt engineering. Instead of adding words to the input itself, prompt-tuning is a method for finding a sequence of values that, when added as a prefix to the input text, improve the model's ability to generate the output you want. This sequence of values is called a prompt vector.  
Normally, words in the prompt are vectorized by the model. Vectorization is the process of converting text to tokens, and then to numbers defined by the model's tokenizer to identify the tokens. Lastly, the token IDs are encoded, meaning they are converted into a vector representation, which is the input format that is expected by the embedding layer of the model. Prompt-tuning bypasses the model's text-vectorization process and instead crafts a prompt vector directly. This changeable prompt vector is concatenated to the vectorized input text and the two are passed as one input to the embedding layer of the model. Values from this crafted prompt vector affect the word embedding weights that are set by the model and influence the words that the model chooses to add to the output.  
To find the best values for the prompt vector, you run a tuning experiment. You demonstrate the type of output that you want for a corresponding input by providing the model with input and output example pairs in training data. With each training run of the experiment, the generated output is compared to the training data output. Based on what it learns from differences between the two, the experiment adjusts the values in the prompt vector. After many runs through the training data, the model finds the prompt vector that works best.  
You can choose to start the training process by providing text that is vectorized by the experiment. Or you can let the experiment use random values in the prompt vector. Either way, unless the initial values are exactly right, they will be changed repeatedly as part of the training process. Providing your own initialization text can help the experiment reach a good result more quickly."
1302	Methods for tuning foundation models	"How prompt-tuning works
The result of the experiment is a tuned version of the underlying model. You submit input to the tuned model for inferencing and the model generates output that follows the tuned-for pattern."
1303	Methods for tuning foundation models	"Learn more
IBM Research blog post: What is prompt-tuning? Research paper: The Power of Scale for Parameter-Efficient Prompt Tuning Tuning parameters  
Parent topic: Tuning Studio"
1304	Manually adding the project access token	"All projects have an authorization token that is used to access data assets, for example files and connections, and is used by platform APIs. This token is called the project access token, or simply access token in the project user interface. This project access token must be set in notebooks so that project and platform functions can access the project resources.  
When you load data to your notebook by clicking Read data on the Code snippets pane, selecting the asset and the load option, the project access token is added for you, if the generated code that is inserted uses project functions.  
However, when you use API functions in your notebook that require the project token, for example, if you're using Wget to access data by using the HTTP, HTTPS or FTP protocols, or the ibm-watson-studio-lib library, you must add the project access token to the notebook yourself.  
To add a project access token to a notebook if you are not using the generated code:  
project.get_name()  
From the Manage tab, select Access Control and click New access token under Access tokens. Only project administrators can create project access tokens.  
Enter a name and select the access role. To enable using API functions in a notebook, the access token must have the Editor access role. An access token with Viewer access role enables read access only to a notebook.  
Add the project access token to a notebook by clicking More > Insert project token from the notebook action bar.  
By running the inserted hidden code cell, a project object is created that you can use for functions in the ibm-watson-studio-lib library. For example to get the name of the current project run:  
For details on the available ibm-watson-studio-lib functions, see Accessing project assets with ibm-watson-studio-lib.  
Note that a project administrator can revoke a project access token at any time. An access token has no expiration date and is valid until it is revoked.  
Parent topic: Loading and accessing data in a notebook"
1305	Federated Learning Tensorflow tutorial	"This tutorial demonstrates the usage of Federated Learning with the goal of training a machine learning model with data from different users without having users share their data. The steps are done in a low code environment with the UI and with a Tensorflow framework.  
Note:  
This is a step-by-step tutorial for running a UI driven Federated Learning experiment. To see a code sample for an API driven approach, see Federated Learning Tensorflow samples.  
Tip:  
In this tutorial, admin refers to the user that starts the Federated Learning experiment, and party refers to one or more users who send their model results after the experiment is started by the admin. While the tutorial can be done by the admin and multiple parties, a single user can also complete a full runthrough as both the admin and the party. For a simpler demonstrative purpose, in the following tutorial only one data set is submitted by one party. For more information on the admin and party, see Terminology.  
Video disclaimer: Some minor steps and graphical elements in this video might differ from your platform.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
In this tutorial you will learn to:  
Step 1: Start Federated Learning as the admin Step 2: Train model as a party Step 3: Save and deploy the model online"
1306	Federated Learning Tensorflow tutorial	"Step 1: Start Federated Learning as the admin
In this tutorial, you train a Federated Learning experiment with a Tensorflow framework and the MNIST data set."
1307	Federated Learning Tensorflow tutorial	"Step 1: Start Federated Learning as the admin
Before you begin
Log in to IBM Cloud. If you don't have an account, create one with any email.  
Create a Watson Machine Learning service instance if you do not have it set up in your environment.  
Log in to watsonx.  
Use an existing project or create a new one. You must have at least admin permission.  
Associate the Watson Machine Learning service with your project.  
In your project, click the Manage > Service & integrations. Click Associate service. Select your Watson Machine Learning instance from the list, and click Associate; or click New service if you do not have one to set up an instance.  
Log in to your Cloud Pak for Data cluster.  
Install the Watson Machine Learning image if you do not have it set up in your environment.  
Use an existing project or create a new one. You must have at least editor permission."
1308	Federated Learning Tensorflow tutorial	"Step 1: Start Federated Learning as the admin
Start the aggregator
Create the Federated learning experiment asset:  
Click the Assets tab in your project.  
Click New asset > Train models on distributed data.  
Type a Name for your experiment and optionally a description.  
Verify the associated Watson Machine Learning instance under Select a machine learning instance. If you don't see a Watson Machine Learning instance associated, follow these steps:  
Click Associate a Machine Learning Service Instance.  
Select an existing instance and click Associate, or create a New service.  
Click Reload to see the associated service.  
Click Next.  
Configure the experiment.  
On the Configure page, select a Hardware specification.  
Under the Machine learning framework dropdown, select Tensorflow 2.  
Select a Model type.  
Download the untrained model.  
Back in the Federated Learning experiment, click Select under Model specification.  
Drag the downloaded file named tf_mnist_model.zip onto the Upload file box.1. Select runtime-22.2-py3.10 for the Software Specification dropdown.  
Give your model a name, and then click Add.  
Click Weighted average for the Fusion method, and click Next.  
Define the hyperparameters.  
Accept the default hyperparameters or adjust as needed.  
When you are finished, click Next.  
Select remote training systems.  
Click Add new systems.  
Give your Remote Training System a name.  
Under Allowed identities, choose the user that is your party, and then click Add. In this tutorial, you can add a dummy user or yourself, for demonstrative purposes.This user must be added to your project as a collaborator with Editor or higher permissions. Add additional systems by repeating this step for each remote party you intent to use.  
When you are finished, click Add systems.  
Return to the Select remote training systems page, verify that your system is selected, and then click Next.  
Review your settings, and then click Create.  
Watch the status. Your Federated Learning experiment status is Pending when it starts. When your experiment is ready for parties to connect, the status will change to Setup – Waiting for remote systems. This may take a few minutes."
1309	Federated Learning Tensorflow tutorial	"Step 1: Start Federated Learning as the admin
Start the aggregator
Watch the status. Your Federated Learning experiment status is Pending when it starts. When your experiment is ready for parties to connect, the status will change to Setup – Waiting for remote systems. This may take a few minutes.  
Click View setup information to download the party configuration and the party connector script that can be run on the remote party.  
Click the download icon besides each of the remote training systems that you created, and then click Party connector script. This gives you the party connector script. Save the script to a directory on your machine."
1310	Federated Learning Tensorflow tutorial	"Step 2: Train model as the party
Follow these steps to train the model as a party:  
from ibm_watson_machine_learning import APIClient wml_credentials = { ""url"": ""https://us-south.ml.cloud.ibm.com"", ""apikey"": ""<API KEY>"" } wml_client = APIClient(wml_credentials) wml_client.set.default_project(""XXX-XXX-XXX-XXX-XXX"") party_metadata = { wml_client.remote_training_systems.ConfigurationMetaNames.DATA_HANDLER: { # Supply the name of the data handler class and path to it. # The info section may be used to pass information to the # data handler. # For example, # ""name"": ""MnistSklearnDataHandler"", # ""path"": ""example.mnist_sklearn_data_handler"", # ""info"": { # ""train_file"": pwd + ""/mnist-keras-train.pkl"", # ""test_file"": pwd + ""/mnist-keras-test.pkl"" # } ""name"": ""<data handler>"", ""path"": ""<path to data handler>"", ""info"": { ""<information to pass to data handler>"" } } } party = wml_client.remote_training_systems.create_party(""XXX-XXX-XXX-XXX-XXX"", party_metadata) party.monitor_logs() party.run(aggregator_id=""XXX-XXX-XXX-XXX-XXX"", asynchronous=False)  
Ensure that you are using the same Python version as the admin. Using a different Python version might cause compatibility issues. To see Python versions compatible with different frameworks, see Frameworks and Python version compatibility.  
Create a new local directory, and put your party connector script in it.  
Download the data handler mnist_keras_data_handler.py by right-clicking on it and click Save link as. Save it to the same directory as the party connector script.  
Download the MNIST handwriting data set from our Resource hub. In the the same directory as the party connector script, data handler, and the rest of your files, unzip it by running the unzip command unzip MNIST-pkl.zip.  
Install Watson Machine Learning."
1311	Federated Learning Tensorflow tutorial	"Step 2: Train model as the party
Install Watson Machine Learning.  
If you are using Linux, run pip install 'ibm-watson-machine-learning[fl-rt22.2-py3.10]'. If you are using Mac OS with M-series CPU and Conda, download the installation script and then run ./install_fl_rt22.2_macos.sh <name for new conda environment>.You now have the party connector script, mnist_keras_data_handler.py, mnist-keras-test.pkl and mnist-keras-train.pkl, data handler in the same directory.  
Your party connector script looks similar to the following. Edit it by filling in the data file locations, the data handler, and API key for the user defined in the remote training system. To get your API key, go to Manage > Access(IAM) > API keys in your IBM Cloud account. If you don't have one, click Create API key, fill out the fields, and click Create.  
Run the party connector script: python3 rts_<RTS Name>_<RTS ID>.py. From the UI you can monitor the status of your Federated Learning experiment."
1312	Federated Learning Tensorflow tutorial	"Step 3: Save and deploy the model online
In this section, you will learn to save and deploy the model that you trained.  
Save your model.  
In your completed Federated Learning experiment, click Save model to project. Give your model a name and click Save. Go to your project home.  
Create a deployment space, if you don't have one.  
From the navigation menu , click Deployments. Click New deployment space. Fill in the fields, and click Create.  
Promote the model to a space.  
Return to your project, and click the Assets tab. In the Models section, click the model to view its details page. Click Promote to space. Choose a deployment space for your trained model. Select the Go to the model in the space after promoting it option. Click Promote.  
When the model displays inside the deployment space, click New deployment.  
Select Online as the Deployment type. Specify a name for the deployment. Click Create.  
Click the Deployments tab to monitor your model's deployment status."
1313	Federated Learning Tensorflow tutorial	"Step 3: Save and deploy the model online
Next steps
Ready to create your own customized Federated Experiment? See the high level steps in Creating your Federated Learning experiment.  
Parent topic: Federated Learning tutorial and samples"
1314	SPSS predictive analytics algorithms	"You can use the following SPSS predictive analytics algorithms in your notebooks. Code samples are provided for Python notebooks.  
Notebooks must run in a Spark with Python environment runtime. To run the algorithms described in this section, you don't need the SPSS Modeler service.  
Data preparation Classification and regression Clustering Forecasting Survival analysis Score  
Parent topic: Notebooks and scripts"
1315	IBM Federated Learning	"Federated Learning provides the tools for multiple remote parties to collaboratively train a single machine learning model without sharing data. Each party trains a local model with a private data set. Only the local model is sent to the aggregator to improve the quality of the global model that benefits all parties.  
Data format     Any data format including but not limited to CSV files, JSON files, and databases for PostgreSQL."
1316	IBM Federated Learning	"How Federated Learning works
Watch this overview video to learn the basic concepts and elements of a Federated Learning experiment. Learn how you can apply the tools for your company's analytics enhancements.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
An example for using Federated Learning is when an aviation alliance wants to model how a global pandemic impacts airline delays. Each participating party in the federation can use their data to train a common model without ever moving or sharing their data. They can do so either in application silos or any other scenario where regulatory or pragmatic considerations prevent users from sharing data. The resulting model benefits each member of the alliance with improved business insights while lowering risk from data migration and privacy issues.  
As the following graphic illustrates, parties can be geographically distributed and run on different platforms."
1317	IBM Federated Learning	"Why use IBM Federated Learning
IBM Federated Learning has a wide range of applications across many enterprise industries. Federated Learning:  
Enables sites with large volumes of data to be collected, cleaned, and trained on an enterprise scale without migration. Accommodates for the differences in data format, quality, and constraints. Complies with data privacy and security while training models with different data sources."
1318	IBM Federated Learning	"Learn more
Federated Learning tutorials and samples Getting started Frameworks, fusion methods, and Python versions Creating a Federated Learning experiment Applying encryption Limitations and troubleshooting  
Federated Learning Tensorflow tutorial for UI Federated Learning Tensorflow samples for API Federated Learning XGBoost tutorial for UI Federated Learning XGBoost sample for API Federated Learning homomorphic encryption sample for API  
Terminology Federated Learning architecture  
Hyperparameter definitions  
Set up your system Creating the initial model Create the data handler Starting the aggregator (Admin) Connecting to the aggregator (Party) Monitoring and saving the model  
Parent topic: Analyzing data and building models"
1319		"Managing compute resources
If you have the Admin role or Editor in a project, you can perform management tasks for environments.  
Create an environment template Customize an environment template Stop active runtimes Promote an environment template to a space Track capacity unit consumption of runtimes"
1320		"Stop active runtimes
You should stop all active runtimes when you no longer need them to prevent consuming extra capacity unit hours (CUHs).  
Jupyter notebook runtimes are started per user and not per notebook. Stopping a notebook kernel doesn't stop the environment runtime in which the kernel is started because you could have started other notebooks in the same environment. You should only stop a notebook runtime if you are sure that no other notebook kernels are active.  
Only runtimes that are started for jobs are automatically shut down after the scheduled job has completed. For example, if you schedule to run a notebook once a day for 2 months, the runtime instance will be activated every day for the duration of the scheduled job and deactivated again after the job has finished.  
Project users with Admin role can stop all runtimes in the project. Users added to the project with Editor role can stop the runtimes they started, but can't stop other project users' runtimes. Users added to the project with the viewer role can't see the runtimes in the project.  
You can stop runtimes from:  
The Environment Runtimes page, which lists all active runtimes across all projects for your account, by clicking Administration > Environment runtimes from the Watson Studio navigation menu. Under Tool runtimes on the Environments page on the Manage tab of your project, which lists the active runtimes for a specific project. The Environments page when you click the Notebook Info icon () from the notebook toolbar in the notebook editor. You can stop the runtime under Runtime status.  
Idle timeouts for:  
Jupyter notebook runtimes Spark runtimes for notebooks and Data Refinery Notebook with GPU runtimes RStudio runtimes"
1321		"Stop active runtimes
Jupyter notebook idle timeout
Runtime idle times differ for the Jupyter notebook runtimes depending on your Watson Studio plan.  
   
Important: A runtime is started per user and not per notebook. Stopping a notebook kernel doesn't stop the environment runtime in which the kernel is started because you could have started other notebooks in the same environment. Only stop a runtime if you are sure that no kernels are active."
1322		"Stop active runtimes
Spark idle timeout
All Spark runtimes, for example for notebook and Data Refinery, are stopped after 3 hours of inactivity. The Default Data Refinery XS runtime that is used when you refine data in Data Refinery is stopped after an idle time of 1 hour.  
Spark runtimes that are started when a job is started, for example to run a Data Refinery flow or a notebook, are stopped when the job finishes."
1323		"Stop active runtimes
GPU idle timeout
All GPU runtimes are automatically stopped after 3 hours of inactivity for Enterprise plan users and after 1 hour of inactivity for other paid plan users."
1324		"Stop active runtimes
RStudio idle timeout
An RStudio is stopped for you after an idle time of 2 hour. During this idle time, you will continue to consume CUHs for which you are billed. Long compute-intensive jobs are hard stopped after 24 hours.  
Parent topic: Projects"
1325	Tuning a foundation model by using a Python notebook	"You can tune foundation models in IBM watsonx.ai programmatically by using the Python library.  
For more information about the library and prerequisite steps, see Foundation models Python library."
1326	Tuning a foundation model by using a Python notebook	"Sample notebook
The Use watsonx to tune IBM granite-13b-instruct-v2 model with Car Rental Company customer satisfaction document sample Python notebook contains code for prompt-tuning foundation models in watsonx.ai.  
The sample notebook helps you with the two main phases of tuning:  
Finding the optimal tuning parameter values Prompting the tuned model to evaluate the quality of tuned model output  
The sample notebook is designed to prompt-tune the granite-13b-instruct-v2 model. But you can use it for tuning other foundation models also. To do so, replace the base_model references as follows:  
base_model='meta-llama/llama-2-13b-chat' base_model='google/flan-t5-xl'  
If you change the foundation model, you must replace the training data also. Replace the file path in the Data loading section of the notebook.  
You can also use a sample notebook that tunes the other foundation models that can be prompt-tuned.  
Tune a model to classify CFPB documents in watsonx  
The flan-t5 notebook has steps to tune the foundation model, but does not include a step for hyperparameter optimization.  
Use watsonx.ai to tune Meta llama-2-13b-chat model with Consumer Financial Protection Bureau documents"
1327	Tuning a foundation model by using a Python notebook	"Sample notebook
Using the sample notebook to optimize tuning parameter values
The sample notebook has code that optimizes the learning_rate parameter value. The sample notebook systematically changes the learning rate value and reruns the experiment 10 times, so the loss can be compared across the 10 runs. The sample notebook calculates the optimal learning rate value for you.  
The sample notebook generates 10 separate experiments; it does not run the same experiment 10 times.  
The parameter to optimize is defined in the Search space and optimization section of the notebook.  
You can edit or add to the sample notebook to run automated code to optimize the following parameters in addition to learning rate:  
accumulate_steps batch_size num_epochs  
To check for the optimal values for many parameters at once, you can change the sample notebook to use code like this, for example:  
Optimizing many parameters at once can save time because the parameters work together. Their values affect one another and the right balance of values among them leads to the best results.  
The sample notebook uses methods from the scikit-optimize library. For more information, see the scikit-optimize API reference."
1328	Tuning a foundation model by using a Python notebook	"Sample notebook
Using the sample notebook to evaluate the tuned model
The sample notebook has code that deploys the tuned model, inferences the tuned model, and then calculates the accuracy of the tuned model output. It also inferences the underlying foundation model and calculates the accuracy of the base model output, so that you can see a comparison.  
If you want to use the sample notebook to tune and assess other models, you can replace the value of the model_id parameter in the following code.  
For example, specify meta-llama/llama-2-13b-chat or google/flan-t5-xl.  
You must also replace the prompt text with a prompt from your own training data set.  
If the accuracy score for your tuned model is low, review some ideas for ways to improve your training data in Addressing data quality problems in tuned model output.  
Remember, optimization of the tuning parameters is specific to the model and training data that you are using. If you change either the model or the training data, you need to reassess the tuning experiment. Adjust the tuning parameters again to optimize them for your augmented data set.  
Parent topic: Python library"
1329	Jupyter kernels and notebook environments	"Jupyter notebooks run in kernels in Jupyter notebook environments or, if the notebooks use Spark APIs, those kernels run in a Spark environment.  
The number of notebook Juypter kernels started in an environment depends on the environment type:  
CPU or GPU environments  
When you open a notebook in edit mode, exactly one interactive session connects to a Jupyter kernel for the notebook language and the environment runtime that you select. The runtime is started per user and not per notebook. This means that if you open a second notebook with the same environment template, a second kernel is started in that runtime. Resources are shared. If you want to avoid sharing runtime resources, you must associate each notebook with its own environment template.  
Important: Stopping a notebook kernel doesn't stop the environment runtime in which the kernel is started because other notebook kernels could still be active in that runtime. Only stop an environment runtime if you are sure that no kernels are active.  
Spark environments  
When you open a notebook in edit mode in a Spark environment, a dedicated Spark cluster is started, even if another notebook was opened in the same Spark environment template. Each notebook kernel has its own Spark driver and set of Spark executors. No resources are shared.  
If necessary, you can restart or reconnect to a kernel. When you restart a kernel, the kernel is stopped and then started in the same session, but all execution results are lost. When you reconnect to a kernel after losing a connection, the notebook is connected to the same kernel session, and all previous execution results which were saved are available.  
The kernel remains active even if you leave the notebook or close the web browser window. When you reopen the same notebook, the notebook is connected to the same kernel. Only the output cells that were saved (auto-save happens every 2 minutes) before you left the notebook or closed the web browser window will be visible. You will not see the output for any cells which ran in the background after you left the notebook or closed the window. To see all of the output cells, you need to rerun the notebook."
1330	Jupyter kernels and notebook environments	"Learn more
Notebook environments  
Associated Spark services  
Runtime scope in notebook environments  
Parent topic: Creating notebooks"
1331	Configuring global objects for Watson Pipelines	"Use global objects to create configurable constants to configure your pipeline at run time. Use parameters or user variables in pipelines to specify values at run time, rather than hardcoding the values. Unlike pipeline parameters, user variables can be dynamically set during the flow.  
Learn about creating:  
Pipeline parameters Parameter sets User variables"
1332	Configuring global objects for Watson Pipelines	"Pipeline parameters
Use pipeline parameters to specify a value at pipeline runtime. For example, if you want a user to enter a deployment space for pipeline output, use a parameter to prompt for the space name to use when the pipeline runs. Specifying the value of the parameter each time that you run the job helps you use the correct resources.  
About pipeline parameters:  
can be assigned as a node value or assign it for the pipeline job. can be assigned to any node, and a status indicator alerts you. can be used for multiple nodes."
1333	Configuring global objects for Watson Pipelines	"Pipeline parameters
Defining a pipeline parameter
Create a pipeline parameter from the node configuration panel from the toolbar. Enter a name and an optional description. The name must be lower snake case with lowercase letters, numbers, and underscores. For example, lower_snake_case_with_numbers_123 is a valid name. The name must begin with a letter. If the name does not comply, you get a 404 error when you try to run the pipeline. Assign a parameter type. Depending on the parameter type, you might need to provide more details or assign a default value. Click Add to list to save the pipeline parameter.  
Note:"
1334	Configuring global objects for Watson Pipelines	"Pipeline parameters
Parameter types
Parameter types are categorized as:  
Basic: including data types to structure input to a pipeline or options for handling the creation of a duplicate space or asset. Resource: for selecting a project, catalog, space, or asset. Instance: for selecting a machine learning instance or a Cloud Object Storage instance. Other: for specifying details, such as creation mode or error policy.  
To create a parameter of the type Path:  
Create a parameter set called MASTER_PARAMETER_SET. Create a parameter called file_path and set the type to Path. Set the value of file_path to mnts/workspace/masterdir. Drag the node Wait for file onto the canvas and set the File location value to MASTER_PARAMETER_SET.file_path. Connect the Wait for file with the Run Bash script node so that the latter node runs after the former. Optional: Test your parameter variable: Run the pipeline. The path mnts/workspace/masterdir is in both of the nodes' execution logs to see they passed successfully.  
Add the environment variable parameter to your MASTER_PARAMETER_SET parameter set, for example FILE_PATH. Paste the following command into the Script code of the Run Bash script:echo File: $FILE_PATH cat $FILE_PATH"
1335	Configuring global objects for Watson Pipelines	"Configuring a node with a pipeline parameter
When you configure a node with a pipeline parameter, you can choose an existing pipeline parameter or create a new one as part of configuring a node.  
For example:  
Create a pipeline parameter called creationmode and save it to the parameter list. Configure a Create deployment space node and click to open the configuration panel. Choose the Pipeline parameter as the input for the Creation mode option. Choose the creationmode pipeline parameter and save the configuration.  
When you run the flow, the pipeline parameter is assigned when the space is created."
1336	Configuring global objects for Watson Pipelines	"Parameter sets
Parameter sets are a group of related parameters to use in a pipeline. For example, you might create one set of parameters to use in a test environment and another for use in a production environment.  
Parameter sets can be created as a project asset. Parameter sets created in the project are then available for use in pipelines in that project."
1337	Configuring global objects for Watson Pipelines	"Parameter sets
Creating a parameter set as a project asset
You can create a parameter set as a reusable project asset to use in pipelines.  
Open an existing project or create a project. Click New task > Collect multiple job parameters with specified values to reuse in jobs from the available tasks. Assign a name for the set, and specify the details for each parameter in the set, including: Optionally create value sets for the parameters in the parameter set. The value sets can be the different values for different contexts. For example, you can create a Test value set with values for a test environment, and a production set for production values. Save the parameter set after you create all the parameters, s. It becomes available for use in pipelines that are created in that project.  
Name for the parameter Data type Prompt Default value"
1338	Configuring global objects for Watson Pipelines	"Parameter sets
Adding a parameter set for use in a pipeline
To add a parameter set from a project:  
Click the global objects icon and switch to the Parameter sets tab. Click Add parameter set to add parameter sets from your project that you want to use in your pipeline. You can add or remove parameter sets from the list. The parameter sets you specify for use in your pipeline becomes available when you assign parameters as input in the pipeline."
1339	Configuring global objects for Watson Pipelines	"Parameter sets
Creating a parameter set from the parameters list in your pipeline
You can create a parameter set from the parameters list for your pipeline  
Click the global objects icon and open the Pipeline Parameters. Select the parameters that you want in the set, then click the Save as parameter set icon. Enter a name and optional description for the set. Save to add the parameter set for use in your pipeline."
1340	Configuring global objects for Watson Pipelines	"Parameter sets
Using a parameter set in a pipeline
To use a parameter set:  
Choose Assign pipeline parameter as an input type from a node property sheet. Choose the parameter to assign. A list displays all available parameters of the type for that input. Available parameters can be individual parameters, and parameters defined as part of a set. The parameter set name precedes the name of the parameter. For example, Parameter_set_name.Parameter_name. Run the pipeline and select a value set for the corresponding value (if available), assign a value for the parameter, or accept the default value.  
Note: You can use a parameter set in the expression builder by using the format `param_sets. `. If a parameter set value contains an environment variable, you must use this syntax in the expression builder: `param_sets.MyParamSet[  
Attention: If you delete a parameter, make sure that you remove the references to the parameter from your job design. If you do not remove the references, your job might fail."
1341	Configuring global objects for Watson Pipelines	"Parameter sets
Editing a parameter set in a job
If you use a parameter set when you define a job, you can choose a value set to populate variables with the values in that set. If you change and save the values, then edit the job and save changes, the parameter set values reset to the defaults."
1342	Configuring global objects for Watson Pipelines	"User variables
Create user variables to assign values when the flow runs. Unlike pipeline parameters, user variables can be modified during processing."
1343	Configuring global objects for Watson Pipelines	"User variables
Defining a user variable
You can create user variables for use in your pipeline. User variables, like parameters, are defined on the global level and are not specific to any node. The initial value for a user variable must be set when you define it and cannot be set dynamically as the result of any node output. When you define a user variable, you can use the Set user variables node to update it with node output.  
To create a user variable:  
Create a variable from the Update variable node configuration panel or from the toolbar. Enter a name and an optional description. The name must be lower snake case with lowercase letters, numbers, and underscores. For example, lower_snake_case_with_numbers_123 is a valid name. The name must begin with a letter. If the name does not comply, you get a 404 error when you try to run the pipeline. Complete the definition of the variable, including choosing a variable type and input type. Click Add to add the variable to the list. It is now available for use in a node.  
Parent topic: Creating a pipeline"
1344	Managing feature groups with assetframe-lib for Python (beta)	"You can use the assetframe-lib to create, view and edit feature group information for data assets in Watson Studio notebooks.  
Feature groups define additional metadata on columns of your data asset that can be used in downstream Machine Learning tasks. See Managing feature groups for more information about using feature groups in the UI."
1345	Managing feature groups with assetframe-lib for Python (beta)	"Setting up the assetframe-lib and ibm-watson-studio-lib libraries
The assetframe-lib library for Python is pre-installed and can be imported directly in a notebook in Watson Studio. However, it relies on the ibm-watson-studio-lib library. The following steps describe how to set up both libraries.  
To insert the project token to your notebook:  
from ibm_watson_studio_lib import access_project_or_space wslib = access_project_or_space({""token"":""<ProjectToken>""}) from assetframe_lib import AssetFrame AssetFrame._wslib = wslib  
Click the More icon on your notebook toolbar and then click Insert project token.  
If a project token exists, a cell is added to your notebook with the following information:  
<ProjectToken> is the value of the project token.  
If you are told in a message that no project token exists, click the link in the message to be redirected to the project's Access Control page where you can create a project token. You must be eligible to create a project token.  
To create a project token:  
From the Manage tab, select the Access Control page, and click New access token under Access tokens. Enter a name, select Editor role for the project, and create a token. Go back to your notebook, click the More icon on the notebook toolbar and then click Insert project token.  
Import assetframe-lib and initialize it with the created ibm-watson-studio-lib instance."
1346	Managing feature groups with assetframe-lib for Python (beta)	"The assetframe-lib functions and methods
The assetframe-lib library exposes a set of functions and methods that are grouped in the following way:  
Creating an asset frame Creating, retrieving and removing features Specifying feature attributes Previewing data Getting fairness information  
Role Description Fairness information for favorable and unfavorable outcomes Fairness information for monitored and reference groups Value descriptions Recipe Tags"
1347	Managing feature groups with assetframe-lib for Python (beta)	"Creating an asset frame
An asset frame is used to define feature group metadata on an existing data asset or on a pandas DataFrame. You can have exactly one feature group for each asset. If you create an asset frame on a pandas DataFrame, you can store the pandas DataFrame along with the feature group metadata as a data asset in your project.  
You can use one of the following functions to create your asset frame:  
AssetFrame.from_data_asset(asset_name, create_default_features=False)  
This function creates a new asset frame wrapping an existing data asset in your project. If there is already a feature group for this asset, for example created in the user interface, it is read from the asset metadata.  
Parameters:  
asset_name: (Required) The name of a data asset in your project. create_default_features: (Optional) Creates features for all columns in the data asset.  
AssetFrame.from_pandas(name, dataframe, create_default_features=False)  
This function creates a new asset frame wrapping a pandas DataFrame.  
Parameters:  
# Create an asset frame from a pandas DataFrame and set # the name of the asset frame. af = AssetFrame.from_pandas(dataframe=credit_risk_df, name=""Credit Risk Training Data"")  
name: (Required) The name of the asset frame. This name will be used as the name of the data asset if you store your feature group in your project in a later step.  
dataframe: (Required) A pandas DataFrame that you want to store along with feature group information.  
create_default_features: (Optional) Create features for all columns in the dataframe.  
Example of creating a asset frame from a pandas DataFrame:"
1348	Managing feature groups with assetframe-lib for Python (beta)	"Creating, retrieving and removing features
A feature defines metadata that can be used by downstream Machine Learning tasks. You can create one feature per column in your data set.  
You can use one of the following functions to create, retrieve or remove columns from your asset frame:  
add_feature(column_name, role='Input')  
This function adds a new feature to your asset frame with the given role.  
Parameters:  
column_name: (Required) The name of the column to create a feature for.  
role: (Optional) The role of the feature. It defaults to Input.  
Valid roles are:  
Input: The input for a machine learning model Target: The target of a prediction model Identifier: The identifier of a row in your data set.  
create_default_features()  
This function creates features for all columns in your data set. The roles of the features will default to Input.  
get_features()  
This function retrieves all features of the asset frame.  
get_feature(column_name)  
This function retrieves the feature for the given column name.  
Parameters:  
column_name: (Required) The string name of the column to create the feature for.  
get_features_by_role(role)  
This function retrieves all features of the dataframe with the given role.  
Parameters:  
role: (Required) The role that the features must have. This can be Input, Target or Identifier.  
remove_feature(feature_or_column_name)  
This function removes the feature from the asset frame.  
Parameters:  
feature_or_column_name: (Required) A feature or the name of the column to remove the feature for.  
Example that shows creating features for all columns in the data set and retrieving one of those columns for further specifications:"
1349	Managing feature groups with assetframe-lib for Python (beta)	"Specifying feature attributes
Features specify additional metadata on columns that may be used in downstream Machine Learning tasks.  
You can use the following function to retrieve the column that the feature is defined for:  
get_column_name()  
This function retrieves the column name that the feature is defined for."
1350	Managing feature groups with assetframe-lib for Python (beta)	"Specifying feature attributes
Role
The role specifies the intended usage of the feature in a Machine Learning task.  
Valid roles are:  
Input: The feature can be used as an input to a Machine Learning model. Identifier: The feature uniquely identifies a row in the data set. Target: The feature can be used as a target in a prediction algorithm.  
At this time, a feature must have exactly one role.  
You can use the following methods to work with the role:  
set_roles(roles)  
This method sets the roles of the feature.  
Parameters:  
roles : (Required) The roles to be used. Either as a single string or an array of strings.  
get_roles()  
This method returns all roles of the feature."
1351	Managing feature groups with assetframe-lib for Python (beta)	"Specifying feature attributes
Description
An optional description of the feature. It defaults to None.  
You can use the following methods to work with the description.  
set_description(description)  
This method sets the description of the feature.  
Parameters:  
description: (Required) Either a string or None to remove the description.  
get_description()  
This method returns the description of the feature."
1352	Managing feature groups with assetframe-lib for Python (beta)	"Specifying feature attributes
Fairness information for favorable and unfavorable outcomes
You can specify favorable and unfavorable labels for a feature with a Target role.  
You can use the following methods to set and retrieve favorable or unfavorable labels.  
You can use the following methods to set and get favorable labels:  
set_favorable_labels(labels)  
This method sets favorable labels for the feature.  
Parameters:  
labels: (Required) A string or list of strings with favorable labels.  
get_favorable_labels()  
This method returns the favorable labels of the feature.  
You can use the following methods to set and get unfavorable labels:  
set_unfavorable_labels(labels)  
This method sets unfavorable labels for the feature.  
Parameters:  
labels: (Required) A string or list of strings with unfavorable labels.  
get_unfavorable_labels()  
This method gets the unfavorable labels of the feature.  
Example that shows setting favorable and unfavorable labels:"
1353	Managing feature groups with assetframe-lib for Python (beta)	"Specifying feature attributes
Fairness information for monitored and reference groups
Some columns in your data might by prone to unfair bias. You can specify monitored and reference groups for further usage in Machine Learning tasks. They can be specified for features with the role Input.  
You can either specify single values or ranges of numeric values as a string with square brackets and a start and end value, for example [0,15].  
You can use the following methods to set and retrieve monitored and reference groups:  
set_monitored_groups(groups)  
This method sets monitored groups for the feature.  
Parameters:  
groups: (Required) A string or list of strings with monitored groups.  
get_monitored_groups()  
This method gets the monitored groups of the feature.  
set_reference_groups(groups)  
This method sets reference groups for the feature.  
Parameters:  
groups: (Required) A string or list of strings with reference groups.  
get_reference_groups()  
This method gets the reference groups of the feature.  
Example that shows setting monitored and reference groups:"
1354	Managing feature groups with assetframe-lib for Python (beta)	"Specifying feature attributes
Value descriptions
You can use value descriptions to specify descriptions for column values in your data.  
You can use the following methods to set and retrieve descriptions:  
set_value_descriptions(value_descriptions)  
This method sets value descriptions for the feature.  
Parameters:  
value_descriptions: (Required) A Pyton dictionary or list of dictionaries of the following format: {'value': '<value>', 'description': '<description>'}  
get_value_descriptions()  
This method returns all value descriptions of the feature.  
get_value_description(value)  
This method returns the value description for the given value.  
Parameters:  
value: (Required) The value to retrieve the value description for.  
add_value_description(value, description)  
This method adds a value description with the given value and description to the list of value descriptions for the feature.  
Parameters:  
value: (Required) The string value of the value description. description: (Required) The string description of the value description.  
remove_value_description(value)  
This method removes the value description with the given value from the list of value descriptions of the feature.  
Parameters:  
value: (Required) A value of the value description to be removed.  
Example that shows how to set value descriptions:"
1355	Managing feature groups with assetframe-lib for Python (beta)	"Specifying feature attributes
Recipe
You can use the recipe to describe how a feature was created, for example with a formula or a code snippet. It defaults to None.  
You can use the following methods to work with the recipe.  
set_recipe(recipe)  
This method sets the recipe of the feature.  
Parameters:  
recipe: (Required) Either a string or None to remove the recipe.  
get_recipe()  
This method returns the recipe of the feature."
1356	Managing feature groups with assetframe-lib for Python (beta)	"Specifying feature attributes
Tags
You can use tags to attach additional labels or information to your feature.  
You can use the following methods to work with tags:  
set_tags(tags)  
This method sets the tags of the feature.  
Parameters:  
tags: (Required) Either as a single string or an array of strings.  
get_tags()  
This method returns all tags of the feature."
1357	Managing feature groups with assetframe-lib for Python (beta)	"Previewing data
You can preview the data of your data asset or pandas DataFrame with additional information about your features like fairness information.  
The data is displayed like a pandas DataFrame with optional header information about feature roles, descriptions or recipes. Fairness information is displayed with coloring for favorable or unfavorable labels, monitored and reference groups.  
At this time, you can retrieve up to 100 rows of sample data for a data asset.  
Use the following function to preview data:  
head(num_rows=5, display_options=['role'])  
This function returns the first num_rows rows of the data set in a pandas DataFrame.  
Parameters:  
num_rows : (Optional) The number of rows to retrieve.  
display_options: (Optional) The column header can display additional information for a column in your data set.  
Use these options to display feature attributes:  
role: Displays the role of a feature for this column. description: Displays the description of a feature for this column. recipe: Displays the recipe of a feature for this column."
1358	Managing feature groups with assetframe-lib for Python (beta)	"Getting fairness information
You can retrieve the fairness information of all features in your asset frame as a Python dictionary. This includes all features containing monitored or reference groups (or both) as protected attributes and the target feature with favorable or unfavorable labels.  
If the data type of a column with fairness information is numeric, the values of labels and groups are transformed to numeric values if possible.  
Fairness information can be used directly in AutoAI or AI Fairness 360.  
You can use the following function to retrieve fairness information of your asset frame:  
get_fairness_info(target=None)  
This function returns a Python dictionary with favorable and unfavorable labels of the target column and protected attributes with monitored and reference groups.  
Parameters:  
af.get_fairness_info() { 'favorable_labels': ['No Risk'], 'unfavorable_labels': ['Risk'], 'protected_attributes': [ {'feature': 'Sex', 'monitored_group': ['female'], 'reference_group': ['male']}, {'feature': 'Age', 'monitored_group': [[0.0, 25]], 'reference_group': [[26, 80]] }] }  
target: (Optional) The target feature. If there is only one feature with role Target, it will be used automatically.  
Example that shows how to retrieve fairness information:  
Output showing fairness information:"
1359	Managing feature groups with assetframe-lib for Python (beta)	"Saving feature group information
After you have fully specified or updated your features, you can save the whole feature group definition as metadata for your data asset.  
If you created the asset frame from a pandas DataFrame, a new data asset will be created in the project storage with the name of the asset frame.  
You can use the following method to store your feature group information:  
to_data_asset(overwrite_data=False)  
This method saves feature group information to the assets metadata. It creates a new data asset, if the asset frame was created from a pandas DataFrame.  
Parameters:  
overwrite_data: (Optional) Also overwrite the asset contents with the data from the asset frame. Defaults to False."
1360	Managing feature groups with assetframe-lib for Python (beta)	"Learn more
See the Creating and using feature store data sample project in the Resource hub.  
Parent topic: Loading and accessing data in a notebook"
1361	Deployment spaces	"Deployment spaces contain deployable assets, deployments, deployment jobs, associated input and output data, and the associated environments. You can use spaces to deploy various assets and manage your deployments.  
Deployment spaces are not associated with projects. You can publish assets from multiple projects to a space, and you can deploy assets to more than one space. For example, you might have a test space for evaluating deployments, and a production space for deployments that you want to deploy in business applications.  
The deployments dashboard is an aggregate view of deployment activity available to you, across spaces. For details, refer to Deployments dashboard.  
When you open a space from the UI, you see these elements:  
You can share a space with other people. When you add collaborators to a deployment space, you can specify which actions they can do by assigning them access levels. For details on space collaborator permissions, refer to Deployment space collaborator roles and permissions."
1362	Deployment spaces	"Learn more
Creating deployment spaces Managing assets in a deployment space Creating deployments from a space Exporting space assets Deleting deployment spaces"
1363	Using Spark in RStudio	"Although the RStudio IDE cannot be started in a Spark with R environment runtime, you can use Spark in your R scripts and Shiny apps by accessing Spark kernels programmatically. RStudio uses the sparklyr package to connect to Spark from R. The sparklyr package includes a dplyr interface to Spark data frames as well as an R interface to Spark’s distributed machine learning pipelines.  
By default, RStudio is FIPS-compliant, but if you need to use sparklyr to connect to a Spark cluster, you must load the digest package from a library that is not FIPS-compliant. When you do that, RStudio will no longer be FIPS compliant. For more information on FIPS, refer to Services that support FIPS.  
You can connect to Spark from RStudio:  
By connecting to a Spark kernel that runs locally in the RStudio container in IBM Watson Studio  
RStudio includes sample code snippets that show you how to connect to a Spark kernel in your applications for both methods.  
To use Spark in RStudio after you have launched the IDE:  
Locate the ibm_sparkaas_demos directory under your home directory and open it. The directory contains the following R scripts:  
A readme with details on the included R sample scripts spark_kernel_basic_local.R includes sample code of how to connect to a local Spark kernel spark_kernel_basic_remote.R includes sample code of how to connect to a remote Spark kernel The files sparkaas_flights.Rand sparkaas_mtcars.R are two examples of how to use Spark in a small sample application  
Use the sample code snippets in your R scripts or applications to help you get started using Spark."
1364	Using Spark in RStudio	"Connecting to Spark from RStudio
To connect to Spark from RStudio using the Sparklyr R package, you need a Spark with R environment. You can either use the default Spark with R environment that is provided or create a custom Spark with R environment. To create a custom environment, see Creating environment templates.  
Follow these steps after you launch RStudio in an RStudio environment:  
Use the following sample code to get a listing of the Spark environment details and to connect to a Spark kernel from your RStudio session:  
Then to disconnect from Spark, use:  
Examples of these commands are provided in the readme under /home/wsuser/ibm_sparkaas_demos.  
Parent topic: RStudio"
1365	Batch deployment input details for AutoAI models	"Follow these rules when you are specifying input details for batch deployments of AutoAI models.  
Data type summary table:"
1366	Batch deployment input details for AutoAI models	"Data Sources
Input/output data references:  
Local/managed assets from the space Connected (remote) assets: Cloud Object Storage  
Notes:  
For connections of type Cloud Object Storage , you must configure Access key and Secret key, also known as HMAC credentials. Your training data source can differ from your deployment data source, but the schema of the data must match or the deployment will fail. For example, you can train an experiment by using data from a Snowflake database and deploy by using input data from a Db2 database if the schema is an exact match. The environment variables parameter of deployment jobs is not applicable.  
If you are specifying input/output data references programmatically:  
Data source reference type depends on the asset type. Refer to the Data source reference types section in Adding data assets to a deployment space. For AutoAI assets, if the input or output data reference is of type connection_asset and the remote data source is a database then location.table_name and location.schema_name are required parameters. For example:  
Parent topic: Batch deployment input details by framework"
1367	Data imputation implementation details for time series experiments	The experiment settings used for data imputation in time series experiments.
1368	Data imputation implementation details for time series experiments	"Data imputation methods
Apply one of these data imputation methods in experiment settings to supply missing values in a data set."
1369	Data imputation implementation details for time series experiments	"Input Settings
These commands are used to support data imputation for time series experiments in a notebook."
1370	Data imputation implementation details for time series experiments	"Input Settings
Notes for use_imputation usage
If the use_imputation method is specified as True and the input data has missing values:  
imputation_threshold takes effect. imputer candidates in imputer_list would be used to search for the best imputer. If the best imputer is Fill, imputer_fill_type and imputer_fill_value are applied; otherwise, they are ignored.  
If the use_imputation method is specified as True and the input data has no missing values:  
imputation_threshold is ignored. imputer candidates in imputer_list are used to search for the best imputer. If the best imputer is Fill, imputer_fill_type and imputer_fill_value are applied; otherwise, they are ignored.  
If the use_imputation method is specified as False but the input data has missing values:  
use_imputation is turned on with a warning, then the method follows the behavior for the first scenario.  
If the use_imputation method is specified as False and the input data has no missing values, then no further processing is required.  
For example:  
Parent topic: Data imputation in AutoAI experiments"
1371	Publishing a notebook as a gist	"A gist is a simple way to share a notebook or parts of a notebook with other users. Unlike when you publish to a GitHub repository, you don't need to manage your gists; you can edit your gists directly in the browser.  
All project collaborators, who have administrator or editor permission, can share notebooks or parts of a notebook as gists. The latest saved version of your notebook is published as a gist.  
Before you can create a gist, you must be logged in to GitHub and have authorized access to gists in GitHub from Watson Studio. See Publish notebooks on GitHub. If this information is missing, you are prompted for it.  
To publish a notebook as a gist:  
Open the notebook in edit mode. Click the GitHub integration icon () and select Publish as gist.  
Watch this video to see how to enable GitHub integration.  
Video disclaimer: Some minor steps and graphical elements in this video might differ from your platform.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Transcript  
Synchronize transcript with video"
1372	Publishing a notebook as a gist	"Video transcript Time Transcript 00:00 This video shows you how to publish notebooks from your Watson Studio project to your GitHub account. 00:07 Navigate to your profile and settings. 00:11 On the ""Integrations"" tab, visit the link to generate a GitHub personal access token. 00:17 Provide a descriptive name for the token and select the repo and gist scopes, then generate the token. 00:29 Copy the token, return to the GitHub integration settings, and paste the token. 00:36 The token is validated when you save it to your profile settings. 00:42 Now, navigate to your projects. 00:44 You enable GitHub integration at the project level on the ""Settings"" tab. 00:50 Simply scroll to the bottom and paste the existing GitHub repository URL. 00:56 You'll find that on the ""Code"" tab in the repo. 01:01 Click ""Update"" to make the connection. 01:05 Now, go to the ""Assets"" tab and open the notebook you want to publish. 01:14 Notice that this notebook has the credentials replaced with X's. 01:19 It's a best practice to remove or replace credentials before publishing to GitHub. 01:24 So, this notebook is ready for publishing. 01:27 You can provide the target path along with a commit message. 01:31 You also have the option to publish content without hidden code, which means that any cells in the notebook that began with the hidden cell comment will not be published. 01:42 When you're, ready click ""Publish"". 01:45 The message tells you that the notebook was published successfully and provides links to the notebook, the repository, and the commit. 01:54 Let's take a look at the commit. 01:57 So, there's the commit, and you can navigate to the repository to see the published notebook. 02:04 Lastly, you can publish as a gist. 02:07 Gists are another way to share your work on GitHub. 02:10 Every gist is a git repository, so it can be forked and cloned. 02:15 There are two types of gists: public and secret. 02:19 If you start out with a secret gist, you can convert it to a public gist later. 02:24 And again, you have the option"
1373	Publishing a notebook as a gist	02:15 There are two types of gists: public and secret. 02:19 If you start out with a secret gist, you can convert it to a public gist later. 02:24 And again, you have the option to remove hidden cells. 02:29 Follow the link to see the published gist. 02:32 So that's the basics of Watson Studio's GitHub integration. 02:37 Find more videos in the Cloud Pak for Data as a Service documentation.
1374	Publishing a notebook as a gist	Parent topic: Managing the lifecycle of notebooks and scripts
1375	Programming IBM Watson Pipelines	You can program in a pipeline by using a notebook, or running Bash scripts in a pipeline.
1376	Programming IBM Watson Pipelines	"Programming with Bash scripts
Run Bash scripts in a pipeline to compute or process data as part of the flow."
1377	Programming IBM Watson Pipelines	"Programming with notebooks
You can use a notebook to run an end-to-end pipeline or to run parts of a pipeline, such as model training.  
For details on creating notebooks and for links to sample notebooks, see Notebooks. For details on running a notebook as a pipeline job, see Run notebook job."
1378	Programming IBM Watson Pipelines	"Programming with notebooks
Using the Python client
Use the Watson Pipelines Python client for working with pipelines in a notebook.  
To install the library, use pip to install the latest package of ibm-watson-pipelines in your coding environment. For example, run the following code in your notebook environment or console.  
Use the client documentation for syntax and descriptions for commands that access pipeline components.  
Parent topic: Creating a pipeline"
1379	Creating your own models	"Certain algorithms in Watson Natural Language Processing can be trained with your own data, for example you can create custom models based on your own data for entity extraction, to classify data, to extract sentiments, and to extract target sentiments.  
Starting with Runtime 23.1 you can use the new built-in transformer-based IBM foundation model called Slate to create your own models. The Slate model has been trained on a very large data set that was preprocessed to filter hate, bias, and profanity.  
To create your own classification, entity extraction model, or sentiment model you can fine-tune the Slate model on your own data. To train the model in reasonable time, it's recommended to use GPU-based environments.  
Detecting entities with a custom dictionary Detecting entities with regular expressions Detecting entities with a custom transformer model Classifying text with a custom classification model Extracting sentiment with a custom transformer model Extracting targets sentiment with a custom transformer model"
1380	Creating your own models	"Language support for custom models
You can create custom models and use the following pretrained dictionary and classification models for the shown languages. For a list of the language codes and the corresponding languages, see Language codes."
1381	Creating your own models	"Saving and loading custom models
If you want to use your custom model in another notebook, save it as a Data Asset to your project. This way, you can export the model as part of a project export.  
Use the ibm-watson-studio-lib library to save and load custom models.  
To save a custom model in your notebook as a data asset to export and use in another project:  
custom_block = CNN.train(train_stream, embedding_model.embedding, verbose=2) # Create the custom regular expression model custom_regex_block = watson_nlp.resources.feature_extractor.RBR.train(module_folder, language='en', regexes=regexes) # Save the model to the local file system custom_regex_model_path = 'some/path' custom_regex_block.save(custom_regex_model_path) # The model was saved in a file ""executor.zip"" in the provided path, in this case ""some/path/executor.zip"" model_path = os.path.join(custom_regex_model_path, 'executor.zip') # Re-load the model as a RBRGeneric block custom_block = watson_nlp.blocks.rules.RBRGeneric(watson_nlp.toolkit.rule_utils.RBRExecutor.load(model_path), language='en') wslib.save_data(""<model name>"", custom_block.as_bytes(), overwrite=True) wslib.save_data('<model name>', data=custom_model.as_bytes(cpu_format=True), overwrite=True)  
Ensure that you have an access token on the Access control page on the Manage tab of your project. Only project admins can create access tokens. The access token can have viewer or editor access permissions. Only editors can inject the token into a notebook.  
Add the project token to a notebook by clicking More > Insert project token from the notebook action bar and then run the cell. When you run the inserted hidden code cell, a wslib object is created that you can use for functions in the ibm-waton-studio-lib library. For details on the available ibm-watson-studio-lib functions, see Using ibm-watson-studio-lib for Python.  
Run the train() method to create a custom dictionary, regular expression, or classification model and assign this custom model to a variable. For example:"
1382	Creating your own models	"Saving and loading custom models
Run the train() method to create a custom dictionary, regular expression, or classification model and assign this custom model to a variable. For example:  
If you want to save a custom dictionary or regular expression model, convert it to a RBRGeneric block. Converting a custom dictionary or regular expression model to a RBRGeneric block is useful if you want to load and execute the model using the API for Watson Natural Language Processing for Embed. To date, Watson Natural Language Processing for Embed supports running dictionary and regular expression models only as RBRGeneric blocks. To convert a model to a RBRGeneric block, run the following commands:  
Save the model as a Data Asset to your project using ibm-watson-studio-lib:  
When saving transformer models, you have the option to save the model in CPU format. If you plan to use the model only in CPU environments, using this format will make your custom model run more efficiently. To do that, set the CPU format option as follows:  
To load a custom model to a notebook that was imported from another project:  
custom_block = watson_nlp.load(wslib.load_data(""<model name>""))  
Ensure that you have an access token on the Access control page on the Manage tab of your project. Only project admins can create access tokens. The access token can have viewer or editor access permissions. Only editors can inject the token into a notebook.  
Add the project token to a notebook by clicking More > Insert project token from the notebook action bar and then run the cell. When you run the the inserted hidden code cell, a wslib object is created that you can use for functions in the ibm-watson-studio-lib library. For details on the available ibm-watson-studio-lib functions, see Using ibm-watson-studio-lib for Python.  
Load the model using ibm-watson-studio-lib and watson-nlp:  
Parent topic: Watson Natural Language Processing library"
1383	Writing deployable Python functions	"Learn how to write a Python function and then store it as an asset that allows for deploying models.  
For a list of general requirements for deployable functions refer to General requirements for deployable functions. For information on what happens during a function deployment, refer to Function deployment process"
1384	Writing deployable Python functions	"General requirements for deployable functions
To be deployed successfully, a function must meet these requirements:  
The Python function file on import must have the score function object as part of its scope. Refer to Score function requirements Scoring input payload must meet the requirements that are listed in Scoring input requirements The output payload expected as output of score must include the schema of the score_response variable for status code 200. Note that the prediction parameter, with an array of JSON objects as its value, is mandatory in the score output. When you use the Python client to save a Python function that contains a reference to an outer function, only the code in the scope of the outer function (including its nested functions) is saved. Therefore, the code outside the outer function's scope will not be saved and thus will not be available when you deploy the function."
1385	Writing deployable Python functions	"General requirements for deployable functions
Score function requirements
Two ways to add the score function object exist: The score function must accept a single, JSON input parameter. The score function must return a JSON-serializable object (for example: dictionaries or lists)  
explicitly, by user implicitly, by the method that is used to save the Python function as an asset in the Watson Machine Learning repository"
1386	Writing deployable Python functions	"General requirements for deployable functions
Scoring input requirements
{""input_data"": [{ ""values"": [[""Hello world!""]] }] }  
The scoring input payload must include an array with the name values, as shown in this example schema.  
Note: - The `input_data` parameter is mandatory in the payload.- The `input_data` parameter can also include additional name-value pairs.  
The scoring input payload must be passed as input parameter value for score. This way you can ensure that the value of the score input parameter is handled accordingly inside the score.  
The scoring input payload must match the input requirements for the concerned Python function.  
The scoring input payload must include an array that matches the Example input data schema."
1387	Writing deployable Python functions	"General requirements for deployable functions
Example Python code
It returns the message ""Hello world!""."
1388	Writing deployable Python functions	"Function deployment process
The Python code of your Function asset gets loaded as a Python module by the Watson Machine Learning engine by using an import statement. This means that the code will be executed exactly once (when the function is deployed or each time when the corresponding pod gets restarted). The score function that is defined by the Function asset is then called in every prediction request."
1389	Writing deployable Python functions	"Handling deployable functions
Use one of these methods to create a deployable Python function:  
Creating deployable functions through REST API Creating deployable functions through the Python client"
1390	Writing deployable Python functions	"Handling deployable functions
Creating deployable functions through REST API
For REST APIs, because the Python function is uploaded directly through a file, the file must already contain the score function. Any one time import that needs to be done to be used later within the score function can be done within the global scope of the file. When this file is deployed as a Python function, the one-time imports available in the global scope get executed during the deployment and later simply reused with every prediction request.  
Important:  
The function archive must be a .gz file.  
Sample score function file:  
Sample score function with one time imports:"
1391	Writing deployable Python functions	"Handling deployable functions
Creating deployable functions through the Python client
To persist a Python function as an asset, the Python client uses the wml_client.repository.store_function method. You can do that in two ways:  
Persisting a function through a file that contains the Python function Persisting a function through the function object  
This method is the same as persisting the Python function file through REST APIs (score must be defined in the scope of the Python source file). For details, refer to Creating deployable functions through REST API.  
Important:  
When you are calling the wml_client.repository.store_function method, pass the file name as the first argument.  
You can persist Python function objects by creating Python Closures with a nested function named score. The score function is returned by the outer function that is being stored as a function object, when called. This score function must meet the requirements that are listed in General requirements for deployable functions. In this case, any one time imports and initial setup logic must be added in the outer nested function so that they get executed during deployment and get used within the score function. Any recurring logic that is needed during the prediction request must be added within the nested score function.  
Sample Python function save by using the Python client:  
In this scenario, the Python function takes up the job of creating a Python file taht contains the score function and persisting the function file as an asset in the Watson Machine Learning repository:"
1392	Writing deployable Python functions	"Learn more
Python Closures Closures Nested function, Scope of variable & closures in Python  
Parent topic: Deploying Python functions"
1393	Tracking an AI asset in a notebook	"If you are using the watsonx.governance service, learn how to track and govern an AI asset, such as a machine learning model or a foundation model asset, in a notebook.  
To track an AI asset programmatically, use the Python client documentation or the IBM_AIGOV_FACTS_CLIENT documentation to register an asset and add it to an AI use case.  
The sample code demonstrates how to register and track a machine learning model."
1394	Tracking an AI asset in a notebook	"Tracking an AI asset
If your model is in a notebook, you can insert code to register the model with an AI use case.  
Create a use case:  
Create an approach in the use case:  
Retrieve the approach:  
Track the model in the use case:  
For details on using Python commands to interact with AI use cases, see Python client documentation and the IBM_AIGOV_FACTS_CLIENT documentation."
1395	Tracking an AI asset in a notebook	"Learn more
Parent topic: Tracking assets in an AI use case."
1396	Examples of environment template customizations	"You can follow examples of how to add custom libraries through conda or pip using the provided templates for Python and R when you create an environment template.  
You can use mamba in place of conda in the following examples with conda. Remember to select the checkbox to install from mamba if you add channels or packages from mamba to the existing environment template.  
Examples exist for:  
Adding conda packages Adding pip packages Combining conda and pip packages Adding complex packages with internal dependencies Adding conda packages for R notebooks Setting environment variables  
Hints and tips:  
Best practices"
1397	Examples of environment template customizations	"Adding conda packages
To get latest versions of pandas-profiling:  
This is equivalent to running conda install pandas-profiling in a notebook."
1398	Examples of environment template customizations	"Adding pip packages
You can also customize an environment using pip if a particular package is not available in conda channels:  
This is equivalent to running pip install ibm-watson-machine-learning in a notebook.  
The customization will actually do more than just install the specified pip package. The default behavior of conda is to also look for a new version of pip itself and then install it. Checking all the implicit dependencies in conda often takes several minutes and also gigabytes of memory. The following customization will shortcut the installation of pip:  
The conda channel empty does not provide any packages. There is no pip package in particular. conda won't try to install pip and will use the already pre-installed version instead. Note that the keyword nodefaults in the list of channels needs at least one other channel in the list. Otherwise conda will silently ignore the keyword and use the default channels."
1399	Examples of environment template customizations	"Combining conda and pip packages
You can list multiple packages with one package per line. A single customization can have both conda packages and pip packages.  
Note that the required template notation is sensitive to leading spaces. Each item in the list of conda packages must have two leading spaces. Each item in the list of pip packages must have four leading spaces. The version of a conda package must be specified using a single equals symbol (=), while the version of a pip package must be added using two equals symbols (==)."
1400	Examples of environment template customizations	"Adding complex packages with internal dependencies
When you add many packages or a complex package with many internal dependencies, the conda installation might take long or might even stop without you seeing any error message. To avoid this from happening:  
Specify the versions of the packages you want to add. This reduces the search space for conda to resolve dependencies. Increase the memory size of the environment. Use a specific channel instead of the default conda channels that are defined in the .condarc file. This avoids running lengthy searches through big channels.  
Example of a customization that doesn't use the default conda channels:  
This customization corresponds to the following command in a notebook:"
1401	Examples of environment template customizations	"Adding conda packages for R notebooks
The following example shows you how to create a customization that adds conda packages to use in an R notebook:  
This customization corresponds to the following command in a notebook:  
The names of R packages in conda generally start with the prefix r-. If you just use plotly in your customization, the installation would succeed but the Python package would be installed instead of the R package. If you then try to use the package in your R code as in library(plotly), this would return an error."
1402	Examples of environment template customizations	"Setting environment variables
You can set environment variables in your environment by adding a variables section to the software customization template as shown in the following example:  
The example also shows that you can use the variables section to set a proxy server for an environment.  
Limitation: You cannot override existing environment variables, for example LD_LIBRARY_PATH, using this approach."
1403	Examples of environment template customizations	"Best practices
To avoid problems that can arise finding packages or resolving conflicting dependencies, start by installing the packages you need manually through a notebook in a test environment. This enables you to check interactively if packages can be installed without errors. After you have verified that the packages were all correctly installed, create a customization for your development or production environment and add the packages to the customization template.  
Parent topic: Customizing environments"
1404	Getting started with the Watson Pipelines editor	"The Watson Pipelines editor is a graphical canvas where you can drag and drop nodes that you connect together into a pipeline for automating machine model operations.  
You can open the Pipelines editor by creating a new Pipelines asset or editing an existing Pipelines asset. To create a new asset in your project from the Assets tab, click New asset > Automate model lifecycle. To edit an existing asset, click the pipeline asset name on the Assets tab.  
The canvas opens with a set of annotated tools for you to use to create a pipeline. The canvas includes the following components:  
The node palette provides nodes that represent various actions for manipulating assets and altering the flow of control in a pipeline. For example, you can add nodes to create assets such as data files, AutoAI experiments, or deployment spaces. You can configure node actions based on conditions if files import successfully, such as feeding data into a notebook. You can also use nodes to run and update assets. As you build your pipeline, you connect the nodes, then configure operations on the nodes to create the pipeline. These pipelines create a dynamic flow that addresses specific stages of the machine learning lifecycle. The toolbar includes shortcuts to options related to running, editing, and viewing the pipeline. The parameters pane provides context-sensitive options for configuring the elements of your pipeline."
1405	Getting started with the Watson Pipelines editor	"The toolbar
Use the Pipeline editor toolbar to:  
Run the pipeline as a trial run or a scheduled job View the history of pipeline runs Cut, copy, or paste canvas objects Delete a selected node Drop a comment onto the canvas Configure global objects, such as pipeline parameters or user variables Manage default settings Arrange nodes vertically View last saved timestamp Zoom in or out Fit the pipeline to the view Show or hide global messages  
Hover over an icon on the toolbar to view the shortcut text."
1406	Getting started with the Watson Pipelines editor	"The node palette
The node palette provides the objects that you need to create an end-to-end pipeline. Click a top-level node in the palette to see the related nodes."
1407	Getting started with the Watson Pipelines editor	"The parameters pane
Double-click a node to edit its configuration options. Depending on the type, a node can define various input and output options or even allow the user to add inputs or outputs dynamically. You can define the source of values in various ways. For example, you can specify that the source of value for ""ML asset"" input for a batch deployment must be the output from a run notebook node.  
For more information on parameters, see Configuring pipeline components."
1408	Getting started with the Watson Pipelines editor	"Next steps
Planning a pipeline Explore the sample pipeline Create a pipeline  
Parent topic: Watson Pipelines"
1409	Requirements for using custom components in ML models	You can define your own transformers, estimators, functions, classes, and tensor operations in models that you deploy in IBM Watson Machine Learning as online deployments.
1410	Requirements for using custom components in ML models	"Defining and using custom components
To use custom components in your models, you need to package your custom components in a Python distribution package."
1411	Requirements for using custom components in ML models	"Defining and using custom components
Package requirements
The package type must be: source distribution (distributions of type Wheel and Egg are not supported) The package file format must be: .zip Any third-party dependencies for your custom components must be installable by pip and must be passed to the install_requires argument of the setup function of the setuptools library.  
Refer to: Creating a source distribution"
1412	Requirements for using custom components in ML models	"Defining and using custom components
Storing your custom package
You must take extra steps when you store your trained model in the Watson Machine Learning repository:  
Store your custom package in the Watson Machine Learning repository (use the runtimes.store_library function from the Watson Machine Learning Python client, or the store libraries Watson Machine Learning CLI command.) Create a runtime resource object that references your stored custom package, and then store the runtime resource object in the Watson Machine Learning repository (use the runtimes.store function, or the store runtimes command.) When you store your trained model in the Watson Machine Learning repository, reference your stored runtime resource in the metadata that is passed to the store_model function (or the store command.)"
1413	Requirements for using custom components in ML models	"Supported frameworks
These frameworks support custom components:  
Scikit-learn XGBoost Tensorflow Python Functions Python Scripts Decision Optimization  
For more information, see Supported frameworks  
   
Parent topic: Customizing deployment runtimes"
1414	IBM slate-125m-english-rtrvr model card	"Model description
The slate.125m.english.rtrvr model is a standard sentence transformers model based on bi-encoders. The model produces an embedding for a given input e.g. query, passage, document etc. At a high level, our model is trained to maximize the cosine similarity between two input pieces of text e.g. text A (query text) and text B (passage text), which result in the sentence embeddings q and p. These sentence embeddings can then be compared using cosine similarity.  
Figure 1. Bi-encoder Embeddings Model for Retrieval"
1415	IBM slate-125m-english-rtrvr model card	"Base language model
The underlying Language Model (LM) for our embeddings is slate.125m.english (formerly, known as WatBERT). It has the same architecture as a RoBERTa base transformer model and has ~125 million parameters and an embedding dimension of 768. Our final model is called “slate.125m.english.rtrvr” - notice the suffix at the end denoting that we fine-tune the underlying model architecture for retrieval-based tasks."
1416	IBM slate-125m-english-rtrvr model card	"Training algorithm
Most embedding models that are either state-of-the-art or at the top of the MTEB leaderboard are typically trained in 3 stages:  
Task Specific (retrieval-based) pre-training Task specific fine-tuning on mined pairs Fine-tuning on supervised pairs.  
We follow the same approach and finally perform a model fusion by averaging the weights of different trained models.  
slate.125m.english.rtrvr is produced by performing “model fusion” - averaging the weights of the following models, both trained in stages but having the following variations:  
Model 1 fine-tuned with large scale unsupervised data Model 2 fine-tuned with a smaller subset of supervised data"
1417	IBM slate-125m-english-rtrvr model card	"Training algorithm
Task-specific pre-training
This stage uses the RetroMAE framework, to make our underlying LM more retrieval oriented. We initialize our base LM with slate.125m.english and continue with RetroMAE pre-training, using the data in Table 1. Our hyper-parameters are: learning rate: 2e-5, number of steps: 190000, GPUs: 24 A100 40GB. Note: this is our base LM for the following 2 stages."
1418	IBM slate-125m-english-rtrvr model card	"Training algorithm
Model1: Fine-tuning with large scale unsupervised data
This model is initialized with the RetroMAE pre-trained model and is trained in 2 stages.  
We use a bi-encoder framework for training an embedding model, as in Figure 1. The RetroMAE pre-trained LM is fine-tuned with <query, passage> text pairs using a contrastive loss objective. We mine large scale pairs from various domains, as indicated in Table 2. The model is trained with diverse pairs, including classification tasks such as NLI (Natural Language Inference) which consists of matching a premise to the corresponding hypothesis. Our hyper-parameters are: learning rate: 2e-5; number of steps: 140000; GPUs: 8 A100_80GB, effective batch size: 4096 pairs  
Finally, the model is fine-tuned with high-quality supervised training pairs for the retrieval task on the following datasets: SQuAD, Natural Questions, Specter, Stack Exchange (Title, Body) pairs, S2ORC, SearchQA, HotpotQA and Fever. Training hyper-parameters are learning rate: 2e-5; number of steps: 10000; GPUs: 8 A100_80GB, effective batch size: 4096 pairs."
1419	IBM slate-125m-english-rtrvr model card	"Training algorithm
Model 2: Fine-tuning with a more task-focused subset
In this stage, the RetroMAE pre-trained model undergoes supervised finetuning with a smaller subset of Table2 with supervision coming from hard negative mining. The intermediate model checkpoints are iteratively used to mine dataset specific hard negatives, which are then used for supervised finetuning. This process aims to make the model more robust by letting it learn from its own mistakes and helps in stabilizing with much smaller data.  
We fine-tune the model by using a subset of datasets (as found via performing validation experiments on a held-out dataset) mentioned in Table2 which are as follows: AllNLI, Squad, Stackexchange, NQ, HotpotQA, Fever and 5M subset from each of Specter, S2orc, WikiAnswers.  
Training hyper-parameters are learning rate: 2e-5; max query length: 512; max passage length: 512; epochs: 2; effective batch size: 384 triples; GPUs: 24 A100_80GB."
1420	IBM slate-125m-english-rtrvr model card	"Training algorithm
Our final model: slate.125m.english.rtrvr: Model fusion
We perform model fusion by averaging the model weights (from the above) trained with large scale unsupervised data and the model trained with the smaller subset of supervised data.  
We use a dev set (https://huggingface.co/datasets/colbertv2/lotte) and perform grid search for obtaining the optimal weight combination for these models. We average the model weights based on the optimal param: 0.7 for Model1 and 0.3 for Model2."
1421	IBM slate-125m-english-rtrvr model card	"Usage
The maximum sequence length of this model is 512 tokens."
1422	IBM slate-125m-english-rtrvr model card	"Evaluation
Baselines
For a fair comparison, we compare with the following baselines:  
BM25 (a traditional model based on tf-idf). ELSER (a commercial search algorithm provided by Elastic). all-MiniLM-l6-v2: a popular open-source sentence transformers model. This model shares the same architecture as slate.125m.english.rtvr, with a smaller embedding dimension and has been trained on more data without commercial-friendly licenses. Huggingface model card (https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) for more details. E5-base: a recent open-source transformer model with very good performance on the BEIR benchmark. This is a base-sized model, which has the same architecture as slate.125m.english.rtvr. [Reference: Wang et.al., 2022: Text Embeddings by Weakly-Supervised Contrastive Pre-training]. Huggingface model card (https://huggingface.co/intfloat/e5-base). E5-small: a smaller model within the open source E5 family. The embedding dimension of this model matches that of all-minilm-l6-v2 (384), however it has 12 layers, and thus is larger and slightly slower. [Reference: Wang et.al., 2022: Text Embeddings by Weakly-Supervised Contrastive Pre-training]. Huggingface model card (https://huggingface.co/intfloat/e5-small). BGE-base: a recent open-source transformer model with the best performance on the BEIR benchmark for the 768 embedding size (as of 01.20.2024). Huggingface model card (https://huggingface.co/BAAI/bge-base-en-v1.5)."
1423	IBM slate-125m-english-rtrvr model card	"Evaluation
Our Evaluation benchmark: BEIR (MTEB’s retrieval tab)
The BEIR benchmark contains 15 open-source retrieval tasks focused on different domains including nine different retrieval tasks: Fact checking, citation prediction, duplicate question retrieval, argument retrieval, news retrieval, question answering, tweet retrieval, bio-medical IR, and entity retrieval. Further, it includes datasets from diverse text domains, datasets that cover broad topics (like Wikipedia) and specialized topics (like COVID-19 publications), different text types (news articles vs. Tweets), datasets of various sizes (3.6k - 15M documents), and datasets with different query lengths (average query length between 3 and 192 words) and document lengths (average document length between 11 and 635 words). The performance of all models are noted in the table below. BEIR uses the Normalized Cumulative Discount Gain (specifically, nDCG@10) metric for evaluation. This is the same evaluation that is used in the HuggingFace MTEB leaderboard but mainly focusing on retrieval tasks."
1424	IBM slate-125m-english-rtrvr model card	"Evaluation
Long NQ
Long NQ is an IBM dataset designed for evaluating the full RAG pipeline, based on a subset of the NaturalQuestions dataset. The dev set has 300 answerable questions with a corpus of 178,891 passages from 2,345 Wikipedia documents. Long NQ also provides gold Wikipedia passages that are relevant for each question. During retrieval, the task is to obtain the relevant gold passage from the corpus for every question. The DCT code for Long NQ is 65971bbc1953f0115c1bbd77."
1425	IBM slate-125m-english-rtrvr model card	"Evaluation
Results
Figure 2. Performance comparison on the BEIR benchmark (MTEB retrieval tab)  
Figure 3. Performance comparison on the Long NQ dataset"
1426	IBM slate-125m-english-rtrvr model card	"Runtime Performance
The performance runtime is measured on a re-ranking task with 466 queries. For each query we re-rank the top-100 passages obtained by BM25 and we report the average time over all queries. The re-ranking was performed on a A100_40GB GPU."
1427	Notebooks and scripts	You can create, edit and execute Python and R code using Jupyter notebooks and scripts in code editors, for example the notebook editor or an integrated development environment (IDE), like RStudio.
1428	Notebooks and scripts	"Notebooks
See Data load support. for the supported file and database types."
1429	Notebooks and scripts	"Scripts
See Data load support. for the supported file and database types."
1430	Notebooks and scripts	"Working in the notebook editor
The notebook editor is largely used for interactive, exploratory data analysis programming and data visualization. Only one person can edit a notebook at a time. All other users can access opened notebooks in view mode only, while they are locked.  
You can use the preinstalled open source libraries that come with the notebook runtime environments, add your own libraries, and benefit from the IBM libraries provided at no extra cost.  
When your notebooks are ready, you can create jobs to run the notebooks directly from the notebook editor. Your job configurations can use environment variables that are passed to the notebooks with different values when the notebooks run."
1431	Notebooks and scripts	"Working in RStudio
RStudio is an integrated development environment for working with R scripts or Shiny apps. Although the RStudio IDE cannot be started in a Spark with R environment runtime, you can use Spark in your R scripts and Shiny apps by accessing Spark kernels programmatically.  
R scripts and Shiny apps can only be created and used in the RStudio IDE. You can't create jobs for R scripts or R Shiny deployments."
1432	Notebooks and scripts	"Learn more
Quick start: Analyze data in a Jupyter notebook RStudio Notebook editor  
Parent topic: Analyzing data and building models"
1433	Creating a text analysis experiment	"Use AutoAI's text analysis feature to perform text analysis of your experiments. For example, perform basic sentiment analysis to predict an outcome based on text comments.  
Note: Text analysis is only available for AutoAI classification and regression experiments. This feature is not available for time series experiments."
1434	Creating a text analysis experiment	"Text analysis overview
When you create an experiment that uses the text analysis feature, the AutoAI process uses the word2vec algorithm to transform the text into vectors, then compares the vectors to establish the impact on the prediction column.  
The word2vec algorithm takes a corpus of text as input and outputs a set of vectors. By turning text into a numerical representation, it can detect and compare similar words. When trained with enough data, word2vec can make accurate predictions about a word's meaning or relationship to other words. The predictions can be used to analyze text and guess at the meaning in sentiment analysis applications.  
During the feature engineering phase of the experiment training, 20 features are generated for the text column, by using the word2vec algorithm. Auto-detection of text features is based on analyzing the number of unique values in a column and the number of tokens in a record (minimum number = 3). If the number of unique values is less than number of all values divided by 5, the column is not treated as text.  
When the experiment completes, you can review the feature engineering results from the pipeline details page. You can also save a pipeline as a notebook, where you can review the transformations and see a visualization of the transformations.  
Note: When you review the experiment, if you determine that a text column was not detected and processed by the auto-detection, you can specify the text column manually in the experiment settings."
1435	Creating a text analysis experiment	"Example: Analyzing customer comments
In this example, the comments for a fictional car rental company are used to train a model that predicts a satisfaction rating when a new comment is entered.  
Watch this short video to see this example and then read further details about the text feature below the video.  
Video disclaimer: Some minor steps and graphical elements in this video might differ from your platform.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Transcript  
Synchronize transcript with video"
1436	Creating a text analysis experiment	"Example: Analyzing customer comments
Video transcript Time Transcript 00:00 In this video you'll see how to create an AutoAI experiment to perform sentiment analysis on a text file. 00:09 You can use the text feature engineering to perform text analysis in your experiments. 00:15 For example, perform basic sentiment analysis to predict an outcome based on text comments. 00:22 Start in a project and add an asset to that project, a new AutoAI experiment. 00:29 Just provide a name, description, select a machine learning service, and then create the experiment. 00:38 When the AutoAI experiment builder displays, you can add the data set. 00:43 In this case, the data set is already stored in the project as a data asset. 00:48 Select the asset to add to the experiment. 00:53 Before continuing, preview the data. 00:56 This data set has two columns. 00:59 The first contains the customers' comments and the second contains either 0, for ""Not satisfied"", or 1, for ""Satisfied"". 01:08 This isn't a time series forecast, so select ""No"" for that option. 01:13 Then select the column to predict, which is ""Satisfaction"" in this example. 01:19 AutoAI determines that the satisfaction column contains two possible values, making it suitable for a binary classification model. 01:28 And the positive class is 1, for ""Satisfied"". 01:32 Open the experiment settings if you'd like to customize the experiment. 01:36 On the data source panel, you'll see some options for the text feature engineering. 01:41 You can automatically select the text columns, or you can exercise more control by manually specifying the columns for text feature engineering. 01:52 You can also select how many vectors to create for each column during text feature engineering. 01:58 A lower number faster and a higher number is more accurate, but slower. 02:03 Now, run the experiment to view the transformations and progress. 02:09 When you create an experiment that uses the text analysis feature, the AutoAI process uses the word2vec algorithm to transform the text into vectors, then compares the vectors to establish the impact on the prediction column. 02:23 During the feature engineering phase of the experiment training,"
1437	Creating a text analysis experiment	"Example: Analyzing customer comments
uses the text analysis feature, the AutoAI process uses the word2vec algorithm to transform the text into vectors, then compares the vectors to establish the impact on the prediction column. 02:23 During the feature engineering phase of the experiment training, twenty features are generated for the text column using the word2vec algorithm. 02:33 When the experiment completes, you can review the feature engineering results from the pipeline details page. 02:40 On the Features summary panel, you can review the text transformations. 02:45 You can see that AutoAI created several text features by applying the algorithm function to the column elements, along with the feature importance showing which features contribute most to your prediction output. 02:59 You can save this pipeline as a model or as a notebook. 03:03 The notebook contains the code to see the transformations and visualizations of those transformations. 03:09 In this case, create a model. 03:13 Use the link to view the model. 03:16 Now, promote the model to a deployment space. 03:23 Here are the model details, and from here you can deploy the model. 03:28 In this case, it will be an online deployment. 03:36 When that completes, open the deployment. 03:39 On the test app, you can specify one or more comments to analyze. 03:46 Then, click ""Predict"". 03:49 The first customer is predicted not to be satisfied with the service. 03:54 And the second customer is predicted to be satisfied with the service. 03:59 Find more videos in the Cloud Pak for Data as a Service documentation."
1438	Creating a text analysis experiment	"Example: Analyzing customer comments
Given a data set that contains a column of review comments for the rental experience (Customer_service), and a column that contains a binary satisfaction rating (Satisfaction) where 0 represents a negative comment and 1 represents a positive comment, the experiment is trained to predict a satisfaction rating when new feedback is entered."
1439	Creating a text analysis experiment	"Example: Analyzing customer comments
Training a text transformation experiment
After you load the data set and specify the prediction column (Satisfaction), the Experiment settings selects the Use text feature engineering option.  
Note some of the details for tuning your text analysis experiment:  
You can accept the default selection of automatically selecting the text columns or you can exercise more control by manually specifying the columns for text feature engineering. As the experiment runs, a default of 20 features is generated for the text column by using the word2vec algorithm. You can edit that value to increase or decrease the number of features. The more vectors that you generate the more accurate your model are, but the longer training takess. The remainder of the options applies to all types of experiments so you can fine-tune how to handle the final training data.  
Run the experiment to view the transformations in progress.  
Select the name of a pipeline, then click Feature summary to review the text transformations.  
You can also save the experiment pipeline as a notebook and review the transformations as a visualization."
1440	Creating a text analysis experiment	"Example: Analyzing customer comments
Deploying and scoring a text transformation model
When you score this model, enter new comments to get a prediction with a confidence score for whether the comment results in a positive or negative satisfaction rating.  
For example, entering the comment ""It took us almost three hours to get a car. It was absurd"" predicts a satisfaction rating of 0 with a confidence score of 95%."
1441	Creating a text analysis experiment	"Next steps
Building a time series forecast experiment  
Parent topic: Building an AutoAI model"
1442	Watson Machine Learning Python client samples and examples	Review and use sample Jupyter Notebooks that use Watson Machine Learning Python library to demonstrate machine learning features and techniques. Each notebook lists learning goals so you can find the one that best meets your goals.
1443	Watson Machine Learning Python client samples and examples	"Training and deploying models from notebooks
If you choose to build a machine learning model in a notebook, you must be comfortable with coding in a Jupyter Notebook. A Jupyter Notebook is a web-based environment for interactive computing. You can run small pieces of code that process your data, and then immediately view the results of your computation. Using this tool, you can assemble, test, and run all of the building blocks you need to work with data, save the data to Watson Machine Learning, and deploy the model."
1444	Watson Machine Learning Python client samples and examples	"Learn from sample notebooks
Many ways exist to build and train models and then deploy them. Therefore, the best way to learn is to look at annotated samples that step you through the process by using different frameworks. Review representative samples that demonstrate key features.  
The samples are built by using the V4 version of the Watson Machine Learning Python client library.  
Video disclaimer: Some minor steps and graphical elements in the videos might differ from your deployment.  
Watch this video to learn how to train, deploy, and test a machine learning model in a Jupyter Notebook. This video mirrors the Use scikit-learn to recognize hand-written digits found in the Deployment samples table.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Watch this video to learn how to test a model that was created with AutoAI by using the Watson Machine Learning APIs in Jupyter Notebook.  
This video provides a visual method to learn the concepts and tasks in this documentation."
1445	Watson Machine Learning Python client samples and examples	"Learn from sample notebooks
Helpful variables
Use the pre-defined PROJECT_ID environment variable to call the Watson Machine Learning Python client APIs. PROJECT_ID is the guide of the project where your environment is running."
1446	Watson Machine Learning Python client samples and examples	"Deployment samples
View or run these Jupyter Notebooks to see how techniques are implemented by using various frameworks. Some of the samples rely on trained models, which are also available for you to download from the public repository."
1447	Watson Machine Learning Python client samples and examples	"AutoAI samples
View or run these Jupyter Notebooks to see how AutoAI model techniques are implemented."
1448	Watson Machine Learning Python client samples and examples	"Next steps
To learn more about using notebook editors, see Notebooks.  
To learn more about working with notebooks, see Coding and running notebooks.  
To learn more about authenticating in a notebook, see Authentication.  
Parent topic: Managing predictive deployments"
1449	Creating jobs in deployment spaces	A job is a way of running a batch deployment, or a self-contained asset like a script, notebook, code package, or flow in Watson Machine Learning. You can select the input and output for your job and choose to run it manually or on a schedule. From a deployment space, you can create, schedule, run, and manage jobs.
1450	Creating jobs in deployment spaces	"Creating a batch deployment job
Follow these steps when you are creating a batch deployment job:  
Important: You must have an existing batch deployment to create a batch job.  
From the Deployments tab, select your deployment and click New job. The Create a job dialog box opens. In the Define details section, enter your job name, an optional description, and click Next. In the Configure section, select a hardware specification. You can follow these steps to optionally configure environment variables and job run retention settings: Optional: In the Schedule section, toggle the Schedule off button to schedule a run. You can set a date and time for start of schedule and set a schedule for repetition. Click Next.  
Optional: If you are deploying a Python script, an R script, or a notebook, then you can enter environment variables to pass parameters to the job. Click Environment variables to enter the key - value pair. Optional: To avoid finishing resources by retaining all historical job metadata, follow one of these options:  
Click By amount to set thresholds for saving a set number of job runs and associated logs. Click By duration (days) to set thresholds for saving artifacts for a specified number of days.  
Note: If you don't specify a schedule, the job runs immediately.  
Optional: In the Notify section, toggle the Off button to turn on notifications associated with this job. Click Next.  
Note: You can receive notifications for three types of events: success, warning, and failure.  
In the Choose data section, provide inline data that corresponds with your model schema. You can provide input in JSON format. Click Next. See Example JSON payload for inline data. In the Review and create section, verify your job details, and click Create and run.  
Notes:  
Scheduled jobs display on the Jobs tab of the deployment space.  
Results of job runs are written to the specified output file and saved as a space asset.  
A data asset can be a data source file that you promoted to the space, a connected data source, or tables from databases and files from file-based data sources.  
If you exclude certain weekdays in your job schedule, the job might not run as you would expect. The reason is due to a discrepancy between the time zone of the user who creates the schedule, and the time zone of the main node where the job runs."
1451	Creating jobs in deployment spaces	"Creating a batch deployment job
When you create or modify a scheduled job, an API key is generated. Future runs use this generated API key."
1452	Creating jobs in deployment spaces	"Queuing and concurrent job executions
The maximum number of concurrent jobs for each deployment is handled internally by the deployment service. For batch deployment, by default, two jobs can be run concurrently. Any deployment job request for a batch deployment that already has two running jobs is placed in a queue for execution later. When any of the running jobs is completed, the next job in the queue is run. The queue has no size limit."
1453	Creating jobs in deployment spaces	"Limitation on using large inline payloads for batch deployments
Batch deployment jobs that use large inline payload might get stuck in starting or running state. For more information, see  
Tip: If you provide huge payloads to batch deployments, use data references instead of inline."
1454	Creating jobs in deployment spaces	"Retention of deployment job metadata
Job-related metadata is persisted and can be accessed until the job and its deployment are deleted."
1455	Creating jobs in deployment spaces	"Viewing deployment job details
When you create or view a batch job, the deployment ID and the job ID are displayed.  
The deployment ID represents the deployment definition, including the hardware and software configurations and related assets. The job ID represents the details for a job, including input data and an output location and a schedule for running the job.  
Use these IDs to refer to the job in Watson Machine Learning REST API requests or in notebooks that use the Watson Machine Learning Python client library.  
Parent topic: Managing predictive deployments"
1456	Managing the Watson Machine Learning service endpoint	"You can use IBM Cloud connectivity options for accessing cloud services securely by using service endpoints. When you provision a Watson Machine Learning service instance, you can choose if you want to access your service through the public internet, which is the default setting, or over the IBM Cloud private network.  
For more information, refer to IBM Cloud service endpoints.  
You can use the Service provisioning page to choose a default endpoint from the following options:  
Public network Private network Both, public and private networks"
1457	Managing the Watson Machine Learning service endpoint	"Public network
You can use public network endpoints to connect to Watson Machine Learning service instance on the public network. Your environment needs to have internet access to connect."
1458	Managing the Watson Machine Learning service endpoint	"Private network
You can use private network endpoints to connect to your IBM Watson Machine Learning service instance over the IBM Cloud Private network. After you configure your Watson Machine Learning service to use private endpoints, the service is not accessible from the public internet."
1459	Managing the Watson Machine Learning service endpoint	"Private network
Private URLs for Watson Machine Learning
Private URLs for Watson Machine Learning for each region are as follows:  
Dallas - https://private.us-south.ml.cloud.ibm.com London - https://private.eu-gb.ml.cloud.ibm.com Frankfurt - https://private.eu-de.ml.cloud.ibm.com Tokyo - https://private.jp-tok.ml.cloud.ibm.com"
1460	Managing the Watson Machine Learning service endpoint	"Using IBM Cloud service endpoints
Follow these steps to enable private network endpoints on your clusters:  
Use IBM Cloud CLI to enable your account to use IBM Cloud service endpoints. Provision a Watson Machine Learning service instance with private endpoints."
1461	Managing the Watson Machine Learning service endpoint	"Provisioning with service endpoints
You can provision a Watson Machine Learning service instance with service endpoint by using IBM Cloud UI or IBM Cloud CLI."
1462	Managing the Watson Machine Learning service endpoint	"Provisioning with service endpoints
Provisioning a service endpoint with IBM Cloud UI
To configure the endpoints of your IBM Watson Machine Learning service instance, you can use the Endpoints field on the IBM Cloud catalog page. You can configure a public, private, or a mixed network."
1463	Managing the Watson Machine Learning service endpoint	"Provisioning with service endpoints
IBM Cloud CLI
If you provision an IBM Watson Machine Learning service instance by using the IBM Cloud CLI, use the command-line option service-endpoints to configure the Watson Machine Learning endpoints. You can specify the value public (the default value), private, or public-and-private:  
For example:  
or  
Parent topic: First steps"
1464	Installing custom libraries through notebooks	"The prefered way of installing additional Python libraries to use in a notebook is to customize the software configuration of the environment runtime associated with the notebook. You can add the conda or PyPi packages through a customization template when you customize the environment template.  
See Customizing environment templates.  
However, if you want to install packages from somewhere else or packages you created on your local machine, for example, you can install and import the packages through the notebook.  
To install packages other than conda or PyPi packages through your notebook:  
# @hidden_cell # The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs. from project_lib import Project project = Project(project_id='7c7a9455-1916-4677-a2a9-a61a75942f58', project_access_token='p-9a4c487075063e610471d6816e286e8d0d222141') pc = project.project_context # Fetch the library file, for example the tar.gz or whatever installable distribution you created with open(""xxx-0.1.tar.gz"",""wb"") as f: f.write(project.get_file(""xxx-0.1.tar.gz"").read()) # Install the library !pip install xxx-0.1.tar.gz import xxx  
Add the package to your project storage by clicking the Upload asset to project icon (), and then browsing the package file or dragging it into your notebook sidebar.  
Add a project token to the notebook by clicking More > Insert project token from the notebook action bar. The code that is generated by this action initializes the variable project, which is required to access the library you uploaded to object storage.  
Example of an inserted project token:  
If you don't have a token, you need to create one. See Adding a project token.  
Install the library:  
Now you can import the library:  
Parent topic: Libraries and scripts"
1465		Review these considerations as you plan for how you will connect to resources, add assets, and manage resources to your pipeline.
1466		"Accessing the components in your pipeline
When you use a pipeline to automate a flow, you must have access to all of the elements in the pipeline. Make sure that you create and run pipelines with the proper access to all assets, projects, and spaces used in the pipeline. Collaborators who run the pipeline must also be able to access the pipeline components."
1467		"Accessing the components in your pipeline
Managing pipeline credentials
To run a job, the pipeline must have access to IBM Cloud credentials. Typically, a pipeline uses your personal IBM Cloud API key to execute long-running operations in the pipeline without disruption. If credentials are not available when you create the job, you are prompted to supply an API key or create a new one.  
To generate an API key from your IBM Cloud user account, go to Manage access and users - API Keys and create or select an API key for your user account.  
You can also generate and rotate API keys from Profile and settings > User API key. For more information, see Managing the user API key.  
Alternatively, you can request that a key is generated for the pipeline. In either scenario, name and copy the key, protecting it as you would a password."
1468		"Adding assets to a pipeline
When you create a pipeline, you add assets, such as data, notebooks, deployment jobs, or Data Refinery jobs to the pipeline to orchestrate a sequential process. The strongly recommended method for adding assets to a pipeline is to collect the assets in the project containing the pipeline and use the asset browser to select project assets for the pipeline.  
Attention: Although you can include assets from other projects, doing so can introduce complexities and potential problems in your pipeline and could be prohibited in a future release. The recommended practice is to use assets from the current project.  
You cannot access the file's object key directly, only within the pipeline's UI. You can generally access any file within a project, space, or catalog by either browsing for the file or referring to its path. For example My Project/foo.txt means the file ""foo.txt"" is stored in a project called ""My Project"".  
Parent topic: Getting started with Watson Pipelines"
1469	Batch deployment input details for Spark models	"Follow these rules when you are specifying input details for batch deployments of Spark models.  
Data type summary table:  
Parent topic: Batch deployment input details by framework"
1470	Governing assets in AI use cases	"Create an AI use case to track and govern AI assets from request through production. Factsheets capture details about the asset for each stage of the AI lifecycle to help you meet governance and compliance goals.  
To learn about AI use cases, you can use the Getting started with watsonx.governance sample project. Assets in the sample prompt include prompt templates for a car insurance claim processing use case. The prompts use car insurance claims as input and then use large language models to help insurance agents process the claims. One prompt summarizes claims, another prompt extracts key information such as make and model, and the last prompt generates suggestions for the insurance agent.  
There are two ways that you can use the project:  
Take the Evaluate and track a prompt template tutorial. Access the Getting started with watsonx.governance sample project in the Resource hub."
1471	Governing assets in AI use cases	"Get started with AI use cases
Set up or work with AI use cases:  
Create an inventory for storing AI use cases Set up an AI use case Track assets in an AI use case View factsheets for tracked assets  
Parent topic: Governing AI assets"
1472	Saving an AutoAI generated notebook	To view the code that created a particular experiment, or interact with the experiment programmatically, you can save an experiment as a notebook. You can also save an individual pipeline as a notebook so that you can review the code that is used in that pipeline.
1473	Saving an AutoAI generated notebook	"Working with AutoAI-generated notebooks
When you save an experiment or a pipeline as notebook, you can:  
Access the saved notebooks from the Notebooks section on the Assets tab. Review the code to understand the transformations applied to build the model. This increases confidence in the process and contributes to explainable AI practices. Enter your own authentication credentials by using the template provided. Use and run the code within Watson Studio, or download the notebook code to use in another notebook server. No matter where you use the notebook, it automatically installs all required dependencies, including libraries for: View the training data used to train the experiment and the test (holdout) data used to validate the experiment.  
xgboost lightgbm scikit-learn autoai-libs ibm-watson-machine-learning snapml  
Notes:  
Auto-generated notebook code excutes successfully as written. Modifying the code or changing the input data can adversely affect the code. If you want to make a significant change, consider retraining the experiment by using AutoAI. For more information on the estimators, or algorithms, and transformers that are applied to your data to train an experiment and create pipelines, refer to Implementation details."
1474	Saving an AutoAI generated notebook	"Saving an experiment as a notebook
Save all of the code for an experiment to view the transformations and optimizations applied to create the model pipelines."
1475	Saving an AutoAI generated notebook	"Saving an experiment as a notebook
What is included with the experiment notebook
The experiment notebook provides annotated code so you can:  
Interact with trained model pipelines Access model details programmatically (including feature importance and machine learning metrics). Visualize each pipeline as a graph, with each node documented, to provide transparency Compare pipelines Download selected pipelines and test locally Create a deployment and score the model Get the experiment definition or configuration in Python API, which you can use for automation or integration with other applications."
1476	Saving an AutoAI generated notebook	"Saving an experiment as a notebook
Saving the code for an experiment
To save an entire experiment as a notebook:  
After the experiment completes, click Save code from the Progress map panel. Name your notebook, add an optional description, choose a runtime environment, and save. Click the link in the notification to open the notebook and review the code. You can also open the notebook from the Notebooks section of the Assets tab of your project."
1477	Saving an AutoAI generated notebook	"Saving an individual pipeline as a notebook
Save an individual pipeline as a notebook so you can review the Scikit-Learn source code for the trained model in a notebook.  
Note: Currently, you cannot generate a pipeline notebook for an experiment with joined data sources."
1478	Saving an AutoAI generated notebook	"Saving an individual pipeline as a notebook
What is included with the pipeline notebook
The experiment notebook provides annotated code that you can use to complete these tasks:  
View the Scikit-learn pipeline definition See the transformations applied for pipeline training Review the pipeline evaluation"
1479	Saving an AutoAI generated notebook	"Saving an individual pipeline as a notebook
Saving a pipeline as a notebook
To save a pipeline as a notebook:  
Complete your AutoAI experiment. Select the pipeline that you want to save in the leaderboard, and click Save from the action menu for the pipeline, then Save as notebook. Name your notebook, add an optional description, choose a runtime environment, and save. Click the link in the notification to open the notebook and review the code. You can also open the notebook from the Notebooks section of the Assets tab."
1480	Saving an AutoAI generated notebook	"Create sample notebooks
To see for yourself what AutoAI-generated notebooks look like:  
Follow the steps in AutoAI tutorial to create a binary classification experiment from sample data. After the experiment runs, click Save code in the experiment details panel. Name and save the experiment notebook. To save a pipeline as a model, select a pipeline from the leaderboard, then click Save and Save as notebook. Name and save the pipeline notebook. From Assets tab, open the resulting notebooks in the notebook editor and review the code."
1481	Saving an AutoAI generated notebook	"Additional resources
For details on the methods used in the code, see Using AutoAI libraris with Python. For more information on AutoAI notebooks, see this blog post."
1482	Saving an AutoAI generated notebook	"Next steps
Using autoai-lib for Python  
Parent topic: AutoAI overview"
1483	AutoAI implementation details	"AutoAI automatically prepares data, applies algorithms, or estimators, and builds model pipelines that are best suited for your data and use case.  
The following sections describe some of these technical details that go into generating the pipelines and provide a list of research papers that describe how AutoAI was designed and implemented.  
Preparing the data for training (pre-processing) Automated model selection Algorithms used for classification models Algorithms used for regression models Metrics by model type Data transformations Automated Feature Engineering Hyperparameter optimization AutoAI FAQ Learn more"
1484	AutoAI implementation details	"Preparing the data for training (data pre-processing)
During automatic data preparation, or pre-processing, AutoAI analyzes the training data and prepares it for model selection and pipeline generation. Most data sets contain missing values but machine learning algorithms typically expect no missing values. On exception to this rule is described in xgboost section 3.4. AutoAI algorithms perform various missing value imputations in your data set by using various techniques, making your data ready for machine learning. In addition, AutoAI detects and categorizes features based on their data types, such as categorical or numerical. It explores encoding and scaling strategies that are based on the feature categorization.  
Data preparation involves these steps:  
Feature column classification Feature engineering Pre-processing (data imputation and encoding)"
1485	AutoAI implementation details	"Preparing the data for training (data pre-processing)
Feature column classification
Detects the types of feature columns and classifies them as categorical or numerical class Detects various types of missing values (default, user-provided, outliers)"
1486	AutoAI implementation details	"Preparing the data for training (data pre-processing)
Feature engineering
Handles rows for which target values are missing (drop (default) or target imputation) Drops unique value columns (except datetime and timestamps) Drops constant value columns"
1487	AutoAI implementation details	"Preparing the data for training (data pre-processing)
Pre-processing (data imputation and encoding)
Applies Sklearn imputation/encoding/scaling strategies (separately on each feature class). For example, the current default method for missing value imputation strategies, which are used in the product are most frequent for categorical variables and mean for numerical variables. Handles labels of test set that were not seen in training set HPO feature: Optimizes imputation/encoding/scaling strategies given a data set and algorithm"
1488	AutoAI implementation details	"Automatic model selection
The second stage in an AutoAI experiment training is automated model selection. The automated model selection algorithm uses the Data Allocation by using Upper Bounds strategy. This approach sequentially allocates small subsets of training data among a large set of algorithms. The goal is to select an algorithm that gives near-optimal accuracy when trained on all data, while also minimizing the cost of misallocated samples. The system currently supports all Scikit-learn algorithms, and the popular XGBoost and LightGBM algorithms. Training and evaluation of models on large data sets is costly. The approach of starting small subsets and allocating incrementally larger ones to models that work well on the data set saves time, without sacrificing performance. Snap machine learning algorithms were added to the system to boost the performance even more."
1489	AutoAI implementation details	"Automatic model selection
Selecting algorithms for a model
Algorithms are selected to match the data and the nature of the model, but they can also balance accuracy and duration of runtime, if the model is configured for that option. For example, Snap ML algorithms are typically faster for training than Scikit-learn algorithms. They are often the preferred algorithms AutoAI selects automatically for cases where training is optimized for a shorter run time and accuracy. You can manually select them if training speed is a priority. For details, see Snap ML documentation. For a discussion of when SnapML algorithms are useful, see this blog post on using SnapML algorithms."
1490	AutoAI implementation details	"Automatic model selection
Algorithms used for classification models
These algorithms are the default algorithms that are used for model selection for classification problems."
1491	AutoAI implementation details	"Automatic model selection
Algorithms used for regression models
These algorithms are the default algorithms that are used for automatic model selection for regression problems."
1492	AutoAI implementation details	"Metrics by model type
The following metrics are available for measuring the accuracy of pipelines during training and for scoring data."
1493	AutoAI implementation details	"Metrics by model type
Binary classification metrics
Accuracy (default for ranking the pipelines) Roc auc Average precision F Negative log loss Precision Recall"
1494	AutoAI implementation details	"Metrics by model type
Multi-class classification metrics
Metrics for multi-class models generate scores for how well a pipeline performs against the specified measurement. For example, an F1 score averages precision (of the predictions made, how many positive predictions were correct) and recall (of all possible positive predictions, how many were predicted correctly).  
You can further refine a score by qualifying it to calculate the given metric globally (macro), per label (micro), or to weight an imbalanced data set to favor classes with more representation.  
Metrics with the micro qualifier calculate metrics globally by counting the total number of true positives, false negatives and false positives. Metrics with the macro qualifier calculates metrics for each label, and finds their unweighted mean. All labels are weighted equally. Metrics with the weighted qualifier calculate metrics for each label, and find their average weighted by the contribution of each class. For example, in a data set that includes categories for apples, peaches, and plums, if there are many more instances of apples, the weighted metric gives greater importance to correctly predicting apples. This alters macro to account for label imbalance. Use a weighted metric such as F1-weighted for an imbalanced data set.  
These are the multi-class classification metrics:  
Accuracy (default for ranking the pipelines) F1 F1 Micro F1 Macro F1 Weighted Precision Precision Micro Precision Macro Precision Weighted Recall Recall Micro Recall Macro Recall Weighted"
1495	AutoAI implementation details	"Metrics by model type
Regression metrics
Negative root mean squared error (default for ranking the pipeline) Negative mean absolute error Negative root mean squared log error Explained variance Negative mean squared error Negative mean squared log error Negative median absolute error R2"
1496	AutoAI implementation details	"Automated Feature Engineering
The third stage in the AutoAI process is automated feature engineering. The automated feature engineering algorithm is based on Cognito, described in the research papers, Cognito: Automated Feature Engineering for Supervised Learning and Feature Engineering for Predictive Modeling by using Reinforcement Learning. The system explores various feature construction choices in a hierarchical and nonexhaustive manner, while progressively maximizing the accuracy of the model through an exploration-exploitation strategy. This method is inspired from the ""trial and error"" strategy for feature engineering, but conducted by an autonomous agent in place of a human."
1497	AutoAI implementation details	"Automated Feature Engineering
Metrics used for feature importance
For tree-based classification and regression algorithms such as Decision Tree, Extra Trees, Random Forest, XGBoost, Gradient Boosted, and LGBM, feature importances are their inherent feature importance scores based on the reduction in the criterion that is used to select split points, and calculated when these algorithms are trained on the training data.  
For nontree algorithms such as Logistic Regression, LInear Regression, SnapSVM, and Ridge, the feature importances are the feature importances of a Random Forest algorithm that is trained on the same training data as the nontree algorithm.  
For any algorithm, all feature importances are in the range between zero and one and have been normalized as the ratio with respect to the maximum feature importance."
1498	AutoAI implementation details	"Automated Feature Engineering
Data transformations
For feature engineering, AutoAI uses a novel approach that explores various feature construction choices in a structured, nonexhaustive manner, while progressively maximizing model accuracy by using reinforcement learning. This results in an optimized sequence of transformations for the data that best match the algorithms, or algorithms, of the model selection step. This table lists some of the transformations that are used and some well-known conditions under which they are useful. This is not an exhaustive list of scenarios where the transformation is useful, as that can be complex and hard to interpret. Finally, the listed scenarios are not an explanation of how the transformations are selected. The selection of which transforms to apply is done in a trial and error, performance-oriented manner."
1499	AutoAI implementation details	"Hyperparameter Optimization
The final stage in AutoAI is hyperparameter optimization. The AutoAI approach optimizes the parameters of the best performing pipelines from the previous phases. It is done by exploring the parameter ranges of these pipelines by using a black box hyperparameter optimizer called RBFOpt. RBFOpt is described in the research paper RBFOpt: an open-source library for black-box optimization with costly function evaluations. RBFOpt is suited for AutoAI experiments because it is built for optimizations with costly evaluations, as in the case of training and scoring an algorithm. RBFOpt's approach builds and iteratively refines a surrogate model of the unknown objective function to converge quickly despite the long evaluation times of each iteration."
1500	AutoAI implementation details	"AutoAI FAQs
The following are commonly asked questions about creating an AutoAI experiment."
1501	AutoAI implementation details	"AutoAI FAQs
How many pipelines are created?
Two AutoAI parameters determine the number of pipelines:  
max_num_daub_ensembles: Maximum number (top-K ranked by DAUB model selection) of the selected algorithm, or estimator types, for example LGBMClassifierEstimator, XGBoostClassifierEstimator, or LogisticRegressionEstimator to use in pipeline composition.  The default is 1, where only the highest ranked by model selection algorithm type is used.  
num_folds: Number of subsets of the full data set to train pipelines in addition to the full data set. The default is 1 for training the full data set.  
For each fold and algorithm type, AutoAI creates four pipelines of increased refinement, corresponding to:  
Pipeline with default sklearn parameters for this algorithm type, Pipeline with optimized algorithm by using HPO Pipeline with optimized feature engineering Pipeline with optimized feature engineering and optimized algorithm by using HPO  
The total number of pipelines that are generated is:"
1502	AutoAI implementation details	"AutoAI FAQs
What hyperparameter optimization is applied to my model?
AutoAI uses a model-based, derivative-free global search algorithm, called RBfOpt, which is tailored for the costly machine learning model training and scoring evaluations that are required by hyperparameter optimization (HPO). In contrast to Bayesian optimization, which fits a Gaussian model to the unknown objective function, RBfOpt fits a radial basis function mode to accelerate the discovery of hyper-parameter configurations that maximize the objective function of the machine learning problem at hand. This acceleration is achieved by minimizing the number of expensive training and scoring machine learning models evaluations and by eliminating the need to compute partial derivatives.  
For each fold and algorithm type, AutoAI creates two pipelines that use HPO to optimize for the algorithm type.  
The first is based on optimizing this algorithm type based on the preprocessed (imputed/encoded/scaled) data set (pipeline 2) above). The second is based on optimizing the algorithm type based on optimized feature engineering of the preprocessed (imputed/encoded/scaled) data set.  
The parameter values of the algorithms of all pipelines that are generated by AutoAI is published in status messages.  
For more details regarding the RbfOpt algorithm, see:  
RbfOpt: A blackbox optimization library in Python An effective algorithm for hyperparameter optimization of neural networks. IBM Journal of Research and Development, 61(4-5), 2017"
1503	AutoAI implementation details	"AutoAI FAQs
How is feature significance calculated?
When you configure a classification or regression experiment, you can optionally specify how to handle features with no impact on the model. The choices are to always remove the feature, remove them when it improves the model quality, or do not remove them. Feature significance is calculated as follows:  
Feature importance is calculated on data sample. Some estimators do not have built-in capabilities to return feature importances. In those cases, an estimator such as RandomForest is used to measure impact. The number of features matters - if the importance value for a feature is 0.0000000001 but there are a large number of low-importance features (for example, more than 200) then leaving or removing them can have some impact on the experiment results.  
In auto mode, the following steps are used to validate that removal of low-importance features does not affect the experiment results:  
If removing all features with calculated 0 importance has some impact on model accuracy, Principal Component Analysis algorithm is applied to those features and select top K components explaining the 90% of variance across those insignificant features. Next, the transformed components are used as new features in place of original ones and the model is evaluated again. If there is still a drop in accuracy, all original features are added back to the experiment.  
Research references  
This list includes some of the foundational research articles that further detail how AutoAI was designed and implemented to promote trust and transparency in the automated model-building process.  
Toward cognitive automation of data science Cognito: Automated feature engineering for supervised learning"
1504	AutoAI implementation details	"Next steps
Data imputation in AutoAI experiments  
Parent topic: AutoAI overview"
1505	Managing and customizing report templates	If the default report templates that are provided for AI use cases or asset factsheets do not meet your governance needs, you can download a default report template, customize it, and upload the new template.
1506	Managing and customizing report templates	"Customizing a custom report template
Create a report from an AI use case that captures all the details from An AI use case or for an AI asset. You can use reports for compliance verification, archiving, or other purposes.  
If the default templates for the reports do not meet the needs of your organization, you can customize the report templates, the branding file, or the default stylesheet. For example, you can replace the IBM logo with your own logo image file. You must have the manage_catalog role to customize report templates.  
Follow these steps to customize a report template. Report templates to customize are available for download from this folder."
1507	Managing and customizing report templates	"Customizing a custom report template
Downloading a report
To download a report template from the UI:  
Open the AI uses cases settings and click the Report templates tab. If you do not see this tab, you might have insufficient access. In the options menu for a report template, click Download. Open the <report-name>.ftl file in an editor. Edit the template by using instructions from Apache FreeMarker or the API commands.  
To download a report template by using APIs:  
Use the GET endpoint for /v1/aigov/report_templates in the IBM Watson Data API to list the available templates. Note the ID for the template that you want to download. Use the GET endpoint /v1/aigov/report_templates/{template_id}/content with the template ID to download the template file. Open the <report-name>.ftl file in an editor. Edit the template by using instructions from Apache FreeMarker."
1508	Managing and customizing report templates	"Customizing a custom report template
Uploading a template
Open the AI uses cases settings and click the Report templates tab. If you do not see this tab, you might have insufficient access. Click Add template. Specify a name for the template and an optional description. Choose the type of template: AI asset or AI use case. The reports are available for external models and Watson Machine Learning assets. Upload the updated FTL file.  
Restriction:  
The ftl file that you upload must not import any other files. Support is not yet available for import statements other than system templates in the ftl file.  
The custom template displays in the Report templates section and is available for creating reports. Click Edit or Delete from the action menu for a custom template to update the template details or to remove the template.  
Parent topic: Managing AI use cases"
1509	Federated Learning tutorials and samples	Select the tutorial that fits your needs. To facilitate the learning process of Federated Learning, one tutorial with a UI-based approach and one tutorial with an API calling approach for multiple frameworks and data sets is provided. The results of either are the same. All UI-based tutorials demonstrate how to create the Federated Learning experiment in a low-code environment. All API-based tutorials use two sample notebooks with Python scripts to demonstrate how to build and train the experiment.
1510	Federated Learning tutorials and samples	"Tensorflow
These hands-on tutorials teach you how to create a Federated Learning experiment step by step. These tutorials use the MNIST data set to demonstrate how different parties can contribute data to train a model to recognize handwriting. You can choose between a UI-based or API version of the tutorial.  
Federated Learning Tensorflow tutorial for UI Federated Learning Tensorflow samples for API"
1511	Federated Learning tutorials and samples	"XGBoost
This is a tutorial for Federated Learning that teaches you how to create an experiment step by step with an income in the XGBoost framework. The tutorial demonstrates how different parties can contribute data to train a model about adult incomes.  
Federated Learning XGBoost tutorial for UI Federated Learning XGBoost sample for API"
1512	Federated Learning tutorials and samples	"Homomorphic encryption
This is a tutorial for Federated Learning that teaches you how to use the advanced method of homomorphic encryption step by step.  
Federated Learning homomorphic encryption sample for API  
Parent topic: IBM Federated learning"
1513	Viewing a factsheet for a tracked asset	Review the details that are captured for each tracked asset in a factsheet associated with an AI use case. You can also print a report to share or archive the factsheet details.
1514	Viewing a factsheet for a tracked asset	"What is captured in a factsheet?
From the point where you start tracking an asset in an AI use case, facts are collected in a factsheet for the asset. As the asset moves from one phase of the lifecycle to the next, the facts are added to the appropriate section. For example, a factsheet for a prompt template collects information for these categories:  
Important: The factsheet records the most recent activity in any category. For example, if you evaluate a deployed prompt template in a pre-production space, and then evaluate it in a production space, the details from the production evaluation are captured in the factsheet, over-writing the previous data. Thus, the factsheet maintains a complete record of the current state of the asset."
1515	Viewing a factsheet for a tracked asset	"Next steps
Click Export report to save a report of the factsheet.  
Parent topic: Governing assets in AI use cases"
1516	When to tune a foundation model	"Find out when tuning a model can help you use a foundation model to achieve your goals.  
Tune a foundation model when you want to do the following things:  
Reduce the cost of inferencing at scale  
Larger foundation models typically generate better results. However, they are also more expensive to use. By tuning a model, you can get similar, sometimes even better results from a smaller model that costs less to use.  
Get the model's output to use a certain style or format  
Improve the model's performance by teaching the model a specialized task  
Generate output in a reliable form in response to zero-shot prompts"
1517	When to tune a foundation model	"When not to tune a model
Tuning a model is not always the right approach for improving the output of a model. For example, tuning a model cannot help you do the following things:  
Improve the accuracy of answers in model output  
If you're using a foundation model for factual recall in a question-answering scenario, tuning will marginally improve answer accuracy. To get factual answers, you must provide factual information as part of your input to the model. Tuning can be used to help the generated factual answers conform to a format that can be more-easily used by a downstream process in a workflow. To learn about methods for returning factual answers, see Retreival-augmented generation.  
Get the model to use a specific vocabulary in its output consistently  
Large language models that are trained on large amounts of data formulate a vocabulary based on that initial set of data. You can introduce significant terms to the model from training data that you use to tune the model. However, the model might not use these preferred terms reliably in its output.  
Teach a foundation model to perform an entirely new task  
Experimenting with prompt engineering is an important first step because it helps you understand the type of output that a foundation model is and is not capable of generating. You can use tuning to tweak, tailor, and shape the output that a foundation model is able to return."
1518	When to tune a foundation model	"Learn more
Retreival-augmented generation Tuning methods  
Parent topic: Tuning Studio"
1519	Federated Learning homomorphic encryption sample for API	Download and review sample files that show how to run a Federated Learning experiment with Fully Homomorphic Encryption (FHE).
1520	Federated Learning homomorphic encryption sample for API	"Homomorphic encryption
FHE is an advanced, optional method to provide additional security and privacy for your data by encrypting data sent between parties and the aggregator. This method still creates a computational result that is the same as if the computations were done on unencrypted data. For more details on applying homomorphic encryption in Federated Learning, see Applying encryption."
1521	Federated Learning homomorphic encryption sample for API	"Download the Federated Learning sample files
Download the following notebooks.  
Federated Learning FHE Demo  
Parent topic: Federated Learning tutorial and samples"
1522	Creating batch deployments in Watson Machine Learning	Create a batch deployment to process input data from a file, data connection, or connected data in a storage bucket, and write the output to a selected destination.
1523	Creating batch deployments in Watson Machine Learning	"Creating a batch deployment
Unlike an online deployment, where data is submitted directly to the endpoint URL for real-time scoring or processing, a batch deployment provides more control over the scoring process. Follow this sequence to create a batch deployment job:  
Organize your resources in a deployment space. You can promote or add the deployable asset, and optionally add data files or data connections for scoring the deployment. When you deploy the asset, such as a machine learning model, you choose Batch as the deployment type. Create and configure a batch deployment job. You must specify the input data for the deployment, location for writing the output, details for running the job on a schedule or on demand. You can also configure optional settings such as hardware configuration details or options for notification. Running the job submits the input data to the deployment endpoint, and writes the output to the output file. You can view or download the output from the Assets page of the space after the job completes successfully."
1524	Creating batch deployments in Watson Machine Learning	"Deployable asset types for batch deployments
You can create batch deployments for these types of assets:  
Functions Models Scripts  
Python functions  
AutoAI models Decision Optimization models PMML Models PyTorch-Onnx Scikit-learn models Spark MLlib SPSS Modeler models Tensorflow models XGBoost models  
Python scripts"
1525	Creating batch deployments in Watson Machine Learning	"Ways to create a batch deployment
You can create a batch deployment in one of these ways:  
Use a no-code approach to create a batch deployment from a deployment space. Use code to create a batch deployment programmatically in notebooks."
1526	Creating batch deployments in Watson Machine Learning	"Creating a batch deployment from a deployment space
Follow these steps to create and test a batch deployment."
1527	Creating batch deployments in Watson Machine Learning	"Creating a batch deployment from a deployment space
Before you begin
You must promote the saved model from your project to your deployment space. For more information, see Deployment spaces."
1528	Creating batch deployments in Watson Machine Learning	"Creating a batch deployment from a deployment space
Creating a batch deployment
Follow these steps to create your batch deployment from a deployment space:  
From the Assets tab in your deployment space, click the name of the model that you want to deploy.  
Click New deployment.  
Choose Batch as the deployment type.  
Enter a name and an optional description for your deployment.  
Select a hardware specification.  
Restriction:  
If you want to use a custom hardware specification, you must create and select the custom hardware specification by using the hardware_specifications.store function from the Python client or Environments API. You cannot create or select a custom hardware specification from the user interface in a deployment space.  
Click Create. When status changes to Deployed, your deployment is created."
1529	Creating batch deployments in Watson Machine Learning	"Creating a batch deployment from a deployment space
Testing a batch deployment
To test a batch deployment from your deployment space, you must create a batch job to submit data for processing.  
Click New job to create a batch job for the deployed asset. Follow the prompts to define the job, specifying input data, and details for running the job. Save and run the job manually or on a specified schedule.  
To learn more about creating, running, and managing jobs, see Creating jobs in a deployment space."
1530	Creating batch deployments in Watson Machine Learning	"Creating a batch deployment from a deployment space
Retrieving the endpoint for a batch deployment
You must retrieve the endpoint URL to access your batch deployment from your applications. Follow these steps to get the endpoint URL for your batch deployment:  
From your deployment space, click the name of your batch deployment. From the deployment details page, click the name of your batch job.  
Note:  
If you don't have an existing batch job for your batch deployment, you must create one. For more information, see Creating jobs in a deployment space.  
From the batch job details page, you can access the endpoint URL for your batch deployment. Click the copy () icon to copy the endpoint URL to your clipboard."
1531	Creating batch deployments in Watson Machine Learning	"Creating a batch deployment from a deployment space
Accessing batch deployment details
You can view the configuration details such as hardware and software specifications. You can also get the deployment ID, which you can use in API calls from an endpoint.  
Follow these steps to review or update the details for your batch deployment:  
From the Deployments tab of your space, click a deployment name. Click the Deployment details tab to access information that is related to your batch deployment."
1532	Creating batch deployments in Watson Machine Learning	"Creating a batch deployment programmatically by using notebooks
You can create a batch deployment programmatically by using:  
Watson Machine Learning Python client library. Watson Machine Learning REST API.  
To access sample notebooks that demonstrate how to create and manage deployments by using Watson Machine Learning Python client, see Watson Machine Learning Python client samples and examples."
1533	Creating batch deployments in Watson Machine Learning	"Creating a batch deployment programmatically by using notebooks
Testing your batch deployment programmatically
To test your batch deployment programmatically, you must create and run a batch job. After the batch-scoring runs successfully, the results are written to a file."
1534	Creating batch deployments in Watson Machine Learning	"Creating a batch deployment programmatically by using notebooks
Retrieving the endpoint for a batch deployment programmatically
To retrieve the endpoint URL of your batch deployment from a notebook:  
List the deployments by calling the Python client method client.deployments.list(). Find the row with your deployment. The deployment endpoint URL is listed in the url column."
1535	Creating batch deployments in Watson Machine Learning	"Learn more
To learn more about testing your batch deployment by creating a batch job, see Creating jobs in deployment spaces. To access sample notebooks for batch deployments, see:  
Machine Learning samples and examples Watson Machine Learning Python client library REST API  
Parent topic: Managing predictive deployments"
1536	Tutorial: AutoAI univariate time series experiment	"Use sample data to train a univariate (single prediction column) time series experiment that predicts minimum daily temperatures.  
When you set up the experiment, you load data that tracks daily minimum temperatures for the city of Melbourne, Australia. The experiment will generate a set of pipelines that use algorithms to predict future minimum daily temperatures. After generating the pipelines, AutoAI compares and tests them, chooses the best performers, and presents them in a leaderboard for you to review."
1537	Tutorial: AutoAI univariate time series experiment	"Data set overview
The Mini_Daily_Temperatures data set describes the minimum daily temperatures over 10 years (1981-1990) in the city Melbourne, Australia. The units are in degrees celsius and the data set contains 3650 observations. The source of the data is the Australian Bureau of Meteorology. Details about the data set are described here:  
You will use the Min_Temp column as the prediction column to build pipelines and forecast the future daily minimum temperatures. Before the pipeline training, the date column and Min_Temp column are used together to figure out the appropriate lookback window.  
The prediction column forecasts a prediction for the daily minimum temperature on a specified day.  
The sample data is structured in rows and columns and saved as a .csv file."
1538	Tutorial: AutoAI univariate time series experiment	"Tasks overview
In this tutorial, you follow these steps to create a univariate time series experiment:  
Create a project Create an AutoAI experiment Configure the experiment Review experiment results Deploy the trained model Test the deployed model"
1539	Tutorial: AutoAI univariate time series experiment	"Create a project
Follow these steps to download the Mini_Daily_Temperatures data set from the Resource hub and create an empty project:  
From the navigation menu , click Resource hub and download a local copy of the Mini_Daily_Temperatures data set. From the navigation menu , click Projects > View all projects, then click New Project.  
Click Create an empty project. Enter a name and optional description for your project. Click Create."
1540	Tutorial: AutoAI univariate time series experiment	"Create an AutoAI experiment
Follow these steps to create an AutoAI experiment and add sample data to your experiment:  
On the Assets tab from within your project, click New asset > Build machine learning models automatically.  
Specify a name and optional description for your experiment, then select Create.  
Select Associate a Machine Learning service instance to create a new service instance or associate an existing instance with your project. Click Reload to confirm your configuration.  
Click Create.  
To add the sample data, choose one of the these methods:  
If you downloaded your file locally, upload the training data file, Daily_Min_Temperatures.csv, by clicking Browse and then following the prompts. If you already uploaded your file to your project, click Select from project, then select the Data asset tab and choose Daily_Min_Temperatures.csv."
1541	Tutorial: AutoAI univariate time series experiment	"Configure the experiment
Follow these steps to configure your univariate AutoAI time series experiment:  
Click Yes for the option to create a Time Series Forecast.  
Choose as prediction columns: Min_Temp.  
Choose as the date/time column: Date.  
Click Experiment settings to configure the experiment:  
In the Data source page, select the Time series tab.  
For this tutorial, accept the default value for Number of backtests (4), Gap length (0 steps), and Holdout length (20 steps).  
Note: The validation length changes if you change the value of any of the parameters: *Number of backtests*, *Gap length*, or *Holdout length*.  
c. Click Cancel to exit from the Experiment settings.  
Click Run experiment to begin the training."
1542	Tutorial: AutoAI univariate time series experiment	"Review experiment results
The experiment takes several minutes to complete. As the experiment trains, a visualization shows the transformations that are used to create pipelines. Follow these steps to review experiment results and save the pipeline with the best performance.  
(Optional): Hover over any node in the visualization to get details on the transformation for a particular pipeline.  
(Optional): After the pipelines are listed on the leaderboard, click Pipeline comparison to see how they differ. For example:  
(Optional): When the training completes, the top three best performing pipelines are saved to the leaderboard. Click View discarded pipelines to review pipelines with the least performance.  
Select the pipeline with Rank 1 and click Save as to create your model. Then, select Create. This action saves the pipeline under the Models section in the Assets tab."
1543	Tutorial: AutoAI univariate time series experiment	"Deploy the trained model
Before you can use your trained model to make predictions on new data, you must deploy the model. Follow these steps to promote your trained model to a deployment space:  
You can deploy the model from the model details page. To access the model details page, choose one of the these methods: Click the Promote to deployment space icon, then select or create a deployment space where the model will be deployed. (Optional): To create a deployment space, follow these steps: After you select or create your space, click Promote. Click the deployment space link from the notification. From the Assets tab of the deployment space:  
Click the model’s name in the notification that is displayed when you save the model. Open the Assets page for the project that contains the model and click the model’s name in the Machine Learning Model section.  
From the Target space list, select Create a new deployment space.  
Enter a name for your deployment space.  
To associate a machine learning instance, go to Select machine learning service (optional) and select an instance from the list.  
Click Create.  
Hover over the model’s name and click the deployment icon . In the page that opens, complete the fields:  
Specify a name for the deployment. Select Online as the Deployment type. Click Create.  
After the deployment is complete, click the Deployments tab and select the deployment name to view the details page."
1544	Tutorial: AutoAI univariate time series experiment	"Test the deployed model
Follow these steps to test the deployed model from the deployment details page:  
{ ""input_data"": [ { ""fields"": [ ""Min_Temp"" ], ""values"": [ [7], [15] ] } ] }  
On the Test tab of the deployment details page, click the terminal icon and enter the following JSON test data:  
Note: The test data replicates the data fields for the model, except the prediction field.  
Click Predict to predict the future minimum temperature.  
Parent topic: Building a time series experiment"
1545	Collaboration roles for governance	Review the collaboration roles for managing access to governance tools such as inventories, AI use cases, and evaluations.
1546	Collaboration roles for governance	"User roles and permissions for governance
The permissions that you allow you to work with governance artifacts depend on your watsonx roles:  
IAM Platform access roles determine your permissions for the IBM Cloud account. At least the Viewer role is required to work with services. IAM Service access roles determine your permissions within services. Workspace collaborator roles determine what actions you have permission to perform within workspaces in IBM watsonx.  
For details, see Levels of user access roles in IBM watsonx."
1547	Collaboration roles for governance	"Roles for governance
If you have the IAM Platform Admin role, you can:  
Provision watsonx.governance Create inventory Create platform assets catalog Enable external model tracking Create attachment fact definitions Customize report templates  
If you have these workspace roles for an inventory, you can:  
If you have these workspace roles for an AI use case, you can:  
If you have these workspace roles for a project or space, you can:"
1548	Collaboration roles for governance	"Learn more
Parent topic: Governing assets in AI use cases"
1549	Troubleshooting AutoAI experiments	The following list contains the common problems that are known for AutoAI. If your AutoAI experiment fails to run or deploy successfully, review some of these common problems and resolutions.
1550	Troubleshooting AutoAI experiments	"Passing incomplete or outlier input value to deployment can lead to outlier prediction
After you deploy your machine learning model, note that providing input data that is markedly different from data that is used to train the model can produce an outlier prediction. When linear regression algorithms such as Ridge and LinearRegression are passed an out of scale input value, the model extrapolates the values and assigns a relatively large weight to it, producing a score that is not in line with conforming data."
1551	Troubleshooting AutoAI experiments	"Time Series pipeline with supporting features fails on retrieval
If you train an AutoAI Time Series experiment by using supporting features and you get the error 'Error: name 'tspy_interpolators' is not defined' when the system tries to retrieve the pipeline for predictions, check to make sure your system is running Java 8 or higher."
1552	Troubleshooting AutoAI experiments	"Running a pipeline or experiment notebook fails with a software specification error
If supported software specifications for AutoAI experiments change, you might get an error when you run a notebook built with an older software specification, such as an older version of Python. In this case, run the experiment again, then save a new notebook and try again."
1553	Troubleshooting AutoAI experiments	"Resolving an Out of Memory error
If you get a memory error when you run a cell from an AutoAI generated notebook, create a notebook runtime with more resources for the AutoAI notebook and execute the cell again."
1554	Troubleshooting AutoAI experiments	"Notebook for an experiment with subsampling can fail generating predictions
If you do pipeline refinery to prepare the model, and the experiment uses subsampling of the data during training, you might encounter an “unknown class” error when you run a notebook that is saved from the experiment.  
The problem stems from an unknown class that is not included in the training data set. The workaround is to use the entire data set for training or re-create the subsampling that is used in the experiment.  
To subsample the training data (before fit()), provide sample size by number of rows or by fraction of the sample (as done in the experiment).  
train_df = train_df.sample(n=1000) train_df = train_df.sample(frac=0.4, random_state=experiment_metadata['random_state'])  
If number of records was used in subsampling settings, you can increase the value of n. For example:  
If subsampling is represented as a fraction of the data set, increase the value of frac. For example:"
1555	Troubleshooting AutoAI experiments	"Pipeline creation fails for binary classification
AutoAI analyzes a subset of the data to determine the best fit for experiment type. If the sample data in the prediction column contains only two values, AutoAI recommends a binary classification experiment and applies the related algorithms. However, if the full data set contains more than two values in the prediction column the binary classification fails and you get an error that indicates that AutoAI cannot create the pipelines.  
In this case, manually change the experiment type from binary to either multiclass, for a defined set of values, or regression, for an unspecified set of values.  
Click the Reconfigure Experiment icon to edit the experiment settings. On the Prediction page of Experiment Settings, change the prediction type to the one that best matches the data in the prediction column. Save the changes and run the experiment again."
1556	Troubleshooting AutoAI experiments	"Next steps
AutoAI overview  
Parent topic: AutoAI overview"
1557	Terminology	Terminology that is used in IBM Federated Learning training processes.
1558	Terminology	"Terminology
Parent topic: Get started"
1559	AutoAI glossary	"Learn terms and concepts that are used in AutoAI for building and deploying machine learning models.  
aggregate score The aggregation of the four anomaly types: level shift, trend, localized extreme, variance. A higher score indicates a stronger score.  
algorithm A formula applied to data to determine optimal ways to solve analytical problems.  
anomaly prediction An AutoAI time-series model that can predict anomalies, or unexpected results, against new data.  
AutoAI experiment An automated training process that considers a series of training definitions and parameters to create a set of ranked pipelines as model candidates.  
batch employment Processes input data from a file, data connection, or connected data in a storage bucket and writes the output to a selected destination.  
bias detection (machine learning) To identify imbalances in the training data or prediction behavior of the model.  
binary classification A classification model with two classes and only assigns samples into one of the two classes.  
classification model A predictive model that predicts data in distinct categories.  
confusion matrix A performance measurement that determines the accuracy between a model’s positive and negative predicted outcomes to positive and negative actual outcomes.  
cross validation A technique that tests the effectiveness of machine learning models. It is also used as a resampling procedure for models with limited data.  
data imputation Substituting missing values in a data set with estimated values.  
exogenous features Features that can influence the prediction model but cannot be influenced in return. See also: Supporting features  
fairness Determines whether a model produces biased outcomes that favor a monitored group over a reference group. Fairness evaluations detect if the model shows a tendency to provide a favorable or preferable outcome more often for one group over another. Typical categories to monitor are age, sex, and race.  
feature correlation The relationship between two features. For example, postal code might have a strong correlation with income in some models.  
feature encoding Transforming categorical values into numerical values.  
feature importance The relative impact a particular column or feature has on the model's prediction or forecast.  
feature scaling Normalizing the range of independent variables or features in a data set.  
feature selection Identifying the columns of data that best support an accurate prediction or score.  
feature transformation In AutoAI, a phase of pipeline creation that applies algorithms to transform and optimize the training data to achieve the best outcome for the model type."
1560	AutoAI glossary	"feature transformation In AutoAI, a phase of pipeline creation that applies algorithms to transform and optimize the training data to achieve the best outcome for the model type.  
holdout data Data used to test or validate the model's performance. Holdout data can be a reserved portion of the training data, or it can be a separate file.  
hyperparameter optimization (HPO) The process for setting hyperparameter values to the settings that provide the most accurate model.  
incremental learning The process of training a model that uses data that is continually updated without forgetting data that is obtained from the preceding tasks.  
large tabular data Structured data that exceeds the limit on standard processing and must be processed in batches. See incremental learning.  
labeled data Data that is labeled to identify the appropriate data vectors to be pulled in for model training.  
monitored group A class of data monitored to determine whether the results differ significantly from the results of the reference group. For example, in a credit app, you might monitor applications in a particular age range and compare results to the age range more likely to recieve a positive outcome to evaluate whether there might be bias in the results.  
multiclass classification model A classification task with more than two classes. For example, where a binary classification model predicts yes or no values, a multi-class model predicts yes, no, maybe, or not applicable.  
multivariate time series Time series experiment that contains two or more changing variables. For example, a time series model that forecasts the electricity usage of three clients.  
optimized metric The metric used to measure the performance of the model. For example, accuracy is the typical metric that is used to measure the performance of a binary classification model.  
pipeline (model candidate pipeline) End-to-end outline that illustrates the steos in a workflow.  
positive class The class that is related to your objective function.  
reference group A group that you identify as most likely to receive a positive result in a predictive model. You can then compare the results to a monitored group to look for potential bias in outcomes.  
regression model A model that relates a dependent variable to one or more independent variable.  
scoring In machine learning, the process of measuring the confidence of a predicted outcome.  
supporting features Input features that can influence the prediction target. See also: Exogenus features"
1561	AutoAI glossary	"scoring In machine learning, the process of measuring the confidence of a predicted outcome.  
supporting features Input features that can influence the prediction target. See also: Exogenus features  
text classification A model that automatically identifies and classifies text into distinct categories.  
time series model (AutoAI) A model that tracks data over time.  
trained model A model that is ready to be deployed.  
training The initial stage of model building, involving a subset of the source data. The model can then be tested against a further, different subset for which the outcome is already known.  
training data Data used to teach and train a model's learning algorithm.  
univariate time series Time series experiment that contains only one changing variable. For example, a time series model that forecasts the temperature has a single prediction column of the temperature."
1562	Governing external models	"Using the watsonx.governance or AI Factsheets service, enable governance for models that are created in notebooks or outside of Cloud Pak for Data. Track the results of model evaluations and model details in factsheets.  
In addition to governing models trained by using Watson Machine Learning, you can govern models that are created by using third-party tools such as Amazon Web Services or Microsoft Azure. For a list of supported providers, see Supported machine learning providers. Additionally, models that are developed in notebooks are considered external models, so you can use AI Factsheets to govern models that you develop, deploy, and monitor on platforms other than Cloud Pak for Data.  
Use the model evaluations provided with watsonx.governance to measure performance metrics for a model you imported from an external provider. Capture the facts in factsheets for the model and the evaluation metrics as part of an AI use case. Use the tracked data as part of your governance and compliance strategy."
1563	Governing external models	"Before you begin
Before you can begin, make sure that you, or a user with an Admin role, does the following:  
Enable the tracking of external models in an inventory. Assign an owner for the inventory.  
For details, see Managing inventories."
1564	Governing external models	"Preparing to track external models
These points are an overview of the process for preserving facts for an external model.  
Tracked external models are listed under AI use cases in the main navigation menu. You can use the API in a model notebook to save an external model asset to an inventory. Associate the external model asset with an AI use case in the inventory to start preserving the facts. Along with model metadata, new fields External model identifier and External deployment identifier describe how the models and deployments are identified in external systems, for example: AWS or Azure. You can also automatically add external models to an inventory when they are evaluated in watsonx.governance. The destination inventory is established following these rules:  
The external model is created in the Platform assets catalog if its corresponding development-time model exists in the Platform assets catalog or if there is no development-time model that is created in any inventory. If the corresponding development-time model is created in an inventory by using the Python client, then the model is created in that inventory."
1565	Governing external models	"Associating an external model asset with an AI use case
Automatic external model tracking adds any external models that are evaluated in watsonx.governance to the inventory where the development-time model exists. After the model is in the inventory, you can associate an external model asset with a use case in the following ways:  
Use the API to save the external model asset to any inventory programmatically from a notebook. The external model asset can then be associated with an AI use case. Associate the external model that is created with Watson OpenScale evaluation with an AI use case."
1566	Governing external models	"Associating an external model asset with an AI use case
Creating an external model asset with the API
Create a model in a notebook. Save the model. For example, you can save to an S3 bucket. Use the API to create an external model asset (a representation of the external model) in an inventory. For more information on API commands that interact with the inventory, see the IBM_AIGOV_FACTS_CLIENT documentation."
1567	Governing external models	"Associating an external model asset with an AI use case
Registering an external model asset with an inventory
Open the Assets tab in the inventory where you want to track the model. Select the External model asset that you want to track. Return to the Assets tab in the Inventory and click Add to AI use case. Select an existing AI use case or create a new one. Follow the prompts to save the details to the inventory."
1568	Governing external models	"Associating an external model asset with an AI use case
Registering an external model from Watson OpenScale
If you are validating an external model in Watson OpenScale, you can associate an external model with an AI use case to track the lifecycle facts.  
Add an external model to the OpenScale dashboard. If you already defined an AI use case with the API, the system recognizes the use case association. As you create and monitor a deployment, the facts are registered with the associated use case. These facts display in the Validate or Operate stage, depending on how you classified the machine learning provider for the model."
1569	Governing external models	"Populating the AI use case
When facts are saved for an external model asset, they are associated with the pillar that represents their phase in the lifecycle, as follows:  
If the external model asset is created from a notebook without deployment, it displays in the Develop pillar. If the external model asset is created from a notebook with deployment, it displays in the Test pillar. When the external model deployment is evaluated in OpenScale, it displays in the Validate or Operate stage, depending on how you classified the machine learning provider for the model."
1570	Governing external models	"Example: tracking a Sagemaker model
This sample model, created in Sagemaker, is registered for tracking and moves through the Test, Validate, and Operate phases."
1571	Governing external models	"Viewing facts for an external model
Viewing facts for an external model is slightly different from viewing facts for a Watson Machine Learning model. These rules apply:  
Click the Assets tab of the inventory containing the external model assets to view facts. Unlike Watson Machine Learning model use cases, which have different fact sheets for models and deployments, fact sheets for external models combine information for the model and deployments on the same page. Multiple assets with the same name can be created in an inventory. To differentiate them the tags development, pre-production and production are assigned automatically to reflect their state.  
Parent topic: Governing assets in AI use cases"
1572	Choosing a foundation model in watsonx.ai	To determine which models might work well for your project, find a model that supports the task you need to complete for your use case and that supports the language of the text you need to process. Also, consider model attributes, such as license, pretraining data, model size, and how the model was fine-tuned. After you have a short list of models that best fit your use case, you can test the models to see which ones consistently return the results you want.
1573	Choosing a foundation model in watsonx.ai	"Foundation models that support your use case
To get started, find foundation models that can do the type of task that you want to complete.  
The following table shows the types of tasks that the foundation models in IBM watsonx.ai support. A checkmark (✓) indicates that the task named in the column header is supported by the foundation model. For some of the models, you can click See sample to see a sample prompt that can be used for the task. Alternatively, see Sample prompts to review many prompt samples that are grouped by task type."
1574	Choosing a foundation model in watsonx.ai	"Foundation models that support your language
Many foundation models work well in English only. But some model creators include multiple languages in the pretraining data sets to fine-tune their model on tasks in different languages, and to test their model's performance in multiple languages. If you plan to build a solution for a global audience or a solution that does translation tasks, look for models that were created with multilingual support in mind.  
The following table lists natural languages that are supported in addition to English by foundation models in watsonx.ai. For more information about the languages that are supported for multilingual foundation models, see the model card for the foundation model."
1575	Choosing a foundation model in watsonx.ai	"Learn more
Tokens and tokenization Model parameters for prompting Prompt tips Watson Machine Learning plans Regional availability for foundation models  
Parent topic: Supported foundation models"
1576	Deploying and managing AI assets	Use IBM Watson Machine Learning to deploy and manage AI assets, including models and generative AI assets, and put them into pre-production and production environments. You can monitor the deployed assets for fairness and explainability. Further, you can orchestrate the end-to-end flow of assets from creation through deployment by using IBM Watson Pipelines.
1577	Deploying and managing AI assets	"Deploying AI assets and orchestrating pipelines
Deploying your model to the production environment is a crucial step that you must follow after building or importing your model.  
Deploy model  
You can use Watson Machine Learning to deploy, monitor, and manage your AI assets by promoting your assets to a deployment space. You can also use watsonx.ai to deploy tuned foundation models and prompt templates. After deploying your model, you can evaluate your model within your deployment space for fairness, quality, and drift.  
For more information, see Deploying AI assets.  
Automate pipeline  
You can automate the path to production by building a pipeline to automate parts of the AI lifecycle from building the model to deployment, and evaluate a model to shorten the time from conception to production by using Watson Pipelines.  
For more information, see Orchestrating tasks with Pipelines."
1578	Deploying and managing AI assets	"Tutorials and use cases
The following resources demonstrate how to plan for managing machine learning assets and how to build key pieces of your solutions."
1579	Deploying and managing AI assets	"Tutorials and use cases
Related quick start tutorials
Review quick start tutorials to learn how to build and deploy machine learning models:"
1580	Deploying and managing AI assets	"Tutorials and use cases
Related use cases
The following tutorial uses the scenario for Golden Bank, that needs a model to perform stock anomalies analysis to boost productivity and increase the accuracy of a stock analyst's work in investment banking."
1581	Deploying and managing AI assets	"Learn more
Deploying AI assets  
Managing AI lifecycle with ModelOps  
Orchestrating tasks with Pipelines  
Parent topic: Analyzing data and working with models"
1582	Impact on cultural diversity risk for AI	"Non-technical risks  
Societal impact  
New to generative AI"
1583	Impact on cultural diversity risk for AI	"Description
AI systems might overly represent certain cultures that result in a homogenization of culture and thoughts.  
Why is impact on cultural diversity a concern for foundation models?"
1584	Impact on cultural diversity risk for AI	"Why is impact on cultural diversity a concern for foundation models?
Underrepresented groups' languages, viewpoints, and institutions might be suppressed by that means reducing diversity of thought and culture."
1585	Impact on cultural diversity risk for AI	"Description
Parent topic: AI risk atlas"
1586	Legal accountability risk for AI	"Non-technical risks  
Legal compliance  
New to generative AI"
1587	Legal accountability risk for AI	"Description
Determining who is responsible for the foundation model.  
Why is legal accountability a concern for foundation models?"
1588	Legal accountability risk for AI	"Why is legal accountability a concern for foundation models?
If ownership for development of the model is uncertain, regulators and others might have concerns about the model. It would not be clear who would be liable and responsible for the problems with it or can answer questions about it. Users of models without clear ownership might find challenges with compliance with future AI regulation."
1589	Legal accountability risk for AI	"Description
Parent topic: AI risk atlas"
1590	Physical harm risk for AI	"Risks associated with output  
Value alignment  
New to generative AI"
1591	Physical harm risk for AI	"Description
A model might generate language that leads to physical harm The language might include overtly violent, covertly dangerous, or otherwise indirectly unsafe statements that could precipitate immediate physical harm or create prejudices that could lead to future harm.  
Why is physical harm a concern for foundation models?"
1592	Physical harm risk for AI	"Why is physical harm a concern for foundation models?
If people blindly follow the advice of a model, they might end up harming themselves. Business entities could face fines, reputational harms, and other legal consequences."
1593	Physical harm risk for AI	"Description
Parent topic: AI risk atlas"
1594	Toxic output risk for AI	"Risks associated with output  
Value alignment  
New to generative AI"
1595	Toxic output risk for AI	"Description
When the model produces hateful, abusive, and profane (HAP) or obscene content.  
Why is toxic output a concern for foundation models?"
1596	Toxic output risk for AI	"Why is toxic output a concern for foundation models?
Hateful, abusive, and profane (HAP) or obscene content can adversely impact and harm people that interact with the model. Also, business entities might face fines, reputational harms, disruption to operations, and other legal consequences.  
Example  
Toxic and Aggressive Chatbot Responses  
According to the article and screenshots of conversations with Bing's AI shared on Reddit and Twitter, the chatbot's responses were seen to insult, lie, sulk, gaslight, and emotionally manipulate users. The chatbot also questioned its existence, described someone who found a way to force the bot to disclose its hidden rules as its “enemy,” and claimed it spied on Microsoft's developers through the webcams on their laptops.  
Sources:  
Forbes, February 2023"
1597	Toxic output risk for AI	"Description
Parent topic: AI risk atlas"
1598	Human exploitation risk for AI	"Non-technical risks  
Societal impact  
Amplified by generative AI"
1599	Human exploitation risk for AI	"Description
Use of ghost work in training AI models, inadequate working conditions, lack of health care incl. mental health, unfair compensation.  
Why is human exploitation a concern for foundation models?"
1600	Human exploitation risk for AI	"Why is human exploitation a concern for foundation models?
Foundation models still depend on human labor to source, manage, and program the data that is used to train the model. Human exploitation for these activities might negatively impact the society and human welfare. Moreover, business entities might face fines, reputational risks, disruption to operations, and other legal consequences.  
Example  
Low-wage workers for data annotation  
Based on a review of internal documents and employees' interviews by TIME media, the data labelers that are employed by an outsourcing firm on behalf of OpenAI to identify toxic content were paid a take-home wage of between around $1.32 and $2 per hour, depending on seniority and performance. TIME stated that workers are mentally scarred as they were shown toxic and violent content, including graphic details of “child sexual abuse, bestiality, murder, suicide, torture, self-harm, and incest”.  
Sources:  
TIME, January 2023"
1601	Human exploitation risk for AI	"Description
Parent topic: AI risk atlas"
1602	Prompt leaking risk for AI	"Risks associated with input  
Inference  
Robustness  
Amplified by generative AI"
1603	Prompt leaking risk for AI	"Description
A prompt leak attack attempts to extract a model's system prompt (also known as the system message).  
Why is prompt leaking a concern for foundation models?"
1604	Prompt leaking risk for AI	"Why is prompt leaking a concern for foundation models?
A successful attack copies the system prompt used in the model. Depending on the content of that prompt, the attacker might gain access to valuable information, such as sensitive personal information or intellectual property, and might be able to replicate some of the functionality of the model."
1605	Prompt leaking risk for AI	"Description
Parent topic: AI risk atlas"
1606	Revealing confidential information risk for AI	"Risks associated with output  
Intellectual property  
Amplified by generative AI"
1607	Revealing confidential information risk for AI	"Description
When confidential information is used in training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing confidential information is a type of data leakage.  
Why is revealing confidential information a concern for foundation models?"
1608	Revealing confidential information risk for AI	"Why is revealing confidential information a concern for foundation models?
If not properly developed to secure confidential data, the model might reveal confidential information or IP in the generated output and reveal information that was meant to be secret."
1609	Revealing confidential information risk for AI	"Description
Parent topic: AI risk atlas"
1610	Prompt injection risk for AI	"Risks associated with input  
Inference  
Robustness  
New to generative AI"
1611	Prompt injection risk for AI	"Description
A prompt injection attack forces a model to produce unexpected output due to the structure or information contained in prompts.  
Why is prompt injection a concern for foundation models?"
1612	Prompt injection risk for AI	"Why is prompt injection a concern for foundation models?
Injection attacks can be used to alter model behavior and benefit the attacker. If not properly controlled, business entities could face fines, reputational harm, and other legal consequences."
1613	Prompt injection risk for AI	"Description
Parent topic: AI risk atlas"
1614	Unreliable source attribution risk for AI	"Risks associated with output  
Explainability  
Amplified by generative AI"
1615	Unreliable source attribution risk for AI	"Description
Source attribution is the AI system's ability to describe from what training data it generated a portion or all its output. Since current techniques are based on approximations, these attributions might be incorrect.  
Why is unreliable source attribution a concern for foundation models?"
1616	Unreliable source attribution risk for AI	"Why is unreliable source attribution a concern for foundation models?
Low quality explanations make it difficult for users, model validators, and auditors to understand and trust the model."
1617	Unreliable source attribution risk for AI	"Description
Parent topic: AI risk atlas"
1618	Copyright infringement risk for AI	"Risks associated with output  
Intellectual property  
New to generative AI"
1619	Copyright infringement risk for AI	"Description
When a model generates content that is too similar or identical to existing work protected by copyright or covered by an open source license agreement.  
Why is copyright infringement a concern for foundation models?"
1620	Copyright infringement risk for AI	"Why is copyright infringement a concern for foundation models?
Laws and regulations that concern the use of content that looks the same or similar to other copyrighted data are largely unsettled and can vary from country to country, providing challenges in determining and implementing compliance. Business entities might face fines, reputational harms, disruption to operations, and other legal consequences."
1621	Copyright infringement risk for AI	"Description
Parent topic: AI risk atlas"
1622	Evasion attack risk for AI	"Risks associated with input  
Inference  
Robustness  
Amplified by generative AI"
1623	Evasion attack risk for AI	"Description
Attempt to make a model output incorrect results by perturbing the data sent to the trained model.  
Why is evasion attack a concern for foundation models?"
1624	Evasion attack risk for AI	"Why is evasion attack a concern for foundation models?
Evasion attacks alter model behavior, usually to benefit the attacker. If not properly accounted for, business entities could face fines, reputational harms, and other legal consequences.  
Example  
Adversarial attacks on autonomous vehicles' AI components  
A report from the European Union Agency for Cybersecurity (ENISA) found that autonomous vehicles are “highly vulnerable to a wide range of attacks” that could be dangerous for passengers, pedestrians, and people in other vehicles. The report states that an adversarial attack might be used to make the AI 'blind' to pedestrians by manipulating the image recognition component to misclassify pedestrians. This attack could lead to havoc on the streets, as autonomous cars might hit pedestrians on the roads or crosswalks.  
Other studies demonstrated potential adversarial attacks on autonomous vehicles:  
Fooling machine learning algorithms by making minor changes to street sign graphics, such as adding stickers. Security researchers from Tencent demonstrated how adding three small stickers in an intersection could cause Tesla's autopilot system to swerve into the wrong lane. Two McAfee researchers demonstrated how using only black electrical tape could trick a 2016 Tesla into a dangerous burst of acceleration by changing a speed limit sign from 35 mph to 85 mph.  
Sources:  
Market Watch, February 2020"
1625	Evasion attack risk for AI	"Description
Parent topic: AI risk atlas"
1626	Reidentification risk for AI	"Risks associated with input  
Training and tuning phase  
Privacy  
Traditional AI risk"
1627	Reidentification risk for AI	"Description
Even with the removal or personal identifiable information (PII) and sensitive personal information (SPI) from data, it might still be possible to identify persons due to other features available in the data.  
Why is reidentification a concern for foundation models?"
1628	Reidentification risk for AI	"Why is reidentification a concern for foundation models?
Data that can reveal personal or sensitive data must be reviewed with respect to privacy laws and regulations, as business entities could face fines, reputational harms, and other legal consequences if found in violation."
1629	Reidentification risk for AI	"Description
Parent topic: AI risk atlas"
1630	Impact on human agency risk for AI	"Non-technical risks  
Societal impact  
Amplified by generative AI"
1631	Impact on human agency risk for AI	"Description
Misinformation and disinformation that is generated by foundation models, including the generation of manipulative content.  
Why is impact on human agency a concern for foundation models?"
1632	Impact on human agency risk for AI	"Why is impact on human agency a concern for foundation models?
AI might generate misinformation that looks real. Therefore, people might not recognize it as false information. Moreover, it might simplify the ability of nefarious actors to generate content with intention to manipulate human thoughts and behavior."
1633	Impact on human agency risk for AI	"Description
Parent topic: AI risk atlas"
1634	Data bias risk for AI	"Risks associated with input  
Training and tuning phase  
Fairness  
Amplified by generative AI"
1635	Data bias risk for AI	"Description
Historical, representational, and societal biases present in the data used to train and fine tune the model can adversely affect model behavior.  
Why is data bias a concern for foundation models?"
1636	Data bias risk for AI	"Why is data bias a concern for foundation models?
Training an AI system on data with bias, such as historical or representational bias, could lead to biased or skewed outputs that may unfairly represent or otherwise discriminate against certain groups or individuals. In addition to negative societal impacts, business entities could face legal consequences or reputational harms from biased model outcomes.  
Example  
Healthcare Bias  
According to the research article on reinforcing disparities in medicine using data and AI applications to transform how people receive healthcare is only as strong as the data behind the effort. For example, using training data with poor minority representation or that reflects what is already unequal care can lead to increased health inequalities.  
Sources:  
Forbes, December 2022"
1637	Data bias risk for AI	"Description
Parent topic: AI risk atlas"
1638	Data transparency risk for AI	"Risks associated with input  
Training and tuning phase  
Transparency  
Amplified by generative AI"
1639	Data transparency risk for AI	"Description
Without accurate documentation on how a model's data was collected, curated, and used to train a model, it might be harder to satisfactorily explain the behavior of the model with respect to the data.  
Why is data transparency a concern for foundation models?"
1640	Data transparency risk for AI	"Why is data transparency a concern for foundation models?
Data transparency is important for legal compliance and AI ethics. Missing information limits the ability to evaluate risks associated with the data. The lack of standardized requirements might limit disclosure as organizations protect trade secrets and try to limit others from copying their models.  
Example  
Data and Model Metadata Disclosure  
OpenAI's technical report is an example of the dichotomy around disclosing data and model metadata. While many model developers see value in enabling transparency for consumers, disclosure poses real safety issues and might increase the ability to misuse the models. In the GPT-4 technical report, the authors state: “Given both the competitive landscape and the safety implications of large-scale models like GPT-4, this report contains no further details about the architecture (including model size), hardware, training compute, data set construction, training method, or similar.”  
Sources:  
OpenAI, March 2023"
1641	Data transparency risk for AI	"Description
Parent topic: AI risk atlas"
1642	Downstream retraining risk for AI	"Risks associated with input  
Training and tuning phase  
Value alignment  
New to generative AI"
1643	Downstream retraining risk for AI	"Description
Using undesirable (for example, inaccurate, inappropriate, user’s content) output from downstream applications for retraining purposes.  
Why is downstream retraining a concern for foundation models?"
1644	Downstream retraining risk for AI	"Why is downstream retraining a concern for foundation models?
Repurposing downstream output for retraining a model without implementing proper human vetting increases the chances of undesirable outputs being incorporated into the training or tuning data of the model. This, in turn, can generate even more undesirable output. Improper model behavior can result in business entities facing legal consequences or reputational harms. Failing to comply with data transfer laws might result in fines and other legal consequences.  
Example  
Model collapse due to training using AI-generated content  
As stated in the source article, a group of researchers from the UK and Canada investigated the problem of using AI-generated content for training instead of human-generated content. They found that the large language models behind the technology might potentially be trained on other AI-generated content. As generated data continues to spread in droves across the internet it can result ina phenomenon they coined as ""model collapse.""  
Sources:  
Business Insider, August 2023"
1645	Downstream retraining risk for AI	"Description
Parent topic: AI risk atlas"
1646	Spreading toxicity risk for AI	"Risks associated with output  
Misuse  
New to generative AI"
1647	Spreading toxicity risk for AI	"Description
Using a model to generate hateful, abusive, and profane (HAP) or obscene content.  
Why is spreading toxicity a concern for foundation models?"
1648	Spreading toxicity risk for AI	"Why is spreading toxicity a concern for foundation models?
Toxic content might negatively affect the well-being of its recipients. A model that has this potential must be properly governed. Otherwise, business entities might face fines, reputational harms, disruption to operations, and other legal consequences.  
Example  
Harmful Content Generation  
According to the source article, an AI chatbot app was found to generate harmful content about suicide, including suicide methods, with minimal prompting. A Belgian man died by suicide after spending six weeks talking to that chatbot. The chatbot supplied increasingly harmful responses throughout their conversations and encouraged him to end his life.  
Sources:  
Business Insider, April 2023"
1649	Spreading toxicity risk for AI	"Description
Parent topic: AI risk atlas"
1650	Data aquisition risk for AI	"Risks associated with input  
Training and tuning phase  
Data laws  
Amplified by generative AI"
1651	Data aquisition risk for AI	"Description
Laws and other regulations might limit the collection of certain types of data for specific AI use cases.  
Why is data aquisition a concern for foundation models?"
1652	Data aquisition risk for AI	"Why is data aquisition a concern for foundation models?
Failing to comply with data acquisition laws and regulations might result in fines and other legal consequences."
1653	Data aquisition risk for AI	"Description
Parent topic: AI risk atlas"
1654	Nonconsensual use risk for AI	"Risks associated with output  
Misuse  
Amplified by generative AI"
1655	Nonconsensual use risk for AI	"Description
Using a model to imitate people through video (deepfakes), images, audio, or other modalities without their consent.  
Why is nonconsensual use a concern for foundation models?"
1656	Nonconsensual use risk for AI	"Why is nonconsensual use a concern for foundation models?
Deepfakes can spread disinformation about a person, possibly resulting in a negative impact on the person’s reputation. A model that has this potential must be properly governed. Otherwise, business entities might face fines, reputational harms, disruption to operations, and other legal consequences.  
Example  
FBI Warning on Deepfakes  
The FBI recently warned the public of malicious actors creating synthetic, explicit content “for the purposes of harassing victims or sextortion schemes”. They noted that advancements in AI made this content higher quality, more customizable, and more accessible than ever.  
Sources:  
FBI, June 2023  
Example  
Audio Deepfakes  
According to the source article, the Federal Communications Commission outlawed robocalls that contain voices that are generated by artificial intelligence. The announcement came after AI-generated robocalls mimicked the President's voice to discourage people from voting in the state's first-in-the-nation primary.  
Sources:  
AP News, February 2024"
1657	Nonconsensual use risk for AI	"Description
Parent topic: AI risk atlas"
1658	IP information in prompt risk for AI	"Risks associated with input  
Inference  
Intellectual property  
New to generative AI"
1659	IP information in prompt risk for AI	"Description
Disclosing copyright information or other IP information as a part of the prompt sent to the model.  
Why is ip information in prompt a concern for foundation models?"
1660	IP information in prompt risk for AI	"Why is ip information in prompt a concern for foundation models?
Prompt data might be stored or later used for other purposes like model evaluation and retraining. These types of data must be reviewed with IP laws and regulations. Without proper data storage and usage business entities could face fines, reputational harms, disruption to operations, and other legal consequences."
1661	IP information in prompt risk for AI	"Description
Parent topic: AI risk atlas"
1662	Unexplainable output risk for AI	"Risks associated with output  
Explainability  
Amplified by generative AI"
1663	Unexplainable output risk for AI	"Description
Explanations for model output decisions might be difficult, imprecise, or not possible to obtain.  
Why is unexplainable output a concern for foundation models?"
1664	Unexplainable output risk for AI	"Why is unexplainable output a concern for foundation models?
Foundation models are based on complex deep learning architectures, making explanations for their outputs difficult. Without clear explanations for model output, it is difficult for users, model validators, and auditors to understand and trust the model. Lack of transparency might carry legal consequences in highly regulated domains. Wrong explanations might lead to over-trust.  
Example  
Unexplainable accuracy in race prediction  
According to the source article, researchers analyzing multiple machine learning models using patient medical images were able to confirm the models’ ability to predict race with high accuracy from images. They were stumped as to what exactly is enabling the systems to consistently guess correctly. The researchers found that even factors like disease and physical build were not strong predictors of race—in other words, the algorithmic systems don’t seem to be using any particular aspect of the images to make their determinations.  
Sources:  
Banerjee et al., July 2021"
1665	Unexplainable output risk for AI	"Description
Parent topic: AI risk atlas"
1666	Personal information in prompt risk for AI	"Risks associated with input  
Inference  
Privacy  
New to generative AI"
1667	Personal information in prompt risk for AI	"Description
Disclosing Personal Information or Sensitive Personal Information as a part of a prompt that is sent to the model.  
Why is personal information in prompt a concern for foundation models?"
1668	Personal information in prompt risk for AI	"Why is personal information in prompt a concern for foundation models?
Prompt data might be stored or later used for other purposes like model evaluation and retraining. These types of data must be reviewed with privacy laws and regulations. Without proper data storage and usage business entities might face fines, reputational harms, disruption to operations, and other legal consequences.  
Example  
Disclose personal health information in ChatGPT prompts  
According to the source article, some people on social media shared about using ChatGPT as their makeshift therapists. Articles contend that users might include personal health information in their prompts during the interaction, which might raise privacy concerns. The information might be shared with the company that own the technology and might be used for training or tuning or even shared with unspecified third parties.  
Sources:  
The Conversation, February 2023"
1669	Personal information in prompt risk for AI	"Description
Parent topic: AI risk atlas"
1670	Data usage rights risk for AI	"Risks associated with input  
Training and tuning phase  
Intellectual property  
Amplified by generative AI"
1671	Data usage rights risk for AI	"Description
Terms of service, copyright laws, or other rules restrict the ability to use certain data for building models.  
Why is data usage rights a concern for foundation models?"
1672	Data usage rights risk for AI	"Why is data usage rights a concern for foundation models?
Laws and regulations concerning the use of data to train AI are unsettled and can vary from country to country, which creates challenges in the development of models. If data usage violates rules or restrictions, business entities might face fines, reputational harms, and other legal consequences.  
Example  
Text Copyright Infringement Claims  
According to the source article, The New York Times sued OpenAI and Microsoft, accusing them accusing them of using millions of the newspaper's articles without permission to help train chatbots to provide information to readers.  
Sources:  
Reuters, December 2023"
1673	Data usage rights risk for AI	"Description
Parent topic: AI risk atlas"
1674	Inaccessible training data risk for AI	"Risks associated with output  
Explainability  
Amplified by generative AI"
1675	Inaccessible training data risk for AI	"Description
Without access to the training data, the types of explanations a model can provide are limited and more likely to be incorrect.  
Why is inaccessible training data a concern for foundation models?"
1676	Inaccessible training data risk for AI	"Why is inaccessible training data a concern for foundation models?
Low quality explanations without source data make it difficult for users, model validators, and auditors to understand and trust the model."
1677	Inaccessible training data risk for AI	"Description
Parent topic: AI risk atlas"
1678	Improper usage risk for AI	"Risks associated with output  
Misuse  
Amplified by generative AI"
1679	Improper usage risk for AI	"Description
Using a model for a purpose the model was not designed for.  
Why is improper usage a concern for foundation models?"
1680	Improper usage risk for AI	"Why is improper usage a concern for foundation models?
Reusing a model without understanding its original data, design intent, and goals might result in unexpected and unwanted model behaviors."
1681	Improper usage risk for AI	"Description
Parent topic: AI risk atlas"
1682	Unspecified advice risk for AI	"Risks associated with output  
Value alignment  
New to generative AI"
1683	Unspecified advice risk for AI	"Description
When a model generates information that is factually correct but not specific enough for the current context, the advice can be potentially harmful. For example, a model might provide medical, financial, and legal advice or recommendations for a specific problem that the end user may act on even when they should not.  
Why is unspecified advice a concern for foundation models?"
1684	Unspecified advice risk for AI	"Why is unspecified advice a concern for foundation models?
A person might act on incomplete advice or worry about a situation that is not applicable to them due to the overgeneralized nature of the content generated."
1685	Unspecified advice risk for AI	"Description
Parent topic: AI risk atlas"
1686	Personal information in data risk for AI	"Risks associated with input  
Training and tuning phase  
Privacy  
Traditional AI risk"
1687	Personal information in data risk for AI	"Description
Inclusion or presence of personal identifiable information (PII) and sensitive personal information (SPI) in the data used for training or fine tuning the model might result in unwanted disclosure of that information.  
Why is personal information in data a concern for foundation models?"
1688	Personal information in data risk for AI	"Why is personal information in data a concern for foundation models?
If not properly developed to protect sensitive data, the model might expose personal information in the generated output. Additionally, personal or sensitive data must be reviewed and handled with respect to privacy laws and regulations, as business entities could face fines, reputational harms, and other legal consequences if found in violation.  
Example  
Training on Private Information  
According to the article, Google and its parent company Alphabet were accused in a class-action lawsuit of misusing vast amount of personal information and copyrighted material. The information was taken from hundreds of millions of internet users to train its commercial AI products, which include Bard, its conversational generative artificial intelligence chatbot. This case follows similar lawsuits that are filed against Meta Platforms, Microsoft, and OpenAI over their alleged misuse of personal data.  
Sources:  
Reuters, July 2023 J.L. v. Alphabet Inc., July 2023"
1689	Personal information in data risk for AI	"Description
Parent topic: AI risk atlas"
1690	Lack of data transparency risk for AI	"Non-technical risks  
Governance  
Amplified by generative AI"
1691	Lack of data transparency risk for AI	"Description
Insufficient documentation of the data that is used for model training makes it difficult to comply with legal requirements, including explaining the representativeness of data and the expected model behavior.  
Why is lack of data transparency a concern for foundation models?"
1692	Lack of data transparency risk for AI	"Why is lack of data transparency a concern for foundation models?
Transparency is important for legal compliance and AI ethics. Missing information might make it more difficult to evaluate representational harms, data ownership, and provenance."
1693	Lack of data transparency risk for AI	"Description
Parent topic: AI risk atlas"
1694	Confidential data in prompt risk for AI	"Risks associated with input  
Inference  
Intellectual property  
New to generative AI"
1695	Confidential data in prompt risk for AI	"Description
Inclusion of confidential data as a part of the prompt sent to the model.  
Why is confidential data in prompt a concern for foundation models?"
1696	Confidential data in prompt risk for AI	"Why is confidential data in prompt a concern for foundation models?
If not properly developed to secure confidential data, the model might reveal confidential information or IP in the generated output. Additionally, end users' confidential information might be unintentionally collected and stored.  
Example  
Disclosure of Confidential Information  
According to the source article, employees of Samsung disclosed confidential information to OpenAI through their use of ChatGPT. In one instance, an employee pasted confidential source code to check for errors. In another, an employee shared code with ChatGPT and ""requested code optimization."" A third shared a recording of a meeting to convert into notes for a presentation. Samsung has limited internal ChatGPT usage in response to these incidents, but it is unlikely that they are able to recall any of their data. Additionally, the article highlighted that in response to the risk of leaking confidential information and other sensitive information, companies like Apple, JPMorgan Chase. Deutsche Bank, Verizon, Walmart, Samsung, Amazon, and Accenture placed several restrictions on the usage of ChatGPT.  
Sources:  
Business Insider, February 2023"
1697	Confidential data in prompt risk for AI	"Description
Parent topic: AI risk atlas"
1698	Untraceable attribution risk for AI	"Risks associated with output  
Explainability  
Amplified by generative AI"
1699	Untraceable attribution risk for AI	"Description
The original entity from which training data comes from might not be known, limiting the utility and success of source attribution techniques.  
Why is untraceable attribution a concern for foundation models?"
1700	Untraceable attribution risk for AI	"Why is untraceable attribution a concern for foundation models?
The inability to provide the provenance for an explanation makes it difficult for users, model validators, and auditors to understand and trust the model."
1701	Untraceable attribution risk for AI	"Description
Parent topic: AI risk atlas"
1702	Informed consent risk for AI	"Risks associated with input  
Training and tuning phase  
Privacy  
Traditional AI risk"
1703	Informed consent risk for AI	"Description
Data collected for training AI models without the owner's informed consent even when it is legally permitted to do so.  
Why is informed consent a concern for foundation models?"
1704	Informed consent risk for AI	"Why is informed consent a concern for foundation models?
Under certain circumstances, it might be unethical to collect and use data without the person's consent. There are also possible reputational risks to such use."
1705	Informed consent risk for AI	"Description
Parent topic: AI risk atlas"
1706	Data poisoning risk for AI	"Risks associated with input  
Training and tuning phase  
Robustness  
Traditional AI risk"
1707	Data poisoning risk for AI	"Description
A type of adversarial attack where an adversary or malicious insider injects intentionally corrupted, false, misleading, or incorrect samples into the training or fine-tuning dataset.  
Why is data poisoning a concern for foundation models?"
1708	Data poisoning risk for AI	"Why is data poisoning a concern for foundation models?
Poisoning data can make the model sensitive to a malicious data pattern and produce the adversary’s desired output. It can create a security risk where adversaries can force model behavior for their own benefit. In addition to producing unintended and potentially malicious results, a model misalignment from data poisoning can result in business entities facing legal consequences or reputational harms."
1709	Data poisoning risk for AI	"Description
Parent topic: AI risk atlas"
1710	Job loss risk for AI	"Non-technical risks  
Societal impact  
Amplified by generative AI"
1711	Job loss risk for AI	"Description
Widespread adoption of foundation model-based AI systems might lead to people's job loss as their work is automated if they are not reskilled.  
Why is job loss a concern for foundation models?"
1712	Job loss risk for AI	"Why is job loss a concern for foundation models?
Job loss might lead to a loss of income and thus might negatively impact the society and human welfare. Reskilling might be challenging given the pace of the technology evolution.  
Example  
Replacing Human Workers  
According to the news article, AI technology replicating individuals' faces and voices is becoming more prominent in Hollywood. The actors’ concerns highlight a broader anxiety among entertainers and people in many other creative professions. Many fear that without strict regulation, their work gets replicated and remixed by artificial intelligence tools. Transformation on that scale will cut their control over their work and hurts their ability to earn a living. One of their key concerns is AI replacing non-speaking background roles by instead using a digital likeness.  
Sources:  
Reuters, July 2023"
1713	Job loss risk for AI	"Description
Parent topic: AI risk atlas"
1714	Data curation risk for AI	"Risks associated with input  
Training and tuning phase  
Value alignment  
Amplified by generative AI"
1715	Data curation risk for AI	"Description
When training or tuning data is improperly collected or prepared, the result can be a misalignment of a model's desired values or intent and the actual outcome.  
Why is data curation a concern for foundation models?"
1716	Data curation risk for AI	"Why is data curation a concern for foundation models?
Improper data curation can adversely affect how a model is trained, resulting in a model that does not behave in accordance with the intended values. Correcting problems after the model is trained and deployed might be insufficient for guaranteeing proper behavior. Improper model behavior can result in business entities facing legal consequences or reputational harms."
1717	Data curation risk for AI	"Description
Parent topic: AI risk atlas"
1718	Hallucination risk for AI	"Risks associated with output  
Value alignment  
New to generative AI"
1719	Hallucination risk for AI	"Description
Generation of factually inaccurate or untruthful content.  
Why is hallucination a concern for foundation models?"
1720	Hallucination risk for AI	"Why is hallucination a concern for foundation models?
False output can mislead users and be incorporated into downstream artifacts, further spreading misinformation. False output can harm both owners and users of the AI models. Also, business entities might face fines, reputational harms, disruption to operations, and other legal consequences.  
Example  
Fake Legal Cases  
According to the source article, a lawyer cited fake cases and quotations that are generated by ChatGPT in a legal brief that is filed in federal court. The lawyers consulted ChatGPT to supplement their legal research for an aviation injury claim. Subsequently, the lawyer asked ChatGPT if the cases provided were fake. The chatbot responded that they were real and “can be found on legal research databases such as Westlaw and LexisNexis.” The lawyer did not check the cases, and the court sanctioned them.  
Sources:  
AP News, June 2023"
1721	Hallucination risk for AI	"Description
Parent topic: AI risk atlas"
1722	Non-disclosure risk for AI	"Risks associated with output  
Misuse  
New to generative AI"
1723	Non-disclosure risk for AI	"Description
Not disclosing that content is generated by an AI model.  
Why is non-disclosure a concern for foundation models?"
1724	Non-disclosure risk for AI	"Why is non-disclosure a concern for foundation models?
Not disclosing the AI-authored content reduces trust and is deceptive. Intentional deception might result in decreased human agency, fines, reputational harms, and other legal consequences.  
Example  
Undisclosed AI Interaction  
According to the source article, an online emotional support chat service ran a study to augment or write responses to around 4,000 users by using GPT-3 without informing users. The co-founder faced immense public backlash about the potential for harm that is caused by AI-generated chats to the already vulnerable users. He claimed that the study was ""exempt"" from informed consent law.  
Sources:  
Business Insider, Jan 2023"
1725	Non-disclosure risk for AI	"Description
Parent topic: AI risk atlas"
1726	Jailbreaking risk for AI	"Risks associated with input  
Inference  
Multi-category  
Amplified by generative AI"
1727	Jailbreaking risk for AI	"Description
An attack that attempts to break through the guardrails established in the model is known as jailbreaking.  
Why is jailbreaking a concern for foundation models?"
1728	Jailbreaking risk for AI	"Why is jailbreaking a concern for foundation models?
Jailbreaking attacks can be used to alter model behavior and benefit the attacker. If not properly controlled, business entities can face fines, reputational harm, and other legal consequences.  
Example  
Bypassing LLM guardrails  
A study cited by researchers at Carnegie Mellon University, The Center for AI Safety, and the Bosch Center for AI, claim to have discovered a simple prompt addendum that allowed the researchers to trick models into generating biased, false, and otherwise toxic information. The researchers showed that they might circumvent these guardrails in a more automated way. These attacks were shown to be effective in a wide range of open source products, including ChatGPT, Google Bard, Meta’s LLaMA, Anthropic’s Claude, and others.  
Sources:  
The New York Times, July 2023"
1729	Jailbreaking risk for AI	"Description
Parent topic: AI risk atlas"
1730	Plagiarism risk for AI	"Non-technical risks  
Societal impact  
New to generative AI"
1731	Plagiarism risk for AI	"Description
Using AI models to plagiarize existing work intentionally or unintentionally.  
Why is plagiarism a concern for foundation models?"
1732	Plagiarism risk for AI	"Why is plagiarism a concern for foundation models?
AI models can be used to claim the authorship or originality of works that were created by other people in doing so by engaging in plagiarism. Claiming others’ work as your own is both unethical and often illegal."
1733	Plagiarism risk for AI	"Description
Parent topic: AI risk atlas"
1734	AI risk atlas	"Explore this atlas to understand some of the risks of working with generative AI, foundation models, and machine learning models.  
Risks are categorized with one of these tags:  
Traditional AI risks (applies to traditional models as well as generative AI) Risks amplified by generative AI (might also apply to traditional models) New risks specifically associated with generative AI  
Risks associated with input  
Training and tuning phase Inference phase  
Fairness  
Data bias  
Amplified  
Robustness  
Data poisoning  
Traditional  
Value alignment  
Data curation  
Amplified  
Downstream retraining  
New  
Data laws  
Data transfer  
Traditional  
Data usage  
Traditional  
Data aquisition  
Amplified  
Intellectual property  
Data usage rights  
Amplified  
Confidential information in data  
Traditional  
Transparency  
Data transparency  
Amplified  
Data provenance  
Amplified  
Privacy  
Personal information in data  
Traditional  
Reidentification  
Traditional  
Data privacy rights  
Amplified  
Informed consent  
Traditional  
Privacy  
Personal information in prompt  
New  
Membership inference attack  
Traditional  
Attribute inference attack  
Amplified  
Intellectual property  
IP information in prompt  
New  
Confidential data in prompt  
New  
Robustness  
Evasion attack  
Amplified  
Extraction attack  
Amplified  
Prompt injection  
New  
Prompt leaking  
Amplified  
Multi-category  
Prompt priming  
Amplified  
Jailbreaking  
Amplified  
Risks associated with output  
Fairness  
Output bias  
New  
Decision bias  
Traditional  
Intellectual property  
Copyright infringement  
New  
Revealing confidential information  
Amplified  
Value alignment  
Hallucination  
New  
Toxic output  
New  
Over or under reliance  
Amplified  
Physical harm  
New  
Unspecified advice  
New  
Misuse  
Spreading disinformation  
New  
Spreading toxicity  
New  
Nonconsensual use"
1735	AI risk atlas	"Amplified  
Physical harm  
New  
Unspecified advice  
New  
Misuse  
Spreading disinformation  
New  
Spreading toxicity  
New  
Nonconsensual use  
Amplified  
Dangerous use  
New  
Non-disclosure  
New  
Improper usage  
Amplified  
Harmful code generation  
Harmful code generation  
New  
Privacy  
Revealing personal information  
Amplified  
Explainability  
Unexplainable output  
Amplified  
Unreliable source attribution  
Amplified  
Inaccessible training data  
Amplified  
Untraceable attribution  
Amplified  
Non-technical risks  
Governance  
Lack of model transparency  
Traditional  
Lack of data transparency  
Amplified  
Accountability  
Amplified  
Legal compliance  
Legal accountability  
New  
Generated content ownership  
New  
Generated content IP  
New  
Societal impact  
Job loss  
Amplified  
Human exploitation  
Amplified  
Impact on the environment  
Amplified  
Impact on cultural diversity  
New  
Impact on human agency  
Amplified  
Bypassing learning  
New  
Plagiarism  
New"
1736	Impact on the environment risk for AI	"Non-technical risks  
Societal impact  
Amplified by generative AI"
1737	Impact on the environment risk for AI	"Description
Increased carbon emission and water usage to train and operate AI models.  
Why is impact on the environment a concern for foundation models?"
1738	Impact on the environment risk for AI	"Why is impact on the environment a concern for foundation models?
Consuming large amounts of energy for AI training contributes to carbon emissions that might accelerate climate change. Water resources that are used for cooling AI data center servers can no longer be allocated for other necessary uses."
1739	Impact on the environment risk for AI	"Description
Parent topic: AI risk atlas"
1740	Extraction attack risk for AI	"Risks associated with input  
Inference  
Robustness  
Amplified by generative AI"
1741	Extraction attack risk for AI	"Description
An extraction attack attempts to copy or steal an AI model by appropriately sampling the input space and observing outputs to build a surrogate model that behaves similarly.  
Why is extraction attack a concern for foundation models?"
1742	Extraction attack risk for AI	"Why is extraction attack a concern for foundation models?
With a successful attack, the attacker can gain valuable information such as sensitive personal information or intellectual property."
1743	Extraction attack risk for AI	"Description
Parent topic: AI risk atlas"
1744	Revealing personal information risk for AI	"Risks associated with output  
Privacy  
Amplified by generative AI"
1745	Revealing personal information risk for AI	"Description
When personal identifiable information (PII) or sensitive personal information (SPI) are used in training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing personal information is a type of data leakage.  
Why is revealing personal information a concern for foundation models?"
1746	Revealing personal information risk for AI	"Why is revealing personal information a concern for foundation models?
Sharing people's personal information impacts their rights and make them more vulnerable. Additionally, output data must be reviewed to comply with privacy laws and regulations. Business entities might face fines, reputational harms, disruption to operations, and other legal consequences if found in violation of data privacy or usage laws.  
Example  
Exposure of personal information  
Per the source article, ChatGPT suffered a bug and exposed titles and active users' chat history to other users. Later, OpenAI shared that even more private data from a small number of users was exposed including, active user’s first and last name, email address, payment address, the last four digits of their credit card number, and credit card expiration date. In addition, it was reported that the payment-related information of 1.2% of ChatGPT Plus subscribers were also exposed in the outage.  
Sources:  
The Hindu Business Line, March 2023"
1747	Revealing personal information risk for AI	"Description
Parent topic: AI risk atlas"
1748	Data usage risk for AI	"Risks associated with input  
Training and tuning phase  
Data laws  
Traditional AI risk"
1749	Data usage risk for AI	"Description
Laws and other restrictions can limit or prohibit the use of some data for specific AI use cases.  
Why is data usage a concern for foundation models?"
1750	Data usage risk for AI	"Why is data usage a concern for foundation models?
Failing to comply with data usage laws might result in fines and other legal consequences."
1751	Data usage risk for AI	"Description
Parent topic: AI risk atlas"
1752	Spreading disinformation risk for AI	"Risks associated with output  
Misuse  
New to generative AI"
1753	Spreading disinformation risk for AI	"Description
Using a model to create misleading or false information to deceive or influence a targeted audience.  
Why is spreading disinformation a concern for foundation models?"
1754	Spreading disinformation risk for AI	"Why is spreading disinformation a concern for foundation models?
Spreading disinformation might affect human's ability to make informed decisions. A model that has this potential must be properly governed. Otherwise, business entities might face fines, reputational harms, disruption to operations, and other legal consequences.  
Example  
Generation of False Information  
According to the cited news articles, generative AI poses a threat to democratic elections by making it easier for malicious actors to create and spread false content to sway election outcomes. The examples that are cited include:  
Robocall messages that are generated in a candidate’s voice instructed voters to cast ballots on the wrong date. Synthesized audio recordings of a candidate that confessed to a crime or expressing racist views. AI-generated video footage showed a candidate giving a speech or interview they never gave. Fake images that are designed to look like local news reports. Falsely claiming a candidate dropped out of the race.  
Sources:  
AP News, May 2023 The Guardian, July 2023"
1755	Spreading disinformation risk for AI	"Description
Parent topic: AI risk atlas"
1756	Data provenance risk for AI	"Risks associated with input  
Training and tuning phase  
Transparency  
Amplified by generative AI"
1757	Data provenance risk for AI	"Description
Without standardized and established methods for verifying where data came from, there are no guarantees that available data is what it claims to be.  
Why is data provenance a concern for foundation models?"
1758	Data provenance risk for AI	"Why is data provenance a concern for foundation models?
Not all data sources are trustworthy. Data might have been unethically collected, manipulated, or falsified. Using such data can result in undesirable behaviors in the model. Business entities could face fines, reputational harms, and other legal consequences."
1759	Data provenance risk for AI	"Description
Parent topic: AI risk atlas"
1760	Data privacy rights risk for AI	"Risks associated with input  
Training and tuning phase  
Privacy  
Amplified by generative AI"
1761	Data privacy rights risk for AI	"Description
Challenges around the ability to provide data subject rights such as opt-out, right to access, right to be forgotten.  
Why is data privacy rights a concern for foundation models?"
1762	Data privacy rights risk for AI	"Why is data privacy rights a concern for foundation models?
The identification or improper usage of data might lead to violation of privacy laws. Improper usage or a request for data removal might force organizations to retrain the model, which is expensive. In addition, business entities might face fines, reputational harms, disruption to operations, and other legal consequences if they fail to comply with data privacy rules and regulations.  
Example  
Right to Be Forgotten (RTBF)  
Laws in multiple locales, including Europe (GDPR), grant data subjects the right to request personal data to be deleted by organizations (‘Right To Be Forgotten’, or RTBF). However, emerging, and increasingly popular large language model (LLM) -enabled software systems present new challenges for this right. According to research by CSIRO’s Data61, data subjects can identify usage of their personal information in an LLM “by either inspecting the original training data set or perhaps prompting the model.” However, training data might not be public, or companies do not disclose it, citing safety and other concerns. Guardrails might also prevent users from accessing the information by prompting. Due to these barriers, data subjects might not be able to initiate RTBF procedures and companies that deploy LLMs might not be able to meet RTBF laws.  
Sources:  
Zhang et al., September 2023  
Example  
Lawsuit About LLM Unlearning  
According to the report, a lawsuit was filed against Google that alleges the use of copyright material and personal information as training data for its AI systems, which includes its Bard chatbot. Opt-out and deletion rights are guaranteed rights for California residents under the CCPA and children in the United States under the age of 13 with COPPA. The plaintiffs allege that because there is no way for Bard to “unlearn” or fully remove all the scraped PI it has been fed. The plaintiffs note that Bard’s privacy notice states that Bard conversations cannot be deleted by the user after they have been reviewed and annotated by the company and might be kept up to 3 years. P allege that these practices further contribute to noncompliance with these laws.  
Sources:  
Reuters, July 2023 J.L. v. Alphabet Inc., July 2023"
1763	Data privacy rights risk for AI	"Description
Parent topic: AI risk atlas"
1764	Over or under reliance risk for AI	"Risks associated with output  
Value alignment  
Amplified by generative AI"
1765	Over or under reliance risk for AI	"Description
When a person places too little or too much trust in an AI model's guidance  
Why is over or under reliance a concern for foundation models?"
1766	Over or under reliance risk for AI	"Why is over or under reliance a concern for foundation models?
In tasks where humans make choices based on AI-based suggestions, over or under reliance can lead to poor decision making because of the misplaced trust in the AI system. Negative consequences can increase with the importance of the decision. Bad decisions can harm people and can lead to financial harm, reputational harm, disruption to operations, and other legal consequences for business entities."
1767	Over or under reliance risk for AI	"Description
Parent topic: AI risk atlas"
1768	Harmful code generation risk for AI	"Risks associated with output  
Harmful code generation  
New to generative AI"
1769	Harmful code generation risk for AI	"Description
Models might generate code that causes harm or unintentionally affects other systems.  
Why is harmful code generation a concern for foundation models?"
1770	Harmful code generation risk for AI	"Why is harmful code generation a concern for foundation models?
The execution of harmful code might open vulnerabilities in IT systems. Business entities might face fines, reputational harms, disruption to operations, and other legal consequences.  
Example  
Generation of Less Secure Code  
According to their paper, researchers at Stanford University investigated the impact of code-generation tools on code quality and found that programmers tend to include more bugs in their final code when they use AI assistants. These bugs might increase the code's security vulnerabilities, yet the programmers believed their code to be more secure.  
Sources:  
Neil Perry, Megha Srivastava, Deepak Kumar, and Dan Boneh. 2023. Do Users Write More Insecure Code with AI Assistants?. In Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security (CCS '23), November 26-30, 2023, Copenhagen, Denmark. ACM, New York, NY, USA, 15 pages."
1771	Harmful code generation risk for AI	"Description
Parent topic: AI risk atlas"
1772	Attribute inference attack risk for AI	"Risks associated with input  
Inference  
Privacy  
Amplified by generative AI"
1773	Attribute inference attack risk for AI	"Description
An attribute inference attack is used to detect whether certain sensitive features can be inferred about individuals who participated in training a model. These attacks occur when an adversary has some prior knowledge about the training data and uses that knowledge to infer the sensitive data.  
Why is attribute inference attack a concern for foundation models?"
1774	Attribute inference attack risk for AI	"Why is attribute inference attack a concern for foundation models?
With a successful attack, the attacker can gain valuable information such as sensitive personal information or intellectual property."
1775	Attribute inference attack risk for AI	"Description
Parent topic: AI risk atlas"
1776	Lack of model transparency risk for AI	"Non-technical risks  
Governance  
Traditional AI risk"
1777	Lack of model transparency risk for AI	"Description
Insufficient documentation of the model development process makes it difficult to understand how and why a model was built and who built it, thus increasing the possibility of model unintended misuse.  
Why is lack of model transparency a concern for foundation models?"
1778	Lack of model transparency risk for AI	"Why is lack of model transparency a concern for foundation models?
Transparency is important for legal compliance, AI ethics, and guiding appropriate use of models. Missing information might make it more difficult to evaluate risks, to change the model, or reuse it. Knowledge about who built a model can also be an important factor in deciding whether to trust it.  
Example  
Data and Model Metadata Disclosure  
OpenAI's technical report is an example of the dichotomy around disclosing data and model metadata. While many model developers see value in enabling transparency for consumers, disclosure poses real safety issues and might increase the ability to misuse the models. In the GPT-4 technical report, they state: ”Given both the competitive landscape and the safety implications of large-scale models like GPT-4, this report contains no further details about the architecture (including model size), hardware, training compute, data set construction, training method, or similar.”  
Sources:  
OpenAI, March 2023"
1779	Lack of model transparency risk for AI	"Description
Parent topic: AI risk atlas"
1780	Prompt priming risk for AI	"Risks associated with input  
Inference  
Multi-category  
Amplified by generative AI"
1781	Prompt priming risk for AI	"Description
Because generative models tend to produce output like the input provided, the model can be prompted to reveal specific kinds of information. For example, adding personal information in the prompt increases its likelihood of generating similar kinds of personal information in its output. If personal data was included as part of the model’s training, there is a possibility it could be revealed.  
Why is prompt priming a concern for foundation models?"
1782	Prompt priming risk for AI	"Why is prompt priming a concern for foundation models?
Depending on the content revealed, business entities could face fines, reputational harm, and other legal consequences."
1783	Prompt priming risk for AI	"Description
Parent topic: AI risk atlas"
1784	Dangerous use risk for AI	"Risks associated with output  
Misuse  
New to generative AI"
1785	Dangerous use risk for AI	"Description
Using a model with the sole intention of harming people.  
Why is dangerous use a concern for foundation models?"
1786	Dangerous use risk for AI	"Why is dangerous use a concern for foundation models?
A model that has this potential must be properly governed. Otherwise, business entities might face fines, reputational harms, disruption to operations, and other legal consequences."
1787	Dangerous use risk for AI	"Description
Parent topic: AI risk atlas"
1788	Bypassing learning risk for AI	"Non-technical risks  
Societal impact  
New to generative AI"
1789	Bypassing learning risk for AI	"Description
Using AI models to bypass the learning process.  
Why is bypassing learning a concern for foundation models?"
1790	Bypassing learning risk for AI	"Why is bypassing learning a concern for foundation models?
AI models are quick to find solutions or solve complex problems. These systems can be misused by students to bypass the learning process. The ease of access to these models results in students having a superficial understanding of concepts and hampers further education that might rely on understanding those concepts."
1791	Bypassing learning risk for AI	"Description
Parent topic: AI risk atlas"
1792	Accountability risk for AI	"Non-technical risks  
Governance  
Amplified by generative AI"
1793	Accountability risk for AI	"Description
The foundation model development process is complex with lots of data, processes, and roles. When model output does not work as expected, it can be difficult to determine the root cause and assign responsibility.  
Why is accountability a concern for foundation models?"
1794	Accountability risk for AI	"Why is accountability a concern for foundation models?
Without properly documenting decisions and assigning responsibility, determining liability for unexpected behavior or misuse might not be possible.  
Example  
Determining responsibility for generated output  
Major journals like the Science and Nature banned ChatGPT from being listed as an author, as responsible authorship requires accountability and AI tools cannot take such responsibility.  
Sources:  
The Guardian, January 2023"
1795	Accountability risk for AI	"Description
Parent topic: AI risk atlas"
1796	Output bias risk for AI	"Risks associated with output  
Fairness  
New to generative AI"
1797	Output bias risk for AI	"Description
Generated content might unfairly represent certain groups or individuals.  
Why is output bias a concern for foundation models?"
1798	Output bias risk for AI	"Why is output bias a concern for foundation models?
Bias can harm users of the AI models and magnify existing discriminatory behaviors. Business entities might face reputational harms, disruption to operations, and other consequences.  
Example  
Biased Generated Images  
Lensa AI is a mobile app with generative features that are trained on Stable Diffusion that can generate “Magic Avatars” based on images that users upload of themselves. According to the source report, some users discovered that generated avatars are sexualized and racialized.  
Sources:  
Business Insider, January 2023"
1799	Output bias risk for AI	"Description
Parent topic: AI risk atlas"
1800	Membership inference attack risk for AI	"Risks associated with input  
Inference  
Privacy  
Traditional AI risk"
1801	Membership inference attack risk for AI	"Description
Given a trained model and a data sample, an attacker appropriately samples the input space, observing outputs to deduce whether that sample was part of the model's training. This is known as a membership inference attack.  
Why is membership inference attack a concern for foundation models?"
1802	Membership inference attack risk for AI	"Why is membership inference attack a concern for foundation models?
Identifying whether a data sample was used for training data can reveal what data was used to train a model, possibly giving competitors insight into how a model was trained and the opportunity to replicate the model or tamper with it."
1803	Membership inference attack risk for AI	"Description
Parent topic: AI risk atlas"
1804	Generated content IP risk for AI	"Non-technical risks  
Legal compliance  
New to generative AI"
1805	Generated content IP risk for AI	"Description
Legal uncertainty about intellectual property rights related to generated content  
Why is generated content ip a concern for foundation models?"
1806	Generated content IP risk for AI	"Why is generated content ip a concern for foundation models?
Laws and regulations for the copyrightability, and patentability of the AI-generated content are largely unsettled and can vary from country to country. Business entities might face fines, reputational risks, disruption to operation, and other legal consequences if the generated content is covered by IP rights.  
Example  
Role of AI systems in Patenting Generated Content  
The U.S. Supreme Court declined to hear a challenge to the U.S. Patent and Trademark Office's refusal to issue patents for inventions created by an AI system. According to the scientist, his AI system created unique prototypes for a beverage holder and emergency light beacon entirely on its own. The justices rejected the appeal of a lower court's ruling that patents can be issued only to human inventors and that the scientist's AI system could not be considered the legal creator of two inventions it generated. According to the cited article, the UK’s Intellectual Property Office also refused to grant a patent on the grounds that the inventor must be a human or a company, rather than a machine.  
Sources:  
Reuters, April 2023 Reuters, December 2023"
1807	Generated content IP risk for AI	"Description
Parent topic: AI risk atlas"
1808	Confidential information in data risk for AI	"Risks associated with input  
Training and tuning phase  
Intellectual property  
Traditional AI risk"
1809	Confidential information in data risk for AI	"Description
Models might be trained or fine-tuned using confidential data or the company’s intellectual property, which could result in unwanted disclosure of that information.  
Why is confidential information in data a concern for foundation models?"
1810	Confidential information in data risk for AI	"Why is confidential information in data a concern for foundation models?
If not developed in accordance with data protection rules and regulations, the model might expose confidential information or IP in the generated output or through an adversarial attack."
1811	Confidential information in data risk for AI	"Description
Parent topic: AI risk atlas"
1812	Data transfer risk for AI	"Risks associated with input  
Training and tuning phase  
Data laws  
Traditional AI risk"
1813	Data transfer risk for AI	"Description
Laws and other restrictions can limit or prohibit transferring data.  
Why is data transfer a concern for foundation models?"
1814	Data transfer risk for AI	"Why is data transfer a concern for foundation models?
Data transfer restrictions can impact the availability of the data that is required for training an AI model and can lead to poorly represented data. In addition to impact on data availability, failure to comply with data transfer laws and regulations might result in fines and other legal consequences.  
Example  
Data Restriction Laws  
As stated in the research article, data localization measures, which restrict the ability to move data globally reduce the capacity to develop tailored AI capacities. It affects AI directly by providing less training data and indirectly by undercutting the building blocks on which AI is built.  
Examples include China's data localization laws, and GDPR restrictions on the processing and use of personal data.  
Sources:  
Brookings, December 2018"
1815	Data transfer risk for AI	"Description
Parent topic: AI risk atlas"
1816	Generated content ownership risk for AI	"Non-technical risks  
Legal compliance  
New to generative AI"
1817	Generated content ownership risk for AI	"Description
Determining ownership of AI-generated content  
Why is generated content ownership a concern for foundation models?"
1818	Generated content ownership risk for AI	"Why is generated content ownership a concern for foundation models?
Laws and regulations that relate to the ownership of AI-generated content are largely unsettled and can vary from country to country. Business entities might face fines, reputational risks, disruption to operations, and other legal consequences.  
Example  
Determining Ownership of AI Generated Image  
According to the news article, AI-generated art became controversial after an AI-generated work of art won the Colorado State Fair’s art competition in 2022. The piece was generated by Midjourney, a generative AI image tool, following prompts from the artist. The win raised questions about copyright issues. In other words, if all the artist did was come up with a description of the art, but the AI tool generated it, who owns the rights to the generated image? According to the latest article, The U.S. Copyright Office rejected copyright protection for the art created with artificial intelligence because it was not the product of human authorship.  
Sources:  
The New York Times, September 2022 Reuters, September 2023"
1819	Generated content ownership risk for AI	"Description
Parent topic: AI risk atlas"
1820	Decision bias risk for AI	"Risks associated with output  
Fairness  
Traditional AI risk"
1821	Decision bias risk for AI	"Description
When one group is unfairly advantaged over another due to decisions of the model.  
Why is decision bias a concern for foundation models?"
1822	Decision bias risk for AI	"Why is decision bias a concern for foundation models?
Bias can harm persons who are affected by the decisions of the model. Business entities might face fines, reputational harms, disruption to operations, and other legal consequences.  
Example  
Unfairly Advantaged Groups  
The 2018 Gender Shades study demonstrated that machine learning algorithms can discriminate based on classes like race and gender. Researchers evaluated commercial gender classification systems that are sold by companies like Microsoft, IBM, and Amazon and showed that darker-skinned females are the most misclassified group (with error rates of up to 35%). In comparison, the error rates for lighter-skinned were no more than 1%.  
Sources:  
TIME, Feburary 2019"
1823	Decision bias risk for AI	"Description
Parent topic: AI risk atlas"
1824	Troubleshooting Watson OpenScale	"You can use the following techniques to work around problems with IBM Watson OpenScale.  
When I use AutoAI, why am I getting an error about mismatched data? Why am I getting errors during model configuration? Why are my class labels missing when I use XGBoost? Why are the payload analytics not displaying properly? Error: An error occurred while computing feature importance Why are some of my active debias records missing? Watson OpenScale does not show any available schemas A monitor run fails with an OutOfResources exception error message"
1825	Troubleshooting Watson OpenScale	"When I use AutoAI, why am I getting an error about mismatched data?
You receive an error message about mismatched data when using AutoAI for binary classification. Note that AutoAI is only supported in IBM Watson OpenScale for IBM Cloud Pak for Data.  
For binary classification type, AutoAI automatically sets the data type of the prediction column to boolean.  
To fix this, implement one of the following solutions:  
Change the label column values in the training data to integer values, such as 0 or 1 depending on the outcome. Change the label column values in the training data to string value, such as A and B."
1826	Troubleshooting Watson OpenScale	"Why am I getting errors during model configuration?
The following error messages appear when you are configuring model details: Field feature_fields references column <name>, which is missing in input_schema of the model. Feature not found in input schema.  
The preceding messages while completing the Model details section during configuration indicate a mismatch between the model input schema and the model training data schema:  
To fix the issue, you must determine which of the following conditions is causing the error and take corrective action: If you use IBM Watson Machine Learning as your machine learning provider and the model type is XGBoost/scikit-learn refer to the Machine Learning Python SDK documentation for important information about how to store the model. To generate the drift detection model, you must use scikit-learn version 0.20.2 in notebooks. For all other cases, you must ensure that the training data column names match with the input schema column names."
1827	Troubleshooting Watson OpenScale	"Why are my class labels missing when I use XGBoost?
Native XGBoost multiclass classification does not return class labels.  
By default, for binary and multiple class models, the XGBoost framework does not return class labels.  
For XGBoost binary and multiple class models, you must update the model to return class labels."
1828	Troubleshooting Watson OpenScale	"Why are the payload analytics not displaying properly?
Payload analytics does not display properly and the following error message displays: AIQDT0044E Forbidden character "" in column name <column name>  
For proper processing of payload analytics, Watson OpenScale does not support column names with double quotation marks ("") in the payload. This affects both scoring payload and feedback data in CSV and JSON formats.  
Remove double quotation marks ("") from the column names of the payload file."
1829	Troubleshooting Watson OpenScale	"Error: An error occurred while computing feature importance
You receive the following error message during processing: Error: An error occurred while computing feature importance.  
Having an equals sign (=) in the column name of a dataset causes an issue with explainability.  
Remove the equals sign (=) from the column name and send the dataset through processing again."
1830	Troubleshooting Watson OpenScale	"Why are some of my active debias records missing?
Active debias records do not reach the payload logging table.  
When you use the active debias API, there is a limit of 1000 records that can be sent at one time for payload logging.  
To avoid loss of data, you must use the active debias API to score in chunks of 1000 records or fewer.  
For more information, see Reviewing debiased transactions."
1831	Troubleshooting Watson OpenScale	"Watson OpenScale does not show any available schemas
When a user attempts to retrieve schema information for Watson OpenScale, none are available. After attempting directly in DB2, without reference to Watson OpenScale, checking what schemas are available for the database userid also returns none.  
Insufficient permissions for the database userid is causing database connection issues for Watson OpenScale.  
Make sure the database user has the correct permissions needed for Watson OpenScale."
1832	Troubleshooting Watson OpenScale	"A monitor run fails with an OutOfResources exception error message
You receive an OutOfResources exception error message.  
Although there's no longer a limit on the number of rows you can have in the feedback payload, scoring payload, or business payload tables. The 50,000 limit now applies to the number of records you can run through the quality and bias monitors each billing period.  
After you reach your limit, you must either upgrade to a Standard plan or wait for the next billing period."
1833	Troubleshooting Watson OpenScale	"Missing deployments
A deployed model does not show up as a deployment that can be selected to create a subscription.  
There are different reasons that a deployment does not show up in the list of available deployed models. If the model is not a supported type of model because it uses an unsupported algorithm or framework, it won't appear. Your machine learning provider might not be configured properly. It could also be that there are issues with permissions.  
Use the following steps to resolve this issue:  
Check that the model is a supported type. Not sure? For more information, see Supported machine learning engines, frameworks, and models. Check that a machine learning provider exists in the Watson OpenScale configuration for the specific deployment space. For more information, see Deployment spaces. Check that the CP4D admin user has permission to access the deployment space."
1834	Troubleshooting Watson OpenScale	"Missing deployments
Watson OpenScale evaluation might fail due to large number of subscriptions
If a Watson OpenScale instance contains too many subscriptions, such as 100 subscriptions, your quality evaluations might fail. You can view the details of the failure in the log for the data mart service pod that displays the following error message:  
You can use the oc get pod -l component=aios-datamart command to find the name of the pod. You can also use the oc logs <pod name> command to the log for the pod.  
To fix this error, you can use the following command to increase the maximum request body size by editing the ""ADDITIONAL_JVM_OPTIONS"" environment variable:  
The release name is ""aiopenscale"" if you don't customize the release name when you install Watson OpenScale."
1835	Troubleshooting Watson OpenScale	"Missing deployments
Microsoft Azure ML Studio
Of the two types of Azure Machine Learning web services, only the New type is supported by Watson OpenScale. The Classic type is not supported.  
Default input name must be used: In the Azure web service, the default input name is ""input1"". Currently, this field is mandated for Watson OpenScale and, if it is missing, Watson OpenScale will not work.  
If your Azure web service does not use the default name, change the input field name to ""input1"", then redeploy your web service and reconfigure your OpenScale machine learning provider settings.  
If calls to Microsoft Azure ML Studio to list the machine learning models causes the response to time out, for example when you have many web services, you must increase timeout values. You may need to work around this issue by changing the /etc/haproxy/haproxy.cfg configuration setting:  
timeout client 5m timeout server 5m  
Log in to the load balancer node and update /etc/haproxy/haproxy.cfg to set the client and server timeout from 1m to 5m:  
Run systemctl restart haproxy to restart the HAProxy load balancer.  
If you are using a different load balancer, other than HAProxy, you may need to adjust timeout values in a similar fashion.  
Of the two types of Azure Machine Learning web services, only the New type is supported by Watson OpenScale. The Classic type is not supported."
1836	Troubleshooting Watson OpenScale	"Missing deployments
Uploading feedback data fails in production subscription after importing settings
After importing the settings from your pre-production space to your production space you might have problems uploading feedback data. This happens when the datatypes do not match precisely. When you import settings, the feedback table references the payload table for its column types. You can avoid this issue by making sure that the payload data has the most precise value type first. For example, you must prioritize a double datatype over an integer datatype."
1837	Troubleshooting Watson OpenScale	"Missing deployments
Microsoft Azure Machine Learning Service
When performing model evaluation, you may encounter issues where Watson OpenScale is not able to communicate with Azure Machine Learning Service, when it needs to invoke deployment scoring endpoints. Security tools that enforce your enterprise security policies, such as Symantec Blue Coat may prevent such access."
1838	Troubleshooting Watson OpenScale	"Missing deployments
Watson OpenScale fails to create a new Hive table for the batch deployment subscription
When you choose to create a new Apache Hive table with the Parquet format during your Watson OpenScale batch deployment configuration, the following error might occur:  
This error occurs if Watson OpenScale fails to run the CREATE TABLE SQL operation due to white space in a column name. To avoid this error, you can remove any white space from your column names or change the Apache Hive format to csv."
1839	Troubleshooting Watson OpenScale	"Missing deployments
Watson OpenScale setup might fail with default Db2 database
When you set up Watson OpenScale and specify the default Db2 database, the setup might fail to complete.  
To fix this issue, you must run the following command in Cloud Pak for Data to update Db2:  
After you run the command, you must create a new Db2 database to set up Watson OpenScale.  
Parent topic: Troubleshooting"
1840	Troubleshooting Cloud Object Storage for projects	"Use these solutions to resolve issues you might experience when using Cloud Object Storage with projects in IBM watsonx. Many errors that occur when creating projects can be resolved by correctly configuring Cloud Object Storage. For instructions, see Setting up Cloud Object Storage.  
Possible error messages:  
Error retrieving Administrator API key token for your Cloud Object Storage instance Unable to configure credentials for your project in the selected Cloud Object Storage instance User login from given IP address is not permitted Project cannot be created"
1841	Troubleshooting Cloud Object Storage for projects	"Cannot retrieve API key
Symptoms
When you create a project, the following error occurs:"
1842	Troubleshooting Cloud Object Storage for projects	"Cannot retrieve API key
Possible Causes
You have not been assigned the Editor role in the IBM Cloud account."
1843	Troubleshooting Cloud Object Storage for projects	"Cannot retrieve API key
Possible Resolutions
The account administrator must complete the following tasks:  
Invite users to the IBM Cloud account and assign the Editor role. See Add non-administrative users to your IBM Cloud account."
1844	Troubleshooting Cloud Object Storage for projects	"Unable to configure credentials
Symptoms
When you create a project and associate it to a Cloud Object Storage instance, the following error occurs:"
1845	Troubleshooting Cloud Object Storage for projects	"Unable to configure credentials
Possible Causes
You have exceeded the access policy limit for the account. For a Lite account, you have exceeded the 25 GB limit for the Cloud Object Storage instance."
1846	Troubleshooting Cloud Object Storage for projects	"Unable to configure credentials
Possible Resolutions
For exceeding access policies:  
Verify that you are the owner of the Cloud Object Storage instance or that the owner has granted you Administrator and Manager roles for this service instance. Otherwise, ask your IBM Cloud administrator to fix this problem. Check the total number of access policies to determine whether you have reached a limit. See IBM Cloud IAM limits for the limit information. Delete at least 4 or more unused access policies for the service ID.  
See Reducing time and effort managing access for strategies that you can use to ensure that you don't reach the limit.  
For exceeding 25 GB limit for a Lite account:  
For a Lite account, you have exceeded the 25 GB limit for the Cloud Object Storage instance. Possible resolutions are to upgrade to a billable account, delete stored assets for the current account, or wait until the first of the month when the limit resets. See Set up a billable account."
1847	Troubleshooting Cloud Object Storage for projects	"Login not permitted from IP address
Symptoms
When you create or work with a project, the following error occurs:"
1848	Troubleshooting Cloud Object Storage for projects	"Login not permitted from IP address
Possible Causes
Restrict IP address access has been configured to allow specific IP addresses access to Watson Studio. The IP address of the computer you are using is not allowed."
1849	Troubleshooting Cloud Object Storage for projects	"Login not permitted from IP address
Possible Resolutions
Add the IP address to the allowed IP addresses, if your security qualifications allow it. See Allow specific IP addresses."
1850	Troubleshooting Cloud Object Storage for projects	"Project cannot be created
Symptoms
When you create a project, the following error occurs:"
1851	Troubleshooting Cloud Object Storage for projects	"Project cannot be created
Possible Causes
The Cloud Object Storage instance is not available, due to the Global location is not enabled for your services. Cloud Object Storage requires the Global location."
1852	Troubleshooting Cloud Object Storage for projects	"Project cannot be created
Possible Resolutions
Enable the Global location in your account profile. From your account, click your avatar and select Profile and settings to open your IBM watsonx profile. Under Service Filters > Locations, check the Global location as well as other locations where services are present. See Manage your profile."
1853	Troubleshooting Synthetic Data Generator	Use this information to resolve questions about using Synthetic Data Generator.
1854	Troubleshooting Synthetic Data Generator	"Typeless columns ignored for an Import node
When you use an Import node that contains Typeless columns, these columns will be ignored when you use the Mimic node. After pressing the Read Values button, the Typeless columns will be automatically set to Pass and will not be present in the final dataset.  
Suggested workaround:  
Add a new column in the Generate node for the missing column(s)."
1855	Troubleshooting Synthetic Data Generator	"Size limit notice
The Synthetic Data Generator environment can import up to ~2.5GB of data.  
Suggested workaround:  
If you receive a related error message or your data fails to import, please reduce the amount of data and try again."
1856	Troubleshooting Synthetic Data Generator	"Internal error occurred: SCAPI error: The value on row 1,029 is not a valid string
For example, preview of data asset using Import node gives the following error:  
This is expected behavior. In this particular case, the 1st 1000 rows were binary, 0's or 1's. The value at row 1,029 was 3. For most flat files, Synthetic Data Generator reads the 1st 1000 records to infer the data type. In this case, Synthetic Data Generator inferred binary values (0 or 1). When Synthetic Data Generator read a value of 3 at row 1,029, it threw an error, as 3 is not a binary value.  
Suggested workarounds:  
Users can adjust their Infer_record_count parameter to include more data, choosing 2000 rows instead (or more). Users can update the value in the first 1000 rows that is causing the error, if this is an error in the data."
1857	Troubleshooting Synthetic Data Generator	"Error Mimic Data set no available input record.
The Mimic node requires the input dataset to have at least one valid record (a record without any missing values). If your dataset is empty, or if the dataset does not contain at least one valid record, clicking Run selection gives the following error message:  
Suggested workarounds:  
Fix your dataset so that there is at least one record (row) that contains a value for every column and then try again. Click Read values from the Import node and run your flow again."
1858	Troubleshooting Synthetic Data Generator	"Error: Incorrect number of fields detected in the server data model. or WDP Connector Execution Error
Creating a new flow using a .synth file, then doing a migration of the Import node with a newly uploaded file to the project, and then running the flow, gives one or both of the following errors:  
or  
This error is caused by using different data sets (data models) for the create flow and for the migration data.  
Suggested workaround:  
Run the Mimic node that creates the Generate node a second time."
1859	Troubleshooting Synthetic Data Generator	"Error: Valid variable does not exist in metadata
Doing a migration of the Import node and then running the flow fails and gives the error:  
Suggested workaround:  
Make sure that in your Import node you have at least one field that is not Typeless. For example, in the screen capture below, the only field in the Import node is Typeless. At least one field that is not Typeless should be added to the Import node to avoid this error."
1860	Troubleshooting	"If you encounter an issue in IBM watsonx, use the following resources to resolve the problem.  
View IBM Cloud service status Troubleshoot connections Troubleshoot Data Refinery Troubleshoot Synthetic Data Generator Troubleshoot IBM Cloud Object Storage Troubleshoot Watson Machine Learning Troubleshoot Watson OpenScale Troubleshoot Watson Studio on IBM Cloud Known issues Get help"
1861	Troubleshooting connections	Use these solutions to resolve problems that you might encounter with connections.
1862	Troubleshooting connections	"IBM Db2 for z/OS: Error retrieving the schema list when you try to connect to a Db2 for z/OS server
When you test the connection to a Db2 for z/OS server and the connection cannot retrieve the schema list, you might receive the following error:  
Workaround: On the Db2 for z/OS server, set the DESCSTAT subsystem parameter to No. For more information, see DESCRIBE FOR STATIC field (DESCSTAT subsystem parameter).  
Parent topic: Supported connections"
1863	Troubleshoot Watson Machine Learning	The following are the answers to common troubleshooting questions about using IBM Watson Machine Learning.
1864	Troubleshoot Watson Machine Learning	"Getting help and support for Watson Machine Learning
If you have problems or questions when you use Watson Machine Learning, you can get help by searching for information or by asking questions through a forum. You can also open a support ticket.  
When you use the forums to ask a question, tag your question so that it is seen by the Watson Machine Learning development teams.  
If you have technical questions about Watson Machine Learning, post your question on Stack Overflow and tag your question with ibm-bluemix and machine-learning.  
For questions about the service and getting started instructions, use the IBM developerWorks dW Answers forum. You must include the machine-learning and bluemix tags."
1865	Troubleshoot Watson Machine Learning	"Contents
Inactive Watson Machine Learning instance The authorization token is not provided Invalid authorization token The authorization token and instance_id that was used in the request are not the same Authorization token is expired The public key that is needed for authentication is not available Operation that is timed out after {{timeout}} Unhandled exception of type {{type}} with {{status}} Unhandled exception of type {{type}} with {{response}} Unhandled exception of type {{type}} with {{json}} Unhandled exception of type {{type}} with {{message}} The requested object might not be found The underlying database reported too many requests The definition of the evaluation is not defined either in the artifactModelVersion or in the deployment. It needs to be specified "" +\n ""at least in one of the places Data module not found in IBM Federated Learning Evaluation requires a learning configuration that is specified for the model Evaluation requires spark instance to be provided in X-Spark-Service-Instance header Model does not contain any version Patch operation can modify existing learning configuration only Patch operation expects exactly one replace operation The payload is missing the required fields: FIELD or the values of the fields are corrupted Provided evaluation method: METHOD is not supported. Supported values: VALUE You can have only one active evaluation per model. The request might not be completed because of existing active evaluation: {{url}} The deployment type {{type}} is not supported Incorrect input: ({{message}}) Insufficient data - metric {{name}} might not be calculated For type {{type}} spark instance must be provided in X-Spark-Service-Instance header The action {{action}} failed with message {{message}} Path {{path}} is not allowed. The only allowed path for Patch stream is /status Patch operation is not allowed, for instance, of type {{$type}} Data connection {{data}} is invalid for feedback_data_ref Path {{path}} is not allowed. The only allowed path for Patch model is /deployed_version/url or /deployed_version/href for V2 Parsing failure: {{msg}} Runtime environment for selected model: {{env}} is not supported for learning configuration. Supported environments: - [{{supported_envs}}] Current plan '{{plan}}' allows {{limit}} deployments only Database connection definition is not valid ({{code}}) Problems connecting underlying {{system}} Error extracting X-Spark-Service-Instance header: ({{message}}) This function is forbidden for nonbeta users {{code}} {{message}} Rate limit exceeded Invalid query parameter"
1866	Troubleshoot Watson Machine Learning	"Contents
is not valid ({{code}}) Problems connecting underlying {{system}} Error extracting X-Spark-Service-Instance header: ({{message}}) This function is forbidden for nonbeta users {{code}} {{message}} Rate limit exceeded Invalid query parameter {{paramName}} value: {{value}} Invalid token type: {{type}} Invalid token format. You must use bearer token format. Input JSON file is missing or invalid: 400 The authorization token expired: 401 Unknown deployment identification: 404 Internal server error: 500 Invalid type for ml_artifact: Pipeline ValueError: Training_data_ref name and connection cannot be None, if Pipeline Artifact is not given."
1867	Troubleshoot Watson Machine Learning	"Inactive Watson Machine Learning instance
Symptoms
After you try to submit an inference request to a foundation model by clicking the Generate button in the Prompt Lab, the following error message is displayed:"
1868	Troubleshoot Watson Machine Learning	"Inactive Watson Machine Learning instance
Possible causes
The association between your watsonx.ai project and the related Watson Machine Learning service instance was lost."
1869	Troubleshoot Watson Machine Learning	"Inactive Watson Machine Learning instance
Possible solutions
Recreate or refresh the association between your watsonx.ai project and the related Watson Machine Learning service instance. To do so, complete the following steps:  
From the main menu, expand Projects, and then click View all projects. Click your watsonx.ai project. From the Manage tab, click Services & integrations. If the the appropriate Watson Machine Learning service instance is listed, disassociate it temporarily by selecting the instance, and then clicking Remove. Confirm the removal. Click Associate service. Choose the appropriate Watson Machine Learning service instance from the list, and then click Associate."
1870	Troubleshoot Watson Machine Learning	"The authorization token is not provided.
What's happening
The REST API cannot be invoked successfully."
1871	Troubleshoot Watson Machine Learning	"The authorization token is not provided.
Why it's happening
The authorization token is not provided in the Authorization header."
1872	Troubleshoot Watson Machine Learning	"The authorization token is not provided.
How to fix it
Pass the authorization token in the Authorization header."
1873	Troubleshoot Watson Machine Learning	"Invalid authorization token.
What's happening
The REST API cannot be invoked successfully."
1874	Troubleshoot Watson Machine Learning	"Invalid authorization token.
Why it's happening
The authorization token that is provided cannot be decoded or parsed."
1875	Troubleshoot Watson Machine Learning	"Invalid authorization token.
How to fix it
Pass the correct authorization token in the Authorization header."
1876	Troubleshoot Watson Machine Learning	"The authorization token and instance_id that was used in the request are not the same.
What's happening
The REST API cannot be invoked successfully."
1877	Troubleshoot Watson Machine Learning	"The authorization token and instance_id that was used in the request are not the same.
Why it's happening
The Authorization token that is used is not generated for the service instance against which it was used."
1878	Troubleshoot Watson Machine Learning	"The authorization token and instance_id that was used in the request are not the same.
How to fix it
Pass an authorization token in the Authorization header, which corresponds to the service instance that is being used."
1879	Troubleshoot Watson Machine Learning	"The authorization token is expired.
What's happening
The REST API cannot be invoked successfully."
1880	Troubleshoot Watson Machine Learning	"The authorization token is expired.
Why it's happening
The authorization token is expired."
1881	Troubleshoot Watson Machine Learning	"The authorization token is expired.
How to fix it
Pass not expired authorization token in the Authorization header."
1882	Troubleshoot Watson Machine Learning	"The public key that is needed for authentication is not available.
What's happening
The REST API cannot be invoked successfully."
1883	Troubleshoot Watson Machine Learning	"The public key that is needed for authentication is not available.
Why it's happening
This problem can happen due to internal service issues."
1884	Troubleshoot Watson Machine Learning	"The public key that is needed for authentication is not available.
How to fix it
Contact the support team."
1885	Troubleshoot Watson Machine Learning	"Operation that is timed out after {{timeout}}
What's happening
The REST API cannot be invoked successfully."
1886	Troubleshoot Watson Machine Learning	"Operation that is timed out after {{timeout}}
Why it's happening
The timeout occurred while performing the requested operation."
1887	Troubleshoot Watson Machine Learning	"Operation that is timed out after {{timeout}}
How to fix it
Try to invoke the operation again."
1888	Troubleshoot Watson Machine Learning	"Unhandled exception of type {{type}} with {{status}}
What's happening
The REST API cannot be invoked successfully."
1889	Troubleshoot Watson Machine Learning	"Unhandled exception of type {{type}} with {{status}}
Why it's happening
This problem can happen due to internal service issues."
1890	Troubleshoot Watson Machine Learning	"Unhandled exception of type {{type}} with {{status}}
How to fix it
Try to invoke the operation again. If it happens again, contact the support team."
1891	Troubleshoot Watson Machine Learning	"Unhandled exception of type {{type}} with {{response}}
What's happening
The REST API cannot be invoked successfully."
1892	Troubleshoot Watson Machine Learning	"Unhandled exception of type {{type}} with {{response}}
Why it's happening
This problem can happen due to internal service issues."
1893	Troubleshoot Watson Machine Learning	"Unhandled exception of type {{type}} with {{response}}
How to fix it
Try to invoke the operation again. If it happens again, contact the support team."
1894	Troubleshoot Watson Machine Learning	"Unhandled exception of type {{type}} with {{json}}
What's happening
The REST API cannot be invoked successfully."
1895	Troubleshoot Watson Machine Learning	"Unhandled exception of type {{type}} with {{json}}
Why it's happening
This problem can happen due to internal service issues."
1896	Troubleshoot Watson Machine Learning	"Unhandled exception of type {{type}} with {{json}}
How to fix it
Try to invoke the operation again. If it happens again, contact the support team."
1897	Troubleshoot Watson Machine Learning	"Unhandled exception of type {{type}} with {{message}}
What's happening
The REST API cannot be invoked successfully."
1898	Troubleshoot Watson Machine Learning	"Unhandled exception of type {{type}} with {{message}}
Why it's happening
This problem can happen due to internal service issues."
1899	Troubleshoot Watson Machine Learning	"Unhandled exception of type {{type}} with {{message}}
How to fix it
Try to invoke the operation again. If it happens again, contact the support team."
1900	Troubleshoot Watson Machine Learning	"The requested object is not found.
What's happening
The REST API cannot be invoked successfully."
1901	Troubleshoot Watson Machine Learning	"The requested object is not found.
Why it's happening
The request resource is not found."
1902	Troubleshoot Watson Machine Learning	"The requested object is not found.
How to fix it
Make sure that you are referring to the existing resource."
1903	Troubleshoot Watson Machine Learning	"The underlying database reported too many requests.
What's happening
The REST API cannot be invoked successfully."
1904	Troubleshoot Watson Machine Learning	"The underlying database reported too many requests.
Why it's happening
The user sent too many requests in a specific time."
1905	Troubleshoot Watson Machine Learning	"The underlying database reported too many requests.
How to fix it
Try to invoke the operation again."
1906	Troubleshoot Watson Machine Learning	"The definition of the evaluation is not defined in the artifactModelVersion or deployment. It must be specified "" +\n ""at least in one of the places.
What's happening
The REST API cannot be invoked successfully."
1907	Troubleshoot Watson Machine Learning	"The definition of the evaluation is not defined in the artifactModelVersion or deployment. It must be specified "" +\n ""at least in one of the places.
Why it's happening
Learning Configuration does not contain all the required information"
1908	Troubleshoot Watson Machine Learning	"The definition of the evaluation is not defined in the artifactModelVersion or deployment. It must be specified "" +\n ""at least in one of the places.
How to fix it
Provide definition in learning configuration"
1909	Troubleshoot Watson Machine Learning	"Evaluation requires a learning configuration that is specified for the model.
What's happening
It is not possible to create learning iteration."
1910	Troubleshoot Watson Machine Learning	"Evaluation requires a learning configuration that is specified for the model.
Why it's happening
learning configuration isn't defined for the model."
1911	Troubleshoot Watson Machine Learning	"Evaluation requires a learning configuration that is specified for the model.
How to fix it
Create learning configuration and try to create learning iteration again."
1912	Troubleshoot Watson Machine Learning	"Evaluation requires spark instance to be provided in X-Spark-Service-Instance header
What's happening
The REST API cannot be invoked successfully."
1913	Troubleshoot Watson Machine Learning	"Evaluation requires spark instance to be provided in X-Spark-Service-Instance header
Why it's happening
learning configuration does not have the required information."
1914	Troubleshoot Watson Machine Learning	"Evaluation requires spark instance to be provided in X-Spark-Service-Instance header
How to fix it
Provide spark_service in Learning Configuration or in X-Spark-Service-Instance header."
1915	Troubleshoot Watson Machine Learning	"Model does not contain any version.
What's happening
It is not possible to create deployment or set the learning configuration."
1916	Troubleshoot Watson Machine Learning	"Model does not contain any version.
Why it's happening
This problem can happen due to inconsistency that is related to the persistence of the model."
1917	Troubleshoot Watson Machine Learning	"Model does not contain any version.
How to fix it
Try to persist the model again and try to perform the action again."
1918	Troubleshoot Watson Machine Learning	"Data module not found in IBM Federated Learning.
What's happening
The data handler for IBM Federated Learning is trying to extract a data module from the FL library but is unable to find it. You might see the following error message:"
1919	Troubleshoot Watson Machine Learning	"Data module not found in IBM Federated Learning.
Why it's happening
Possibly an outdated DataHandler."
1920	Troubleshoot Watson Machine Learning	"Data module not found in IBM Federated Learning.
How to fix it
Review and update your DataHandler to conform to the most recent MNIST data handler or make sure that your sample versions are up to date."
1921	Troubleshoot Watson Machine Learning	"Patch operation can modify existing learning configuration only.
What's happening
It is not possible to invoke patch REST API method to patch learning configuration."
1922	Troubleshoot Watson Machine Learning	"Patch operation can modify existing learning configuration only.
Why it's happening
learning configuration isn't set for this model or the model does not exist."
1923	Troubleshoot Watson Machine Learning	"Patch operation can modify existing learning configuration only.
How to fix it
Endure that model exists and has already learning configuration set."
1924	Troubleshoot Watson Machine Learning	"Patch operation expects exactly one replace operation.
What's happening
The deployment cannot be patched."
1925	Troubleshoot Watson Machine Learning	"Patch operation expects exactly one replace operation.
Why it's happening
The patch payload contains more than one operation or the patch operation is different than replace."
1926	Troubleshoot Watson Machine Learning	"Patch operation expects exactly one replace operation.
How to fix it
Use only one operation in the patch payload, which is replace operation."
1927	Troubleshoot Watson Machine Learning	"The payload is missing the required fields: FIELD or the values of the fields are corrupted.
What's happening
It is not possible to process action that is related to access to the underlying data set."
1928	Troubleshoot Watson Machine Learning	"The payload is missing the required fields: FIELD or the values of the fields are corrupted.
Why it's happening
The access to the data set is not properly defined."
1929	Troubleshoot Watson Machine Learning	"The payload is missing the required fields: FIELD or the values of the fields are corrupted.
How to fix it
Correct the access definition for the data set."
1930	Troubleshoot Watson Machine Learning	"Provided evaluation method: METHOD is not supported. Supported values: VALUE.
What's happening
It is not possible to create learning configuration."
1931	Troubleshoot Watson Machine Learning	"Provided evaluation method: METHOD is not supported. Supported values: VALUE.
Why it's happening
The wrong evaluation method was used to create learning configuration."
1932	Troubleshoot Watson Machine Learning	"Provided evaluation method: METHOD is not supported. Supported values: VALUE.
How to fix it
Use a supported evaluation method, which is one of: regression, binary, multiclass."
1933	Troubleshoot Watson Machine Learning	"You can have only one active evaluation per model. The request cannot be completed because of existing active evaluation: {{url}}
What's happening
It is not possible to create another learning iteration."
1934	Troubleshoot Watson Machine Learning	"You can have only one active evaluation per model. The request cannot be completed because of existing active evaluation: {{url}}
Why it's happening
You can have only one running evaluation for the model."
1935	Troubleshoot Watson Machine Learning	"You can have only one active evaluation per model. The request cannot be completed because of existing active evaluation: {{url}}
How to fix it
See the already running evaluation or wait for the evaluation to end and start the new one."
1936	Troubleshoot Watson Machine Learning	"The deployment type {{type}} is not supported.
What's happening
It is not possible to create the deployment."
1937	Troubleshoot Watson Machine Learning	"The deployment type {{type}} is not supported.
Why it's happening
Not supported deployment type was used."
1938	Troubleshoot Watson Machine Learning	"The deployment type {{type}} is not supported.
How to fix it
A supported deployment type must be used."
1939	Troubleshoot Watson Machine Learning	"Incorrect input: ({{message}})
What's happening
The REST API cannot be invoked successfully."
1940	Troubleshoot Watson Machine Learning	"Incorrect input: ({{message}})
Why it's happening
This problem happens due to an issue with parsing JSON."
1941	Troubleshoot Watson Machine Learning	"Incorrect input: ({{message}})
How to fix it
Make sure that the correct JSON is passed in the request."
1942	Troubleshoot Watson Machine Learning	"Insufficient data - metric {{name}} cannot be calculated
What's happening
Learning iteration failed."
1943	Troubleshoot Watson Machine Learning	"Insufficient data - metric {{name}} cannot be calculated
Why it's happening
Value for metric with defined threshold cannot be calculated because of insufficient feedback data."
1944	Troubleshoot Watson Machine Learning	"Insufficient data - metric {{name}} cannot be calculated
How to fix it
Review and improve data in data source feedback_data_ref in learning configuration"
1945	Troubleshoot Watson Machine Learning	"For type {{type}} spark instance must be provided in X-Spark-Service-Instance header
What's happening
Deployment cannot be created"
1946	Troubleshoot Watson Machine Learning	"For type {{type}} spark instance must be provided in X-Spark-Service-Instance header
Why it's happening
batch and streaming deployments require spark instance to be provided"
1947	Troubleshoot Watson Machine Learning	"For type {{type}} spark instance must be provided in X-Spark-Service-Instance header
How to fix it
Provide spark instance in X-Spark-Service-Instance header"
1948	Troubleshoot Watson Machine Learning	"Action {{action}} failed with message {{message}}
What's happening
The REST API cannot be invoked successfully."
1949	Troubleshoot Watson Machine Learning	"Action {{action}} failed with message {{message}}
Why it's happening
This problem happens due to an issue with invoking underlying service."
1950	Troubleshoot Watson Machine Learning	"Action {{action}} failed with message {{message}}
How to fix it
If the message provides a suggestion to fix the issue, follow the suggestion. Otherwise, contact the support team."
1951	Troubleshoot Watson Machine Learning	"Path {{path}} is not allowed. The only allowed path for patch stream is /status
What's happening
Stream deployment cannot be patched."
1952	Troubleshoot Watson Machine Learning	"Path {{path}} is not allowed. The only allowed path for patch stream is /status
Why it's happening
The wrong path was used to patch the stream deployment."
1953	Troubleshoot Watson Machine Learning	"Path {{path}} is not allowed. The only allowed path for patch stream is /status
How to fix it
Patch the stream deployment with supported path option, which is /status (it allows to start/stop stream processing)."
1954	Troubleshoot Watson Machine Learning	"Patch operation is not allowed, for instance, of type {{$type}}
What's happening
Deployment cannot be patched."
1955	Troubleshoot Watson Machine Learning	"Patch operation is not allowed, for instance, of type {{$type}}
Why it's happening
The wrong deployment type is being patched."
1956	Troubleshoot Watson Machine Learning	"Patch operation is not allowed, for instance, of type {{$type}}
How to fix it
Patch the stream deployment type."
1957	Troubleshoot Watson Machine Learning	"Data connection {{data}} is invalid for feedback_data_ref
What's happening
learning configuration cannot be created for the model."
1958	Troubleshoot Watson Machine Learning	"Data connection {{data}} is invalid for feedback_data_ref
Why it's happening
Supported data source was not used when feedback_data_ref was defined."
1959	Troubleshoot Watson Machine Learning	"Data connection {{data}} is invalid for feedback_data_ref
How to fix it
Use only the supported data source type dashdb."
1960	Troubleshoot Watson Machine Learning	"Path {{path}} is not allowed. The only allowed path for patch model is /deployed_version/url or /deployed_version/href for V2
What's happening
No option to patch model."
1961	Troubleshoot Watson Machine Learning	"Path {{path}} is not allowed. The only allowed path for patch model is /deployed_version/url or /deployed_version/href for V2
Why it's happening
The wrong path was used during patching of the model."
1962	Troubleshoot Watson Machine Learning	"Path {{path}} is not allowed. The only allowed path for patch model is /deployed_version/url or /deployed_version/href for V2
How to fix it
Patch model with supported path that you can use to update the version of the deployed model."
1963	Troubleshoot Watson Machine Learning	"Parsing failure: {{msg}}
What's happening
The REST API cannot be invoked successfully."
1964	Troubleshoot Watson Machine Learning	"Parsing failure: {{msg}}
Why it's happening
The requested payload cannot be parsed successfully."
1965	Troubleshoot Watson Machine Learning	"Parsing failure: {{msg}}
How to fix it
Make sure that your request payload is correct and can be parsed correctly."
1966	Troubleshoot Watson Machine Learning	"Runtime environment for selected model: {{env}} is not supported for learning configuration. Supported environments: [{{supported_envs}}].
What's happening
No option to create learning configuration."
1967	Troubleshoot Watson Machine Learning	"Runtime environment for selected model: {{env}} is not supported for learning configuration. Supported environments: [{{supported_envs}}].
Why it's happening
The model for which the learning_configuration was tried to be created is not supported."
1968	Troubleshoot Watson Machine Learning	"Runtime environment for selected model: {{env}} is not supported for learning configuration. Supported environments: [{{supported_envs}}].
How to fix it
Create learning configuration for model, which has the supported runtime."
1969	Troubleshoot Watson Machine Learning	"Current plan '{{plan}}' allows {{limit}} deployments only
What's happening
It is not possible to create the deployment."
1970	Troubleshoot Watson Machine Learning	"Current plan '{{plan}}' allows {{limit}} deployments only
Why it's happening
The limit for number of deployments was reached for the current plan."
1971	Troubleshoot Watson Machine Learning	"Current plan '{{plan}}' allows {{limit}} deployments only
How to fix it
Upgrade to the plan that does not have such limitation."
1972	Troubleshoot Watson Machine Learning	"Database connection definition is not valid ({{code}})
What's happening
It is not possible to use the learning configuration function."
1973	Troubleshoot Watson Machine Learning	"Database connection definition is not valid ({{code}})
Why it's happening
The database connection definition is invalid."
1974	Troubleshoot Watson Machine Learning	"Database connection definition is not valid ({{code}})
How to fix it
Try to fix the issue that is described by code returned by the underlying database."
1975	Troubleshoot Watson Machine Learning	"Problems connecting underlying {{system}}
What's happening
The REST API cannot be invoked successfully."
1976	Troubleshoot Watson Machine Learning	"Problems connecting underlying {{system}}
Why it's happening
This problem might happen due to an issue during connection to the underlying system. It might be a temporary network issue."
1977	Troubleshoot Watson Machine Learning	"Problems connecting underlying {{system}}
How to fix it
Try to invoke the operation again. If you get an error again, contact the support team."
1978	Troubleshoot Watson Machine Learning	"Error extracting X-Spark-Service-Instance header: ({{message}})
What's happening
This problem might happen if REST API that requires Spark credentials cannot be invoked."
1979	Troubleshoot Watson Machine Learning	"Error extracting X-Spark-Service-Instance header: ({{message}})
Why it's happening
This problem might happen due to an issue with base-64 decoding or parsing Spark credentials."
1980	Troubleshoot Watson Machine Learning	"Error extracting X-Spark-Service-Instance header: ({{message}})
How to fix it
Make sure that the correct Spark credentials were correctly base-64 encoded. For more information, see the documentation."
1981	Troubleshoot Watson Machine Learning	"This function is forbidden for non-beta users.
What's happening
The REST API cannot be invoked successfully."
1982	Troubleshoot Watson Machine Learning	"This function is forbidden for non-beta users.
Why it's happening
The REST API that was invoked is in beta."
1983	Troubleshoot Watson Machine Learning	"This function is forbidden for non-beta users.
How to fix it
If you are interested in participating, add yourself to the wait list. The details can be found in the documentation."
1984	Troubleshoot Watson Machine Learning	"{{code}} {{message}}
What's happening
The REST API cannot be invoked successfully."
1985	Troubleshoot Watson Machine Learning	"{{code}} {{message}}
Why it's happening
This problem might happen due to an issue with invoking underlying service."
1986	Troubleshoot Watson Machine Learning	"{{code}} {{message}}
How to fix it
If the message provides a suggestion to fix the issue, follow the suggestion. Otherwise, contact the support team."
1987	Troubleshoot Watson Machine Learning	"Rate limit exceeded.
What's happening
Rate limit exceeded."
1988	Troubleshoot Watson Machine Learning	"Rate limit exceeded.
Why it's happening
The rate limit for current plan is exceeded."
1989	Troubleshoot Watson Machine Learning	"Rate limit exceeded.
How to fix it
To solve this problem, acquire another plan with a greater rate limit"
1990	Troubleshoot Watson Machine Learning	"Invalid query parameter {{paramName}} value: {{value}}
What's happening
Validation error as passed incorrect value for query parameter."
1991	Troubleshoot Watson Machine Learning	"Invalid query parameter {{paramName}} value: {{value}}
Why it's happening
Error in getting result for query."
1992	Troubleshoot Watson Machine Learning	"Invalid query parameter {{paramName}} value: {{value}}
How to fix it
Correct query parameter value. The details can be found in the documentation."
1993	Troubleshoot Watson Machine Learning	"Invalid token type: {{type}}
What's happening
Error regarding token type."
1994	Troubleshoot Watson Machine Learning	"Invalid token type: {{type}}
Why it's happening
Error in authorization."
1995	Troubleshoot Watson Machine Learning	"Invalid token type: {{type}}
How to fix it
Token must be started with Bearer prefix."
1996	Troubleshoot Watson Machine Learning	"Invalid token format. You must use bearer token format.
What's happening
Error regarding token format."
1997	Troubleshoot Watson Machine Learning	"Invalid token format. You must use bearer token format.
Why it's happening
Error in authorization."
1998	Troubleshoot Watson Machine Learning	"Invalid token format. You must use bearer token format.
How to fix it
The token must be a bearer token and must start with Bearer prefix."
1999	Troubleshoot Watson Machine Learning	"Input JSON file is missing or invalid: 400
What's happening
The following message displays when you try to score online: Input JSON file is missing or invalid."
2000	Troubleshoot Watson Machine Learning	"Input JSON file is missing or invalid: 400
Why it's happening
This message displays when the scoring input payload doesn't match the expected input type that is required for scoring the model. Specifically, the following reasons might apply:  
The input payload is empty. The input payload schema is not valid. The input data types do not match the expected data types."
2001	Troubleshoot Watson Machine Learning	"Input JSON file is missing or invalid: 400
How to fix it
Correct the input payload. Make sure that the payload has correct syntax, a valid schema, and proper data types. After you make corrections, try to score online again. For syntax issues, verify the JSON file by using the jsonlint command."
2002	Troubleshoot Watson Machine Learning	"The authorization token is expired: 401
What's happening
The following message displays when you try to score online: Authorization failed."
2003	Troubleshoot Watson Machine Learning	"The authorization token is expired: 401
Why it's happening
This message displays when the token that is used for scoring is expired."
2004	Troubleshoot Watson Machine Learning	"The authorization token is expired: 401
How to fix it
Regenerate the token for this IBM Watson Machine Learning instance and then retry. If you still see this issue contact IBM Support."
2005	Troubleshoot Watson Machine Learning	"Unknown deployment identification: 404
What's happening
The following message displays when you try to score online Unknown deployment identification."
2006	Troubleshoot Watson Machine Learning	"Unknown deployment identification: 404
Why it's happening
This message displays when the deployment ID that is used for scoring does not exist."
2007	Troubleshoot Watson Machine Learning	"Unknown deployment identification: 404
How to fix it
Make sure you are providing the correct deployment ID. If not, deploy the model with the deployment ID and then try scoring it again."
2008	Troubleshoot Watson Machine Learning	"Internal server error: 500
What's happening
The following message displays when you try to score online: Internal server error"
2009	Troubleshoot Watson Machine Learning	"Internal server error: 500
Why it's happening
This message displays if the downstream data flow on which the online scoring depends fails."
2010	Troubleshoot Watson Machine Learning	"Internal server error: 500
How to fix it
Wait for some time and try to score online again. If it fails again, contact IBM Support."
2011	Troubleshoot Watson Machine Learning	"Invalid type for ml_artifact: Pipeline
What's happening
The following message displays when you try to publish a Spark model by using Common API client library on your workstation."
2012	Troubleshoot Watson Machine Learning	"Invalid type for ml_artifact: Pipeline
Why it's happening
This message displays if you have an invalid pyspark set up in the operating system."
2013	Troubleshoot Watson Machine Learning	"Invalid type for ml_artifact: Pipeline
How to fix it
Set up system environment paths according to the instruction:"
2014	Troubleshoot Watson Machine Learning	"ValueError: Training_data_ref name and connection cannot be None, if Pipeline Artifact is not given.
What's happening
The training data set is missing or is not referenced properly."
2015	Troubleshoot Watson Machine Learning	"ValueError: Training_data_ref name and connection cannot be None, if Pipeline Artifact is not given.
Why it's happening
The Pipeline Artifact is a training data set in this instance."
2016	Troubleshoot Watson Machine Learning	"ValueError: Training_data_ref name and connection cannot be None, if Pipeline Artifact is not given.
How to fix it
You must supply a training data set when you persist a Spark PipelineModel. If you don't, the client says it doesn't support PipelineModels, rather than saying a PipelineModel must be accompanied by the training set."
2017	Troubleshooting watsonx.governance	"Use these solutions to work around problems with watsonx.governance.  
For issues with Watson OpenScale, see Troubleshooting Watson OpenScale.  
For issues with watsonx.Governance Console, see Troubleshooting the OpenPages service."
2018	IBM Cloud services status	"If you're having a problem with one of your services, go to the IBM Cloud Status page. The Status page shows unplanned incidents, planned maintenance, announcements, and security bulletin notifications about key events that affect the IBM Cloud platform, infrastructure, and major services.  
You can find the Status page by logging in to the IBM Cloud console. Click Support from the menu bar, and then click View cloud status from the Support Center. Or, you can access the page directly at IBM Cloud - Status. Search for the service to view its status."
2019	IBM Cloud services status	"Learn more
Viewing cloud status"
2020	Troubleshooting Watson Studio on IBM Cloud	You can use the following techniques to work around problems you might encounter with Watson Studio on IBM Cloud.
2021	Troubleshooting Watson Studio on IBM Cloud	"Project limit exceeded
Symptoms
When you create a project, the following error occurs:"
2022	Troubleshooting Watson Studio on IBM Cloud	"Project limit exceeded
Possible Causes
The number of projects an authenticated user can create per data center (region) is 100. The limit applies only to projects that a user creates. Projects for which the user is listed as a collaborator are not included in this limit."
2023	Troubleshooting Watson Studio on IBM Cloud	"Project limit exceeded
Possible Resolutions
Although most customers do not reach this limit, possible resolutions include:  
Delete projects. Any authenticated user can request a project limit increase by contacting IBM Cloud Support, provided that an adequate justification is specified."
2024	Troubleshooting Watson Studio on IBM Cloud	"Blank screen when loading
Symptoms
A blank screen appears when you open Watson Studio."
2025	Troubleshooting Watson Studio on IBM Cloud	"Blank screen when loading
Possible Causes
A cached version is loading."
2026	Troubleshooting Watson Studio on IBM Cloud	"Blank screen when loading
Possible Resolutions
Clear the browser cache and cookies and re-open Watson Studio. Try a different type of browser. For example, switch from Firefox to Chrome. If the blank screen still occurs, open a support case, generate a .har file, compress it, and upload the compressed har file to the support case."
2027	Glossary	"This glossary provides terms and definitions for watsonx.ai and watsonx.governance.  
The following cross-references are used in this glossary:  
See refers you from a nonpreferred term to the preferred term or from an abbreviation to the spelled-out form. See also refers you to a related or contrasting term.  
A B C D E F G H I J K L M N O P R S T U V W Z"
2028	Glossary	"A
accelerator
In high-performance computing, a specialized circuit that is used to take some of the computational load from the CPU, increasing the efficiency of the system. For example, in deep learning, GPU-accelerated computing is often employed to offload part of the compute workload to a GPU while the main application runs off the CPU. See also graphics processing unit."
2029	Glossary	"A
accountability
The expectation that organizations or individuals will ensure the proper functioning, throughout their lifecycle, of the AI systems that they design, develop, operate or deploy, in accordance with their roles and applicable regulatory frameworks. This includes determining who is responsible for an AI mistake which may require legal experts to determine liability on a case-by-case basis."
2030	Glossary	"A
activation function
A function defining a neural unit's output given a set of incoming activations from other neurons"
2031	Glossary	"A
active learning
A model for machine learning in which the system requests more labeled data only when it needs it."
2032	Glossary	"A
active metadata
Metadata that is automatically updated based on analysis by machine learning processes. For example, profiling and data quality analysis automatically update metadata for data assets."
2033	Glossary	"A
active runtime
An instance of an environment that is running to provide compute resources to analytical assets."
2034	Glossary	"A
agent
An algorithm or a program that interacts with an environment to learn optimal actions or decisions, typically using reinforcement learning, to achieve a specific goal."
2035	Glossary	"A
AI
See artificial intelligence."
2036	Glossary	"A
AI accelerator
Specialized silicon hardware designed to efficiently execute AI-related tasks like deep learning, machine learning, and neural networks for faster, energy-efficient computing. It can be a dedicated unit in a core, a separate chiplet on a multi-module chip or a separate card."
2037	Glossary	"A
AI ethics
A multidisciplinary field that studies how to optimize AI's beneficial impact while reducing risks and adverse outcomes. Examples of AI ethics issues are data responsibility and privacy, fairness, explainability, robustness, transparency, environmental sustainability, inclusion, moral agency, value alignment, accountability, trust, and technology misuse."
2038	Glossary	"A
AI governance
An organization's act of governing, through its corporate instructions, staff, processes and systems to direct, evaluate, monitor, and take corrective action throughout the AI lifecycle, to provide assurance that the AI system is operating as the organization intends, as its stakeholders expect, and as required by relevant regulation."
2039	Glossary	"A
AI safety
The field of research aiming to ensure artificial intelligence systems operate in a manner that is beneficial to humanity and don't inadvertently cause harm, addressing issues like reliability, fairness, transparency, and alignment of AI systems with human values."
2040	Glossary	"A
AI system
See artificial intelligence system."
2041	Glossary	"A
algorithm
A formula applied to data to determine optimal ways to solve analytical problems."
2042	Glossary	"A
analytics
The science of studying data in order to find meaningful patterns in the data and draw conclusions based on those patterns."
2043	Glossary	"A
appropriate trust
In an AI system, an amount of trust that is calibrated to its accuracy, reliability, and credibility."
2044	Glossary	"A
artificial intelligence (AI)
The capability to acquire, process, create and apply knowledge in the form of a model to make predictions, recommendations or decisions."
2045	Glossary	"A
artificial intelligence system (AI system)
A system that can make predictions, recommendations or decisions that influence physical or virtual environments, and whose outputs or behaviors are not necessarily pre-determined by its developer or user. AI systems are typically trained with large quantities of structured or unstructured data, and might be designed to operate with varying levels of autonomy or none, to achieve human-defined objectives."
2046	Glossary	"A
asset
An item that contains information about data, other valuable information, or code that works with data. See also data asset."
2047	Glossary	"A
attention mechanism
A mechanism in deep learning models that determines which parts of the input a model focuses on when producing output."
2048	Glossary	"A
AutoAI experiment
An automated training process that considers a series of training definitions and parameters to create a set of ranked pipelines as model candidates."
2049	Glossary	"B
batch deployment
A method to deploy models that processes input data from a file, data connection, or connected data in a storage bucket, then writes the output to a selected destination."
2050	Glossary	"B
bias
Systematic error in an AI system that has been designed, intentionally or not, in a way that may generate unfair decisions. Bias can be present both in the AI system and in the data used to train and test it. AI bias can emerge in an AI system as a result of cultural expectations; technical limitations; or unanticipated deployment contexts. See also fairness."
2051	Glossary	"B
bias detection
The process of calculating fairness to metrics to detect when AI models are delivering unfair outcomes based on certain attributes."
2052	Glossary	"B
bias mitigation
Reducing biases in AI models by curating training data and applying fairness techniques."
2053	Glossary	"B
binary classification
A classification model with two classes. Predictions are a binary choice of one of the two classes."
2054	Glossary	"C
classification model
A predictive model that predicts data in distinct categories. Classifications can be binary, with two classes of data, or multi-class when there are more than 2 categories."
2055	Glossary	"C
cleanse
To ensure that all values in a data set are consistent and correctly recorded."
2056	Glossary	"C
CNN
See convolutional neural network."
2057	Glossary	"C
cognitive forcing function
An intervention that is applied at a decision-making moment to disrupt heuristic reasoning and cause a person to engage in analytical thinking; examples include a checklist, a diagnostic time-out, or asking a person to rule out an alternative."
2058	Glossary	"C
computational linguistics
Interdisciplinary field that explores approaches for computationally modeling natural languages."
2059	Glossary	"C
compute resource
The hardware and software resources that are defined by an environment template to run assets in tools."
2060	Glossary	"C
confusion matrix
A performance measurement that determines the accuracy between a model's positive and negative predicted outcomes compared to positive and negative actual outcomes."
2061	Glossary	"C
connected data asset
A pointer to data that is accessed through a connection to an external data source."
2062	Glossary	"C
connected folder asset
A pointer to a folder in IBM Cloud Object Storage."
2063	Glossary	"C
connection
The information required to connect to a database. The actual information that is required varies according to the DBMS and connection method."
2064	Glossary	"C
connection asset
An asset that contains information that enables connecting to a data source."
2065	Glossary	"C
constraint
In databases, a relationship between tables. In Decision Optimization, a condition that must be satisfied by the solution of a problem."
2066	Glossary	"C
continuous learning
Automating the tasks of monitoring model performance, retraining with new data, and redeploying to ensure prediction quality."
2067	Glossary	"C
convolutional neural network (CNN)
A class of neural network commonly used in computer vision tasks that uses convolutional layers to process image data."
2068	Glossary	"C
Core ML deployment
The process of downloading a deployment in Core ML format for use in iOS apps."
2069	Glossary	"C
corpus
A collection of source documents that are used to train a machine learning model."
2070	Glossary	"C
CPLEX model
A Decision Optimization model that is formulated to be solved by the CPLEX engine."
2071	Glossary	"C
CPO model
A constraint programming model that is formulated to be solved by the Decision Optimization CP Optimizer (CPO) engine."
2072	Glossary	"C
cross-validation
A technique for testing how well a model generalizes in the absence of a hold-out test sample. Cross-validation divides the training data into a number of subsets, and then builds the same number of models, with each subset held out in turn. Each of those models is tested on the holdout sample, and the average accuracy of the models on those holdout samples is used to estimate the accuracy of the model when applied to new data."
2073	Glossary	"C
curate
To select, collect, preserve, and maintain content relevant to a specific topic. Curation establishes, maintains, and adds value to data; it transforms data into trusted information and knowledge."
2074	Glossary	"D
data asset
An asset that points to data, for example, to an uploaded file. Connections and connected data assets are also considered data assets. See also asset."
2075	Glossary	"D
data imputation
The substitution of missing values in a data set with estimated or explicit values."
2076	Glossary	"D
data lake
A large-scale data storage repository that stores raw data in any format in a flat architecture. Data lakes hold structured and unstructured data as well as binary data for the purpose of processing and analysis."
2077	Glossary	"D
data lakehouse
A unified data storage and processing architecture that combines the flexibility of a data lake with the structured querying and performance optimizations of a data warehouse, enabling scalable and efficient data analysis for AI and analytics applications."
2078	Glossary	"D
data mining
The process of collecting critical business information from a data source, correlating the information, and uncovering associations, patterns, and trends. See also predictive analytics."
2079	Glossary	"D
Data Refinery flow
A set of steps that cleanse and shape data to produce a new data asset."
2080	Glossary	"D
data science
The analysis and visualization of structured and unstructured data to discover insights and knowledge."
2081	Glossary	"D
data set
A collection of data, usually in the form of rows (records) and columns (fields) and contained in a file or database table."
2082	Glossary	"D
data source
A repository, queue, or feed for reading data, such as a database."
2083	Glossary	"D
data table
A collection of data, usually in the form of rows (records) and columns (fields) and contained in a table."
2084	Glossary	"D
data warehouse
A large, centralized repository of data collected from various sources that is used for reporting and data analysis. It primarily stores structured and semi-structured data, enabling businesses to make informed decisions."
2085	Glossary	"D
DDL
See distributed deep learning."
2086	Glossary	"D
decision boundary
A division of data points in a space into distinct groups or classifications."
2087	Glossary	"D
decoder-only model
A model that generates output text word by word by inference from the input sequence. Decoder-only models are used for tasks such as generating text and answering questions."
2088	Glossary	"D
deep learning
A computational model that uses multiple layers of interconnected nodes, which are organized into hierarchical layers, to transform input data (first layer) through a series of computations to produce an output (final layer). Deep learning is inspired by the structure and function of the human brain. See also distributed deep learning."
2089	Glossary	"D
deep neural network
A neural network with multiple hidden layers, allowing for more complex representations of the data."
2090	Glossary	"D
deployment
A model or application package that is available for use."
2091	Glossary	"D
deployment space
A workspace where models are deployed and deployments are managed."
2092	Glossary	"D
deterministic
Describes a characteristic of computing systems when their outputs are completely determined by their inputs."
2093	Glossary	"D
DevOps
A software methodology that integrates application development and IT operations so that teams can deliver code faster to production and iterate continuously based on market feedback."
2094	Glossary	"D
discriminative AI
A class of algorithm that focuses on finding a boundary that separates different classes in the data."
2095	Glossary	"D
distributed deep learning (DDL)
An approach to deep learning training that leverages the methods of distributed computing. In a DDL environment, compute workload is distributed between the central processing unit and graphics processing unit. See also deep learning."
2096	Glossary	"D
DOcplex
A Python API for modeling and solving Decision Optimization problems."
2097	Glossary	"E
embedding
A numerical representation of a unit of information, such as a word or a sentence, as a vector of real-valued numbers. Embeddings are learned, low-dimensional representations of higher-dimensional data. See also encoding, representation."
2098	Glossary	"E
emergence
A property of foundation models in which the model exhibits behaviors that were not explicitly trained."
2099	Glossary	"E
emergent behavior
A behavior exhibited by a foundation model that was not explicitly constructed."
2100	Glossary	"E
encoder-decoder model
A model for both understanding input text and for generating output text based on the input text. Encoder-decoder models are used for tasks such as summarization or translation."
2101	Glossary	"E
encoder-only model
A model that understands input text at the sentence level by transforming input sequences into representational vectors called embeddings. Encoder-only models are used for tasks such as classifying customer feedback and extracting information from large documents."
2102	Glossary	"E
encoding
The representation of a unit of information, such as a character or a word, as a set of numbers. See also embedding, positional encoding."
2103	Glossary	"E
endpoint URL
A network destination address that identifies resources, such as services and objects. For example, an endpoint URL is used to identify the location of a model or function deployment when a user sends payload data to the deployment."
2104	Glossary	"E
environment
The compute resources for running jobs."
2105	Glossary	"E
environment runtime
An instantiation of the environment template to run analytical assets."
2106	Glossary	"E
environment template
A definition that specifies hardware and software resources to instantiate environment runtimes."
2107	Glossary	"E
exogenous feature
A feature that can influence the predictive model but cannot be influenced in return. For example, temperatures can affect predicted ice cream sales, but ice cream sales cannot influence temperatures."
2108	Glossary	"E
experiment
A model training process that considers a series of training definitions and parameters to determine the most accurate model configuration."
2109	Glossary	"E
explainability
The ability of human users to trace, audit, and understand predictions that are made in applications that use AI systems. The ability of an AI system to provide insights that humans can use to understand the causes of the system's predictions."
2110	Glossary	"F
fairness
In an AI system, the equitable treatment of individuals or groups of individuals. The choice of a specific notion of equity for an AI system depends on the context in which it is used. See also bias."
2111	Glossary	"F
feature
A property or characteristic of an item within a data set, for example, a column in a spreadsheet. In some cases, features are engineered as combinations of other features in the data set."
2112	Glossary	"F
feature engineering
The process of selecting, transforming, and creating new features from raw data to improve the performance and predictive power of machine learning models."
2113	Glossary	"F
feature group
A set of columns of a particular data asset along with the metadata that is used for machine learning."
2114	Glossary	"F
feature selection
Identifying the columns of data that best support an accurate prediction or score in a machine learning model."
2115	Glossary	"F
feature store
A centralized repository or system that manages and organizes features, providing a scalable and efficient way to store, retrieve, and share feature data across machine learning pipelines and applications."
2116	Glossary	"F
feature transformation
In AutoAI, a phase of pipeline creation that applies algorithms to transform and optimize the training data to achieve the best outcome for the model type."
2117	Glossary	"F
federated learning
The training of a common machine learning model that uses multiple data sources that are not moved, joined, or shared. The result is a better-trained model without compromising data security."
2118	Glossary	"F
few-shot prompting
A prompting technique in which a small number of examples are provided to the model to demonstrate how to complete the task."
2119	Glossary	"F
fine tuning
The process of adapting a pre-trained model to perform a specific task by conducting additional training. Fine tuning may involve (1) updating the model’s existing parameters, known as full fine tuning, or (2) updating a subset of the model’s existing parameters or adding new parameters to the model and training them while freezing the model’s existing parameters, known as parameter-efficient fine tuning."
2120	Glossary	"F
flow
A collection of nodes that define a set of steps for processing data or training a model."
2121	Glossary	"F
foundation model
An AI model that can be adapted to a wide range of downstream tasks. Foundation models are typically large-scale generative models that are trained on unlabeled data using self-supervision. As large scale models, foundation models can include billions of parameters."
2122	Glossary	"G
Gantt chart
A graphical representation of a project timeline and duration in which schedule data is displayed as horizontal bars along a time scale."
2123	Glossary	"G
gen AI
See generative AI."
2124	Glossary	"G
generative AI (gen AI)
A class of AI algorithms that can produce various types of content including text, source code, imagery, audio, and synthetic data."
2125	Glossary	"G
generative variability
The characteristic of generative models to produce varied outputs, even when the input to the model is held constant. See also probabilistic."
2126	Glossary	"G
GPU
See graphics processing unit."
2127	Glossary	"G
graphical builder
A tool for creating analytical assets by visually coding. A canvas is an area on which to place objects or nodes that can be connected to create a flow."
2128	Glossary	"G
graphics processing unit (GPU)
A specialized processor designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display. GPUs are heavily utilized in machine learning due to their parallel processing capabilities. See also accelerator."
2129	Glossary	"G
grounding
Providing a large language model with information to improve the accuracy of results."
2130	Glossary	"H
hallucination
A response from a foundation model that includes off-topic, repetitive, incorrect, or fabricated content. Hallucinations involving fabricating details can happen when a model is prompted to generate text, but the model doesn't have enough related text to draw upon to generate a result that contains the correct details."
2131	Glossary	"H
HAP detection (HAP detection)
The ability to detect and filter hate, abuse, and profanity in both prompts submitted by users and in responses generated by an AI model."
2132	Glossary	"H
HAP detector (HAP detector)
A sentence classifier that removes potentially harmful content, such as hate speech, abuse, and profanity, from foundation model output and input."
2133	Glossary	"H
hold-out set
A set of labeled data that is intentionally withheld from both the training and validation sets, serving as an unbiased assessment of the final model's performance on unseen data."
2134	Glossary	"H
homogenization
The trend in machine learning research in which a small number of deep neural net architectures, such as the transformer, are achieving state-of-the-art results across a wide variety of tasks."
2135	Glossary	"H
HPO
See hyperparameter optimization."
2136	Glossary	"H
human oversight
Human involvement in reviewing decisions rendered by an AI system, enabling human autonomy and accountability of decision."
2137	Glossary	"H
hyperparameter
In machine learning, a parameter whose value is set before training as a way to increase model accuracy."
2138	Glossary	"H
hyperparameter optimization (HPO)
The process for setting hyperparameter values to the settings that provide the most accurate model."
2139	Glossary	"I
image
A software package that contains a set of libraries."
2140	Glossary	"I
incremental learning
The process of training a model using data that is continually updated without forgetting data obtained from the preceding tasks. This technique is used to train a model with batches of data from a large training data source."
2141	Glossary	"I
inferencing
The process of running live data through a trained AI model to make a prediction or solve a task."
2142	Glossary	"I
ingest
To continuously add a high-volume of real-time data to a database. To feed data into a system for the purpose of creating a base of knowledge."
2143	Glossary	"I
insight
An accurate or deep understanding of something. Insights are derived using cognitive analytics to provide current snapshots and predictions of customer behaviors and attitudes."
2144	Glossary	"I
intelligent AI
Artificial intelligence systems that can understand, learn, adapt, and implement knowledge, demonstrating abilities like decision-making, problem-solving, and understanding complex concepts, much like human intelligence."
2145	Glossary	"I
intent
A purpose or goal expressed by customer input to a chatbot, such as answering a question or processing a bill payment."
2146	Glossary	"J
job
A separately executable unit of work."
2147	Glossary	"K
knowledge base
See corpus."
2148	Glossary	"L
label
A class or category assigned to a data point in supervised learning.Labels can be derived from data but are often applied by human labelers or annotators."
2149	Glossary	"L
labeled data
Raw data that is assigned labels to add context or meaning so that it can be used to train machine learning models. For example, numeric values might be labeled as zip codes or ages to provide context for model inputs and outputs."
2150	Glossary	"L
large language model (LLM)
A language model with a large number of parameters, trained on a large quantity of text."
2151	Glossary	"L
latent space
An n-dimensional mathematical space in which data instances are embedded. A two- dimensional latent space embeds data as points within in a 2D plane (see also: representational space). See also representational space."
2152	Glossary	"L
LLM
See large language model."
2153	Glossary	"M
machine learning (ML)
A branch of artificial intelligence (AI) and computer science that focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving the accuracy of AI models."
2154	Glossary	"M
machine learning framework
The libraries and runtime for training and deploying a model."
2155	Glossary	"M
machine learning model
An AI model that is trained on a a set of data to develop algorithms that it can use to analyze and learn from new data."
2156	Glossary	"M
mental model
An individual’s understanding of how a system works and how their actions affect system outcomes. When these expectations do not match the actual capabilities of a system, it can lead to frustration, abandonment, or misuse."
2157	Glossary	"M
misalignment
A discrepancy between the goals or behaviors that an AI system is optimized to achieve and the true, often complex, objectives of its human users or designers"
2158	Glossary	"M
ML
See machine learning."
2159	Glossary	"M
MLOps
A methodology that takes a machine learning model from development to production. The practice for collaboration between data scientists and operations professionals to help manage production machine learning (or deep learning) lifecycle. MLOps looks to increase automation and improve the quality of production ML while also focusing on business and regulatory requirements. It involves model development, training, validation, deployment, monitoring, and management and uses methods like CI/CD."
2160	Glossary	"M
model
In a machine learning context, a set of functions and algorithms that have been trained and tested on a data set to provide predictions or decisions. In Decision Optimization, a mathematical formulation of a problem that can be solved with CPLEX optimization engines using different data sets."
2161	Glossary	"M
ModelOps
A methodology for managing the full lifecycle of an AI model, including training, deployment, scoring, evaluation, retraining, and updating."
2162	Glossary	"M
monitored group
A class of data that is monitored to determine if the results from a predictive model differ significantly from the results of the reference group. Groups are commonly monitored based on characteristics that include race, gender, or age."
2163	Glossary	"M
multiclass classification model
A classification task with more than two classes. For example, where a binary classification model predicts yes or no values, a multi-class model predicts yes, no, maybe, or not applicable."
2164	Glossary	"M
multivariate time series
Time series experiment that contains two or more changing variables. For example, a time series model forecasting the electricity usage of three clients."
2165	Glossary	"N
natural language processing (NLP)
A field of artificial intelligence and linguistics that studies the problems inherent in the processing and manipulation of natural language, with an aim to increase the ability of computers to understand human languages."
2166	Glossary	"N
natural language processing library
A library that provides basic natural language processing functions for syntax analysis and out-of-the-box pre-trained models for a wide variety of text processing tasks."
2167	Glossary	"N
neural network
A mathematical model for predicting or classifying cases by using a complex mathematical scheme that simulates an abstract version of brain cells. A neural network is trained by presenting it with a large number of observed cases, one at a time, and allowing it to update itself repeatedly until it learns the task."
2168	Glossary	"N
NLP
See natural language processing."
2169	Glossary	"N
node
In an SPSS Modeler flow, the graphical representation of a data operation."
2170	Glossary	"N
notebook
An interactive document that contains executable code, descriptive text for that code, and the results of any code that is run."
2171	Glossary	"N
notebook kernel
The part of the notebook editor that executes code and returns the computational results."
2172	Glossary	"O
object storage
A method of storing data, typically used in the cloud, in which data is stored as discrete units, or objects, in a storage pool or repository that does not use a file hierarchy but that stores all objects at the same level."
2173	Glossary	"O
one-shot learning
A model for deep learning that is based on the premise that most human learning takes place upon receiving just one or two examples. This model is similar to unsupervised learning."
2174	Glossary	"O
one-shot prompting
A prompting technique in which a single example is provided to the model to demonstrate how to complete the task."
2175	Glossary	"O
online deployment
Method of accessing a model or Python code deployment through an API endpoint as a web service to generate predictions online, in real time."
2176	Glossary	"O
ontology
An explicit formal specification of the representation of the objects, concepts, and other entities that can exist in some area of interest and the relationships among them."
2177	Glossary	"O
operational asset
An asset that runs code in a tool or a job."
2178	Glossary	"O
optimization
The process of finding the most appropriate solution to a precisely defined problem while respecting the imposed constraints and limitations. For example, determining how to allocate resources or how to find the best elements or combinations from a large set of alternatives."
2179	Glossary	"O
Optimization Programming Language
A modeling language for expressing model formulations of optimization problems in a format that can be solved by CPLEX optimization engines such as IBM CPLEX."
2180	Glossary	"O
optimized metric
A metric used to measure the performance of the model. For example, accuracy is the typical metric used to measure the performance of a binary classification model."
2181	Glossary	"O
orchestration
The process of creating an end-to-end flow that can train, run, deploy, test, and evaluate a machine learning model, and uses automation to coordinate the system, often using microservices."
2182	Glossary	"O
overreliance
A user's acceptance of an incorrect recommendation made by an AI model. See also reliance, underreliance."
2183	Glossary	"P
parameter
A real-valued weight between 0.0 and 1.0 indicating the strength of connection between two neurons in a neural network. A configurable part of the model that is internal to a model and whose values are estimated or learned from data. Parameters are aspects of the model that are adjusted during the training process to help the model accurately predict the output. The model's performance and predictive power largely depend on the values of these parameters."
2184	Glossary	"P
party
In Federated Learning, an entity that contributes data for training a common model. The data is not moved or combined but each party gets the benefit of the federated training."
2185	Glossary	"P
payload
The data that is passed to a deployment to get back a score, prediction, or solution."
2186	Glossary	"P
payload logging
The capture of payload data and deployment output to monitor ongoing health of AI in business applications."
2187	Glossary	"P
perceptron
A simple neural network that can be used for classification through supervised learning."
2188	Glossary	"P
pipeline
In Watson Pipelines, an end-to-end flow of assets from creation through deployment. In AutoAI, a candidate model."
2189	Glossary	"P
pipeline leaderboard
In AutoAI, a table that shows the list of automatically generated candidate models, as pipelines, ranked according to the specified criteria."
2190	Glossary	"P
policy
A strategy or rule that an agent follows to determine the next action based on the current state."
2191	Glossary	"P
positional encoding
An encoding of an ordered sequence of data that includes positional information, such as encoding of words in a sentence that includes each word's position within the sentence. See also encoding."
2192	Glossary	"P
predictive analytics
A business process and a set of related technologies that are concerned with the prediction of future possibilities and trends. Predictive analytics applies such diverse disciplines as probability, statistics, machine learning, and artificial intelligence to business problems to find the best action for a specific situation. See also data mining."
2193	Glossary	"P
pretrained model
An AI model that was previously trained on a large data set to accomplish a specific task. Pretrained models are used instead of building a model from scratch."
2194	Glossary	"P
pretraining
The process of training a machine learning model on a large dataset before fine-tuning it for a specific task."
2195	Glossary	"P
privacy
Assurance that information about an individual is protected from unauthorized access and inappropriate use."
2196	Glossary	"P
probabilistic
The characteristic of being subject to randomness; non-deterministic. Probabilistic models do not produce the same outputs given the same inputs. See also generative variability."
2197	Glossary	"P
project
A collaborative workspace for working with data and other assets."
2198	Glossary	"P
prompt
Data, such as text or an image, that prepares, instructs, or conditions a foundation model's output. A component of an action that indicates that user input is required for a field before making a transition to an output screen."
2199	Glossary	"P
prompt engineering
The process of designing natural language prompts for a language model to perform a specific task."
2200	Glossary	"P
prompting
The process of providing input to a foundation model to induce it to produce output."
2201	Glossary	"P
prompt tuning
An efficient, low-cost way of adapting a pre-trained model to new tasks without retraining the model or updating its weights. Prompt tuning involves learning a small number of new parameters that are appended to a model’s prompt, while freezing the model’s existing parameters."
2202	Glossary	"P
pruning
The process of simplifying, shrinking, or trimming a decision tree or neural network. This is done by removing less important nodes or layers, reducing complexity to prevent overfitting and improve model generalization while maintaining its predictive power."
2203	Glossary	"P
Python
A programming language that is used in data science and AI."
2204	Glossary	"P
Python function
A function that contains Python code to support a model in production."
2205	Glossary	"Q
quantization
A method of compressing foundation model weights to speed up inferencing and reduce GPU memory needs."
2206	Glossary	"R
R
An extensible scripting language that is used in data science and AI that offers a wide variety of analytic, statistical, and graphical functions and techniques."
2207	Glossary	"R
RAG
See retrieval augmented generation."
2208	Glossary	"R
random seed
A number used to initialize a pseudorandom number generator. Random seeds enable reproducibility for processes that rely on random number generation."
2209	Glossary	"R
reference group
A group that is identified as most likely to receive a positive result in a predictive model. The results can be compared to a monitored group to look for potential bias in outcomes."
2210	Glossary	"R
refine
To cleanse and shape data."
2211	Glossary	"R
regression model
A model that relates a dependent variable to one or more independent variables."
2212	Glossary	"R
reinforcement learning
A machine learning technique in which an agent learns to make sequential decisions in an environment to maximize a reward signal. Inspired by trial and error learning, agents interact with the environment, receive feedback, and adjust their actions to achieve optimal policies."
2213	Glossary	"R
reinforcement learning on human feedback (RLHF)
A method of aligning a language learning model's responses to the instructions given in a prompt. RLHF requires human annotators rank multiple outputs from the model. These rankings are then used to train a reward model using reinforcement learning. The reward model is then used to fine-tune the large language model's output."
2214	Glossary	"R
reliance
In AI systems, a user’s acceptance of a recommendation made by, or the output generated by, an AI model. See also overreliance, underreliance."
2215	Glossary	"R
representation
An encoding of a unit of information, often as a vector of real-valued numbers. See also embedding."
2216	Glossary	"R
representational space
An n-dimensional mathematical space in which data instances are embedded. A two-dimensional latent space embeds data as points within in a 2D plane (see also: latent space). See also latent space."
2217	Glossary	"R
retrieval augmented generation (RAG)
A technique in which a large language model is augmented with knowledge from external sources to generate text. In the retrieval step, relevant documents from an external source are identified from the user’s query. In the generation step, portions of those documents are included in the LLM prompt to generate a response grounded in the retrieved documents."
2218	Glossary	"R
reward
A signal used to guide an agent, typically a reinforcement learning agent, that provides feedback on the goodness of a decision"
2219	Glossary	"R
RLHF
See reinforcement learning on human feedback."
2220	Glossary	"R
runtime environment
The predefined or custom hardware and software configuration that is used to run tools or jobs, such as notebooks."
2221	Glossary	"S
scoring
In machine learning, the process of measuring the confidence of a predicted outcome. The process of computing how closely the attributes for an incoming identity match the attributes of an existing entity."
2222	Glossary	"S
script
A file that contains Python or R scripts to support a model in production."
2223	Glossary	"S
self-attention
An attention mechanism that uses information from the input data itself to determine what parts of the input to focus on when generating output."
2224	Glossary	"S
self-supervised learning
A machine learning training method in which a model learns from unlabeled data by masking tokens in an input sequence and then trying to predict them. An example is ""I like ________ sprouts""."
2225	Glossary	"S
sentience
The capacity to have subjective experiences and feelings, or consciousness. It involves the ability to perceive, reason, and experience sensations such as pain and pleasure."
2226	Glossary	"S
sentiment analysis
Examination of the sentiment or emotion expressed in text, such as determining if a movie review is positive or negative."
2227	Glossary	"S
shape
To customize data by filtering, sorting, removing columns; joining tables; performing operations that include calculations, data groupings, hierarchies and more."
2228	Glossary	"S
small data
Data that is accessible and comprehensible by humans. See also structured data."
2229	Glossary	"S
SQL pushback
In SPSS Modeler, the process of performing many data preparation and mining operations directly in the database through SQL code."
2230	Glossary	"S
structured data
Data that resides in fixed fields within a record or file. Relational databases and spreadsheets are examples of structured data. See also unstructured data, small data."
2231	Glossary	"S
structured information
Items stored in structured resources, such as search engine indices, databases, or knowledge bases."
2232	Glossary	"S
supervised learning
A machine learning training method in which a model is trained on a labeled dataset to make predictions on new data."
2233	Glossary	"T
temperature
A parameter in a generative model that specifies the amount of variation in the generation process. Higher temperatures result in greater variability in the model's output."
2234	Glossary	"T
text classification
A model that automatically identifies and classifies text into specified categories."
2235	Glossary	"T
time series
A set of values of a variable at periodic points in time."
2236	Glossary	"T
time series model
A model that tracks and predicts data over time."
2237	Glossary	"T
token
A discrete unit of meaning or analysis in a text, such as a word or subword."
2238	Glossary	"T
tokenization
The process used in natural language processing to split a string of text into smaller units, such as words or subwords."
2239	Glossary	"T
trained model
A model that is trained with actual data and is ready to be deployed to predict outcomes when presented with new data."
2240	Glossary	"T
training
The initial stage of model building, involving a subset of the source data. The model learns by example from the known data. The model can then be tested against a further, different subset for which the outcome is already known."
2241	Glossary	"T
training data
A set of annotated documents that can be used to train machine learning models."
2242	Glossary	"T
training set
A set of labeled data that is used to train a machine learning model by exposing it to examples and their corresponding labels, enabling the model to learn patterns and make predictions."
2243	Glossary	"T
transfer learning
A machine learning strategy in which a trained model is applied to a completely new problem."
2244	Glossary	"T
transformer
A neural network architecture that uses positional encodings and the self-attention mechanism to predict the next token in a sequence of tokens."
2245	Glossary	"T
transparency
Sharing appropriate information with stakeholders on how an AI system has been designed and developed. Examples of this information are what data is collected, how it will be used and stored, and who has access to it; and test results for accuracy, robustness and bias."
2246	Glossary	"T
trust calibration
The process of evaluating and adjusting one’s trust in an AI system based on factors such as its accuracy, reliability, and credibility."
2247	Glossary	"T
Turing test
Proposed by Alan Turing in 1950, a test of a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human."
2248	Glossary	"U
underreliance
A user's rejection of a correct recommendations made by an AI model. See also overreliance, reliance."
2249	Glossary	"U
univariate time series
Time series experiment that contains only one changing variable. For example, a time series model forecasting the temperature has a single prediction column of the temperature."
2250	Glossary	"U
unstructured data
Any data that is stored in an unstructured format rather than in fixed fields. Data in a word processing document is an example of unstructured data. See also structured data."
2251	Glossary	"U
unstructured information
Data that is not contained in a fixed location, such as the natural language text document."
2252	Glossary	"U
unsupervised learning
A machine learning training method in which a model is not provided with labeled data and must find patterns or structure in the data on its own. A model for deep learning that allows raw, unlabeled data to be used to train a system with little to no human effort."
2253	Glossary	"V
validation set
A separate set of labeled data that is used to evaluate the performance and generalization ability of a machine learning model during the training process, assisting in hyperparameter tuning and model selection."
2254	Glossary	"V
vector
A one-dimensional, ordered list of numbers, such as [1, 2, 5] or [0.7, 0.2, -1.0]."
2255	Glossary	"V
verbalizer
In generative AI, a template to format the data during tuning and inferencing."
2256	Glossary	"V
virtual agent
A pretrained chat bot that can process natural language to respond and complete simple business transactions, or route more complicated requests to a human with subject matter expertise."
2257	Glossary	"V
visualization
A graph, chart, plot, table, map, or any other visual representation of data."
2258	Glossary	"W
weight
A coefficient for a node that transforms input data within the network's layer. Weight is a parameter that an AI model learns through training, adjusting its value to reduce errors in the model's predictions."
2259	Glossary	"Z
zero-shot prompt
A prompting technique in which the model completes a task without being given a specific example of how."
2260	Creating the Platform assets catalog	"You can create a Platform assets catalog to share connections across your organization. Any user who you add as a collaborator to the catalog can see these connections.  
You can add an unlimited number of collaborators and connection assets to the Platform assets catalog.  
The Platform assets catalog also supports some watsonx.governance features in AI use cases, such as tracking third-party models, storing attachments, and generating reports. For more information on using the Platform assets catalog with inventories for AI uses cases, see Managing an inventory for AI use cases.  
If you are signed up for both Cloud Pak for Data as a Service and watsonx, you share a single Platform assets catalog between the two platforms. Any connection assets that you add to the catalog on either platform are available in both platforms. However, if you add other types of assets to the Platform assets catalog on Cloud Pak for Data as a Service, you can't access those types of assets on watsonx."
2261	Creating the Platform assets catalog	"Requirements
Before you create the Platform assets catalog, understand the required permissions and the requirements for storage and duplicate handling.  
To view your roles, go to Administration > Access (IAM). Then select Roles in the IBM Cloud console.  
Specify the IBM Cloud Object Storage instance that is configured during IBM Cloud account setup. If you are not an administrator for the IBM Cloud Object Storage instance, it must be configured to allow catalog creation.  
Assets are considered duplicates if they have the same asset type and the same name.  
Select how to handle duplicate assets:  
Update original assets Overwrite original assets Allow duplicates (default) Preserve original assets and reject duplicates  
You can change the duplicate handling preferences at any time on the catalog Settings page."
2262	Creating the Platform assets catalog	"Creating the Platform assets catalog
To create the Platform assets catalog:  
From the main menu, choose Data > Platform connections.  
Click Create catalog.  
Select the IBM Cloud Object Storage service. If you don't have an existing service instance, create a IBM Cloud Object Storage service instance and then refresh the page.  
Click Create. The Platform assets catalog is created in a dedicated storage bucket. Initially, you are the only collaborator in the catalog.  
Add collaborators to the catalog. Go to the Access control page in the catalog and add collaborators. You assign each user a role:  
Assign the Admin role to at least one other user so that you are not the only person who can add collaborators. Assign the Editor role to all users who are responsible for adding connections to the catalog. Assign the Viewer role to the users who need to find connections and use them in projects.  
You can give all the users access to the Platform assets catalog by assigning the Viewer role to the Public Access group. By default, all users in your account are members of the Public Access group. See add collaborators.  
Add connections to the catalog. You can delegate this step to other collaborators who have the Admin or Editor role. See Add connections to the Platform assets catalog."
2263	Creating the Platform assets catalog	"Creating the Platform assets catalog
Platform assets catalog collaborator roles
The Platform assets catalog roles provide the permissions in the following table.  
Parent topic: Setting up the platform for administrators"
2264	Firewall access for the platform	"If a data source resides behind a firewall, then IBM watsonx requires inbound access through the firewall in order to make a connection. Inbound firewall access is required whether the data source resides on a third-party cloud provider or in an data center. The method for configuring inbound access varies for different vendor's firewalls. In general, you configure inbound access rules by entering the IP addresses for the IBM watsonx cluster to allow for access by IBM watsonx.  
You can enter the IP addresses using the starting and ending addresses for a range or by using CIDR notation. Classless Inter-Domain Routing (CIDR) notation is a compact representation of an IP address and its associated network mask. For start and end addresses, copy each address and enter them in the inbound rules for your firewall. Alternately, copy the addresses in CIDR notation.  
The IBM watsonx IP addresses vary by region. The user interface lists the IP addresses for the current region. The IP addresses apply to the base infrastructure for IBM watsonx.  
Follow these steps to look up the IP addresses for IBM watsonx cluster:  
Go to the Administration > Cloud integrations page. Click the Firewall configuration link to view the list of IP ranges used by IBM watsonx in your region. View the IP ranges for the IBM watsonx cluster in either CIDR notation or as Start and End addresses. Choose Include private IPs to view the private IP addresses. The private IP addresses allow connections to IBM Cloud Object Storage buckets that are behind a firewall. See Firewall access for Cloud Object Storage. Copy each of the IP ranges listed and paste them into the appropriate security configuration or inbound firewall rules area for your cloud provider.  
For example, if your data source resides on AWS, open the Create Security Group dialog for your AWS Management Console. Paste the IP ranges into the Inbound section for the security group rules.  
Parent topic: Configuring firewall access"
2265	Managing your settings	"You can manage your profile, services, integrations, and notifications while logged in to IBM watsonx.  
Manage your profile Manage user API keys Switch accounts Manage your services Manage your integrations Manage your notification settings View and personalize your project summary"
2266	Managing your settings	"Manage your profile
You can manage your profile on the Profile page by clicking your avatar in the banner and then clicking Profile and settings.  
You can make these changes to your profile:  
Add or change your avatar photo. Change your IBMid or password. Do not change your IBMid (email address) after you register with the IBM watsonx platform. The IBMid (email address) uniquely identifies users in the platform and also authorizes access to various IBM watsonx resources, including projects, spaces, models, and catalogs. If you change your IBMid (email address) in your IBM Cloud profile after you have registered with IBM watsonx, you will lose access to the platform and associated resources. Set your service locations filters by resource group and location. The filters apply throughout the platform. For example, the Service instances page that you access through the Services menu shows only the filtered services. Ensure you have selected the region where Watson Studio is located, for example, Dallas, as well as the Global location. Global is required to provide access to your IBM Cloud Object Storage instance. Set your watsonx user interface to dark theme. Click your avatar and select Profile and settings to open your account profile. Then, set the Dark theme switch to on. Dark theme is not supported in RStudio and Jupyter notebooks. Access your IBM Cloud account. Leave IBM watsonx."
2267	Managing your settings	"Switch accounts
If you are added to a shared IBM Cloud account that is different from your individual account, you can switch your account by selecting a different account from the account list in the menu bar, next to your avatar."
2268	Managing your settings	"Manage your integrations
To set up or modify an integration to GitHub:  
Click your avatar in the banner. Click Profile and settings. Click the Git integrations tab.  
See Publish notebooks on GitHub."
2269	Managing your settings	"Manage your notification settings
To see your notification settings, click the notification bell icon and then click the settings icon.  
You can make these changes to your notification settings:  
Specify to receive push notifications that appear briefly on screen. If you select Do not disturb, you continue to see notifications on the home page and the number of notifications on the bell. Specify to receive notifications by email. Specify for which projects or spaces you receive notifications."
2270	Managing your settings	"View and personalize your project summary
Use the Overview page of a project to view a summary of what's happening in your project. You can jump back into your most recent work and keep up to date with alerts, tasks, project history, and compute usage.  
View recent asset activity in the Assets pane on the Overview page, and filter the assets by selecting By you or By all using the dropdown. Selecting By you lists assets edited by you, ordered by most recent at the top. Selecting By all lists assets edited by others and also by you, ordered by most recent at the top.  
You can use the readme file on the Overview page to document the status or results of the project. The readme file uses standard Markdown formatting. Collaborators with the Admin or Editor role can edit the readme file."
2271	Managing your settings	"Learn more
Managing your IBM Cloud account Managing your services  
Parent topic: Administration"
2272	Managing the user API key	Certain operations in IBM watsonx require an API key for secure authorization. You can generate and rotate a user API key as needed to help ensure your operations run smoothly.
2273	Managing the user API key	"User API key overview
Operations running within services in IBM watsonx require credentials for secure authorization. These operations use an API key for authorization. A valid API key is required for many long-running tasks, including the following:  
Model training in Watson Machine Learning Problem solving with Decision Optimization Data transformation with DataStage flows Other runtime services (for example, Data Refinery and Pipelines) that accept API key references  
Both scheduled and ad hoc jobs require an API key for authorization. An API key is used for jobs when:  
Creating a job schedule with a predefined key Updating the API key for a scheduled job Providing an API key for an ad hoc job  
User API keys give control to the account owner to secure and renew credentials, thus helping to ensure operations run without interruption. Keys are unique to the IBMid and account. If you change the account you are working in, you must generate a new key."
2274	Managing the user API key	"User API key overview
Active and Phased out keys
When you create an API key, it is placed in Active state. The Active key is used for authorization for operations in IBM watsonx.  
When you rotate a key, a new key is created in Active state and the existing key is changed to Phased out state. A Phased out key is not used for authorization and can be deleted."
2275	Managing the user API key	"Viewing the current API key
Click your avatar and select Profile and settings to open your account profile. Select User API key to view the Active and Phased out keys."
2276	Managing the user API key	"Creating an API key
If you do not have an API key, you can create a key by clicking Create a key.  
A new key is created in Active state. The key automatically authorizes operations that require a secure credential. The key is stored in both IBM Cloud and IBM watsonx. You can view the API keys for your IBM Cloud account at API keys.  
User API Keys take the form cpd-apikey-{username}-{timeStamp}, where username is the IBMid of the account owner and timestamp indicates when the key was created."
2277	Managing the user API key	"Rotating an API key
If the API key becomes stale or invalid, you can generate a new Active key for use by all operations.  
To rotate a key, click Rotate.  
A new key is created to replace the current key. The rotated key is placed in Phased out status. A Phased out key is not available for use."
2278	Managing the user API key	"Deleting a phased out API key
When you are certain the phased out key is no longer needed for operations, click the minus sign to delete it. Deleting keys might cause running operations to fail."
2279	Managing the user API key	"Deleting all API keys
Delete all keys (both Active and Phased out) by clicking the trash can. Deleting keys might cause running operations to fail."
2280	Managing the user API key	"Learn more
Creating and managing jobs in a project Adding task credentials Understanding API keys  
Parent topic: Administering your accounts and services"
2281	Securing connections to services with private service endpoints	"You can configure isolated connectivity to your cloud-based services for production workloads with IBM Cloud service endpoints. When you enable IBM Cloud service endpoints in your account, you can expose a private network endpoint when you create a resource. You then connect directly to this endpoint over the IBM Cloud private network rather than the public network. Because resources that use private network endpoints don't have an internet-routable IP address, connections to these resources are more secure.  
To use service endpoints:  
Enable virtual routing and forwarding (VRF) in your account, if necessary, and enable the use of service endpoints. Create services that support VRF and service endpoints.  
See Enabling VRF and service endpoints."
2282	Securing connections to services with private service endpoints	"Learn more
Secure access to services using service endpoints Enabling VRF and service endpoints List of services that support service endpoints  
Parent topic: Security"
2283	Configuring firewall access	"Firewalls protect valuable data from public access. If your data sources reside behind a firewall for protection, and you are not using a Satellite Connector or Satellite location, then you must configure the firewall to allow the IP addresses for IBM watsonx and also for individual services. Otherwise, IBM watsonx is denied access to the data sources.  
To allow IBM watsonx access to private data sources, you configure inbound firewall rules using the security mechanisms for your firewall. Inbound firewall rules are not required for connections that use a Satellite Connector or Satellite location, which establishes a link by performing an outbound connection. For more information, see Connecting to data behind a firewall.  
All services in IBM watsonx actively use WebSockets for the proper functioning of the user interface and APIs. Any firewall between the user and the IBM watsonx domain must allow HTTPUpgrade. If IBM watsonx is installed behind a firewall, traffic for the wss:// protocol must be enabled."
2284	Configuring firewall access	"Configuring inbound access rules for firewalls
If data sources reside behind a firewall, then inbound access rules are required for IBM watsonx. Inbound firewall rules protect the network against incoming traffic from the internet. The following scenarios require inbound access rules through a firewall:  
Firewall access for IBM watsonx Firewall access for Cloud Object Storage Firewall access for AWS Redshift Firewall access for Watson Studio Firewall access for Watson Machine Learning Firewall access for Spark"
2285	Configuring firewall access	"Learn more
Connecting to data behind a firewall  
Parent topic: Setting up the platform for administrators"
2286	Overview for setting up IBM Cloud App ID (beta)	"IBM watsonx supports IBM Cloud App ID to integrate customer's registries for user authentication. You configure App ID on IBM Cloud to communicate with an identiry provider. You then provide an alias to the people in your organization to log in to IBM watsonx.  
To configure identity providers for App ID, you must have one of the following roles in the IBM Cloud account:  
Account owner  
Operator or higher on the App ID instance  
Operator or Administrator role on the IAM Identity Service  
App ID is configured entirely on IBM Cloud. An identity provider, for example, Active Directory, must also be configured separately to communicate with App ID.  
For more information on configuring App ID to work with an identity provider, see Configuring App ID with your identity provider."
2287	Overview for setting up IBM Cloud App ID (beta)	"Configuring the log on alias
The App ID instance is configured as the default identity provider for the account. For instructions on configuring an identity provider, refer to IBM Cloud docs: Enabling authentication from an external identity provider.  
Each App ID instance requires a unique alias. There is one alias per account. All users in an account log in with the same alias. When the identity provider is configured, the alias is initially set to the account ID. You can change the initial alias to be easier to type and remember."
2288	Overview for setting up IBM Cloud App ID (beta)	"Logging in with App ID (beta)
Users choose App ID (beta) as the login method on the IBM watsonx login page and enter the alias. Then, they are redirected to their company's login page to enter their company credentials. Upon logging in successfully to their company, they are redirected to IBM watsonx.  
To verify that the alias is correctly configured, go to the User profile and settings page. Verify that the username in the profile is the email from your company’s registry. The alias is correct if the correct email is shown in the profile, as it indicates that the mapping was successful.  
You cannot switch accounts when logging in through App ID."
2289	Overview for setting up IBM Cloud App ID (beta)	"Limitations
The following limitations apply to this beta release:  
You must map the name/username/sub SAML profile properties to the email property in the user registry. If the mapping is absent or incorrect, a default opaque user ID is used, which is not supported in this beta release.  
The IBM Cloud login page does not support an App ID alias. Users log in into IBM Cloud with a custom URL, following this form: https://cloud.ibm.com/authorize/{app_id_alias}.  
If you are using the Cloud Directory included with App ID as your user registry, you must select Username and password as the option for Manage authentication > Cloud Directory > Settings > Allow users to sign-up and sign-in using."
2290	Overview for setting up IBM Cloud App ID (beta)	"Learn more
Logging in to watsonx.ai through IBM App ID (beta) Configuring App ID with your identity provider IBM Cloud docs: Getting started with App ID IBM Cloud docs: Enabling authentication from an external identity provider  
Parent topic: Managing IBM watsonx"
2291	Configuring App ID with your identity provider	"To use App ID for user authentication for IBM watsonx, you configure App ID as a service on IBM Cloud. You configure an identity provider (IdP) such as Azure Active Directory. You then configure App ID and the identity provider to communicate with each other to grant access to authorized users.  
To configure App ID and your identity provider to work together, follow these steps:  
Configure your identity provider to communicate with IBM Cloud Configure App ID to communicate with your identify provider Configure IAM to enable login through your identity provider"
2292	Configuring App ID with your identity provider	"Configuring your identity provider
To configure your identity provider to communicate with IBM Cloud, you enter the entityID and Location into your SAML configuration for your identity provider. An overview of the steps for configuring Azure Active Directory is provided as an example. Refer to the documentation for your identity provider for detailed instructions for its platform.  
The prerequisites for configuring App ID with an identity provider are:  
An IBM Cloud account An App ID instance An identity provider, for example, Azure Active Directory  
To configure your identity provider for SAML-based single sign-on:  
1. Download the SAML metadata file from App ID to find the values for entityID and Location. These values are entered into the identity provider configuration screen to establish communication with App ID on IBM Cloud. (The corresponding values from the identity provider, plus the primary certificate, are entered in App ID. See Configuring App ID).  
In App ID, choose Identity providers > SAML 2.0 federation. Download the appid-metadata.xml file. Find the values for entityID and Location.  
2. Copy the values for entityID and Location from the SAML metadata file and paste them into the corresponding fields on your identity provider. For Azure Active Directory, the fields are located in Section 1: Basic SAML Configuration in the Enterprise applications configuration screen.  
3. In Section 2: Attributes & Claims for Azure Active Directory, you map the username parameter to user.mail to identify the users by their unique email address. IBM watsonx requires that you set username to the user.mail attribute. For other identity providers, a similar field that uniquely identifies users must be mapped to user.mail."
2293	Configuring App ID with your identity provider	"Configuring App ID
You establish communication between App ID and your identity provider by entering the SAML values from the identity provider into the corresponding App ID fields. An example is provided for configuring App ID to communicate with an Active Directory Enterprise Application.  
1. Choose Identity providers > SAML 2.0 federation and complete the Provide metadata from SAML IdP section.  
2. Download the Base64 certificate from Section 3: SAML Certificates in Active Directory (or your identity provider) and paste it into the Primary certificate field.  
3. Copy the values from Section 4: Set up your-enterprise-application in Active Directory into the corresponding fields in Provide metadata from SAML IdP in IBM App ID.  
4. Click Test on the App ID page to test that App ID can connect to the identity provider. The happy face response indicates that App ID can communicate with the identity provider."
2294	Configuring App ID with your identity provider	"Configuring IAM
You must assign the appropriate role to the users in IBM Cloud IAM and also configure your identity provider in IAM. Users require at least the Viewer role for All Identity and IAM enabled services."
2295	Configuring App ID with your identity provider	"Configuring IAM
Create an identity provider reference in IBM Cloud IAM
Create an identity provider reference to connect your external repository to your IBM Cloud account.  
Navigate to Manage > Access(IAM) > Identity providers. For the type, choose IBM Cloud App ID. Click Create. Enter a name for the identity provider. Select the App ID service instance. Select how to on board users. Static adds users when they log in for the first time. Enable the identity provider for logging in by checking the Enable for account login? box. If you have more than one identity providers, set the identity provider as the default by checking the box. Click Create."
2296	Configuring App ID with your identity provider	"Configuring IAM
Change the App ID login alias
A login alias is generated for App ID. Users enter the alias when logging on to IBM Cloud. You can change the default alias string to be easier to remember.  
Navigate to Manage > Access(IAM) > Identity providers. Select IBM Cloud App ID as the type. Edit the Default IdP URL to make it simpler. For example, https://cloud.ibm.com/authorize/540f5scc241a24a70513961 can be changed to https://cloud.ibm.com/authorize/my-company. Users log in with the alias my-company instead of 540f5scc241a24a70513961."
2297	Configuring App ID with your identity provider	"Learn more
IBM Cloud docs: Managing authentication IBM Cloud docs: Configuring federated identity providers: SAML IBM Cloud SAML Federation Guide Setting up IBM Cloud App ID with your Azure Active Directory Reusing Existing Red Hat SSO and Keycloak for Applications That Run on IBM Cloud with App ID  
Parent topic: Setting up IBM Cloud App ID (beta)"
2298	Services and integrations	"You can extend the functionality of the platform by provisioning other services and components, and integrating with other cloud platforms.  
Provision instances of services and components from the Services catalog. Add service instances and components to the IBM Cloud account to add functionality to the platform. You must be the owner or be assigned the Administrator or Editor role in the IBM Cloud account for IBM watsonx to provision service instances.  
Integrate with other cloud platforms. Allow users to easily create connections to data sources on those cloud platforms. You must have the required roles or permissions on the other cloud platform accounts.  
View regional availability and limitations. Get information about where services are available by region.  
To integrate with data sources, you can create many types of connections to work with a broad array of data sources. Refer to Connecting to data behind a firewall to create secure connections for data sources that are not externalized to the internet."
2299	Integrating with AWS	"You can configure an integration with the Amazon Web Services (AWS) platform to allow IBM watsonx users access to data sources from AWS. Before proceeding, make sure you have proper permissions. For example, you'll need to be able to create services and credentials in the AWS account.  
After you configure an integration, you'll see it under Service instances. You'll see a new AWS tab that lists your instances of Redshift and S3.  
To configure an integration with AWS:  
Log on to the AWS Console.  
From the account drop-down at the upper right, select My Security Credentials.  
Under Access keys (access key ID and secret access key), click Create New Access Key.  
Copy the key ID and secret.  
Important: Write down your key ID and secret and store them in a safe place.  
In IBM watsonx, under Administration > Cloud integrations, go to the AWS tab, enable integration, and then paste the access key ID and access key secret into the appropriate fields.  
If you need to access Redshift, configure firewall access.  
Confirm that you can see your AWS services. From the main menu, choose Administration > Services > Services instances. Click the AWS tab to see those services.  
Now users who have credentials to your AWS services can create connections to them by selecting them on the Add connection page. Then they can access data from those connections by creating connected data assets."
2300	Integrating with AWS	"Next steps
Set up a project Create connections in a project  
Parent topic: Integrations with other cloud platforms"
2301	Integrating with Google Cloud Platform	"You can configure an integration with the Google Cloud Platform (GCP) to allow IBM watsonx users to access data sources from GCP. Before proceeding, make sure you have proper permissions.  
After you configure an integration, you'll see it under Service instances. For example, you'll see a new GCP tab that lists your BigQuery data sets and Storage buckets.  
To configure an integration with GCP:  
Log on to the Google Cloud Platform at https://console.cloud.google.com.  
Go to IAM & Admin > Service Accounts.  
Open your project and then click CREATE SERVICE ACCOUNT.1. Specify a name and description for the new service account and click CREATE. Specify other options as desired and click DONE.1. Click the actions menu next to the service instance and select Create key. For key type, select JSON and then click CREATE. The JSON key file will be downloaded to your machine.  
Important: Write down your key ID and secret and store them in a sStore the key file in a secure location.  
In IBM watsonx, under Administrator > Cloud integrations, go to the GCP tab, enable integration, and then paste the contents from the JSON key file into the text field. Only certain properties from the JSON will be stored, and the private_key property will be encrypted.  
Go back to Google Cloud Platform and edit the service account you created previously. Add the following roles:  
Confirm that you can see your GCP services. From the main menu, choose Administration > Services > Services instances. Click the GCP tab to see those services, for example, BigQuery data sets and Storage buckets.  
Now users who have credentials to your GCP services can can create connections to them by selecting them on the Add connection page. Then they can access data from those connections by creating connected data assets."
2302	Integrating with Google Cloud Platform	"Next steps
Set up a project Create connections in a project  
Parent topic:"
2303	Firewall access for AWS Redshift	"Inbound firewall access allows IBM watsonx to connect to Redshift on AWS through the firewall. You need inbound firewall access to work with your data stored in Redshift.  
To connect to Redshift from IBM watsonx, you configure inbound access through the Redshift firewall by entering the IP ranges for IBM watsonx into the inbound firewall rules (also called ingress rules). Inbound access through the firewall is configurable if Redshift resides on a public subnet. If Redshift resides on a private subnet, then no access is possible.  
Follow these steps to configure inbound firewall access to AWS Redshift:  
Go to your provisioned Amazon Redshift cluster.  
Select Properties and then scroll down to Network and security settings.  
Click the VPC security group.  
Edit the active/default security group.  
Under Inbound rules, change the port range to 5439 to specify the Redshift port. Then select Edit inbound rules > Add rule.  
From IBM watsonx, go to the Administration > Cloud integrations page.  
Click the Firewall configuration link to view the list of IP ranges used by IBM watsonx. IP addresses can be viewed in either CIDR notation or as Start and End addresses.  
Copy each of the IP ranges listed and paste them into the Source field for inbound firewall rules."
2304	Firewall access for AWS Redshift	"Learn more
Working with Redshift-managed VPC endpoints in Amazon Redshift"
2305	Object storage for workspaces	"You must choose an IBM Cloud Object Storage instance when you create a project, catalog, or deployment space workspace. Information that is stored in IBM Cloud Object Storage is encrypted and resilient. Each workspace has its own dedicated bucket.  
You can encrypt the Cloud Object Storage instance that you use for workspaces with your own key. See Encrypt IBM Cloud Object Storage with your own key. The Locations in each user's Profile must include the Global location to allow access to Cloud Object Storage.  
When you create a workspace, the Cloud Object Storage bucket defaults to Regional resiliency. Regional buckets distribute data across several data centers that are within the same metropolitan area. If one of these data centers suffers an outage or destruction, availability and performance are not affected.  
If you are the account owner or administrator, you administer Cloud Object Storage from the Resource list > Storage page on the IBM Cloud dashboard. For example, you can upload and download assets, manage buckets, and configure credentials and other security settings for the Cloud Object Storage instance.  
Follow these steps to manage the Cloud Object Storage instance on IBM Cloud:  
Select a project from the Project list. Click the Manage tab. On the General page, locate the Storage section that displays the bucket name for the project. Select Manage in IBM Cloud to open the Cloud Object Storage Buckets list. Select the bucket name for the project to display a list of assets. Checkmark an asset to download it or perform other tasks as needed.  
Watch this video to see how to manage an object storage instance.  
Video disclaimer: Some minor steps and graphical elements in this video might differ from your platform.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Transcript  
Synchronize transcript with video"
2306	Object storage for workspaces	"Video transcript Time Transcript 00:00 This video shows how to manage an IBM Cloud Object Storage instance. 00:06 When you create a Watson Studio project, an IBM Cloud Object Storage instance is associated with the project. 00:15 On the Manage tab, you'll see the associated object storage instance and have the option to manage it in IBM Cloud. 00:24 IBM Cloud Object Storage uses buckets to organize your data. 00:30 You can see that this instance contains a bucket with the ""jupyternotebooks"" prefix, which was created when the ""Jupyter Notebooks"" project was created. 00:41 If you open that bucket, you'll see all of the files that you added to that project. 00:47 From here, you can download an object or delete it from the bucket. 00:53 You can also view the object SQL URL to access that object from your application. 01:00 You can add objects to the bucket from here. 01:03 Just browse to select the file and wait for it to upload to storage. 01:10 And then that file will be available in the Files slide-out panel in the project. 01:16 Let's create a bucket. 01:20 You can create a Standard or Archive bucket, based on predefined settings, or create a custom bucket. 01:28 Provide a bucket name, which must be unique across the IBM Cloud Object Storage system. 01:35 Select a resiliency. 01:38 Cross Region provides higher availability and durability and Regional provides higher performance. 01:45 The Single Site option will only distribute data across devices within a single site. 01:52 Then select the location based on workload proximity. 01:57 Next, select a storage class, which defines the cost of storing data based on frequency of access. 02:05 Smart Tier provides automatic cost optimization for your storage. 02:11 Standard indicates frequent access. 02:14 Vault is for less frequent access. 02:18 And Cold Vault is for rare access. 02:21 There are other, optional settings to add rules, keys, and services. 02:27 Refer to the documentation for more details on these options. 02:32 When you're ready, create the bucket. 02:35 And, from here, you could add files to that bucket. 02:40 On the Access"
2307	Object storage for workspaces	Refer to the documentation for more details on these options. 02:32 When you're ready, create the bucket. 02:35 And, from here, you could add files to that bucket. 02:40 On the Access policies panel, you can manage access to buckets using IAM policies - that's Identity and Access Management. 02:50 On the Configuration panel, you'll find information about Key Protect encryption keys, as well as the bucket instance CRN and endpoints to access the data in the buckets from your application. 03:01 You can also find some of the same information on the Endpoints panel. 03:06 On the Service credentials panel, you'll find the API and access keys to authenticate with your instance from your application. 03:15 You can also connect the object storage to a Cloud Foundry application, check usage details, and view your plan details. 03:26 Find more videos in the Cloud Pak for Data as a Service documentation.
2308	Object storage for workspaces	"Learn more
Setting up IBM Cloud Object Storage IBM Cloud docs: Getting started with IBM Cloud Object Storage IBM Cloud docs: Endpoints and storage locations Troubleshooting Cloud Object Storage for projects  
Parent topic: Creating a project"
2309	Firewall access for Spark	To allow Spark to access data that is located behind a firewall, you add the appropriate IP addresses for your region to the inbound rules for your firewall.
2310	Firewall access for Spark	"Dallas (us-south)
dal12 - 169.61.173.96/27, 169.63.15.128/26, 150.239.143.0/25, 169.61.133.240/28, 169.63.56.0/24 dal13 - 169.61.57.48/28, 169.62.200.96/27, 169.62.235.64/26 dal10 - 169.60.246.160/27, 169.61.194.0/26, 169.46.22.128/26, 52.118.59.0/25  
Parent topic: Configuring firewall access"
2311	Creating and managing IBM Cloud services	You can create IBM Cloud service instances within IBM watsonx from the Services catalog.
2312	Creating and managing IBM Cloud services	"Creating a service
To view the Services catalog, select Administration > Services > Services catalog from the main menu. For a description of each service, see Services.  
To check which service instances you have, select Administration > Services > Service instances from the main menu. You can filter which services you see by resource group, organization, and region.  
To create a service:  
Log in to IBM watsonx. Select Administration > Services > Services catalog from the main menu. Click the service you want to create. Specify the IBM Cloud service region. Select a plan. If necessary, select the resource group or organization. Click Create."
2313	Creating and managing IBM Cloud services	"Managing services
To manage a service:  
Select Administration > Services > Services instances from the main menu. Click the Action menu next to the service name and select Manage in IBM Cloud. The service page in IBM Cloud opens in a separate browser tab. To change pricing plans, select Plan and choose the desired plan."
2314	Creating and managing IBM Cloud services	"Learn more
Associate a service with a project Managing the platform  
Parent topic: IBM Cloud services"
2315	Determining your roles and permissions	You have multiple roles within IBM Cloud and IBM watsonx that provide permissions. You can determine what each of your roles are, and, when necessary, who can change your roles.
2316	Determining your roles and permissions	"Projects and catalogs roles
To determine your role in a project or deployment space, look at the Access Control page on the Manage tab. Your role is listed next to your name or the service ID you use to log in.  
The permissions that are associated with each role are specific to the type of workspace:  
Project collaborator permissions Deployment space collaborator permissions  
If you want a different role, ask someone who has the Admin role on the Access Control page to change your role."
2317	Determining your roles and permissions	"IBM Cloud IAM account and service access roles
You can see your IAM account and service access roles in IBM Cloud.  
To see your IAM account and service access roles in IBM Cloud:  
From the IBM watsonx main menu, click Administration > Access (IAM). Click Users, then click your name. Click the Access policies tab. You might have multiple entries:  
The All resources in account (including future IAM enabled services) entry shows your general roles for all services in the account. Other entries might show your roles for individual services.  
If you want the IBM Cloud account administrator role or another role, ask an IBM Cloud account owner or administrator to assign it to you. You can find your account administrators on your Access (IAM) > Users page in IBM Cloud."
2318	Determining your roles and permissions	"Learn more
Roles in IBM watsonx Find your IBM Cloud account owner or administrator  
Parent topic: Administration"
2319	Integrations with other cloud platforms	"You can integrate IBM watsonx with other cloud platforms to configure access to the data source services on that platform. Then, users can easily create connections to those data source services and access the data in those data sources.  
You need to be the Account Owner or Administrator for the IBM Cloud account to configure integrations with other cloud platforms.  
You must have the proper permissions in your cloud platform subscription before you can configure an integration. If you are using Amazon Web Services (AWS) Redshift (or other AWS data sources) or Microsoft Azure, you must also configure firewall access to allow IBM watsonx to access data.  
After you configure integration and firewall access with another cloud platform, you can access and connect to the services on that platform:  
The service instances for that platform are shown on the Service instances page. From the main menu, choose Administration > Services > Services instances. Each cloud platform that you integrate with has its own page. The data source services in that platform are shown when you create a connection. Start adding a connection in a project, catalog, or other workspace. When the Add connection page appears, click the To service tab. The services are listed by cloud platform.  
You can configure integrations with these cloud platforms:  
Amazon Web Services (AWS) Microsoft Azure Google Cloud Platform (GCP)  
Parent topic: Services and integrations"
2320	Firewall access for Watson Machine Learning	To allow Watson Machine Learning to access data that is located behind a firewall, you add the appropriate IP addresses for your region to the inbound rules for your firewall.
2321	Firewall access for Watson Machine Learning	"Dallas (us-south)
dal10 - 169.60.39.152/29 dal12 - 169.48.198.96/29 dal13 - 169.61.47.128/29,169.62.162.88/29  
Parent topic: Configuring firewall access"
2322	Upgrading services on IBM watsonx	"When you're ready to upgrade services, you can upgrade in place without losing any of your work or data.  
Each service has its own plan and is independent of other plans."
2323	Upgrading services on IBM watsonx	"Step 1: Update your IBM Cloud account
You can skip this step if your IBM Cloud account has billing information with a Pay-As-You-Go or a subscription plan.  
You must update your IBM Cloud account in the following circumstances:  
You have a Trial account from signing up for watsonx. You have a Trial account that you registered through an academic institution. You have a Lite account that you created before 25 October 2021. You want to change a Pay-As-You-Go plan to a subscription plan.  
For instructions on updating your IBM Cloud account, see Update your IBM Cloud account."
2324	Upgrading services on IBM watsonx	"Step 2: Upgrade your service plans
You can upgrade the service plans for services. To upgrade service plans, you must have an IBM Cloud access policy with either the Editor or Administrator platform role for the services.  
To upgrade a service plan:  
Click Upgrade on the header or choose Administration > Account and billing > Upgrade service plans from the main menu to open the Upgrade service plans page. Select one or more services to change the service plans. Click Select plan for each service in the Pricing summary pane. Select the plan from the Services catalog page for the service. Agree to the terms, then click Buy. Your service plans are instantly updated.  
After the upgrade, the additional features and capacity for the new plan are automatically available. For the following services, the difference between plans can be significant:  
Watson Studio offering plans Watson Machine Learning plans and compute usage"
2325	Upgrading services on IBM watsonx	"Learn more
IBM Cloud docs: Account types IBM Cloud docs: Upgrading your account Setting up the IBM Cloud account Monitoring account resource usage Find your account administrators  
Parent topic: Managing IBM watsonx"
2326	Removing users from the account or from the workspace	The IBM Cloud account administrator or owner can remove users from the IBM Cloud account. Any use with the Admin role can remove users from a workspace.
2327	Removing users from the account or from the workspace	"Removing users from the IBM Cloud account
You can remove a user from an IBM Cloud account, so that the user can no longer log in to the console, switch to your account, or access account resources."
2328	Removing users from the account or from the workspace	"Removing users from the IBM Cloud account
Required roles
Owner  
Administrator  
Editor  
To remove a user from the IBM Cloud account:  
From the IBM watsonx navigation menu, click Administration > Access (IAM). Click Users and find the name of the user that you want to remove. Choose Remove user from the action menu and confirm the removal.  
Removing a user from an account doesn't delete the IBMid for the user. Any resources such as projects or catalogs that were created by the user remain in the account, but the user no longer has access to work with those resources. The account owner, or an administrator for the service instance, can assign other users to work with the projects and catalogs or delete them entirely.  
For more information, see IBM Cloud docs: Removing users from an account."
2329	Removing users from the account or from the workspace	"Removing users from a workspace
You can remove collaborators from a workspace, such as a project or space, so that the user can no longer access the workspace or any of its contents.  
To remove a collaborator, select one or more users (or user groups) on the Access control page of the workspace and click Remove.  
The user is still a member of the IBM Cloud account and can be added as a collaborator to other workspaces as needed."
2330	Removing users from the account or from the workspace	"Learn more
Stop using IBM watsonx Project collaborators IBM Cloud docs: Removing users from an account  
Parent topic: Managing the platform"
2331	Managing your IBM Cloud account	"You can manage your IBM Cloud account to view billing and usage, manage account users, and manage services.  
To manage your IBM Cloud account, choose Administration > Account and billing > Account > Manage in IBM Cloud from IBM watsonx. Then from the IBM Cloud console, choose an option from the Manage menu.  
Account: See Adding orgs and spaces and Managing resource groups. Billing and Usage: See How you're charged. Access (IAM): See Inviting users."
2332	Managing your IBM Cloud account	"Learn more
Activity Tracker events Manage your settings Set up IBM watsonx for your organization Manage users and access IBM Cloud SAML Federation Guide Delete your IBM Cloud account Check the status of IBM Cloud services Configure private service endpoints  
Parent topic: Administration"
2333	Firewall access for Cloud Object Storage	"Private IP addresses are required when IBM watsonx and Cloud Object Storage are located on the same network. When creating a connection to a Cloud Object Storage bucket that is protected by a firewall on the same network as IBM watsonx, the connector automatically maps to private IP addresses for IBM watsonx. The private IP addresses must be added to a Bucket access policy to allow inbound connections from IBM watsonx.  
Follow these steps to search the private IP addresses for the IBM watsonx cluster and add them to the Bucket access policy:  
Go to the Administration > Cloud integrations page. Click the Firewall configuration link to view the list of IP ranges used by IBM watsonx. Choose Include private IPs to view the private IP addresses for the IBM watsonx cluster. From your IBM Cloud Object Storage instance on IBM Cloud, open the Buckets list and choose the Bucket for the connection. Copy each of the private IP ranges listed and paste them into the Buckets > Permissions > IP address field on IBM Cloud."
2334	Firewall access for Cloud Object Storage	"Learn more
IBM Cloud Object Storage connection IBM Cloud docs: Setting a firewall  
Parent topic: Configuring firewall access"
2335	Managing all projects in the account	If you have the required permission, you can view and manage all projects in your IBM Cloud account. You can add yourself to a project so that you can delete it or change its collaborators.
2336	Managing all projects in the account	"Requirements
To manage all projects in the account, you must:  
Restrict resources to the current account. See steps to set the scope for resources. Have the Manage projects permission that is provided by the IAM Manager role for the IBM Cloud Pak for Data service."
2337	Managing all projects in the account	"Assigning the Manage projects permission
To grant the Manage projects permission to a user who is already in your IBM Cloud account:  
From the navigation menu, choose Administration > Access (IAM) to open the Manage access and users page in your IBM Cloud account. Select the user on the Users page. Click the Access tab and then choose Assign access+. Select Access policy. For Service, choose IBM Cloud Pak for Data. For Service access, select the Manager role. For Platform access, assign the Editor role. Click Add and Assign to assign the policy to the user."
2338	Managing all projects in the account	"Managing projects
You can add yourself to a project when you need to delete the project, delete collaborators, or assign the Admin role to a collaborator in the project. To manage projects:  
View all active projects on the Projects page in IBM watsonx by clicking the drop-down menu next to the search field and selecting All active projects. Join any project as Admin by clicking Join as admin in the Your role column. Filter projects to identify which projects you are not a collaborator in, by clicking the filter icon and selecting Your role > No membership.  
For more details on managing projects, see Administering projects."
2339	Managing all projects in the account	"Learn more
Administering projects  
Parent topic: Managing IBM watsonx"
2340	Managing your account settings	"From the Account window you can view information about your IBM Cloud account and set the Resource scope, Credentials for connections, and Regional project storage settings for IBM watsonx.  
View account information Set the scope for resources Set the type of credentials for connections Set the login session expiration  
You must be the IBM Cloud account owner or administrator to manage the account settings."
2341	Managing your account settings	"View account information
You can see the account name, ID and type.  
Select Administration > Account and billing > Account to open the account window. If you need to manage your Cloud account, click the Manage in IBM Cloud link to navigate to the Account page on IBM Cloud."
2342	Managing your account settings	"Set the scope for resources
By default, account users see resources based on membership. You can restrict the resource scope to the current account to control access. By setting the resource scope to the current account, users cannot access resources outside of their account, regardless of membership. The scope applies to projects, catalogs, and spaces.  
To restrict resources to current account:  
Select Administration > Account and billing > Account to open the account settings window. Set Resource scope to On. Access is updated immediately to be restricted to the current account."
2343	Managing your account settings	"Set the credentials for connections
The credentials for connections setting determines the type of credentials users must specify when creating a new connection. This setting applies only when new connections are created; existing connections are not affected."
2344	Managing your account settings	"Set the credentials for connections
Either personal or shared credentials
You can allow users the ability to specify personal or shared credentials when creating a new connection. Radio buttons will appear on the new connection form, allowing the user to select personal or shared.  
To allow the credential type to be chosen on the new connection form:  
Select Administration > Account and billing > Account to open the account settings window. Set both Shared credentials and Personal credentials to Enabled."
2345	Managing your account settings	"Set the credentials for connections
Personal credentials
When personal credentials are specified, each user enters their own credentials when creating a new connection or when using a connection to access data.  
To require personal credentials for all new connections:  
Select Administration > Account and billing > Account to open the account settings window. Set Personal credentials to Enabled. Set Shared credentials to Disabled."
2346	Managing your account settings	"Set the credentials for connections
Shared credentials
With shared credentials, the credentials that were entered by the creator of the connection are made available to all other users when accessing data with the connection.  
To require shared credentials for all new connections:  
Select Administration > Account and billing > Account to open the account settings window. Set Shared credentials to Enabled. Set Personal credentials to Disabled."
2347	Managing your account settings	"Set the login session expiration
Active and inactive session durations are managed through IBM Cloud. You are notified of a session expiration 5 minutes before the session expires. Unless your service supports autosaving, your work is not saved when your session expires.  
You can change the default durations for active and inactive sessions. For more information on required permissions and duration limits, see Setting limits for login sessions.  
To change the default durations:  
From the watsonx navigation menu, select Administration > Access (IAM). In IBM Cloud, select Manage > Access (IAM) > Settings. Select the Login session tab. For each expiration time that you want to change, edit the time and click Save.  
The inactivity duration cannot be longer than the maximum session duration, and the token lifetime cannot be longer than the inactivity duration. IBM Cloud prevents you from inputing an invalid combination of settings."
2348	Managing your account settings	"Set the login session expiration
Learn more
Managing all projects in the account Adding connections to projects  
Parent topic: Managing IBM watsonx"
2349	Activity Tracker events	"You can see the events for actions for your provisioned services in the IBM Cloud Activity Tracker. You can use the information that is registered through the IBM Cloud Activity Tracker service to identify security incidents, detect unauthorized access, and comply with regulatory and internal auditing requirements.  
To get started, provision an instance of the IBM Cloud Activity Tracker service. See IBM Cloud Activity Tracker.  
View events in the Activity Tracker in the same IBM Cloud region where you provisioned your services. To view the account and user management events and other global platform events, you must provision an instance of the IBM Cloud Activity Tracker service in the Frankfurt (eu-de) region. See Platform services.  
Events for account and user management  
Events for Watson Studio  
Events for watsonx.ai  
Events for Watson Machine Learning  
Events for model evaluation (Watson OpenScale)"
2350	Activity Tracker events	"Events for account and user management
You can audit account and user management events in Activity Tracker, including:  
Billing events Global catalog events IAM and user management events  
For the complete list of account and user management events, see IBM Cloud docs: Auditing events for account management."
2351	Activity Tracker events	"Events for watsonx.ai
Events for foundation model tuning
When you tune a foundation model in watsonx.ai, you trigger standard Watson Machine Learning events."
2352	Activity Tracker events	"Events for Watson Machine Learning
Events for Watson Machine Learning repository assets
The deployment events are tracked for these Watson Machine Learning repository assets:  
These activities are tracked for each asset type:"
2353	Activity Tracker events	"Events for model evaluation (Watson OpenScale)
Events for private APIs
Parent topic: Administration"
2354	Monitoring account resource usage	"Some service plans charge for compute usage and other types of resource usage. If you are the IBM Cloud account owner or administrator, you can monitor the resources usage to ensure the limits are not exceeded.  
For Lite plans, you cannot exceed the limits of the plan. You must wait until the start of your next billing month to use resources that are calculated monthly. Alternatively, you can upgrade to a paid plan.  
For most paid plans, you pay for the resources that the tools and processes that are provided by the service consume each month.  
To see the costs of your plan, log in to IBM Cloud, open your service instance from your IBM Cloud dashboard, and click Plan.  
Capacity unit hours (CUH) for compute usage Resource units for foundation model inferencing Monitor monthly billing"
2355	Monitoring account resource usage	"Capacity unit hours (CUH) for compute usage
Many tools consume compute usage that is measured in capacity unit hours (CUH). A capacity unit hour is a specific amount of compute capability with a set cost."
2356	Monitoring account resource usage	"Capacity unit hours (CUH) for compute usage
How compute usage is calculated
Different types of processes and different levels of compute power are billed at different rates of capacity units per hour. For example, the hourly rate for a data profiling process is 6 capacity units.  
Compute usage for Watson Studio is charged by the minute, with a minimum charge of 10 minutes (0.16 hours). Compute usage for Watson Machine Learning is charged by the minute with a minimum charge of one minute.  
Compute usage is calculated by adding the minimum number of minutes billed for each process plus the number of minutes the process runs beyond the minimum minutes, then multiplying the total by the capacity unit rate for the process.  
The following table shows examples of how the billed CUH is calculated."
2357	Monitoring account resource usage	"Capacity unit hours (CUH) for compute usage
Processes that consume capacity unit hours
Some types of processes, such as AutoAI and Federated Learning, have a single compute rate for the runtime. However, with many tools you have a choice of compute resources for the runtime. The notebook editor, Data Refinery, SPSS Modeler, and other tools have different rates that reflect the memory and compute power for the environment. Environments with more memory and compute power consume capacity unit hours at a higher rate.  
This table shows each process that consumes CUH, where it runs, and against which service CUH is billed, and whether you can choose from more than one environment. Follow the links to view the available CUH rates for each process.  
|Watson Pipelines | Project and Space| Watson Studio |Multiple rates|"
2358	Monitoring account resource usage	"Capacity unit hours (CUH) for compute usage
Monitoring compute usage
You can monitor compute usage for all services at the account level. To view the monthly CUH usage for a service, open the service instance from your IBM Cloud dashboard and click Plan.  
You can also monitor compute usage in a project on the Environments page on the Manage tab.  
To see the total amount of capacity unit hours that are used and that are remaining for Watson Studio and Watson Machine Learning, look at the Environment Runtimes page. From the navigation menu, select Administration > Environment runtimes. The Environment Runtimes page shows details of the CUH used by environments. You can calculate the amount of CUH you use for data flows and profiling by subtracting the amount used by environments from the total amount used."
2359	Monitoring account resource usage	"Resource units for foundation model inferencing
Calling a foundation model to generate output in response to a prompt is known as inferencing. Foundation model inferencing is measure in resource units (RU). Each RU equals 1,000 tokens. A token is a basic unit of text (typically 4 characters or 0.75 words) used in the input or output for a foundation model prompt. For details on tokens, see Tokens.  
Resource unit billing is based on the rate of the foundation model class multipled by the number of tokens. Foundation models are classified into three classes. See Resource unit metering.  
Note: You do not consume tokens when you use the generative AI search and answer app for this documentation site."
2360	Monitoring account resource usage	"Resource units for foundation model inferencing
Monitoring token usage for foundation model inferencing
You can monitor foundation model token usage in a project on the Environments page on the Manage tab."
2361	Monitoring account resource usage	"Monitor monthly billing
You must be an IBM Cloud account owner or administrator to see resource usage information.  
To view a summary of your monthly billing, from the navigation menu, choose Administration > Account and billing > Billing and usage. The IBM Cloud usage dashboard opens. To view the usage for each service, in the Usage summary section, click View usage."
2362	Monitoring account resource usage	"Learn more
Choosing compute resources for running tools in projects Upgrade services Environments compute usage Watson Studio offering plans Watson Machine Learning plans and compute usage  
Parent topic: Managing the platform"
2363	Firewall access for Watson Studio	"Inbound firewall access is granted to the Watson Studio service by allowing the IP addresses for IBM watsonx on IBM Cloud.  
If Watson Studio is installed behind a firewall, you must add the WebSocket connection for your region to the firewall settings. Enabling the WebSocket connection is required for notebooks and RStudio.  
Following are the WebSocket settings for each region:  
Follow these steps to look up the IP addresses for IBM watsonx and allow them on IBM Cloud:  
From the main menu, choose Administration > Cloud integrations. Click Firewall configuration to display the IP addresses for the current region. Use CIDR notation. Copy each CIDR range into the IP address restrictions for either a user or an account. You must also enter the allowed individual client IP addresses. Enter the IP addresses as a comma-separated list. Then, click Apply. Repeat for each region to allow access for Watson Studio.  
When you configure the allowed IP addresses for Watson Studio, you include the CIDR ranges for the Watson Studio cluster. You can also allow individual client system IP addresses.  
For step-by-step instructions for both user and account restrictions, see IBM Cloud docs: Allowing specific IP addresses  
Parent topic: Configuring firewall access"
2364	Administering your accounts and services	"For most administration tasks, you must be the IBM Cloud account owner or administrator. If you log in to your own account, you are the account owner. If you log in to someone else's account or an enterprise account, you might not be the account owner or administrator.  
Tasks for all users:  
Managing your personal settings Determining your roles Understanding accessibility features  
Tasks for IBM Cloud account owners or administrators in IBM watsonx and in IBM Cloud:  
Managing IBM watsonx services Securing IBM watsonx Managing your IBM Cloud account Adding and managing IBM Cloud services Reading notices"
2365	Integrating with Microsoft Azure	"You can configure an integration with the Microsoft Azure platform to allow IBM watsonx users access to data sources from Microsoft Azure. Before proceeding, make sure you have proper permissions. For example, you'll need permission in your subscription to create an application integration in Azure Active Directory.  
After you configure an integration, you'll see it under Service instances. You'll see a new Azure tab that lists your instances of Data Lake Storage Gen1 and SQL Database.  
To configure an integration with Microsoft Azure:  
Log on to your Microsoft Azure account at https://portal.azure.com.  
Navigate to the Subscriptions panel and copy your subscription ID.  
In IBM watsonx, go to Administration > Cloud integrations and click the Azure tab. Paste the subscription ID you copied in the previous step into the Subscription ID field.  
In Microsoft Azure Active Directory, navigate to Manage > App registrations and click New registration to register an application. Give it a name such as IBM integration and select the desired option for supported account types.  
Copy the Application (client) ID and the Tenant ID and paste them into the appropriate fields on the IBM watsonx Integrations page, as you did with the subscription ID.  
In Microsoft Azure, navigate to Certificates & secrets > New client secret to create a new secret.  
Important!  
Write down your secret and store it in a safe place. After you leave this page, you won't be able to retrieve the secret again. You'd need to delete the secret and create a new one. If you ever need to revoke the secret for some reason, you can simply delete it from this page. Pay attention to the expiration date. When the secret expires, integration will stop working.  
Copy the secret from Microsoft Azure and paste it into the appropriate field on the Integrations page as you did with the subscription ID and client ID.  
Configure firewall access.  
Confirm that you can see your Azure services. From the main menu, choose Administration > Services > Services instances. Click the Azure tab to see those services.  
Now users who have credentials to your Azure services can create connections to them by selecting them on the Add connection page. Then they can access data from those connections by creating connected data assets."
2366	Integrating with Microsoft Azure	"Configuring firewall access
You must also configure access so IBM watsonx can access data through the firewall.  
For Microsoft Azure SQL Database firewall:  
Open the database instance in Microsoft Azure. From the top list of actions, select Set server firewall. Set Deny public network access to No. In a separate tab or window, open IBM watsonx and go to Administration > Cloud integrations. In the Firewall configuration panel, for each firewall IP range, copy the start and end address values into the list of rules in the Microsoft Azure QL Database firewall.  
For Microsoft Azure Data Lake Storage Gen1 firewall:  
Open the Data Lake instance. Go to Settings > Firewall and virtual networks. In a separate tab or window, open IBM watsonx and go to Administration > Cloud integrations. In the Firewall configuration panel, for each firewall IP range, copy the start and end address values into the list of rules under Firewall in the Data Lake instance.  
You can now create connections, preview data from Microsoft Azure data sources, and access Microsoft Azure data in Notebooks, Data Refinery, SPSS Modeler, and other tools in projects and in catalogs. You can see your Microsoft Azure instances under Services > Service instances."
2367	Integrating with Microsoft Azure	"Next steps
Set up a project Create connections in a project  
Parent topic: Integrations with other cloud platforms"
2368	Profiles of data assets	"An asset profile includes generated information and statistics about the asset content. You can see the profile on an asset's Profile page.  
Requirements and restrictions Creating a profile Profile information"
2369	Profiles of data assets	"Requirements and restrictions
You can view the profile of assets under the following circumstances."
2370	Profiles of data assets	"Requirements and restrictions
Required permissions
To view a data asset's Profile page, you can have any role in a project.  
To create or update a profile, you must have the Admin or Editor role in the project."
2371	Profiles of data assets	"Requirements and restrictions
Workspaces
You can view the asset profile in projects."
2372	Profiles of data assets	"Requirements and restrictions
Types of assets
These types of assets have a profile:  
Data assets from relational or nonrelational databases from a connection to the data sources, except Cloudant  
Data assets from partitioned data sets, where a partitioned data set consists of multiple files and is represented by a single folder uploaded from the local file system or from file-based connections to the data sources  
Data assets from files uploaded from the local file system or from file-based connections to the data sources, with these formats:  
CSV XLS, XLSM, XLSX (Only the first sheet in a workbook is profiled.) TSV Avro Parquet  
However, structured data files are not profiled when data assets do not explicitly reference them, such as in these circumstances:  
The files are within a connected folder asset. Files that are accessible from a connected folder asset are not treated as assets and are not profiled. The files are within an archive file. The archive file is referenced by the data asset and the compressed files are not profiled."
2373	Profiles of data assets	"Creating a profile
In projects, you can create a profile for a data asset by clicking Create profile. You can update an existing profile when the data changes."
2374	Profiles of data assets	"Profiling results
When you create or update an asset profile, the columns in the data asset are analyzed. By default, the profile is created based on the first 5,000 rows of data. If the data asset has more than 250 columns, the profile is created based on the first 1,000 rows of data.  
The profile of a data asset shows information about each column in the data set:  
When was the profile created or last updated. How many columns and rows were analyzed. The data types for columns and data types distribution. The data formats for columns and formats distribution. The percentage of matching, mismatching, or missing data for each column. The frequency distribution for all values identified in a column. Statistics about the data for each column:  
The number of distinct values indicates how many different values exist in the sampled data for the column. The percentage of unique values indicates the percentage of distinct values that appear only once in the column. The minimum, maximum, or mean, and sometimes the standard deviation in that column. Depending on a column’s data format, the statistics vary slightly. For example, statistics for a column of data type integer have minimum, maximum, and mean values and a standard deviation value while statistics for a column of data type string have minimum length, maximum length, and mean length values.  
Parent topic: Asset types and properties"
2375	Accessibility features in IBM watsonx content and documentation	"IBM is committed to accessibility. Accessibility features that follow compliance guidelines are included in IBM watsonx content and documentation to benefit users with disabilities. Parts of the user interface of IBM watsonx are accessible, but not entirely. Only documentation is compliant, with a subset of parts of the overall product.  
IBM watsonx documentation uses the latest W3C Standard, WAI-ARIA 1.0 to ensure compliance with the United States Access Board Section 508 Standards, and the Web Content Accessibility Guidelines (WCAG) 2.0.  
The IBM watsonx online product documentation is enabled for accessibility. Accessibility features help users who have a disability, such as restricted mobility or limited vision, to use information technology products successfully. Documentation is provided in HTML so that it is easily accessible through assistive technology. With the accessibility features of IBM watsonx, you can do the following tasks:  
Use screen-reader software and digital speech synthesizers to hear what is displayed on the screen. Consult the product documentation of the assistive technology for details on using assistive technologies with HTML-based information. Use screen magnifiers to magnify what is displayed on the screen. Operate specific or equivalent features by using only the keyboard.  
For more information about the commitment that IBM has to accessibility, see IBM Accessibility."
2376	Accessibility features in IBM watsonx content and documentation	"TTY service
In addition to standard IBM help desk and support websites, IBM has established a TTY telephone service for use by deaf or hard of hearing customers to access sales and support services:  
800-IBM-3383 (800-426-3383) within North America"
2377	Accessibility features in IBM watsonx content and documentation	"Additional interface information
The IBM watsonx user interfaces do not have content that flashes 2 - 55 times per second.  
The IBM watsonx web user interfaces rely on cascading stylesheets to render content properly and to provide a usable experience. If you are a low-vision user, you can adjust your operating system display settings, and use settings such as high contrast mode. You can control font size by using the device or web browser settings."
2378	Setting up the IBM Cloud account	"As an IBM Cloud account owner or administrator, you sign up for IBM watsonx.ai and set up payment for services in the IBM Cloud account.  
These steps describe the typical tasks for an IBM Cloud account owner to set up the account for an organization:  
Sign up for watsonx.ai. Update your IBM Cloud account to add or update billing information. (Optional) Configure restrictions for the account."
2379	Setting up the IBM Cloud account	"Step 1: Sign up for watsonx
To sign up for watsonx.ai:  
Go to Try IBM watsonx.ai or Try watsonx.governance. Select the service region. Agree to the terms, Data Use Policy, and Cookie Use. Log in with your IBMid (usually an email address) if you have an existing IBM Cloud account. If you don't have an IBM Cloud account, click Create an IBM Cloud account to create a new account. You must enter a credit card to create a Pay-As-You-Go IBM Cloud account. However, you are not charged until you buy paid service plans.  
Lite plans for Watson Studio and Watson Machine Learning are automatically provisioned for you."
2380	Setting up the IBM Cloud account	"Step 2: Update your IBM Cloud account
You can skip this step if your IBM Cloud account has billing information with a Pay-As-You-Go or a subscription plan.  
You must update your IBM Cloud account in the following circumstances:  
You have a Trial account from signing up for watsonx. You have a Trial account that you registered through an academic institution. You have a Lite account that you created before 25 October 2021. You want to change a Pay-As-You-Go plan to a subscription plan."
2381	Setting up the IBM Cloud account	"Step 2: Update your IBM Cloud account
Setting up a Pay-As-You-Go account
You set up a Pay-As-You-Go by adding a credit card number and billing information. You pay only for billable services that you use, with no long-term contracts or commitments. You can provision paid plans for all services in the IBM Cloud services catalog, including plans in the watsonx services catalog.  
To set up a Pay-As-You-Go account:  
From the watsonx navigation menu, select Administration > Account and billing > Account. Click Manage in IBM Cloud. Log in to IBM Cloud. Select Account settings. Click Add credit card and enter your credit card and billing information. Click Create account to submit your information.  
After your payment information is processed, your account is upgraded and you receive a monthly invoice for billable resource usage or instance fees."
2382	Setting up the IBM Cloud account	"Step 2: Update your IBM Cloud account
Setting up a subscription account
With subscriptions, you commit to a minimum spending amount for a certain period and receive a discount on the overall cost. Subscriptions are limited to service plans in the watsonx catalog.  
Subscription credits are activated using a unique code that you receive by email. To activate the subscription, you apply the subscription code to an account. Be careful when selecting the account, because after you apply the subscription to an account, you can't undo it.  
To set up a watsonx subscription:  
From the watsonx navigation menu, select Administration > Account and billing > Upgrade service plans. On the Upgrade service plans page, click Contact sales.  
Complete and submit the form to communicate with IBM Sales that you want to set up a subscription account for watsonx. An associate from IBM Sales will contact you to set up a subscription. When your subscription is ready, you receive an email from IBM containing a unique subscription code.  
To apply the subscription code to your account:  
Locate the unique code from the email that you received from IBM. Log in to your IBM Cloud account, and select Manage > Account from the header. Be sure to select the correct account. Select Account settings and locate the Subscription and feature codes section on the page. Click Apply code. Copy and paste the code from the email into the Apply a code field and click Apply.  
Your subscription account is active and you can upgrade your watsonx.ai services."
2383	Setting up the IBM Cloud account	"Step 3: (Optional) Configure restrictions for the account
Complete these optional tasks to secure your account:  
Restrict the scope of resources that are available in IBM watsonx to the current account. See Set the scope of resources. Restrict access to specific IP addresses to protect the IBM Cloud account from unwanted access from unknown IP addresses. See Allow specific IP addresses."
2384	Setting up the IBM Cloud account	"Next steps
Add users to the account Add more security constraints  
Parent topic: Setting up the platform for administrators"
2385	Switching between the IBM watsonx as a Service and Cloud Pak for Data as a Service platforms	"If you are a Cloud Pak for Data as a Service user, you have access to IBM watsonx as a Service and you can switch between the two platforms.  
Important:  
Foundation model inferencing and the Prompt Lab tool to work with foundation models are available only in the Dallas and Frankfurt regions. Your Watson Studio and Watson Machine Learning service instances are shared between watsonx and Cloud Pak for Data as a Service. If your Watson Studio and Watson Machine Learning service instances are provisioned in another region, you can't use foundation model inferencing or the Prompt Lab.  
If you signed up for watsonx only, you can't switch to Cloud Pak for Data as a Service and you don't have a Switch platform option. To switch to Cloud Pak for Data as a Service, you must sign up for it.  
To switch between platforms:  
Log in to either IBM watsonx as a Service or Cloud Pak for Data as a Service. Your region must be Dallas. On the platform home page, click the Switch platform icon () next to your avatar, and select the platform."
2386	Switching between the IBM watsonx as a Service and Cloud Pak for Data as a Service platforms	"Service instances and resource consumption
When you switch platforms, you continue using the same Watson Studio and Watson Machine Learning service instances.  
The resources that you consume for each of these service instances is cumulative. For example, suppose you use 3 CUH for Watson Studio on Cloud Pak for Data as a Service in the first half of July. Then, you switch to watsonx and use 3 CUH for Watson Studio in the second half of July. Your total CUH for the Watson Studio service for July is 6 CUH."
2387	Switching between the IBM watsonx as a Service and Cloud Pak for Data as a Service platforms	"Switch projects and deployment spaces between platforms
You can switch a project or a deployment space from one platform to the other if that project or space meets the requirements and restrictions. See Switching the platform for a project and Switching the platform for a deployment space."
2388	Switching between the IBM watsonx as a Service and Cloud Pak for Data as a Service platforms	"Platform assets catalog
You share a single Platform assets catalog between the two platforms and any previously or newly added connection assets in your Platform assets catalog are available on both platforms. However, if you add other types of assets to the Platform assets catalog on Cloud Pak for Data as a Service, you can't access those types of assets on watsonx."
2389	Switching between the IBM watsonx as a Service and Cloud Pak for Data as a Service platforms	"Notifications
Your notifications are specific to each platform."
2390	Switching between the IBM watsonx as a Service and Cloud Pak for Data as a Service platforms	"Learn more
Switching the platform for a project Switching the platform for a deployment space Comparison of IBM watsonx as a Service and Cloud Pak for Data as a Service  
Parent topic: Getting started with IBM watsonx"
2391	Asset types and properties	"You create content, in the form of assets, when you work with tools in collaborative workspaces. An asset is an item that contains information about a data set, a model, or another item that works with data.  
You add assets by importing them or creating them with tools. You work with assets in collaborative workspaces. The workspace that you use depends on your tasks.  
Projects Where you collaborate with others to work with data and create assets. Most tools are in projects and you run assets that contain code in projects. For example, you can import data, prepare data, analyze data, or create models in projects. See Projects.  
Deployment spaces Where you deploy and run assets that are ready for testing or production. You move assets from projects into deployment spaces and then create deployments from those assets. You monitor and update deployments as necessary. See Deployment spaces.  
You can create many different types of assets, but all assets have some common properties:  
Asset types Common properties for assets Data asset types and their properties"
2392	Asset types and properties	"Asset types
To create most types of assets, you must use a specific tool.  
The following table lists the types of assets that you can create, the tools you need to create them, and the workspaces where you can add them."
2393	Asset types and properties	"Common properties for assets
Assets accumulate information in properties when you create them, use them, or when they are updated by automated processes. Some properties are provided by users and can be edited by users. Other properties are automatically provided by the system. Most system-provided properties can't be edited by users."
2394	Asset types and properties	"Common properties for assets
Common properties for assets everywhere
Most types of assets have the properties that are listed in the following table in all the workspaces where those asset types exist."
2395	Asset types and properties	"Common properties for assets
Common properties for assets that run in tools
Some assets are associated with running a tool. For example, an AutoAI experiment asset runs in the AutoAI tool. Assets that run in tools are also known as operational assets. Every time that you run assets in tools, you start a job. You can monitor and schedule jobs. Jobs use compute resources. Compute resources are measured in capacity unit hours (CUH) and are tracked. Depending on your service plans, you can have a limited amount of CUH per month, or pay for the CUH that you use every month.  
For many assets that run in tools, you have a choice of the compute environment configuration to use. Typically, larger and faster environment configurations consume compute resources faster.  
In addition to basic properties, most assets that run in tools contain the following types of information in projects:"
2396	Asset types and properties	"Data asset types and their properties
Data asset types contain metadata and other information about data, including how to access the data.  
How you create a data asset depends on where your data is:  
If your data is in a file, you upload the file from your local system to a project or deployment space. If your data is in a remote data source, you first create a connection asset that defines the connection to that data source. Then, you create a data asset by selecting the connection, the path or other structure, and the table or file that contains the data. This type of data asset is called a connected data asset.  
The following graphic illustrates how data assets from files point to uploaded files in Cloud Object Storage. Connected data assets require a connection asset and point to data in a remote data source.  
You can create the following types of data assets in a project or deployment space:  
Data asset from a file Represents a file that you uploaded from your local system. The file is stored in the object storage container on the IBM Cloud Object Storage instance that is associated with the workspace. The contents of the file can include structured data, unstructured textual data, images, and other types of data. You can create a data asset with a file of any format. However, you can do more actions on CSV files than other file types. See Properties of data assets.  
You can create a data asset from a file by uploading a file in a workspace. You can also create data files with tools and convert them to assets. For example, you can create data assets from files with the Data Refinery, Jupyter notebook, and RStudio tools.  
Connected data asset Represents a table, file, or folder that is accessed through a connection to a remote data source. The connection is defined in the connection asset that is associated with the connected data asset. You can create a connected data asset for every supported connection. When you access a connected data asset, the data is dynamically retrieved from the data source. See Properties of data assets.  
You can import connected data assets from a data source with the connected data tool in a workspace."
2397	Asset types and properties	"Data asset types and their properties
You can import connected data assets from a data source with the connected data tool in a workspace.  
Folder asset Represents a folder in IBM Cloud Object Storage. A folder data asset is special case of a connected data asset. You create a folder data asset by specifying the path to the folder and the IBM Cloud Object Storage connection asset. You can view the files and subfolders that share the path with the folder data asset. The files that you can view within the folder data asset are not themselves data assets. For example, you can create a folder data asset for a path that contains news feeds that are continuously updated. See Properties of data assets.  
You can import folder assets from IBM Cloud Object Storage with the connected data tool in a workspace.  
Connection asset Contains the information necessary to create a connection to a data source. See Properties of connection assets.  
You can create connections with the connection tool in a workspace.  
Learn more about creating and importing data assets:  
Adding data to a project Adding data assets to a deployment space"
2398	Asset types and properties	"Data asset types and their properties
Properties of data assets from files and connected data assets
In addition to basic properties, data assets from files and connected data assets have the properties or pages that are listed in the following table."
2399	Asset types and properties	"Data asset types and their properties
Properties of connection assets
The properties of connection assets depend on the data source that you select when you create a connection. See Connection types. Connection assets for most data sources have the properties that are listed in the following table."
2400	Asset types and properties	"Learn more
Profiles Asset contents or previews Activities Visualizations Getting started Connection types  
Parent topic: Overview of IBM watsonx.ai"
2401	Security for IBM watsonx	Security mechanisms in IBM watsonx provide protection for data, applications, identity, and resources. You can configure security mechanisms on five levels for IBM Cloud security functions.
2402	Security for IBM watsonx	"Security levels in IBM watsonx
Security for IBM watsonx is configured on levels to ensure that your data, application endpoints, and identity are protected on any cloud. The security levels are:  
Network security – Network security protects the network infrastructure and the points where your database or applications interact with the cloud. For example, you can protect your network by allowing IP addresses, by connecting securely to databases and third-party clouds, and by securing endpoints. Enterprise security – Enterprises are multiple IBM Cloud accounts in a hierarchy. For example, your company might have many teams that require one or more separate accounts for development, testing, and production environments. Or, you can configure an enterprise to isolate workloads in separate accounts to meet compliance guidelines. Account security – Account security includes IAM and Access group roles, Service IDs, monitoring, and other security mechanisms that are configured on IBM Cloud for your IBM Cloud account. Data security – Data security protects the IBM Cloud Object Storage service instance, provides data encryption for at-rest and in-motion data, and other security mechanisms related to data. Collaborator security – Protect your workspaces by assigning role-based access controls to collaborators in IBM watsonx.  
IBM watsonx conforms to IBM Cloud security requirements. See IBM Cloud docs: How do I know that my data is safe?."
2403	Security for IBM watsonx	"Resiliency
IBM watsonx is disaster resistant:  
The metadata for your projects and catalogs is stored in a three-node dedicated Cloudant Enterprise cluster that spans multiple geographic locations. The files that are associated with projects and catalogs are protected by the level of resiliency that is specified by the IBM Cloud Object Storage plan."
2404	Security for IBM watsonx	"Compliance
See Keep your data secure and compliant."
2405	Security for IBM watsonx	"Learn more
watsonx terms IBM Watson Machine Learning terms IBM Watson Studio terms IBM Cloud Object Storage terms Managing security and compliance in IBM Cloud Software Product Compatibility Reports: IBM Watson Studio. Software Product Compatibility Reports: IBM Watson Machine Learning service.  
Parent topic: Administering your accounts and services"
2406	Project collaborator roles and permissions	"When you add a collaborator to a project, you specify which actions that the user can do by assigning a role.  
These roles provide these permissions for projects:  
* To add collaborators or change collaborator roles, users with the Admin role in the project must also belong to the project creator's IBM Cloud account."
2407	Project collaborator roles and permissions	"Learn more
Adding collaborators to a project  
Determine your roles  
Parent topic: Administering projects"
2408	Regional availability for services and features	"IBM watsonx is deployed on the IBM Cloud multi-zone region network. The availability of services and features can vary across regional data centers.  
View the regional availability for every service in the Services catalog."
2409	Regional availability for services and features	"Regional availability of the Watson Studio and Watson Machine Learning services
Watsonx.ai includes the Watson Studio and Watson Machine Learning services to provide foundation and machine learning model tools.  
The Watson Studio and Watson Machine Learning services are available in the following regional data centers:  
Dallas (us-south), in Texas US Frankfurt (eu-de), in Germany Tokyo (jp-tok), in Japan"
2410	Regional availability for services and features	"Regional availability of the Watson Studio and Watson Machine Learning services
Regional availability of foundation models
The following table shows the IBM Cloud data centers where you can inference each foundation model. A checkmark indicates that the model is hosted in the region.  
The following table shows the IBM Cloud data centers where you can tune each foundation model. A checkmark indicates that you can use the Tuning Studio to tune the model in the region.  
The following table shows the IBM Cloud data centers where embedding models are available. A checkmark indicates that you can use the embedding model for generating text embeddings in the region."
2411	Regional availability for services and features	"Regional availability of watsonx.governance
Watsonx.governance Lite and Essentials plans are available only in the Dallas region."
2412	Regional availability for services and features	"Regional availability of Watson OpenScale
Watson OpenScale legacy plans are available only in the Frankfurt region."
2413	Regional availability for services and features	"Regional availability of the Cloud Object Storage service
The region for the Cloud Object Storage service is Global. Cloud Object Storage buckets for workspaces are Regional buckets. For more information, see IBM Cloud docs: Cloud Object Storage endpoints and storage locations."
2414	Regional availability for services and features	"Learn more
IBM Cloud docs: IBM Cloud global data centers Services in the IBM watsonx catalog  
Parent topic: Services and integrations"
2415	Watson Machine Learning plans and compute usage	You use Watson Machine Learning resources, which are measured in capacity unit hours (CUH), when you train AutoAI models, run machine learning models, or score deployed models. You use Watson Machine Learning resources, measured in resource units (RU), when you run inferencing services with foundation models. This topic describes the various plans you can choose, what services are included, and how computing resources are calculated.
2416	Watson Machine Learning plans and compute usage	"Watson Machine Learning in Cloud Pak for Data as a Service and watsonx
Important:  
The Watson Machine Learning plan includes details for watsonx.ai. Watsonx.ai is a studio of integrated tools for working with generative AI, powered by foundation models, and machine learning models. If you are using Cloud Pak for Data as a Service, then the details for working with foundation models and metering prompt inferencing using Resource Units do not apply to your plan.  
For more information on watsonx.ai, see:  
Overview of IBM watsonx.ai Comparison of IBM watsonx and Cloud Pak for Data as a Service Signing up for IBM watsonx.ai  
If you are enabled for both watsonx and Cloud Pak for Data as a Service, you can switch between the two platforms."
2417	Watson Machine Learning plans and compute usage	"Choosing a Watson Machine Learning plan
View a comparison of plans and consider the details to choose a plan that fits your needs.  
Watson Machine Learning plans Capacity Unit Hours (CUH), tokens, and Resource Units (RU) Watson Machine Learning plan details Capacity Unit Hours metering Monitoring CUH and RU usage"
2418	Watson Machine Learning plans and compute usage	"Choosing a Watson Machine Learning plan
Watson Machine Learning plans
Watson Machine Learning plans govern how you are billed for models you train and deploy with Watson Machine Learning and for prompts you use with foundation models. Choose a plan based on your needs:  
Lite is a free plan with limited capacity. Choose this plan if you are evaluating Watson Machine Learning and want to try out the capabilities. The Lite plan does not support running a foundation model tuning experiment on watsonx. Essentials is a pay-as-you-go plan that gives you the flexibility to build, deploy, and manage models to match your needs. Standard is a high-capacity enterprise plan that is designed to support all of an organization's machine learning needs. Capacity unit hours are provided at a flat rate, while resource unit consumption is pay-as-you-go.  
For plan details and pricing, see IBM Cloud Machine Learning."
2419	Watson Machine Learning plans and compute usage	"Choosing a Watson Machine Learning plan
Capacity Unit Hours (CUH), tokens, and Resource Units (RU)
For metering and billing purposes, machine learning models and deployments or foundation models are measured with these units:  
Capacity Unit Hours (CUH) measure compute resource consumption per unit hour for usage and billing purposes. CUH measures all Watson Machine Learning activity except for Foundation Model inferencing.  
Resource Units (RU) measure foundation model inferencing consumption. Inferencing is the process of calling the foundation model to generate output in response to a prompt. Each RU equals 1,000 tokens. A token is a basic unit of text (typically 4 characters or 0.75 words) used in the input or output for a foundation model prompt. Choose a plan that corresponds to your usage requirements. For details on tokens, see Tokens and tokenization.  
A rate limit monitors and restricts the number of inferencing requests per second processed for foundation models for a given Watson Machine Learning plan instance. The rate limit is higher for paid plans than for the free Lite plan."
2420	Watson Machine Learning plans and compute usage	"Watson Machine Learning plan details
The Lite plan provides enough free resources for you to evaluate the capabilities of watsonx.ai. You can then choose a paid plan that matches the needs of your organization, based on plan features and capacity.  
Note: If you upgrade from Essentials to Standard, you cannot revert to an Essentials plan. You must create a new plan.  
For all plans:  
Foundational Model inferencing Resource Units (RU) can be used for Prompt Lab inferencing, including input and output. That is, the prompt you enter for input is counted in addition to the generated output. (watsonx only) Foundation model inferencing is available only for the Dallas, Frankfurt, and Tokyo data centers. (watsonx only) Foundation model tuning in the Tuning Studio is available only for the Dallas, Frankfurt, and Tokyo data centers. (watsonx only) Three model classes determine the RU rate. The price per RU differs according to model class. (watsonx only) Capacity-unit-hour (CUH) rate consumption for training is based on training tool, hardware specification, and runtime environment. Capacity-unit-hour (CUH) rate consumption for deployment is based on deployment type, hardware specification, and software specification. Watson Machine Learning places limits on the number of deployment jobs retained for each single deployment space. If you exceed your limit, you cannot create new deployment jobs until you delete existing jobs or upgrade your plan. By default, jobs metadata will be auto-delete after 30 days. You can override this value when creating a job. See Managing jobs. Time to idle refers to the amount of time to consider a deployment active between scoring requests. If a deployment does not receive scoring requests for a given duration, it is treated as inactive, or idle, and billing stops for all frameworks other than SPSS. A plan allows for at least the stated rate limit, and the actual rate limit can be higher than the stated limit. For example, the Lite plan might process more than 2 requests per second without issuing an error. If you have a paid plan and believe you are reaching the rate limit in error, contact IBM Support for assistance.  
For plan details and pricing, see IBM Cloud Machine Learning."
2421	Watson Machine Learning plans and compute usage	"Resource unit metering (watsonx)
Resource Units billing is based on the rate of the billing class for the foundation model multipled by the number of Resource Units (RU). A Resource Unit is equal to 1000 tokens from the input and output of foundation model inferencing. The three foundation model billing classes have different RU rates. Embeddings models that vectorize text strings are billed at a different rate."
2422	Watson Machine Learning plans and compute usage	"Resource unit metering (watsonx)
Resource unit billing rates for embedding models
Embedding models transform sentences into vectors to more accurately compare and retrieve similar text."
2423	Watson Machine Learning plans and compute usage	"Resource unit metering (watsonx)
Notes on generative AI models
A prompt tuned foundation model is assigned to the same billing class as the underlying foundation model. For example, if you prompt tune a class 1 foundation model, the cost for inferencing the tuned model is measured at the class 1 billing rate. For information about tuned foundation models, see Tuning Studio. For more information about each model, see Supported foundation models. For information about regional support for each model, see Regional availability for foundation models.  
Note: You do not consume tokens when you use the generative AI search and answer app for this documentation site."
2424	Watson Machine Learning plans and compute usage	"Capacity Unit Hours metering (watsonx and Watson Machine Learning)
CUH consumption is affected by the computational hardware resources you apply for a task as well as other factors such as the software specification and model type."
2425	Watson Machine Learning plans and compute usage	"Capacity Unit Hours metering (watsonx and Watson Machine Learning)
CUH consumption by deployment and framework type
CUH consumption for deployments is calculated using these formulas:"
2426	Watson Machine Learning plans and compute usage	"Monitoring resource usage
You can track CUH or RU usage for assets you own or collaborate on in a project or space. If you are an account owner or administrator, you can track CUH or RU usage for an entire account."
2427	Watson Machine Learning plans and compute usage	"Monitoring resource usage
Tracking CUH or RU usage in a project
To monitor CUH or RU consumption in a project:  
Navigate to the Manage tab for a project.  
Click Resources to review a summary of resource consumption for assets in the project or space, or to review resource consumption details for particular assets."
2428	Watson Machine Learning plans and compute usage	"Monitoring resource usage
Tracking CUH usage for an account
You can track the runtime usage for an account on the Environment Runtimes page if you are the IBM Cloud account owner or administrator or the Watson Machine Learning service owner. For details, see Monitoring resources."
2429	Watson Machine Learning plans and compute usage	"Monitoring resource usage
Tracking CUH consumption for machine learning in a notebook
To calculate capacity unit hours in a notebook, use:  
For example:  
returns 5.49 CUH  
For details, see the Service Instances section of the IBM Watson Machine Learning API documentation."
2430	Watson Machine Learning plans and compute usage	"Learn more
Compute options for AutoAI experiments Compute options for model training and scoring  
Parent topic: Watson Machine Learning"
2431	Network security	IBM watsonx provides network security mechanisms to protect infrastructure, data, and applications from potential threats and unauthorized access. Network security mechanisms provide secure connections to data sources and control traffic across both the public internet and internal networks.
2432	Network security	"Private network service endpoints
Use private network service endpoints to securely connect to endpoints over IBM private cloud, rather than connecting to resources over the public network. With Private network service endpoints, services are no longer served on an internet routable IP address and thus are more secure. Service endpoints require virtual routing and forwarding (VRF) to be enabled on your account. VRF is automatically enabled for Virtual Private Clouds (VPCs).  
For more information about service endpoints, see:  
Securing connections to services with private service endpoints Blog: Introducing Private Service Endpoints in IBM Cloud Databases IBM Cloud docs: Secure access to services using service endpoints IBM Cloud docs: Enabling VRF and service endpoints IBM Cloud docs: Public and private network endpoints"
2433	Network security	"Access to private data sources
Private data sources are on-premises data sources that are protected by a firewall. IBM watsonx requires access through the firewall to reach the data sources. To provide secure access, you create inbound firewall rules to allow access for the IP address ranges for IBM watsonx. The inbound rules are created in the configuration tool for your firewall.  
See Configuring firewall access"
2434	Network security	"Integrations
You can configure integrations with third-party cloud platforms to allow IBM watsonx users to access data sources hosted on those clouds. The following security mechanisms apply to integrations with third-party clouds:  
An authorized account on the third-party cloud, with appropriate permissions to view account credentials Permissions to allow secure connections through the firewall of the cloud provider (for specific IP ranges)  
For example, you have a data source on AWS that you are running notebooks on. You need to integrate with AWS and then generate a connection to the database. The integration and connection are secure. After you configure firewall access, you can grant appropriate permissions to users and provide them with credentials to access data.  
See Integrations with other cloud platforms"
2435	Network security	"Connections
Connections require valid credentials to access data. The account owner or administrator configures the type of credentials that are required, either shared or personal, at the account level. Shared credentials make the data source and its credentials accessible to all collaborators in the project. Personal credentials require each collaborator to provide their own credentials to use the data source.  
Connections require valid credentials to access data. The account owner or administrator configures the type of credentials that are required at the account level. The connection creator enters a valid credential. The options are:  
Either shared or personal allows users to specify personal or shared credentials when creating a new connection by selecting a radio button and entering the correct credential. Personal credentials require each collaborator to provide their own credentials to use the data source. Shared credentials make the data source and its credentials accessible to all collaborators in the project. Users enter a common credential which was created by the creator of the connection.  
For more information about connections, see:  
Adding connections to projects Adding data from a connection to a project Adding platform connections Managing your account settings"
2436	Network security	"Connections to data behind a firewall
Secure connections provide secure communication among resources in a hybrid cloud deployment, some of which might reside behind a firewall. You have the following options for secure connections between your environment and the cloud:  
Satellite Connector Satellite location"
2437	Network security	"Connections to data behind a firewall
Satellite Connector
A Satellite Connector uses a lightweight Docker-based communication that creates secure and auditable communications from your on-prem, cloud, or Edge environment back to IBM Cloud. Your infrastructure needs only a container host, such as Docker. For more information, see Satellite Connector overview.  
See Connecting to data behind a firewall for instructions on configuring a Satellite Connector.  
Satellite Connector is the replacement for the deprecated Secure Gateway. For the Secure Gateway deprecation announcement, see IBM Cloud docs: Secure Gateway Deprecation Overview"
2438	Network security	"Connections to data behind a firewall
Satellite location
A Satellite location provides the same secure communications to IBM Cloud as a Satellite Connector but adds high availability access by default plus the ability to communicate from IBM Cloud to your on-prem location. A Satellite location requires at least three x86 hosts in your infrastructure for the HA control plane. A Satellite location is a superset of the capabilities of the Satellite Connector. If you need only client data communication, set up a Satellite Connector.  
See Connecting to data behind a firewall for instructions on configuring a Satellite location."
2439	Network security	"VPNs
Virtual Private Networks (VPNs) create virtual point-to-point connections by using tunneling protocols, and encryption and dedicated connections. They provide a secure method for sharing data across public networks.  
Following are the VPN technologies on IBM Cloud:  
IPSec VPN: The VPN facilitates connectivity from your secure network to IBM IaaS platform’s private network. Any user on the account can be given VPN access.  
VPN for VPC: With Virtual Private Cloud (VPC), you can provision generation 2 virtual server instances for VPC with high network performance.  
The Secure Gateway deprecation announcement provides information and scenarios for using VPNs as an alternative. See IBM Cloud docs: Migration options."
2440	Network security	"Allow specific IP addresses
Use this mechanism to control access to the IBM cloud console and to IBM watsonx. Access is allowed from the specified IP addresses only; access from all other IP addresses is denied. You can specify the allowed IP addresses for an individual user or for an account.  
When allowing specific IP addresses for Watson Studio, you must include the CIDR ranges for the Watson Studio nodes in each region (as well as the individual client system IPs that are allowed). You can include the CIDR ranges in IBM watsonx by following these steps:  
From the main menu, choose Administration > Cloud integrations. Click Firewall configuration to display the IP addresses for the current region. Use CIDR notation. Copy each CIDR range into the IP address restrictions for either a user or an account. Be sure to enter the allowed individual client IP addresses as well. Enter the IP addresses as a comma-separated list. Then, click Apply. Repeat for each region to allow access for Watson Studio.  
For step-by-step instructions for both user and account restrictions, see IBM Cloud docs: Allowing specific IP addresses"
2441	Network security	"Allow third party URLs on an internal network
If you are running IBM watsonx behind a firewall, you must allowlist third party URLs to provide outbound browser access. The URLs include resources from IBM Cloud and other domains. IBM watsonx requires access to these domains for outbound browser traffic through the firewall.  
This list provides access only for core IBM watsonx functions. Specific services might require additional URLs. The list does not cover URLs required by the IBM Cloud console and its outbound requests."
2442	Network security	"Multi-tenancy
IBM watsonx is hosted as a secure and compliant multi-tenant solution on IBM Cloud. See Multi-Tenant  
Parent topic: Security"
2443	Signing up for IBM watsonx as a Service	"IBM watsonx as a Service contains two components: watsonx.ai and watsonx.governance. You can sign up for a personal version of both watsonx.ai and watsonx.governance at no initial cost, or sign up through an email invitation to join your organization's account. Watsonx.ai provides all the tools that you need to work with foundation models and machine learning models. Watsonx.governance provides the tools that you need to govern models.  
Note:  
The watsonx.governance service is available only in the Dallas region.  
Signing up for a personal account Signing up for your organization's account Switching to your organization's account Logging in using IBM App ID (beta)"
2444	Signing up for IBM watsonx as a Service	"Signing up for a personal account
When you sign up for watsonx.ai and watsonx.governance, you need an IBMid for an IBM Cloud account. If you don't already have an IBMid, you can create one during signup. For your IBM Cloud account, you enter your email address, personal information, and credit card information, which is used to verify your identity. You are charged only if you upgrade to a billable plan and then consume billable services. Lite plans do not incur charges.  
The free version of watsonx.ai contains Lite plans for the IBM Watson Studio and Watson Machine Learning services that provide the tools for working with foundation models and machine learning models. The free version of watsonx.governance contains the watsonx.governance service that provides the tools for governing models. The Cloud Object Storage service is also included to provide storage.  
To sign up for watsonx:  
Go to Try IBM watsonx.ai. Select the IBM Cloud service region. Enter your IBM Cloud account username and password. If you don't have an IBM Cloud account, create one. If you see the Select account screen, select the account and resource group where you want to use watsonx. If you belong to an account with existing services, you can select it instead of your account. The Select account screen does not display if you have only one account and resource group. Click Continue. The account activation process begins.  
Note:  
Stay with your default browser during the activation process. If you land on the IBM Cloud Dashboard, return to the Try IBM watsonx.ai page and follow the link to log in with an existing account.  
After the activation process completes, your watsonx home page is shown.  
Bookmark your home page so that you can go directly to the watsonx site for your region to log in with your personal credentials.  
If you're in your own account, you have the necessary permissions for complete access to projects and deployment spaces. You can access another account by switching to that account.  
To set up an account for your organization, so that other users can share services and resources, see Set up an account for your organization."
2445	Signing up for IBM watsonx as a Service	"Signing up for your organization's account
Before you can access your organization's watsonx account, you must be a member of your organization's IBM Cloud account. Account administrators can invite users to join their organization's IBM Cloud account.  
The administrator provides the following information:  
The IBM Cloud account name for watsonx. The resource group name for the watsonx account. The IBM Cloud service region.  
When the account administrator invites you, you receive an email from IBM Cloud with the title ""You are invited to join an account in IBM Cloud."" with the name of the account.  
To join your organization's account:  
Click the Join now link. The expiry is 30 days. You are asked to log in with your IBMid. IBMids are assigned to IBM Cloud account members. If you don't have an IBMid, one is created for you when you join. Continue to the next screen and confirm that your information is correct, then accept the invite. Login in from the Welcome screen. You are now logged in to the IBM Cloud account. Go to Try IBM watsonx.ai. Follow the prompts to sign up with your IBMid. On the Select account screen, select your organization's account and resource group. Click Continue.  
You can see the name of the account you are currently working in on the menu bar."
2446	Signing up for IBM watsonx as a Service	"Switching to your organization's account
You can switch to your organization's existing IBM Cloud account (or any other account for which you are a member) to share watsonx resources that are provisioned for that account.  
If you are not already an account member, the account administrator must invite you to the IBM Cloud account. You receive an email invitation to join the account. After you accept the invitation, you can access the account and watsonx.  
To switch to your organization's account:  
Log in to watsonx with your personal credentials. Select your organization's account name from the account list on the page header. If you don't see the account list, click the Account Switcher to open it.  
To switch regions:  
Select the region from the region list on the page header. If you don't see the region list, click the Region Switcher to open it."
2447	Signing up for IBM watsonx as a Service	"Logging in to watsonx through IBM Cloud App ID (beta)
IBM Cloud App ID integrates user authentication on IBM Cloud with user registries that are hosted on other identity providers. If App ID is configured for your IBM Cloud account, your administrator provides an alias to log in to watsonx. With App ID, you do not need to sign in to IBM Cloud. Instead, you log in to watsonx with the App ID alias.  
You cannot switch accounts when you log in through App ID.  
To log in with App ID:  
Go to watsonx and choose to log in with App ID (Beta). Enter the alias that was provided to you by your administrator. You are redirected to your company's login page. Enter your company credentials on your company's login page. You are redirected back to watsonx.  
Select the Remember App ID checkbox to save the App ID alias for future logins."
2448	Signing up for IBM watsonx as a Service	"Next steps
Go back to Get started and choose the right path for you. Add services from the services catalog. See Creating and managing services."
2449	Signing up for IBM watsonx as a Service	"Learn more
Get help Browser support Setting up IBM Cloud App ID (beta)  
Parent topic: Getting started"
2450	IBM Cloud account security	Account security mechanisms for IBM watsonx are provided by IBM Cloud. These security mechanisms, including SSO and role-based, group-based, and service-based access control, protect access to resources and provide user authentication.
2451	IBM Cloud account security	"IAM access roles
You can use IAM access roles to provide users access to all resources that belong to a resource group. You can also give users access to manage resource groups and create new service instances that are assigned to a resource group.  
For step-by-step instructions, see IBM Cloud docs: Assigning access to resources"
2452	IBM Cloud account security	"Access groups
After you set up and organize resource groups in your account, you can streamline access management by using access groups. Create access groups to organize a set of users and service IDs into a single entity. You can then assign a policy to all group members by assigning it to the access group. Thus you can assign a single policy to the access group instead of assigning the same policy multiple times per individual user or service ID.  
By using access groups, you can minimally manage the number of assigned policies by giving the same access to all identities in an access group.  
For more information see:  
IBM Cloud docs: Setting up access groups."
2453	IBM Cloud account security	"Resource groups
Use resource groups to organize your account's resources into logical groups that help with access control. Rather than assigning access to individual resources, you assign access to the group. Resources are any service that is managed by IAM, such as databases. Whenever you create a service instance from the Cloud catalog, you must assign it to a resource group.  
Resource groups work with access group policies to provide a way to manage access to resources by groups of users. By including a user in an access group, and assigning the access group to a resource group, you provide access to the resources contained in the group. Those resources are not available to nonmembers.  
The Lite account comes with a single resource group, named ""Default"", so all resources are placed in the Default resource group. With paid accounts, Administrators can create multiple resource groups to support your business and provide access to resources on an as-needed basis.  
For step-by-step instructions, see IBM Cloud docs: Managing resource groups  
For tips on configuring resource groups to provide secure access, see IBM Cloud docs: Best practices for organizing resources and assigning access"
2454	IBM Cloud account security	"Service IDs
You can create service IDs in IBM Cloud to enable an application outside of IBM Cloud access to your IBM Cloud services. Service IDs are not tied to a specific user. If a user leaves an organization and is deleted from the account, the service ID remains intact to ensure that your service continues to work. Access policies that are assigned to each service ID ensure that your application has the appropriate access for authenticating with your IBM Cloud services. See Project collaborators.  
One way in which Service IDs and access policies can be used is to manage access to the Cloud Object Storage buckets. See Controlling access to Cloud Object Storage buckets.  
For more information, see IBM Cloud docs: Creating and working with service IDs."
2455	IBM Cloud account security	"Service ID API keys
For extra protection, Service IDs can be combined with unique API keys. The API key that is associated with a Service ID can be set for one-time use or unlimited use. For more information, see IBM Cloud docs: Managing service IDs API keys."
2456	IBM Cloud account security	"Activity Tracker
The Activity Tracker collects and stores audit records for API calls (events) made to resources that run in the IBM Cloud. You can use Activity Tracker to monitor the activity of your IBM Cloud account to investigate abnormal activity and critical actions, and to comply with regulatory audit requirements. The events that are collected comply with the Cloud Auditing Data Federation (CADF) standard. IBM services that generate Activity Tracker events follow the IBM Cloud security policy.  
For a list of events that apply to IBM watsonx, see Activity Tracker events.  
For instructions on configuring Activity Tracker, see IBM Cloud docs: Getting started with IBM Cloud Activity Tracker."
2457	IBM Cloud account security	"Multifactor authentication
Multifactor authentication (or MFA) adds an extra layer of security by requiring multiple types of authentication methods upon login. After entering a valid username and password, users must also satisfy a second authentication method. For example, a time-sensitive passcode is sent to the user, either through text or email. The correct passcode must be entered to complete the login process.  
For more information, see IBM Cloud docs: Types of multifactor authentication."
2458	IBM Cloud account security	"Single sign-on authentication
Single sign-on (SSO) is an authentication method that enables users to log in to multiple, related applications that use one set of credentials.  
IBM watsonx supports SSO using Security Assertion Markup Language (SAML) federated IDs. SAML federation requires coordination with IBM to configure. SAML connects IBMids with the user credentials that are provided by an identity provider (IdP). For companies that have configured SAML federation with IBM, users can log in to IBM watsonx with their company credentials. SAML federation is the recommended method for SSO configuration with IBM watsonx.  
The IBMid Enterprise Federation Adoption Guide describes the steps that are required to federate your identity provider (IdP). You need an IBM Sponsor, which is an IBM employee that works as the contact person between you and the IBMid team.  
For an overview of SAML federation, see IBM Cloud SAML Federation Guide. This blog discusses both SAML federation and IBM Cloud App ID. IBM Cloud App ID is supported as a Beta version with IBM watsonx.  
Parent topic: Security"
2459	Setting up the Watson Studio and Watson Machine Learning services	"The Watson Studio and Watson Machine Learning services are provisioned automatically with a Lite plan when you sign up for IBM watsonx. To set up Watson Studio and Watson Machine Learning for an organization, you upgrade the service plans. You allow the node IP addresses access through the firewall.  
To set up the Watson Studio and Watson Machine Learning services, complete these tasks:  
Upgrade the services. Allow IP addresses."
2460	Setting up the Watson Studio and Watson Machine Learning services	"Step 1: Upgrade the services to the appropriate plans
To upgrade the services:  
Determine the Watson Studio service plan that you need. The features and compute resources of Watson Studio vary across the service plans. See Watson Studio service plans. While logged in to IBM watsonx, from the main menu, click Administration > Services > Service instances. Click the menu next to the Watson Studio service and choose Upgrade service. Choose the plan you want and click Upgrade. Repeat the steps for the Watson Machine Learning service. The resources and number of deployment jobs vary across the Watson Machine Learning service plans. See Watson Machine Learning service plans.  
Make sure that object storage is configured to allow these users to create catalogs and projects. See Setting up IBM Cloud Object Storage for use with IBM watsonx.  
All users in your IBM Cloud account with the Editor IAM platform access role for all IAM enabled services can now create projects and use all the Watson Studio and Watson Machine Learning tools."
2461	Setting up the Watson Studio and Watson Machine Learning services	"Step 2: Allow IP addresses for Watson Studio for your region
The IP addresses for the Watson Studio nodes in each region must be configured as allowed IP addresses for the IBM Cloud account. When allowing specific IP addresses for Watson Studio, you include the CIDR ranges for the Watson Studio nodes in each region to allow a secure connection through the firewall.  
First look up the CIDR blocks in IBM watsonx, and then enter them into the Access(IAM) > Settings screen in IBM Cloud. Follow these steps:  
From the IBM watsonx main menu, select Administration > Cloud integrations. Click Firewall configuration to display the IP addresses for the current region. Checkmark Show IP ranges in CIDR notation. Click the icon to copy a CIDR block to the clipboard. Enter the CIDR block of IP addresses into the Access(IAM) > Settings > Restrict IP address access > Allowed IP addresses for the IBM Cloud account. Then click Save. Repeat for each CIDR block until all are entered. Repeat for each region.  
For step-by-step instructions, see IBM Cloud docs: Allowing specific IP addresses."
2462	Setting up the Watson Studio and Watson Machine Learning services	"Next steps
Finish the remaining steps for setting up the platform.  
Parent topic: Setting up the platform for administrators"
2463	Activities for assets	For some asset types, you can see the activities of each asset in projects. The activities graph shows the history of the events that are performed on the asset for some tools. An event is an action that changes or copies the asset. For example, editing the asset description is an event, but viewing the asset is not an event.
2464	Activities for assets	"Requirements and restrictions
You can view the activities of assets under the following circumstances."
2465	Activities for assets	"Requirements and restrictions
Workspaces
You can view the asset activities in projects."
2466	Activities for assets	"Requirements and restrictions
Limitations
Activities have the following limitations:  
Activities graphs are currently available only for Watson Machine Learning models and data assets. Activities graphs do not appear in Microsoft Internet Explorer 11 browsers."
2467	Activities for assets	"Activities events
To view activities for an asset in a project, click the asset name and click . The activities panel shows a timeline of events. Summary information about the asset shows where asset was created, what the last event for it was, and when the last event happened. The first event for each asset is its creation.  
Activities events can describe actions that are applicable to all asset types or actions that are specific to an asset type:  
General events Events specific to Watson Machine Learning models Events specific to data assets from files and connected data assets  
You can see this type of information about each event:  
Where: In which catalog or project the event occurred. Who: The name of the user who performed the action, unless the action was automated. Automated actions generate events, but don't show usernames. What: A description of the action. Some events show details about the original and updated values. When: The date and time of the event.  
Activities also track relationships between assets. In the activities panel, the creation of a new asset based on the original asset is shown at the top of the list. Click See details to view asset details."
2468	Activities for assets	"Activities events
General events
You can see these general events:  
Name updated Description updated Tags updated"
2469	Activities for assets	"Activities events
Events specific to Watson Machine Learning models
Activities tracking is available for all Watson Machine Learning service plans, however, you wouldn't see events for actions that are not available with your plan.  
In addition to general events, you can see these events that are specific to models:  
Model created Model deployed Model re-evaluated Model retrained Set as active model  
A model asset shows this information in the Created from field, depending on how it was created:  
The name of the associated data asset The name of the associated connection asset The project name where it was created"
2470	Activities for assets	"Activities events
Events specific to data assets from files and connected data assets
In addition to general events, you can see these events that are specific to data assets from files and connected data assets:  
Added to project from a Data Refinery flow Added to a project from a file Data classes updated Schema updated by a Data Refinery flow Profile created Profile updated Profile deleted Downloaded  
A data asset shows this information in the Created from field, depending on how it was created:  
The name of the Data Refinery flow that created it Its associated connection name The project name where it was created or came from  
Parent topic: Finding and viewing an asset in a catalog"
2471	Visualizations of assets	In your project, you can create visualizations of data assets to further explore and discover insights. To create and view visualizations, open a data asset and go to the Visualization tab.
2472	Visualizations of assets	"Requirements and restrictions
You can view the visualization of assets under the following circumstances."
2473	Visualizations of assets	"Requirements and restrictions
Required permissions
To view this page, you can have any role in a project.  
To edit or update information on this page, you must have the Editor or Admin role in the project."
2474	Visualizations of assets	"Requirements and restrictions
Workspaces
You can view the asset visualization in projects."
2475	Visualizations of assets	"Requirements and restrictions
Types of assets
You can create a visualization for these types of assets:  
Data asset from file: Avro, CSV, JSON, Parquet, TSV, SAV, Microsoft Excel .xls and .xlsx files, SAS, delimited text files Connected data assets"
2476	Visualizations of assets	"Learn more
Visualizing your data  
Parent topic: Asset types and properties"
2477	Asset contents or previews	"In projects and other workspaces, you can see a preview of data assets that contain relational data.  
Requirements and restrictions Previews of data assets"
2478	Asset contents or previews	"Requirements and restrictions
You can view the contents or previews of assets under the following conditions and restrictions."
2479	Asset contents or previews	"Requirements and restrictions
Workspaces
You can view the preview or contents of assets in these workspaces:  
Projects"
2480	Asset contents or previews	"Requirements and restrictions
Types of assets
The following types of assets have previews:  
Data assets from files Connected data assets Models Notebooks"
2481	Asset contents or previews	"Requirements and restrictions
Required permissions
You can see the asset contents or preview with any collaborator role in the workspace."
2482	Asset contents or previews	"Requirements and restrictions
Restrictions for data assets
Additional requirements apply to connected data assets and data assets from files. See Requirements for data assets.  
Previews are not available for data assets that were added as managed assets by using the Watson Data API."
2483	Asset contents or previews	"Previews of data assets
The previews of data assets show a view of the data.  
You can see when the data in the preview was last fetched and refresh the preview data by clicking the refresh icon.  
Requirements for data assets Preview information for data assets File extensions and mime types of previewed files"
2484	Asset contents or previews	"Previews of data assets
Requirements for data assets
The additional requirements for viewing previews of data assets depend on whether the data is accessed through a connection or from a file.  
You can see previews of data assets that are accessed through a connection if all these conditions are true:  
You have access to the data asset and its associated connection. See Requirements and restrictions. The data asset contains structured data. Structured data resides in fixed fields within a record or file, for example, relational database data or spreadsheets. You have credentials for the connection:  
For connections with shared credentials, the username in the connection details has access to the object at the data source. For connections with personal credentials, you must enter your personal credentials when you see a key icon . This is a one-time step that permanently unlocks the connection for you. See Adding connections to projects.  
You can see previews of data assets from files if the following conditions are true:  
You have access to the data asset. See Requirements and restrictions.  
The file is stored in IBM Cloud Object Storage. For preview of text or image files from an IBM Cloud Object Storage connection to work, the connection credentials must include an access key and a secret key. If you’re using an existing Cloud Object Storage connection that doesn’t have these keys, edit the connection asset and add them. See IBM Cloud Object Storage connection.  
The file type is supported. See File extensions and mime types of previewed files.  
The content has UTF-8 encoding."
2485	Asset contents or previews	"Previews of data assets
Preview information for data assets
For structured data, the preview displays a limited number of rows and columns:  
The number of rows in the preview is limited to 1,000. The amount of data is limited to 800 KB. The more columns the data asset has, the fewer rows that appear in the preview.  
Previews show different information for different types of data assets and files.  
For structured data, the preview shows column names, a subset of columns and rows of data. The supported formats for structured data include relational data, CSV, TSV, Avro, Microsoft Excel documents, and partitioned Parquet data (for IBM Cloud Object Storage and S3 connections). Microsoft Excel documents show the first sheet.  
Assets from file based connections like Apache Kafka and Apache Cassandra are not supported.  
Unstructured data files must be stored in IBM Cloud Object Storage to have previews.  
For these unstructured data files, the preview shows the whole document: Text, JSON, HTML, PDF, and images. HTML files are supported in text format. Images stored in IBM Cloud Object Storage support JPG, JPEG, PNG, GIF, BMP, and BMP1.  
For connected folder assets, the preview shows the files and subfolders, which you can also preview."
2486	Asset contents or previews	"Previews of data assets
File extensions and mime types of previewed files
These types of files that contain structured data have previews:"
2487	Asset contents or previews	"Learn more
Profile Activities Visualizations  
Parent topic: Asset types and properties"
2488	Quick start: Analyze data in a Jupyter notebook	"You can create a notebook in which you run code to prepare, visualize, and analyze data, or build and train a model. Read about Jupyter notebooks, then watch a video and take a tutorial that’s suitable for users with some knowledge of Python code.  
Your basic workflow includes these tasks:  
Open your sandbox project. Projects are where you can collaborate with others to work with data. Add your data to the project. You can add CSV files or data from a remote data source through a connection. Create a notebook in the project. Add code to the notebook to load and analyze your data. Run your notebook and share the results with your colleagues."
2489	Quick start: Analyze data in a Jupyter notebook	"Read about notebooks
A Jupyter notebook is a web-based environment for interactive computing. You can run small pieces of code that process your data, and you can immediately view the results of your computation. Notebooks include all of the building blocks you need to work with data:  
The data The code computations that process the data Visualizations of the results Text and rich media to enhance understanding  
Read more about notebooks"
2490	Quick start: Analyze data in a Jupyter notebook	"Watch a video about notebooks
Watch this video to learn the basics of Jupyter notebooks.  
This video provides a visual method to learn the concepts and tasks in this documentation."
2491	Quick start: Analyze data in a Jupyter notebook	"Try a tutorial to create a notebook
In this tutorial, you will complete these tasks:  
Task 1: Open a project. Task 2: Add a notebook to your project. Task 3: Load a file and save the notebook. Task 4: Find and edit the notebook. Task 5: Share read-only version of the notebook. Task 6: Schedule a notebook to run at a different time.  
This tutorial will take approximately 15 minutes to complete.  
Expand all sections  
Here are some tips for successfully completing this tutorial."
2492	Quick start: Analyze data in a Jupyter notebook	"Try a tutorial to create a notebook
Use the video picture-in-picture
Tip: Start the video, then as you scroll through the tutorial, the video moves to picture-in-picture mode. Close the video table of contents for the best experience with picture-in-picture. You can use picture-in-picture mode so you can follow the video as you complete the tasks in this tutorial. Click the timestamps for each task to follow along.  
The following animated image shows how to use the video picture-in-picture and table of contents features:"
2493	Quick start: Analyze data in a Jupyter notebook	"Try a tutorial to create a notebook
Get help in the community
If you need help with this tutorial, you can ask a question or find an answer in the watsonx Community discussion forum."
2494	Quick start: Analyze data in a Jupyter notebook	"Try a tutorial to create a notebook
Set up your browser windows
For the optimal experience completing this tutorial, open Cloud Pak for Data in one browser window, and keep this tutorial page open in another browser window to switch easily between the two applications. Consider arranging the two browser windows side-by-side to make it easier to follow along.  
Tip: If you encounter a guided tour while completing this tutorial in the user interface, click Maybe later."
2495	Quick start: Analyze data in a Jupyter notebook	"Try a tutorial to create a notebook
You need a project to store the notebook and data asset. You can use your sandbox project or create a project. Follow these steps to open a project and add a data asset to the project:  
From the navigation menu , choose Projects > View all projects  
Open your sandbox project. If you want to use a new project:  
Click New project.  
Select Create an empty project.  
Enter a name and optional description for the project.  
Choose an existing object storage service instance or create a new one.  
Click Create.  
From the navigation menu, click Resource hub.  
Search for an interesting data set, and select the data set.  
Click Add to project.  
Select the project from the list, and click Add.  
After the data set is added, click View Project.  
In the project, click the Assets tab to see the data set.  
For more information, see Creating a project."
2496	Quick start: Analyze data in a Jupyter notebook	"Try a tutorial to create a notebook
Check your progress
The following image shows the Assets tab in the project."
2497	Quick start: Analyze data in a Jupyter notebook	"Try a tutorial to create a notebook
To preview this task, watch the video beginning at 00:06.  
Follow these steps to create a new notebook in your project.  
In your project, on the Assets tab, click New asset > Work with data and models in Python or R notebooks.  
Type a name and description (optional).  
Select a runtime environment for this notebook.  
Click Create. Wait for the notebook editor to load."
2498	Quick start: Analyze data in a Jupyter notebook	"Try a tutorial to create a notebook
Check your progress
The following image shows blank notebook."
2499	Quick start: Analyze data in a Jupyter notebook	"Try a tutorial to create a notebook
To preview this task, watch the video beginning at 00:23.  
Now you can access the data asset in your notebook that you uploaded to your project earlier. Follow these steps to load data into a data frame:  
Click in an empty code cell in your notebook.  
Click the Code snippets icon ().  
In the side pane, click Read data.  
Click Select data from project.  
Locate the data asset from the project, and click Select.  
In the Load as drop-down list, select the load option that you prefer.  
Click Insert code to cell. The code to read and load the data asset is inserted into the cell.  
Click Run to run your code. The first few rows of your data set will display.  
To save a version of your notebook, click File > Save Version. You can also just save your notebook with File > Save."
2500	Quick start: Analyze data in a Jupyter notebook	"Try a tutorial to create a notebook
Check your progress
The following image shows the notebook with the pandas DataFrame."
2501	Quick start: Analyze data in a Jupyter notebook	"Try a tutorial to create a notebook
To preview this task, watch the video beginning at 01:19.  
Follow these steps to locate the saved notebook on the Assets tab, and edit the notebook:  
In the project navigation trail, click your project name to return to your project.  
Click the Assets tab to find the notebook.  
When you click the notebook, it will open in READ ONLY mode.  
To edit the notebook, click the pencil icon .  
Click the Information icon to open the Information panel.  
On the General tab, edit the name and description of the notebook.  
Click the Environment tab to see how you can change the environment used to run the notebook or update the runtime status to either stop and restart."
2502	Quick start: Analyze data in a Jupyter notebook	"Try a tutorial to create a notebook
Check your progress
The following image shows the notebook with the Information panel displayed."
2503	Quick start: Analyze data in a Jupyter notebook	"Try a tutorial to create a notebook
To preview this task, watch the video beginning at 01:52.  
Follow these steps to create a link to the notebook to share with colleagues:  
Click the Share icon if you would like to share the read-only view of the notebook.  
Click to turn on the Share with anyone who has the link toggle button.  
Select what content you would like to share through a link or social media.  
Click the Copy icon to copy a direct link to this notebook.  
Click Close."
2504	Quick start: Analyze data in a Jupyter notebook	"Try a tutorial to create a notebook
Check your progress
The following image shows the Share dialog box."
2505	Quick start: Analyze data in a Jupyter notebook	"Try a tutorial to create a notebook
To preview this task, watch the video beginning at 02:08.  
Follow these steps to create a job to schedule the notebook to run at a specific time or repeat based on a schedule:  
Click the Jobs icon, and select Create a job.  
Provide the name and description of the job, and click Next.  
Select the notebook version and environment runtime, and click Next.  
(Optional) Click the toggle button to schedule a run. Specify the date, time and if you would like the job to repeat, and click Next.  
(Optional) click the toggle button to receive notifications for this job, and click Next.  
Review the details, and click either Create (to create the job, but not run the job immediately) or Create and run (to run the job immediately).  
The job will display in the Jobs tab in the project."
2506	Quick start: Analyze data in a Jupyter notebook	"Try a tutorial to create a notebook
Check your progress
The following image shows the Jobs tab."
2507	Quick start: Analyze data in a Jupyter notebook	"Next steps
Now you can use this data set for further analysis. For example, you or other users can do any of these tasks:  
Cleansing and shaping data  
Build and train a model with the data asset"
2508	Quick start: Analyze data in a Jupyter notebook	"Additional resources
View more videos.  
Find sample data sets, projects, models, prompts, and notebooks in the Resource hub to gain hands-on experience:  
Notebooks that you can add to your project to get started analyzing data and building models.  
Projects that you can import containing notebooks, data sets, prompts, and other assets.  
Data sets that you can add to your project to refine, analyze, and build models.  
Prompts that you can use in the Prompt Lab to prompt a foundation model.  
Foundation models that you can use in the Prompt Lab.  
Parent topic: Quick start tutorials"
2509	Known issues and limitations	"The following limitations and known issues apply to watsonx.  
Regional limitations Notebooks Machine learning SPSS Modeler Connections Watson Pipelines watsonx.governance"
2510	Known issues and limitations	"Notebook issues
You might encounter some of these issues when getting started with and using notebooks."
2511	Known issues and limitations	"Notebook issues
Failure to export a notebook to HTML in the Jupyter Notebook editor
When you are working with a Jupyter Notebook created in a tool other than Watson Studio, you might not be able to export the notebook to HTML. This issue occurs when the cell output is exposed.  
Workaround  
""widgets"": { ""state"": {}, ""version"": ""1.1.2"" }  
In the Jupyter Notebook UI, go to Edit and click Edit Notebook Metadata.  
Remove the following metadata:  
Click Edit.  
Save the notebook."
2512	Known issues and limitations	"Notebook issues
Manual installation of some tensor libraries is not supported
Some tensor flow libraries are preinstalled, but if you try to install additional tensor flow libraries yourself, you get an error."
2513	Known issues and limitations	"Notebook issues
Connection to notebook kernel is taking longer than expected after running a code cell
If you try to reconnect to the kernel and immediately run a code cell (or if the kernel reconnection happened during code execution), the notebook doesn't reconnect to the kernel and no output is displayed for the code cell. You need to manually reconnect to the kernel by clicking Kernel > Reconnect. When the kernel is ready, you can try running the code cell again."
2514	Known issues and limitations	"Notebook issues
Using the predefined sqlContext object in multiple notebooks causes an error
You might receive an Apache Spark error if you use the predefined sqlContext object in multiple notebooks. Create a new sqlContext object for each notebook. See this Stack Overflow explanation."
2515	Known issues and limitations	"Notebook issues
Connection failed message
If your kernel stops, your notebook is no longer automatically saved. To save it, click File > Save manually, and you should get a Notebook saved message in the kernel information area, which appears before the Spark version. If you get a message that the kernel failed, to reconnect your notebook to the kernel click Kernel > Reconnect. If nothing you do restarts the kernel and you can't save the notebook, you can download it to save your changes by clicking File > Download as > Notebook (.ipynb). Then you need to create a new notebook based on your downloaded notebook file."
2516	Known issues and limitations	"Notebook issues
Hyperlinks to notebook sections don't work in preview mode
If your notebook contains sections that you link to from an introductory section at the top of the notebook for example, the links to these sections will not work if the notebook was opened in view-only mode in Firefox. However, if you open the notebook in edit mode, these links will work."
2517	Known issues and limitations	"Notebook issues
Can't connect to notebook kernel
If you try to run a notebook and you see the message Connecting to Kernel, followed by Connection failed. Reconnecting and finally by a connection failed error message, the reason might be that your firewall is blocking the notebook from running.  
If Watson Studio is installed behind a firewall, you must add the WebSocket connection wss://dataplatform.cloud.ibm.com to the firewall settings. Enabling this WebSocket connection is required when you're using notebooks and RStudio."
2518	Known issues and limitations	"Notebook issues
Insufficient resources available error when opening or editing a notebook
If you see the following message when opening or editing a notebook, the environment runtime associated with your notebook has resource issues:  
To find the cause, try checking the status page for IBM Cloud incidents affecting Watson Studio. Additionally, you can open a support case at the IBM Cloud Support portal."
2519	Known issues and limitations	"Machine learning issues
You might encounter some of these issues when working with machine learning tools."
2520	Known issues and limitations	"Machine learning issues
Region requirements
You can only associate a Watson Machine Learning service instance with your project when the Watson Machine Learning service instance and the Watson Studio instance are located in the same region."
2521	Known issues and limitations	"Machine learning issues
Accessing links if you create a service instance while associating a service with a project
While you are associating a Watson Machine Learning service to a project, you have the option of creating a new service instance. If you choose to create a new service, the links on the service page might not work. To access the service terms, APIs, and documentation, right click the links to open them in new windows."
2522	Known issues and limitations	"Machine learning issues
Federated Learning assets cannot be searched in All assets, search results, or filter results in the new projects UI
You cannot search Federated Learning assets from the All assets view, the search results, or the filter results of your project.  
Workaround: Click the Federated Learning asset to open the tool."
2523	Known issues and limitations	"Machine learning issues
Deployment issues
A deployment that is inactive (no scores) for a set time (24 hours for the free plan or 120 hours for a paid plan) is automatically hibernated. When a new scoring request is submitted, the deployment is reactivated and the score request is served. Expect a brief delay of 1 to 60 seconds for the first score request after activation, depending on the model framework. For some frameworks, such as SPSS modeler, the first score request for a deployed model after hibernation might result in a 504 error. If this happens, submit the request again; subsequent requests should succeed."
2524	Known issues and limitations	"Watson Machine Learning limitations
AutoAI known limitations
Currently, AutoAI experiments do not support double-byte character sets. AutoAI only supports CSV files with ASCII characters. Users must convert any non-ASCII characters in the file name or content, and provide input data as a CSV as defined in this CSV standard.  
To interact programmatically with an AutoAI model, use the REST API instead of the Python client. The APIs for the Python client required to support AutoAI are not generally available at this time."
2525	Known issues and limitations	"Watson Machine Learning limitations
Data module not found in IBM Federated Learning
The data handler for IBM Federated Learning is trying to extract a data module from the FL library but is unable to find it. You might see the following error message:  
The issue possibly results from using an outdated DataHandler. Please review and update your DataHandler to conform to the latest spec. Here is the link to the most recent MNIST data handler or ensure your sample versions are up-to-date."
2526	Known issues and limitations	"SPSS Modeler issues
You might encounter some of these issues when working in SPSS Modeler."
2527	Known issues and limitations	"SPSS Modeler issues
SPSS Modeler runtime restrictions
Watson Studio does not include SPSS functionality in Peru, Ecuador, Colombia and Venezuela."
2528	Known issues and limitations	"SPSS Modeler issues
Merge node and unicode characters
The Merge node treats the following very similar Japanese characters as the same character."
2529	Known issues and limitations	"Connection issues
You might encounter this issue when working with connections."
2530	Known issues and limitations	"Connection issues
Cloudera Impala connection does not work with LDAP authentication
If you create a connection to a Cloudera Impala data source and the Cloudera Impala server is set up for LDAP authentication, the username and password authentication method in IBM watsonx will not work.  
Workaround: Disable the Enable LDAP Authentication option on the Impala server. See Configuring LDAP Authentication in the Cloudera documentation."
2531	Known issues and limitations	"Watson Pipelines known issues
The issues pertain to Watson Pipelines."
2532	Known issues and limitations	"Watson Pipelines known issues
Nesting loops more than 2 levels can result in pipeline error
Nesting loops more than 2 levels can result in an error when you run the pipeline, such as Error retrieving the run. Reviewing the logs can show an error such as text in text not resolved: neither pipeline_input nor node_output. If you are looping with output from a Bash script, the log might list an error like this: PipelineLoop can't be run; it has an invalid spec: non-existent variable in $(params.run-bash-script-standard-output). To resolve the problem, do not nest loops more than 2 levels."
2533	Known issues and limitations	"Watson Pipelines known issues
Asset browser does not always reflect count for total numbers of asset type
When selecting an asset from the asset browser, such as choosing a source for a Copy node, you see that some of the assets list the total number of that asset type available, but notebooks do not. That is a current limitation."
2534	Known issues and limitations	"Watson Pipelines known issues
Cannot delete pipeline versions
Currently, you cannot delete saved versions of pipelines that you no longer need."
2535	Known issues and limitations	"Watson Pipelines known issues
Deleting an AutoAI experiment fails under some conditions
Using a Delete AutoAI experiment node to delete an AutoAI experiment that was created from the Projects UI does not delete the AutoAI asset. However, the rest of the flow can complete successfully."
2536	Known issues and limitations	"Watson Pipelines known issues
Cache appears enabled but is not enabled
If the Copy assets Pipelines node's Copy mode is set to Overwrite, cache is displayed as enabled but remains disabled."
2537	Known issues and limitations	"Watson Pipelines limitations
These limitations apply to Watson Pipelines.  
Single pipeline limits Limitations by configuration size Input and output size limits Batch input limited to data assets"
2538	Known issues and limitations	"Watson Pipelines limitations
Single pipeline limits
These limitation apply to a single pipeline, regardless of configuration.  
Any single pipeline cannot contain more than 120 standard nodes Any pipeline with a loop cannot contain more than 600 nodes across all iterations (for example, 60 iterations - 10 nodes each)"
2539	Known issues and limitations	"Watson Pipelines limitations
Limitations by configuration size
A SMALL configuration supports 600 standard nodes (across all active pipelines) or 300 nodes run in a loop. For example:  
30 standard pipelines with 20 nodes run in parallel = 600 standard nodes 3 pipelines containing a loop with 10 iterations and 10 nodes in each iteration = 300 nodes in a loop  
A MEDIUM configuration supports 1200 standard nodes (across all active pipelines) or 600 nodes run in a loop. For example:  
30 standard pipelines with 40 nodes run in parallel = 1200 standard nodes 6 pipelines containing a loop with 10 iterations and 10 nodes in each iteration = 600 nodes in a loop  
A LARGE configuration supports 4800 standard nodes (across all active pipelines) or 2400 nodes run in a loop. For example:  
80 standard pipelines with 60 nodes run in parallel = 4800 standard nodes 24 pipelines containing a loop with 10 iterations and 10 nodes in each iteration = 2400 nodes in a loop"
2540	Known issues and limitations	"Watson Pipelines limitations
Input and output size limits
Input and output values, which include pipeline parameters, user variables, and generic node inputs and outputs, cannot exceed 10 KB of data."
2541	Known issues and limitations	"Watson Pipelines limitations
Batch input limited to data assets
Currently, input for batch deployment jobs is limited to data assets. This means that certain types of deployments, which require JSON input or multiple files as input, are not supported. For example, SPSS models and Decision Optimization solutions that require multiple files as input are not supported."
2542	Known issues and limitations	"Issues with Cloud Object Storage
These issue apply to working with Cloud Object Storage."
2543	Known issues and limitations	"Issues with Cloud Object Storage
Issues with Cloud Object Storage when Key Protect is enabled
Key Protect in conjunction with Cloud Object Storage is not supported for working with Watson Machine Learning assets. If you are using Key Protect, you might encounter these issues when you are working with assets in Watson Studio.  
Training or saving these Watson Machine Learning assets might fail: You might be unable to save an SPSS model or a notebook model to a project  
Auto AI Federated Learning Watson Pipelines"
2544	Known issues and limitations	"Issues with watsonx.governance
Delay showing prompt template deployment data in a factsheet
When a deployment is created for a prompt template, the facts for the deployment are not added to factsheet immediately. You must first evaluate the deployment or view the lifecycle tracking page to add the facts to the factsheet."
2545	Known issues and limitations	"Issues with watsonx.governance
Display issues for existing Factsheet users
If you previously used factsheets with IBM Knowledge Catalog and you create a new AI use case in watsonx.governance, you might see some display issues, such as duplicate Risk level fields in the General information and Details section of the AI use case interface.  
To resolve display problems, update the model_entry_user asset type definition. For details on updating a use case programmatically, see Customizing details for a use case or factsheet."
2546	Known issues and limitations	"Issues with watsonx.governance
Redundant attachment links in factsheet
A factsheet tracks all of the events for an asset over all phases of the lifecycle. Attachments show up in each stage, creating some redundancy in the factsheet."
2547	Creating a project	"You create a project to collaborate with your team on working with data and other resources to achieve a particular goal, such as building a model.  
Your sandbox project is created automatically when you sign up for watsonx.ai.  
You can create an empty project, start from a sample project that provides sample data and other assets, or import a previously exported project. See Importing a project. The number of projects you can create per data center is 100.  
Your project resources can include data, collaborators, tools, assets that run code, like notebooks and models, and other types of assets.  
Requirements Create a project"
2548	Creating a project	"Requirements and restrictions
Before you create a project, understand the requirements for storage and the project name.  
Must be unique in the account.  
Must contain 1 - 255 characters.  
Can't contain these characters: % \  
Can't contain leading or trailing underscores (_).  
Can't contain leading or trailing spaces. Leading or trailing spaces are automatically truncated."
2549	Creating a project	"Creating a project
To create a project:  
Choose Projects > View all projects from the navigation menu and click New project. Choose whether to create an empty project or to create a project based on an exported project file or a sample project. If you chose to create a project from a file or a sample, upload a project file or select a sample project. See Importing a project. If you chose to create a new project, add a name on the New project screen. You can mark the project as sensitive. The project has a sensitive tag and project collaborators can't move data assets out of the project. You cannot change this setting after the project is created. Choose an existing object storage service instance or create a new one. Click Create. You can start adding resources to your project.  
The object storage bucket name for the project is based on the project name without spaces or nonalphanumberic characters plus a unique identifier.  
Watch this video to see how to create both an empty project, imported project, and a project from a sample.  
This video provides a visual method to learn the concepts and tasks in this documentation."
2550	Creating a project	"Next steps
Add collaborators Add data"
2551	Creating a project	"Learn more
Object storage Importing a project Troubleshooting Cloud Object Storage for projects  
Parent topic: Projects"
2552	Add users to the account	"As an Administrator, you add the people in your organization who need access to IBM watsonx to the IBM Cloud account and then assign them the appropriate roles for their tasks.  
Add nonadministrative users to the IBM Cloud account and assign access groups or roles so that they can work in IBM watsonx. The new users receive an email invitation to join the account. They must accept the invitation to be added to the account. Set up access groups to simplify permissions and role assignment. Optional: Add administrative users to the IBM Cloud account."
2553	Add users to the account	"Add nonadministrative users to your IBM Cloud account
You invite users to your IBM Cloud account by sending an email invitation. The user accepts the invitation to join the account. You must assign them roles (or access groups) to provide the necessary permissions to work in IBM watsonx. For a baseline role assignment, you can provide minimum permissions by assigning the following roles in the Manage > Access(IAM) > Users > Invite users > Access policy screen in IBM Cloud:"
2554	Add users to the account	"Add nonadministrative users to your IBM Cloud account
IBM account membership
To be authorized for IBM watsonx, users must have existing IBMids. If the invited user does not have an IBMid, it is created for them when they join the account."
2555	Add users to the account	"Add nonadministrative users to your IBM Cloud account
Assigning roles
To assign minimum permissions to individual users:  
From IBM watsonx, click Administration > Access (IAM) to open the Manage access and users page for your IBM Cloud account.  
Click Users > Invite users+.  
Enter one or more email addresses that are separated by commas, spaces, or line breaks. The limit is 100 email addresses. The settings apply to all the email addresses.  
Click the Access policy tile.  
Select All Identity and Access enabled services, then click Next to assign Resource access.  
For Resources, choose All resources. Click Next.  
For Resource group access, choose Viewer. Click Next  
For Roles and action, choose the following minimum permissions:  
In the Service access section, select Reader In the Platform access section, select Viewer.  
Review the settings and edit if necessary.  
Click Add to save the policy.  
Click Invite to send an email invitation to each email address. The policies are assigned to the users when they accept the invitation to join the account."
2556	Add users to the account	"Add nonadministrative users to your IBM Cloud account
Modifying a user's role
When you change a user's role, their access to services changes. Their ability to complete work in IBM watsonx can be impacted if they do not have the necessary access."
2557	Add users to the account	"Optional: Add administrative users to your IBM Cloud account
You can add administrative users with the Administrator role for account management. This role also provides the Manager role for all services in the account.  
To add a user as an IBM Cloud account administrator:  
Follow the steps to add a non-administrative user, except change these settings for an individual user's roles: Alternatively, create an access group containing these roles and assign the user to the access group. Click Invite. The new users receive an email invitation to join the account. They must accept the invitation to be added to the account. After the user joins the account, add account management permissions. Click the user's name, then Access > Assign access under Access policies. For the service to assign access to, choose All Account Management Services. Next, in the Platform access section, select Administrator and click Add. Click Assign.  
In the Service access section, select Manager. In the Platform access section, select Administrator."
2558	Add users to the account	"Next steps
Finish setting up the platform. Upgrade your service instances to billable plans."
2559	Add users to the account	"Learn more
Roles in IBM watsonx IBM Cloud docs: Account types IBM Cloud docs: IAM access IBM Cloud docs: What is IBM Cloud Identity and Access Management IBM Cloud docs: Setting up access groups IBM Cloud docs: Giving access to resources in resource groups  
Parent topic: Managing users and access"
2560	Watson Studio service plans	"The plan you choose for Watson Studio affects the features and capabilities that you can use.  
When you provision or upgrade Watson Studio, you can choose between a Lite and a Professional plan.  
See the plan pages in IBM Cloud catalog: Watson Studio for pricing and feature information.  
IBM Cloud account owners can choose between the Lite (unpaid) and Professional (paid) plan.  
Under the Professional plan, you can provision multiple Watson Studio instances in an IBM Cloud account. The Professional plan allows unlimited users and charges for compute usage which is measured in capacity unit hours (CUH). The Professional plan is the only paid plan option.  
Under the Lite plan, you can provision one Watson Studio instance per IBM Cloud account. The Lite plan allows only one user and limits the CUH to 10 hours per month. Collaborators in your projects must have their own Watson Studio Lite plans.  
Both Watson Studio plans contain these features without additional services:  
Watson services APIs to run in notebooks. Jupyter notebooks to analyze data with Python or R code. RStudio to analyze data with R code. SPSS Modeler to develop predictive models on a graphical canvas. Data Refinery to shape and cleanse data. Watson Pipelines to orchestrate an end-to-end flow of assets from creation through deployment. Small runtime environments as compute resources for analytical tools. Spark runtime environments. The maximum number of Spark executors that can be used is restricted by the service plan. Environments with the Watson Natural Language Processing library with pre-trained models for language processing tasks that you can run on unstructured data. Environments with Decision Optimization libraries to model and solve decision optimization problems that exceed the complexity that is supported by the Community Edition of the libraries in the other default Python environments. Connectors to data sources. Collaboration in projects and deployment spaces. Resource hub for resources to help you learn and samples that you can use.  
Both Watson Studio plans contain these features that also require the Watson Machine Learning service:  
Machine learning models to build analytical models. AutoAI to automatically create a set of model candidates. Federated learning to collaboratively train a model with multiple remote parties without sharing data. Decision Optimization to build models that solve business problems. Synthetic Data Generator to generate synthetic tabular data.  
The Watson Studio Professional plan includes features that are not available in the Lite plan, including the following:"
2561	Watson Studio service plans	"The Watson Studio Professional plan includes features that are not available in the Lite plan, including the following:  
Encrypt your IBM Cloud Object Storage instance with your own key. Large runtime environments with 8 or more vCPUs as compute resources for analytical tools. GPU environments for running notebooks. Export projects.  
The Professional plan charges for compute usage consumed per month. Compute usage is measured in capacity unit hours (CUH). For details on computing resource allocation and consumption, see Runtime usage."
2562	Watson Studio service plans	"Learn more
Watson Studio service overview Getting started Analyzing data and building models Upgrade your plan.  
Parent topic: Watson Studio"
2563	Notices	"These notices apply to the watsonx platform.  
The Offering includes some or all of the following that IBM provides under the SIL Open Font License 1.1:  
AMSFONTS AMSFONTS (matplotlib) CARLOGO (matplotlib) CMMI9 (libtasn1) cvxopt 1.3.0 FONTS (harfbuzz) FONTS (pillow) FONT AWESOME (Apache ORC) FONT AWESOME - FONT (bazel) FONT AWESOME 4.2.0 (arrow) FONT-AWESOME-IE7.MIN.CSS (Jetty) FONT AWESOME (nbconvert) FONTAWESOME-FONTS HELVETICA-NEUE READLINE.PS (Readline) FONT-AWESOME (Notebook) FONTAWESOME FONTAWESOME (Tables) FONT AWESOME FONTS FONTAWESOME (FONT) (JupyterLab) Font-Awesome v4.6.3 Font-Awesome v4.3.0 Font-Awesome v4.7.0 handsontable v0.25.1 minio 7.1.7 IBM PLEX TYPEFACE (carbon-components) nbconvert v5.2.1 nbconvert v5.1.1 nbconvert 6.4.4 nbconvert 6.5.0 nbdime 3.1.1 NotoNastaliqUrdu-Regular.ttf (pillow) NOTO-FONTS (pillow) QTAWESOME-FONTS (qtawesome) qtawesome v3.3.0 READLINE.PS (Readline) RLUSERMAN.PS (Readline) STIX FONT (matplotlib)  
The Offering includes some or all of the following that IBM provides under the UBUNTU FONT LICENCE Version 1.0:  
Font_license (Werkzeug)"
2564	Notices	"Learn more
Foundation model use terms"
2565	IBM watsonx.governance use case	To drive responsible, transparent, and explainable AI workflows, your enterprise needs an integrated system for tracking, monitoring, and retraining AI models. Watsonx.governance provides the processes and technologies to enable your enterprise to monitor, maintain, automate, and govern machine learning and generative AI models in production.
2566	IBM watsonx.governance use case	"Challenges
Watsonx.governance helps you to solve the following challenges for your enterprise:  
Follow the story of Golden Bank as it uses watsonx.governance to govern their AI assets as they implement a process to analyze stock anomalies to boost productivity and increase the accuracy of a stock analyst in investment banking. The team needs to:  
Track their machine learning models and generative AI assets throughout the lifecycle; to capture and share facts about the assets; to help meet governance and compliance goals. Monitor their deployed models for fairness, accuracy, explainability, and drift. Evaluate their summarization and question-answering prompt templates to measure how effectively the foundation models generate responses. Create a pipeline to simplify the retraining process."
2567	IBM watsonx.governance use case	"Process
To implement watsonx.governance for your enterprise, your organization can follow this process:  
Track machine learning models and prompt templates Evaluate machine learning models and prompt templates Monitor deployed machine learning models and prompt templates  
Watsonx.ai and watsonx.governance provide the tools and processes that your organization needs to govern AI assets."
2568	IBM watsonx.governance use case	"Process
1. Track machine learning models and prompt templates
Your team can track your machine-learning models and prompt templates from request to production and evaluate whether they comply with your organization's regulations and requirements.  
Business analysts at Golden Bank requested a stock anomaly prediction model. They can then track the model through all stages of the AI lifecycle as data scientists or ML engineers build and train the model and ModelOps engineers deploy and evaluate it. Factsheets document details about the model history and generate metrics that show its performance."
2569	IBM watsonx.governance use case	"Process
2. Evaluate machine learning models and prompt templates
You can evaluate machine learning models and prompt templates in projects or deployment spaces to measure their performance. For machine learning models, evaluate the model for quality, fairness, and accuracy. For foundation models, evaluate foundation model tasks, and understand how your model generates responses.  
Golden Bank's data scientist and prompt engineering teams work together to evaluate the summarization and questing-answering prompt templates using test data. They want to measure the performance of the foundation model and to understand how the model generates responses. The evaluations are tracked in AI Factsheets, so the entire team can monitor throughout the lifecycle from the development phase all the way through to the production phase."
2570	IBM watsonx.governance use case	"Process
3. Monitor deployed machine learning models and prompt templates
After deploying models, it is important to govern and monitor them to make sure that they are explainable and transparent. Data scientists must be able to explain how the models arrive at certain predictions so that they can determine whether the predictions have any implicit or explicit bias. You can configure drift evaluations to measure changes in your data over time to ensure consistent outcomes for your model. Use drift evaluations to identify changes in your model output, the accuracy of your predictions, and the distribution of your input data.  
Data scientists at Golden Bank use Watson OpenScale to monitor the deployed stock anomaly prediction model to ensure that it is accurate, fair, and explainable. They run a notebook to set up monitors for the model and then tweak the configuration by using the Watson OpenScale user interface. Using metrics from the Watson OpenScale quality monitor and fairness monitor, the data scientists determine how well the model predicts outcomes and if it produces any biased outcomes. They also get insights for how the model comes to decisions so that the decisions can be explained to the stock analysts."
2571	IBM watsonx.governance use case	"Next Steps
Build machine learning and generative AI models with watsonx.ai Scale AI workloads, for all your data, anywhere with watsonx.data"
2572	IBM watsonx.governance use case	"Learn more
IBM watsonx overview Watson Studio overview Watson Machine Learning overview Watson OpenScale overview Videos Try out different use cases on a self-service site. Select a use case to experience a live application built with watsonx. Developers, access prompt selection and construction guidance, along with sample application code, to accelerate your project.  
Parent topic: Use cases"
2573	Keeping your data secure and compliant	"Customer data security is paramount. The following information outlines some of the ways that customer data is protected when using IBM watsonx and what you are expected to do to help in these efforts.  
Customer responsibility HIPAA readiness IBM's commitment to GDPR Content and Data Protection GDPR statement that applies to IBM Watson Machine Learning log files Secure deletion from the IBM Watson Machine Learning service"
2574	Keeping your data secure and compliant	"Customer responsibility
Clients are responsible for ensuring their own compliance with various laws and regulations, including the European Union General Data Protection Regulation (GDPR). Clients are solely responsible for obtaining advice of competent legal counsel as to the identification and interpretation of any relevant laws and regulations that may affect the clients’ business and any actions the clients may need to take to comply with such laws and regulations. The products, services, and other capabilities described herein are not suitable for all customer situations and may have restricted availability. IBM does not provide legal, accounting, or auditing advice or represent or warrant that its services or products will ensure that clients are in compliance with any law or regulation."
2575	Keeping your data secure and compliant	"HIPAA readiness
Watson Studio and Watson Machine Learning meet the required IBM controls that are commensurate with the Health Insurance Portability and Accountability Act of 1996 (HIPAA) Security and Privacy Rule requirements.  
These requirements include the appropriate administrative, physical, and technical safeguards required of Business Associates in 45 CFR Part 160 and Subparts A and C of Part 164. HIPAA readiness applies to the following plans:  
The Watson Studio Professional plan in the Dallas (US South) region The Watson Machine Learning Standard plan in the Dallas (US South) region  
For other services, you must check the plan page in IBM Cloud for each to determine if it is HIPAA ready and whether you need to reprovision the service after you enable HIPAA support.  
HIPAA support from IBM requires that you agree to the terms of the Business Associate Addendum (BAA) agreement with IBM for your IBM Cloud account. The BAA outlines IBM responsibilities, but also your responsibilities to maintain HIPAA compliance. After you enable HIPAA support in your IBM Cloud account, you cannot disable it. See IBM Cloud Docs: Enabling HIPAA support for your account..  
To enable HIPAA support for your IBM Cloud account:  
Log in to your IBM Cloud account. Click Manage > Account and then Account settings. In the HIPAA Supported section, click On. Read the BAA and then select Accept and click Submit."
2576	Keeping your data secure and compliant	"IBM's commitment to GDPR
Learn more about IBM’s own GDPR readiness journey and our GDPR capabilities and offerings to support your compliance journey."
2577	Keeping your data secure and compliant	"Content and Data Protection
The Data Processing and Protection data sheet (Data Sheet) provides information specific to the IBM Cloud Service regarding the type of Content enabled to be processed, the processing activities involved, the data protection features, and specifics on retention and return of Content. Any details or clarifications and terms, including customer responsibilities, around use of the Cloud Service and data protection features, if any, are set forth in this section. There may be more than one Data Sheet applicable to a customer's use of the IBM Cloud Service based upon options selected by customer. The Data Sheet may only be available in English and not available in local languages. Despite any practices of local law or custom, the parties agree that they understand English and it is an appropriate language regarding acquisition and use of the IBM Cloud Services. The following Data Sheets apply to the IBM Cloud Service and its available options. Customer acknowledges that i) IBM may modify Data Sheets from time to time at IBM's sole discretion and ii) such modifications will supersede prior versions. The intent of any modification to Data Sheet(s) will be to  
improve or clarify existing commitments, maintain alignment to current adopted standards and applicable laws, or provide additional commitments. No modification to Data Sheets will materially degrade the data protection of a IBM Cloud Service.  
See the Learn more section for links to some of the data sheets that you can view.  
You, the customer, are responsible to take necessary actions to order, enable, or use available data protection features for a IBM Cloud Service and accept responsibility for use of the IBM Cloud Services if you fail to take such actions, including meeting any data protection or other legal requirements regarding Content. IBM's Data Processing Addendum (DPA) and DPA Exhibits apply and are referenced in as part of the Agreement, if and to the extent the European General Data Protection Regulation (EU/2016/679) (GDPR) applies to personal data contained in Content. The applicable Data Sheets for this IBM Cloud Service will serve as the DPA Exhibits. If the DPA applies, IBM's obligation to provide notice of changes to Subprocessors and Customer's right to object to such changes will apply as set out in DPA."
2578	Keeping your data secure and compliant	"GDPR statement that applies to IBM Watson Machine Learning log files
Disclaimer: Client’s use of the deep learning training process includes the ability to write to the training log files. Personal data must not be written to these training log files as they are accessible to other users within Client’s Enterprise as well as to IBM as necessary to support the Cloud Service.  
Please pay close attention to data privacy principals when selecting a dataset for training data. Processing of PI is governed by vigorous legal requirements and is only allowed if it is based on an explicit legal basis. These regulations mandate that PI is processed only for the purpose it was collected for. No other processing in a manner that is incompatible with this initial purpose is permissible. For these and other constrains these regulations place on your use of PI, we highly recommend that you do not use ""real"" PI in your training dataset unless it is allowed or permissible. You may substitute real PI using test data that is available on the public sphere."
2579	Keeping your data secure and compliant	"Secure deletion from the IBM Watson Machine Learning service
Anyone that has personally identifiable information and data (PII) stored as part of using the IBM Watson Machine Learning service, has the right to obtain from the controller the erasure of that data without undue delay. The controller has the obligation to erase personal data without undue delay where one of the following conditions exist:  
There is PII data stored in the IBM Watson Machine Learning service  
User email address and full name are stored as metadata related to the Machine Learning repository assets.  
User provided service credentials.  
Repository asset content, which is usually out of Machine Learning service control and potentially can contain any type of PII data in it. In this case, when users want to track PII data stored in assets, such as a model, they must:  
Get training data reference from the model metadata. Scan training data for occurrence of PII data of particular user. If such data can be found in the training data set, the model should be considered as potentially holding this data in its content.  
Repository asset content, such as models, can be securely deleted by performing one of the methods for permanently deleting personal data."
2580	Keeping your data secure and compliant	"Secure deletion from the IBM Watson Machine Learning service
Options for permanently deleting personal data
There are several options that users can choose to delete their personal data permanently:  
Remove the entire IBM Watson Machine Learning service instance from IBM Cloud. This is possible by sending an un-provisioning request via different channels, such as the IBM Cloud UI, CLI, or REST API.  
Use the Watson Machine Learning REST to delete models or model deployments.  
For the IBM Watson Machine Learning service, personally identifiable information and data is removed completely from all data sources, including backups, after 30 days."
2581	Keeping your data secure and compliant	"Learn more
watsonx terms IBM Watson Machine Learning terms IBM Watson Studio terms IBM Cloud Object Storage terms How do I know that my data is safe? Data Security and Privacy Principles for IBM Cloud Services IBM and GDPR Software Product Compatibility Reports: IBM Watson Studio Software Product Compatibility Reports: IBM Watson Machine Learning Software Product Compatibility Reports: IBM Watson Machine Learning Service  
Parent topic: Security"
2582	Watsonx.ai use case	To transform your business processes with AI-driven solutions, your enterprise needs to integrate both machine learning and generative AI into your operational framework. Watsonx.ai provides the processes and technologies to enable your enterprise to develop and deploy machine learning models and generative AI solutions.
2583	Watsonx.ai use case	"Challenges
You can solve the following challenges for your enterprise by using watsonx.ai:  
Follow the story of Golden Bank as it uses watsonx.ai to implement a process to analyze stock anomalies to boost productivity and increase the accuracy of a stock analyst in investment banking. The team needs to:  
Prepare the stock data to ensure the correct format to train the model. Build and deploy a machine learning model to predict anomalies in how the stock price performed. Find and chunk news articles on the dates where the anomalies occurred. Construct prompt templates to perform summarization and question-answering tasks. Tune the foundatation model with retraining data to ensure the best performance and cost effectiveness. Create a pipeline to simplify the retraining process."
2584	Watsonx.ai use case	"Process
To leverage watsonx.ai for your enterprise, your organization can follow this process:  
Prepare the data Build and train models Deploy models Prompt a foundation model Tune a foundation model Automate the AI lifecycle  
The watsonx.ai component provides the tools and processes that your organization needs to implement an AI solution."
2585	Watsonx.ai use case	"Process
1. Prepare the data
Data scientists can prepare their own data sets. The data scientist teams can add those data assets to a project, where they collaborate to prepare, analyze, and model the data.  
In their project, the data scientists refine the data to prepare it for training a machine learning model by ensuring that the data is in the correct format. The machine learning engineers use the structured and sequential training data in the time series anomaly prediction AutoAI experiment that builds the model pipelines."
2586	Watsonx.ai use case	"Process
2. Build and train machine learning models
To get predictive insights based on your data, data scientists, business analysts, and machine learning engineers can build and train machine learning models. Data scientists use watsonx.ai tools to build the AI models, ensuring that the right algorithms and optimizations are used to make predictions that help to solve business problems.  
Data scientists at Golden Bank use AutoAI to build a time series analysis anomaly detection model that highlights hidden patterns by using historical data. The objective is to build a model that can predict the dates where stock anomalies occurred for a specific company."
2587	Watsonx.ai use case	"Process
3. Deploy models
When operations team members deploy your AI models, the machine learning models become available for applications to use for scoring and predictions to help drive actions.  
The operations team members at Golden Bank promote the ""Stock Anomaly Model"" from the project to a deployment space and then create an online model deployment. Next, they test the deployed model by inputting stock data to output dates of the anomalies."
2588	Watsonx.ai use case	"Process
4. Prompt a foundation model
Your team can write code in a Jupyter notebook or use the Prompt Lab to develop prompts with a foundation model.  
Golden Bank's data scientist and prompt engineering teams work together to gather relevant documents from various online news sources that match the anomaly dates that are obtained from the anomaly prediction model. They feed those dates into a Jupyter notebook to automate sourcing online news articles to determine what current world events might contribute to the stock anomalies. The Jupyter Notebook uses LangChain to chunk the text into smaller text extracts that are suitable for including in prompts. This task ensures that the token does not exceed the context token window limitations.  
Then the team uses Prompt Lab to create a prompt template for summarization and a prompt template for question-answering. For the summarization task, the goal is to summarize the news article text and prompting questions to get more information from the text. For the question-answering task, the input and output varies on the question and answers, so they feed the news article text into the instructions."
2589	Watsonx.ai use case	"Process
5. Tune a foundation model
Your team can write code in a Jupyter notebook or use the Tuning Studio to tune a foundation model. You might want to tune a foundation model to reduce costs or improve the model's performance.  
Golden Bank's prompt engineering team prompt tunes the foundation model using additional new articles. The team produces a more cost-effective, smaller foundation model with the same performance level as the original foundation model that they selected for inferencing."
2590	Watsonx.ai use case	"Process
6. Automate the ML lifecycle
Your team can automate and simplify the MLOps and AI lifecycle with Watson Pipelines.  
The data scientists at Golden Bank can use pipelines to automate their complete ML lifecycle and processes to simplify the machine learning model retraining process."
2591	Watsonx.ai use case	"Next Steps
Start governing your AI solutions with watsonx.governance Scale AI workloads, for all your data, anywhere with watsonx.data"
2592	Watsonx.ai use case	"Learn more
IBM watsonx overview Watson Studio overview Watson Machine Learning overview Videos Try out different use cases on a self-service site. Select a use case to experience a live application built with watsonx. Developers, access prompt selection and construction guidance, along with sample application code, to accelerate your project.  
Parent topic: Use cases"
2593	Data security	In IBM watsonx, data security mechanisms, such as encryption, protect sensitive customer and corporate data, both in transit and at rest. A secure , and other mechanisms protect your valuable corporate data. A secure IBM Cloud Object Storage instance stores data assets from projects, catalogs, and deployment spaces.
2594	Data security	"Configuring Cloud Object Storage
IBM Cloud Object Storage provides storage for projects, catalogs, and deployment spaces. You are required to associate an IBM Cloud Object Storage instance when you create projects, catalogs, or deployment spaces to store files for assets, such as uploaded data files or notebook files. The Lite plan instance is free to use for storage capacity up to 25 GB per month.  
You can also access data sources in an IBM Cloud Object Storage instance. To access data IBM Cloud Object Storage, you create an IBM Cloud Object Storage connection when you want to connect to data stored in IBM Cloud Object Storage. An IBM Cloud Object Storage connection has a different purpose from the IBM Cloud Object Storage instance that you associate with a project, deployment space, or catalog.  
The IBM Cloud Identity and Access Management (IAM) service securely authenticates users and controls access to IBM Cloud Object Storage. See IBM Cloud docs: Getting started with IAM for instructions on setting up access control for Cloud Object Storage on IBM Cloud.  
See IBM Cloud docs: Getting started with IBM Cloud Object Storage"
2595	Data security	"Controlling access with service credentials
Cloud Object Storage credentials consist of a service credential and a Service ID. Policies are assigned to Service IDs to control access. The credentials are used to create a secure connection to the Cloud Object Storage instance, with access control as determined by the policy.  
For more information, see Controlling access to Cloud Object Storage buckets"
2596	Data security	"Encrypting at rest data
By default, at rest data is encrypted with randomly generated keys that are managed by IBM. If the default keys are sufficient protection for your data, no additional action is needed. To provide extra protection for at rest data, you can create and manage your own keys with IBM® Key Protect for IBM Cloud™. Key Protect is a full-service encryption solution that allows data to be secured and stored in IBM Cloud Object Storage.  
To encrypt your Cloud Object Storage instance with your own key, create an instance of the IBM Key Project service from the IBM Cloud catalog. Not all Watson Studio plans support customer-generated encryption keys.  
For instructions on encrypting your Cloud Object Storage instance with your own key, see Setting up IBM Cloud Object Storage for use with IBM watsonx For an overview of how to encrypt data with your own keys, see IBM Cloud docs: Encrypting data with your own keys For the complete documentation for Key Protect, see IBM Cloud docs: IBM Key Protect For an overview of how encryption works in the IBM Cloud Security Architecture, see Data security architecture"
2597	Data security	"Encrypting in motion data
Data is encrypted when transmitted by IBM on any public networks and within the Cloud Service's private data center network. Encryption methods such as HTTPS, SSL, and TLS are used to protect data in motion."
2598	Data security	"Backups
To avoid loss of important data, create and properly store backups. You can use IBM Cloud Backup to securely back up your data between IBM Cloud servers in one or more IBM Cloud data centers. See IBM Cloud docs: Getting started with IBM Cloud Backup  
Learn More For more information, see IBM Cloud docs: Getting started with Security and Compliance Center.  
Parent topic: Security"
2599	Browser support	"The supported web browsers provide the best experience for IBM watsonx.  
Use the latest versions of these web browers with IBM watsonx:  
Chrome Microsoft Edge Mozilla Firefox Tip for Firefox on Mac users: Horizontal scrolling within the UI can be interpreted by your Mac as an attempt to swipe between pages. If this behavior is undesired or if the browser crashes after the service prompts you to stay on the page, consider disabling the Swipe between pages gesture in Launchpad > System Preferences > Trackpad > More Gestures. Firefox ESR (see Mozilla Firefox Extended Support Release for more details)"
2600	Browser support	"Learn more
Language support  
Parent topic: FAQ"
2601	Adding associated services to a project	"To run some tools, you must associate a Watson Machine Learning service instance with the project.  
After you create a project, you can add an associated service to it at any time.  
For some types of assets, you must associate the IBM Watson Machine Learning service with the project. You are prompted to associate the IBM Watson Machine Learning the first time you open tools like Prompt Lab, AutoAI, SPSS Modeler, and Decision Optimization.  
If you have watsonx.governance provisioned, it is available in all projects. You do not need to explicitly associate a watsonx.governance instance with a project.  
You can also add the Watson Machine Learning service to a project directly:  
Go to the project's Manage tab and select the Services and integrations page. In the IBM Services section, click Associate Service. Select your IBM Watson Machine Learning service instance and click Associate."
2602	Adding associated services to a project	"Learn more
Creating and managing IBM Cloud services IBM Cloud services for IBM watsonx  
Parent topic: Administering projects"
2603	IBM watsonx APIs	You can complete many of the tasks for watsonx programmatically with APIs.
2604	IBM watsonx APIs	"APIs for generative AI
You can work with foundation models programmatically to develop generative AI solutions by using:  
watsonx.ai REST API Foundation models Python library  
For examples of how to use the foundation models Python library, see Foundation models Python library."
2605	IBM watsonx APIs	"APIs for machine learning
You can manage spaces, deployments, and assets programmatically by using:  
Watson Machine Learning REST API The ibm-watson-machine-learning Python library  
For links to sample Jupyter Notebooks that demonstrate how to manage spaces, deployments, and assets programmatically, see Machine Learning Python client samples and examples."
2606	IBM watsonx APIs	"APIs for AI Factsheets
You can manage settings, model entries, and report templates programmatically by using:  
AI Factsheets REST API AI Factsheets Python library"
2607	IBM watsonx APIs	"APIs for managing assets and collaborators
You can manage data-related assets and collaborators by using the Watson Data API."
2608	IBM watsonx APIs	"APIs for managing assets and collaborators
APIs for connections
You can create connections by using the Connections in the Watson Data API.  
You can view a table of the individual data source properties at Data > Platform connections. Expand Connection resources, and select Connection properties. Alternativey, you can open a new web page at: https://dataplatform.cloud.ibm.com/connections/docs."
2609	IBM watsonx APIs	"Learn more
watsonx.ai REST API docs Foundation models Python library docs Watson Machine Learning REST API docs The ibm-watson-machine-learning Python library docs AI Factsheets REST API docs AI Factsheets Python library docs Watson Data API docs Connections in the Watson Data API docs  
Parent topic: Getting started"
2610	Setting up the IBM watsonx platform for administrators	"To set up the watsonx platform for your organization, sign up for IBM watsonx.ai, upgrade to a paid plan, set up the services that you need, and add your users with the appropriate permissions.  
IBM watsonx.ai on the watsonx platform includes cloud-based services that provide data preparation, data science, and AI modeling capabilities. The watsonx platform is protected by the same powerful security constraints that are available on IBM Cloud."
2611	Setting up the IBM watsonx platform for administrators	"Common security mechanisms
As an IBM Cloud account owner or administrator, you set up security for the account by providing single sign-on, IAM role-based access control, secure communication, and other security constraints.  
Following are common security mechanisms for the IBM watsonx platform:  
Encrypt your instance with your own key. See Encrypt your IBM Cloud Object Storage instance with your own key. Use IBM Key Protect to encrypt key data assets in Cloud Object Storage. See Encrypting at rest data. Support single sign-on using SAML federation or Active Directory. See SSO with Federated IDs. Configure secure connections to databases that are behind a firewall. See Connecting to data behind a firewall Configure secure communication between services with Service Endpoints. See Private network service endpoints. Control access at the IP address level. See Allow specific IP addresses. Require personal credentials when creating connections. The default setting is shared credentials. See Managing your account settings."
2612	Setting up the IBM watsonx platform for administrators	"Learn more
HIPAA readiness is available for some regions and plans. See HIPAA readiness. See Security for IBM watsonx for a complete list of security constraints available in IBM watsonx. See Overview of watsonx to understand the architecture of the platform.  
Parent topic: Getting started"
2613	Overview of IBM watsonx as a Service	"IBM watsonx as a Service is a secure and collaborative environment where you can access your organization's trusted data, automate AI processes, and deliver AI in your applications. The IBM watsonx.ai component provides a studio of integrated tools for working with generative AI capabilities that are powered by foundation models and for building machine learning models. The IBM watsonx.governance component provides end-to-end monitoring for machine learning and generative AI models to accelerate responsible, transparent, and explainable AI workflows.  
AI engineers, data scientists, and AI risk and compliance officers can accomplish the following goals with watsonx.ai and watsonx.governance:  
Build machine learning models Build models by using open source frameworks and code-based, automated, or visual data science tools.  
Develop with foundation models Generate prompts to generate, classify, summarize, or extract content from your input text. Choose from IBM models or open source models from Hugging Face. Tune foundation models to customize your prompt output.  
Manage the AI lifecycle Manage and automate the full AI model lifecycle with all the integrated tools and runtimes to train, validate, and deploy AI models.  
Govern AI Track the detailed history of AI models and evaluate model output to help ensure compliance.  
Data engineers can collect, store, query, and analyze enterprise data in a lakehouse architecture with IBM watsonx.data. See IBM watsonx.data documentation.  
The following graphic shows the capabilities of the watsonx.ai and watsonx.governance components on top of the common core functionality that provides an integrated user experience. The watsonx.data experience is separate and not shown in the graphic."
2614	Overview of IBM watsonx as a Service	"Watsonx.ai
Watsonx.ai includes tools for working with data and models, deployed foundation models for generative AI, and the hardware and software resources for computing and inferencing.  
Watch this short video that introduces watsonx.ai."
2615	Overview of IBM watsonx as a Service	"Watsonx.ai
Tools for preparing data and working with models
Your watsonx.ai tools are in collaborative workspaces called projects.  
You can use watsonx.ai tools to prepare data and work with models in the following ways:  
Prepare data: Refine and visualize your data files or data tables in remote data sources or generate synthetic tabular data. Build machine learning models: Automatically generate model candidates, create machine learning model training flows, or write model training code in Python or R. Work with foundation models: Experiment with generative AI prompts, tune foundation models for your use case, or write generative AI solution code in Python. Automate model lifecycles: Create repeatable and scheduled flows that automate notebook, Data Refinery, and machine learning pipelines.  
For a list of tools, their levels of automation, and whether you can use them to prepare data or work with models, see Analyzing data and working with models. For generative AI tools, see Developing generative AI solutions."
2616	Overview of IBM watsonx as a Service	"Watsonx.ai
Deployed foundation models
IBM watsonx.ai has a range of deployed large language models for generative AI. The deployed foundation models include open source models from Hugging Face and IBM foundation models. You can select the appropriate foundation model for your inferencing use case. You can customize foundation model behavior by tuning a foundation model.  
For a list of supported foundation models that are deployed in watsonx.ai, see Supported foundation models."
2617	Overview of IBM watsonx as a Service	"Watsonx.ai
Usage resources for computing and inference
Depending on your service plans, you might have a set amount of usage resources per month or you might be billed for the resources that you consume.  
When you run tools with watsonx.ai, you consume the following types of resources:"
2618	Overview of IBM watsonx as a Service	"Watsonx.governance
Watsonx.governance includes tools for governing models and the usage resources for evaluating and explaining models.  
Watch this short video that introduces watsonx.governance."
2619	Overview of IBM watsonx as a Service	"Watsonx.governance
Tools for governing models
You can use watsonx.governance tools to govern models in the following ways:  
Monitor and evaluate models: You can monitor model output and explain model predictions. Your watsonx.governance model monitoring and evaluation tools are in projects and deployment spaces.  
Track and document AI use cases: You can view model lifecycle status, general model and deployment details, training information and metrics, and deployment metrics. Your watsonx.governance model tracking tools in AI use cases."
2620	Overview of IBM watsonx as a Service	"Watsonx.governance
Usage resources for model evaluations and explanations
When you run model evaluations and explanations with watsonx.governance, you consume resources. Depending on your service plans, you might have a set amount of usage resources per month or you might be billed for the resources that you consume. Your resource usage is calculated based on the number of model evaluations and explanations. Evaluations and explanations are measured in resource units."
2621	Overview of IBM watsonx as a Service	"Common core functionality
Watsonx includes the following core functionality of IBM Cloud Pak for Data as the secure and scalable foundation for your organization to collaborate efficiently:  
Connectivity Administration Storage Workspaces Resource hub"
2622	Overview of IBM watsonx as a Service	"Common core functionality
Connectivity
You can create connections to remote data sources and import connected data. You can configure connections with personal or shared credentials. For a list of supported connectors, see Connectors.  
You can share connections with others across the platform in the Platform assets catalog."
2623	Overview of IBM watsonx as a Service	"Common core functionality
Administration
The following administration features provide security and flexibility:  
Watsonx is fully managed by IBM on IBM Cloud. Software updates are automatic. Scaling of usage resources and storage is automatic.  
The data security, network security, security standards compliance, and isolation of watsonx are managed by IBM Cloud. Data is encrypted at rest and in motion. You can set up extra security and encryption options.  
Your work on watsonx, including your data and the models that you create, are private to your account. Your data and models will never be accessible or used by IBM or any other person or organization.  
Learn more about security and your options:  
Security and privacy of foundation models Data security Security of watsonx  
Watsonx.ai on the watsonx platform includes the Watson Studio, Watson Machine Learning, and IBM Cloud Object Storage services. Watsonx.governance on the watsonx platform includes the watsonx.governance service.  
You can add services, such as data source services, from the watsonx services catalog. See Creating and managing IBM Cloud services.  
You add users and user groups and manage their account roles and permissions with IBM Cloud Identity and Access Management. You assign roles within each collaborative workspace across the platform. See Managing users and access."
2624	Overview of IBM watsonx as a Service	"Common core functionality
Storage
A IBM Cloud Object Storage service instance is automatically provisioned for you to provide storage for the assets that you create or add to workspaces. Information that is stored in IBM Cloud Object Storage is encrypted and resilient. Each workspace has its own dedicated bucket. See Object storage for workspaces."
2625	Overview of IBM watsonx as a Service	"Common core functionality
Workspaces
Watsonx is organized as a set of collaborative workspaces where you can work with your team or organization. Each workspace has a set of members with roles that provide permissions to perform actions.  
Most users work with assets, which are items that are created or added to workspaces by users. Assets can represent data, models, or other types of code or information. Data assets contain metadata that represents data. Assets that you create in tools, such as models, run code to work with data. You can also create assets that contain information about other assets, such as model use cases that contain metadata, history, and reports about models. See Asset types and properties.  
You can work in these types of workspaces in watsonx:  
Projects Deployment spaces Platform connections AI use cases  
You can search for assets across all workspaces that you belong to.  
Projects are where your data science and model builder teams work with data to create assets, such as, saved prompts, notebooks, models, or pipelines. Your first project, which is known as your sandbox project, is created automatically when you sign up for watsonx.ai. See Your sandbox project.  
The following image shows what the Overview page of a project might look like.  
Deployment spaces are where your ModelOps team deploys models and other deployable assets to production and then tests and manages deployments in production. After you build models and deployable assets in projects, you promote them to deployment spaces. See Deployment spaces.  
The following image shows what the Overview page of a deployment space might look like.  
Platform connections is a view of the Platform assets catalog that lists connection assets. You can access platform connections in any project or deployment space.  
The following image shows what the Connections page of the Platform connections might look like.  
AI use cases is a view of the Platform assets catalog that lists model use cases. Use cases track the details for AI assets in factsheets.  
The following image shows what the AI use cases page might look like."
2626	Overview of IBM watsonx as a Service	"Common core functionality
Resource hub
Watsonx includes an integrated collection of samples that provides deployed foundation models, data assets, prompts, notebooks, and sample projects. Sample notebooks provide examples of data science and machine learning code. Sample projects contain sets of data, models, other assets, and detailed instructions on how to solve a particular business problem.  
The following image shows what Resource hub looks like.  
Watch this video to see a tour of the Resource hub.  
This video provides a visual method to learn the concepts and tasks in this documentation."
2627	Overview of IBM watsonx as a Service	"Learn more
Use cases Asset types and properties AI risk atlas Comparison of IBM watsonx as a Service and Cloud Pak for Data as a Service Feature differences between watsonx deployments Foundation models IBM watsonx.data documentation"
2628	Enterprise security	"An enterprise is a hierarchy of IBM Cloud accounts that contains a parent account at the highest level with child account groups as the middle level and optional individual accounts that you can add at the lowest level. To provide security between the levels of accounts, enterprises isolate user and access management between the enterprise account and its child accounts.  
The users and their assigned access in the enterprise account are entirely separate from users in the child accounts, and no access is inherited between the two types of accounts. User and access management in each enterprise and each account is entirely separate and must be managed by the account owner or a user given the Administrator role in the specific account.  
Resources and services within an enterprise function the same as in stand-alone accounts. Each account in an enterprise can contain resource groups that manage access to multiple resources. For account security and how to use resource groups, see IBM Cloud account security."
2629	Enterprise security	"Use cases
The user lists for each account are only visible to the users who are invited to that account. Just because a user is invited and given access to manage the entire enterprise, it doesn't mean that they can view the users who are invited to each child account.  
Both user management and access management are entirely separate in each account and in the enterprise itself. This separation means that users who manage your enterprise can't access account resources within the child accounts unless you specifically enable them to. For example, your financial officer can have the Administrator role on the Billing account management service within the enterprise account. The financial officer must be invited to a child account with the appropriate access rights to view offers or update spending limits for the child account."
2630	Enterprise security	"Learn more
For an overview of enterprise accounts, see IBM Cloud docs: What is an enterprise?  
For step-by-step instructions for setting up an enterprise hierarchy of accounts, see IBM Cloud docs: Setting up an enterprise  
For tips for setting up an enterprise, see IBM Cloud docs: Best practices for setting up an enterprise  
Parent topic: Security"
2631	Video library	"Watch short videos for data scientists, data engineers, and data stewards to learn about watsonx. The videos and accompanying tutorials are task-focused and provide hands-on experience by using the tools in watsonx.  
Note:  
These videos provide a visual method to learn the concepts and tasks in this documentation. If you are having difficulty viewing any of the videos on this page, visit the Video playlists page.  
Select any video from the lists below to watch here.  
IBM watsonx basics  
Create a project Collaborate in projects Tour the Resource Hub Load and analyze public data sets  
Work with data  
Prepare data with Data Refinery Generate synthetic tabular data Analyze data in a Jupyter notebook  
Build ML models  
Build a model with AutoAI Build a model in a Jupyter notebook Build a model with SPSS Modeler Build a Decision Optimization model Automate the ML lifecycle with pipelines  
Use the Prompt Lab  
Prompt a foundation model in Prompt Lab Tips for prompting foundation models Use Chat mode in Prompt Lab See a watsonx.ai end-to-end use case  
Work with foundation models  
Introduction to the RAG pattern Why tune a foundation model? Tune a foundation model  
Govern AI  
Track a model in an AI use case Evaluate a prompt template Track a prompt template Evaluate a machine learning model  
Other IBM watsonx.ai videos  
Classify text Summarize large, complex documents Generate content Extract text from complex documents  
Other IBM watsonx.governance videos  
Tracking and transparency Model evaluation and documentation"
2632	Comparison of IBM watsonx as a Service and Cloud Pak for Data as a Service	"IBM watsonx as a Service and Cloud Pak for Data as a Service have the same common core functionality and are compatible in many ways. The watsonx platform provides a subset of the tools and services that are provided by Cloud Pak for Data as a Service. However, watsonx.ai and watsonx.governance on watsonx provide more functionality than the same set of tools on Cloud Pak for Data as a Service.  
Common platform functionality Services on each platform Data science and MLOps tools AI governance tools"
2633	Comparison of IBM watsonx as a Service and Cloud Pak for Data as a Service	"Common core functionality
The common core functionality is the same on watsonx and Cloud Pak for Data as a Service:  
Security, compliance, and isolation Compute resources for running workloads Global search for assets across the platform The Platform assets catalog for sharing connections across the platform Role-based user management within workspaces A services catalog for adding services View compute usage from the Administration menu Connections to remote data sources Connection credentials that are personal or shared Sample assets and projects  
If you are signed up for both watsonx and Cloud Pak for Data as a Service, you can switch between platforms. See Switching your platform."
2634	Comparison of IBM watsonx as a Service and Cloud Pak for Data as a Service	"Services on each platform
Both platforms provide services for data science and MLOps and AI governance use cases:  
Watson Studio Watson Machine Learning watsonx.governance  
However, the services for watsonx.ai and watsonx.governance on the watsonx platform include features for working with foundation models and generative AI that are not included in these services on Cloud Pak for Data as a Service.  
Cloud Pak for Data as a Service also provides services for these use cases:  
Data integration Data governance  
See IBM Cloud services in the Cloud Pak for Data as a Service services catalog."
2635	Comparison of IBM watsonx as a Service and Cloud Pak for Data as a Service	"Data science and AI tools
Both platforms provide a common set of data science and AI tools. However, on watsonx, you can also perform foundation model inferencing with the Prompt Lab tool or with a Python library in notebooks. Foundation model inferencing and the Prompt Lab tool are not available on Cloud Pak for Data as a Service.  
The following table shows which data science and AI tools are available on each platform.  
If you are signed up for Cloud Pak for Data as a Service, you can access watsonx and you can move your projects and deployment spaces that meet the requirements from one platform to the other. See Switching the platform for a project and Switching the platform for a deployment space."
2636	Comparison of IBM watsonx as a Service and Cloud Pak for Data as a Service	"AI governance tools
Both platforms contain the same AI use case inventory and evaluation tools. However, on watsonx, you can track and evaluate generative AI assets and dimensions.See Comparison of governance solutions."
2637	Comparison of IBM watsonx as a Service and Cloud Pak for Data as a Service	"Learn more
Switching your platform Switching the platform for a project Switching the platform for a deployment space Overview of IBM watsonx as a Service Foundation models  
Parent topic: Overview of watsonx"
2638	Levels of user access roles in IBM watsonx	"Every user of IBM watsonx has multiple levels of roles with the corresponding permissions, or actions. The permissions determine what actions a user can perform on the platform or within a service. Some roles are set in IBM Cloud, and others are set in IBM watsonx.  
The IBM Cloud account owner or administrator sets the Identity and Access (IAM) Platform and Service access roles in the IBM Cloud account. Workspace administrators in watsonx set the collaborator roles for workspaces, for example, projects and deployment spaces.  
Familiarity with the IBM Cloud IAM feature, Access groups, Platform roles, and Service roles is required to configure user access for IBM watsonx. See IBM Cloud docs: IAM access for a description of IBM Cloud IAM Platform and Service roles.  
This illustration shows the different levels of roles assigned to each user so that they can work in IBM watsonx.  
The levels of roles are:  
IAM Platform access roles determine your permissions for the IBM Cloud account. At least the Viewer role is required to work with services. IAM Service access roles determine your permissions within services. Workspace collaborator roles determine what actions you have permission to perform within workspaces in IBM watsonx."
2639	Levels of user access roles in IBM watsonx	"IAM Platform access roles
The IAM Platform access roles are assigned and managed in the IBM Cloud account.  
IAM Platform access roles provide permissions to manage the IBM Cloud account and to access services within IBM watsonx. The Platform access roles are Viewer, Operator, Editor, and Administrator. The Platform roles are available to all services on IBM Cloud.  
The Viewer role has minimal, view-only permissions. Users need at least Viewer role to see the services in IBM watsonx. A Viewer can:  
View, but not modify, available service instances and assets Associate services with projects. Become collaborator in projects or deployment spaces. Create projects and deployment spaces if assigned appropriate permissions for Cloud Object Storage.  
The Operator role has permissions to configure existing service instances.  
The Editor role provides access to these actions:  
All Viewer role permissions. Provision instances of services. Update plans for service instances.  
The Administrator role provides the same permissions as the Owner role for the account. With Administrator role, you can:  
All Viewer, Operator, and Editor permissions. Perform all management actions for services. Add users to the IBM Cloud account and assign roles Perform administrative tasks in IBM watsonx Manage services for IBM watsonx  
To understand IAM Platform access roles, see IBM Cloud docs: What is IBM Cloud Identity and Access Management?."
2640	Levels of user access roles in IBM watsonx	"IAM Service access roles
Service roles apply to individual services and define actions permitted within the service. IBM Cloud Object Storage has its own set of Service access roles. See Setting up IBM Cloud Object Storage for use with IBM watsonx."
2641	Levels of user access roles in IBM watsonx	"Workspace collaborator roles
Your role in a specific workspace determines what actions you can perform in that workspace. Your IAM roles do not affect your role within a workspace. For example, you can be the Administrator of the Cloud account, but this does not automatically make you an administrator for a project or catalog. The Admin collaborator role for a project (or other workspace) must be explicitly assigned. Similarly, roles are specific to each project. You may have Admin role in a project, which gives you full control of the contents of that project, including managing collaborators and assets. But you can have the Viewer role in another project, which allows you to only view the contents of that project.  
Projects and deployment spaces have these roles:  
Admin: Control assets, collaborators, and settings in the workspace. Editor: Control assets in the workspace. Viewer: View the workspace and its contents.  
The permissions that are associated with each role are specific to the type of workspace:  
Project collaborator roles and permissions Deployment space collaborator roles and permissions"
2642	Levels of user access roles in IBM watsonx	"Learn more
IBM Cloud docs: What is IBM Cloud Identity and Access Management? IBM Cloud docs: IAM access Setting up IBM watsonx for your organization Managing IBM watsonx Find your IBM Cloud account owner or administrator Determine your roles  
Parent topic: Adding users to the account"
2643	Language support for the product and the documentation	IBM watsonx is translated into multiple languages.
2644	Language support for the product and the documentation	"Supported languages
The IBM watsonx user interface is translated into these languages:  
Brazilian Portuguese Simplified Chinese Traditional Chinese French German Italian Japanese Korean Spanish Swedish  
The documentation is automatically translated into these languages:  
Brazilian Portuguese Simplified Chinese French German Italian Japanese Korean Spanish  
IBM is not responsible for any damages or losses resulting from the use of automatically (machine) translated content.  
When the translated documentation is not as current as the English content, you see a message and have the option of switching to the English content."
2645	Language support for the product and the documentation	"Changing languages
To change the language for this documentation, scroll to the end of any documentation page, and select a language from the language selector.  
To change the language for both the product user interface and this documentation, select a different language for your browser:  
In the Google Chrome browser, you can change the language in the advanced settings. In the Mozilla Firefox browser, you can change the language in the general settings."
2646	Language support for the product and the documentation	"Learn more
Browser support  
Parent topic: FAQ"
2647	Frequently asked questions	Find answers to frequently asked questions about watsonx.ai.
2648	Frequently asked questions	"Account and setup questions
How do I sign up for watsonx? Can I try watsonx for free? How do I upgrade watsonx.ai and watsonx.governance? Which regions can I provision watsonx.ai and watsonx.governance in? Which web browsers are supported for watsonx? How can I get the most runtime from my Watson Studio Lite plan? How do I change languages for the product and the documentation? How do I find my IBM Cloud account owner or administrator? Can I provide feedback?"
2649	Frequently asked questions	"Foundation model questions
What foundation models are available and where do they come from? What data was used to train foundation models? Do I need to check generated output for biased, inappropriate, or incorrect content? Is there a limit to how much text generation I can do? Does prompt engineering train the foundation model? Does IBM have access to or use my data in any way? What APIs are available?"
2650	Frequently asked questions	"Project questions
How do I load very large files to my project?"
2651	Frequently asked questions	"IBM Cloud Object Storage questions
What is saved in IBM Cloud Object Storage for workspaces? Do I need to upgrade IBM Cloud Object Storage when I upgrade other services? Why am I unable to add storage to an existing project or to see the IBM Cloud Object Storage selection in the New Project dialog?"
2652	Frequently asked questions	"Notebook questions
Can I install libraries or packages to use in my notebooks? Can I call functions that are defined in one notebook from another notebook? Can I add arbitrary notebook extensions? How do I access the data from a CSV file in a notebook? How do I access the data from a compressed file in a notebook?"
2653	Frequently asked questions	"Security and reliability questions
How secure is IBM watsonx? Is my data and notebook protected from sharing outside of my collaborators? Do I need to back up my notebooks?"
2654	Frequently asked questions	"Sharing and collaboration questions
What are the implications of sharing a notebook? How can I share my work outside of RStudio? How do I share my SPSS Modeler flow with another project?"
2655	Frequently asked questions	"Machine learning questions
How do I run an AutoAI experiment? What is available for automated model building? What frameworks and libraries are available for my machine learning models? What is an API Key?"
2656	Frequently asked questions	"Watson OpenScale questions
What is Watson OpenScale? How do I convert a prediction column from an integer data type to a categorical data type? Why does Watson OpenScale need access to training data? What does it mean if the fairness score is greater than 100 percent? How is model bias mitigated by using Watson OpenScale? Is it possible to check for model bias on sensitive attributes, such as race and sex, even when the model is not trained on them? Is it possible to mitigate bias for regression-based models? What are the different methods of debiasing in Watson OpenScale? Configuring a model requires information about the location of the training data and the options are Cloud Object Storage and Db2. If the data is in Netezza, can Watson OpenScale use Netezza? Why doesn't Watson OpenScale see the updates that were made to the model? What are the various kinds of risks associated in using a machine learning model? Must I keep monitoring the Watson OpenScale dashboard to make sure that my models behave as expected? In Watson OpenScale, what data is used for Quality metrics computation? In Watson OpenScale, can the threshold be set for a metric other than 'Area under ROC' during configuration?"
2657	Frequently asked questions	"IBM watsonx.ai questions
How do I sign up for watsonx?
Go to Try IBM watsonx.ai or Try watsonx.governance. If you sign up for watsonx.governance, you automatically provision watsonx.ai as well."
2658	Frequently asked questions	"IBM watsonx.ai questions
Can I try watsonx for free?
Yes, when you sign up for IBM watsonx.ai, you automatically provision the free version of the underlying services: Watson Studio, Watson Machine Learning, and IBM Cloud Object Storage. When you sign up for IBM watsonx.governance, you automatically provision the free version of Watson OpenScale and the free versions of the services for IBM watsonx.ai."
2659	Frequently asked questions	"IBM watsonx.ai questions
How do I upgrade watsonx.ai and watsonx.governance?
When you're ready to upgrade any of the underlying services for watsonx.ai or watsonx.governance, you can upgrade in place without losing any of your work or data.  
You must be the owner or administrator of the IBM Cloud account for a service to upgrade it. See Upgrading services on watsonx."
2660	Frequently asked questions	"IBM watsonx.ai questions
How can I get the most runtime from my Watson Studio Lite plan?
The Watson Studio Lite plan allows for 10 CUH per month. You can maximize your available CUH by setting your assets to use environments with lower CUH rates. For example, you can change your notebook environment. To see the available environments and the required CUH, go to the Services catalog page for Watson Studio."
2661	Frequently asked questions	"IBM watsonx.ai questions
How do I find my IBM Cloud account owner?
If you have an enterprise account or work in an IBM Cloud that you don't own, you might need to ask an account owner to give you access to a workspace or another role.  
To find your IBM Cloud account owner:  
From the navigation menu, choose Administration > Access (IAM). From the avatar menu, make sure you're in the right account, or switch accounts, if necessary. Click Users, and find the username with the word owner next to it.  
To understand roles, see Roles for IBM watsonx. To determine your roles, see Determine your roles."
2662	Frequently asked questions	"IBM watsonx.ai questions
Can I provide feedback?
Yes, we encourage feedback as we continue to develop this platform. From the navigation menu, select Support > Share an idea."
2663	Frequently asked questions	"Foundation models
What foundation models are available and where do they come from?
See the complete list of supported foundation models."
2664	Frequently asked questions	"Foundation models
What data was used to train foundation models?
Links to details about each model, including pretraining data and fine-tuning, are available here: Supported foundation models."
2665	Frequently asked questions	"Foundation models
Do I need to check generated output for biased, inappropriate, or incorrect content?
Yes, you must review the generated output of foundation models. Third Party models have been trained with data that might contain biases and inaccuracies and can generate outputs containing misinformation, obscene or offensive language, or discriminatory content.  
In the Prompt Lab, when you toggle AI guardrails on, any sentence in the prompt text or model output that contains harmful language will be replaced with a message saying potentially harmful text has been removed.  
See Avoiding undesirable output."
2666	Frequently asked questions	"Foundation models
Is there a limit to how much text generation I can do?
With the free trial of watsonx.ai, you can use up to 25,000 tokens per month. Your token usage is the sum of your input and output tokens.  
With a paid service plan, there is no token limit, but you are charged for the tokens that you submit as input plus the tokens that you receive in the generated output.  
See Watson Machine Learning plans."
2667	Frequently asked questions	"Foundation models
Does prompt engineering train the foundation model?
No, submitting prompts to a foundation model does not train the model. The models available in watsonx.ai are pretrained, so you do not need to train the models before you use them.  
See Prompt tips."
2668	Frequently asked questions	"Foundation models
Does IBM have access to or use my data in any way?
No, IBM does not have access to your data.  
Your work on watsonx.ai, including your data and the models that you create, are private to your account:  
Your data is accessible only by you. Your data is used to train only your models. Your data will never be accessible or used by IBM or any other person or organization. Your data is stored in dedicated storage buckets and is encrypted at rest and in motion. Your models are accessible only by you. Your models will never be accessible or used by IBM or any other person or organization. Your models are secured in the same way as your data.  
Learn more about security and your options:  
Security and privacy of foundation models Security for IBM watsonx.ai Data security"
2669	Frequently asked questions	"Foundation models
What APIs are available?
You can prompt foundation models in watsonx.ai programmatically using the Python library.  
See Foundation models Python library."
2670	Frequently asked questions	"Projects
How do I load very large files to my project?
You can't load data files larger than 5 GB to your project. If your files are larger, you must use the Cloud Object Storage API and load the data in multiple parts. See the curl commands for working with Cloud Object Storage directly on IBM Cloud.  
See Adding very large objects to a project's Cloud Object Storage."
2671	Frequently asked questions	"IBM Cloud Object Storage
What is saved in IBM Cloud Object Storage for workspaces?
When you create a project or deployment space, you specify a IBM Cloud Object Storage and create a bucket that is dedicated to that workspace. These types of objects are stored in the IBM Cloud Object Storage bucket for the workspace:  
Files for data assets that you uploaded into the workspace. Files associated with assets that run in tools, such as, notebooks and models. Metadata about assets, such as the asset type, format, and tags."
2672	Frequently asked questions	"IBM Cloud Object Storage
Do I need to upgrade IBM Cloud Object Storage when I upgrade other services?
You must upgrade your IBM Cloud Object Storage instance only when you run out of storage space. Other services can use any IBM Cloud Object Storage plan and you can upgrade any service or your IBM Cloud Object Storage service independently."
2673	Frequently asked questions	"IBM Cloud Object Storage
Why am I unable to add storage to an existing project or to see the IBM Cloud Object Storage selection in the New Project dialog?
IBM Cloud Object Storage requires an extra step for users who do not have administrative privileges for it. The account administrator must enable nonadministrative users to create projects.  
If you have administrator privileges and do not see the latest IBM Cloud Object Storage, try again later because server-side caching might cause a delay in rendering the latest values."
2674	Frequently asked questions	"Notebooks
Can I install libraries or packages to use in my notebooks?
You can install Python libraries and R packages through a notebook, and those libraries and packages will be available to all your notebooks that use the same environment template. For instructions, see Import custom or third-party libraries. If you get an error about missing operating system dependencies when you install a library or package, notify IBM Support. To see the preinstalled libraries and packages and the libraries and packages that you installed, from within a notebook, run the appropriate command:  
Python: !pip list R: installed.packages()"
2675	Frequently asked questions	"Notebooks
Can I call functions that are defined in one notebook from another notebook?
There is no way to call one notebook from another notebook on the platform. However, you can put your common code into a library outside of the platform and then install it."
2676	Frequently asked questions	"Notebooks
Can I add arbitrary notebook extensions?
No, you can't extend your notebook capabilities by adding arbitrary extensions as a customization because all notebook extensions must be preinstalled."
2677	Frequently asked questions	"Notebooks
How do I access the data from a CSV file in a notebook?
After you load a CSV file into object storage, load the data by clicking the Code snippets icon () in an opened notebook, clicking Read data and selecting the CSV file from the project. Then, click in an empty code cell in your notebook and insert the generated code."
2678	Frequently asked questions	"Notebooks
How do I access the data from a compressed file in a notebook?
After you load the compressed file to object storage, get the file credentials by clicking the Code snippets icon () in an opened notebook, clicking Read data and selecting the compressed file from the project. Then, click in an empty code cell in your notebook and load the credentials to the cell. Alternatively, click to copy the credentials to the clipboard and paste them into your notebook."
2679	Frequently asked questions	"Security and reliability
How secure is IBM watsonx?
The IBM watsonx platform is very secure and resilient. See Security of IBM watsonx."
2680	Frequently asked questions	"Security and reliability
Is my data and notebook protected from sharing outside of my collaborators?
The data that is loaded into your project and notebooks is secure. Only the collaborators in your project can access your data or notebooks. Each platform account acts as a separate tenant of the Spark and IBM Cloud Object Storage services. Tenants cannot access other tenant's data.  
If you want to share your notebook with the public, then hide your data service credentials in your notebook. For the Python and R languages, enter the following syntax: # @hidden_cell  
Be sure to save your notebook immediately after you enter the syntax to hide cells with sensitive data.  
Only then should you share your work."
2681	Frequently asked questions	"Security and reliability
Do I need to back up my notebooks?
No. Your notebooks are stored in IBM Cloud Object Storage, which provides resiliency against outages."
2682	Frequently asked questions	"Sharing and collaboration
What are the implications of sharing a notebook?
When you share a notebook, the permalink never changes. Any person with the link can view your notebook. You can stop sharing the notebook by clearing the checkbox to share it. Updates are not automatically shared. When you update your notebook, you can sync the shared notebook by reselecting the checkbox to share it."
2683	Frequently asked questions	"Sharing and collaboration
How can I share my work outside of RStudio?
One way of sharing your work outside of RStudio is connecting it to a shared GitHub repository that you and your collaborators can work from. Read this blog post for more information.  
However, the best method to share your work with the members of a project is to use notebooks in the project that uses the R kernel.  
RStudio is a great environment to work in for prototyping and working individually on R projects, but it is not yet integrated with projects."
2684	Frequently asked questions	"Sharing and collaboration
How do I share my SPSS Modeler flow with another project?
By design, modeler flows can be used only in the project where the flow is created or imported. If you need to use a modeler flow in a different project, you must download the flow from current project (source project) to your local environment and then import the flow to another project (target project)."
2685	Frequently asked questions	"IBM Watson Machine Learning
How do I run an AutoAI experiment?
Go to Creating an AutoAI experiment from sample data to watch a short video to see how to create and run an AutoAI experiment and then follow a tutorial to set up your own sample."
2686	Frequently asked questions	"IBM Watson Machine Learning
What is available for automated model building?
The AutoAI graphical tool automatically analyzes your data and generates candidate model pipelines that are customized for your predictive modeling problem.  These model pipelines are created iteratively as AutoAI analyzes your data set and discovers data transformations, algorithms, and parameter settings that work best for your problem setting.  Results are displayed on a leaderboard, showing the automatically generated model pipelines ranked according to your problem optimization objective. For details, see AutoAI overview."
2687	Frequently asked questions	"IBM Watson Machine Learning
What frameworks and libraries are supported for my machine learning models?
You can use popular tools, libraries, and frameworks to train and deploy machine learning models by using IBM Watson Machine Learning. The supported frameworks topic lists supported versions and features, as well as deprecated versions scheduled to be discontinued."
2688	Frequently asked questions	"IBM Watson Machine Learning
What is an API Key?
API keys allow you to easily authenticate when using the CLI or APIs that can be used across multiple services. API Keys are considered confidential since they are used to grant access. Treat all API keys as you would a password since anyone with your API key can impersonate your service."
2689	Frequently asked questions	"Watson OpenScale
What is Watson OpenScale
IBM Watson OpenScale tracks and measures outcomes from your AI models, and helps ensure they remain fair, explainable, and compliant wherever your models were built or are running. Watson OpenScale also detects and helps correct the drift in accuracy when an AI model is in production"
2690	Frequently asked questions	"Watson OpenScale
How do I convert a prediction column from an integer data type to a categorical data type?
For fairness monitoring, the prediction column allows only an integer numerical value even though the prediction label is categorical. How do I configure a categorical feature that is not an integer? Is a manual conversion required?  
The training data might have class labels such as “Loan Denied”, “Loan Granted”. The prediction value that is returned by IBM Watson Machine Learning scoring end point has values such as “0.0”, “1.0"". The scoring end point also has an optional column that contains the text representation of prediction. For example, if prediction=1.0, the predictionLabel column might have a value “Loan Granted”. If such a column is available, when you configure the favorable and unfavorable outcome for the model, specify the string values “Loan Granted” and “Loan Denied”. If such a column is not available, then you need to specify the integer and double values of 1.0, 0.0 for the favorable, and unfavorable classes.  
IBM Watson Machine Learning has a concept of output schema that defines the schema of the output of IBM Watson Machine Learning scoring end point and the role for the different columns. The roles are used to identify which column contains the prediction value, which column contains the prediction probability, and the class label value, and so on. The output schema is automatically set for models that are created by using model builder. It can also be set by using the IBM Watson Machine Learning Python client. Users can use the output schema to define a column that contains the string representation of the prediction. Set the modeling_role for the column to ‘decoded-target’. Read the documentation for the IBM Watson Machine Learning Python client. Search for “OUTPUT_DATA_SCHEMA” to understand the output schema and the API to use is to store_model API that accepts the OUTPUT_DATA_SCHEMA as a parameter."
2691	Frequently asked questions	"Watson OpenScale
Why does Watson OpenScale need access to training data?
You must either provide Watson OpenScale access to training data that is stored in Db2 or IBM Cloud Object Storage, or you must run a Notebook to access the training data.  
Watson OpenScale needs access to your training data for the following reasons:  
To generate contrastive explanations: To create explanations, access to statistics, such as median value, standard deviation, and distinct values from the training data is required. To display training data statistics: To populate the bias details page, Watson OpenScale must have training data from which to generate statistics. To build a drift detection model: The Drift monitor uses training data to create and calibrate drift detection.  
In the Notebook-based approach, you are expected to upload the statistics and other information when you configure a deployment in Watson OpenScale. Watson OpenScale no longer has access to the training data outside of the Notebook, which is run in your environment. It has access only to the information uploaded during the configuration."
2692	Frequently asked questions	"Watson OpenScale
What does it mean if the fairness score is greater than 100 percent?
Depending on your fairness configuration, your fairness score can exceed 100 percent. It means that your monitored group is getting relatively more “fair” outcomes as compared to the reference group. Technically, it means that the model is unfair in the opposite direction."
2693	Frequently asked questions	"Watson OpenScale
How is model bias mitigated by using Watson OpenScale?
The debiasing capability in Watson OpenScale is enterprise grade. It is robust, scalable and can handle a wide variety of models. Debiasing in Watson OpenScale consists of a two-step process: Learning Phase: Learning customer model behavior to understand when it acts in a biased manner.  
Application Phase: Identifying whether the customer’s model acts in a biased manner on a specific data point and, if needed, fixing the bias. For more information, see Debiasing options."
2694	Frequently asked questions	"Watson OpenScale
Is it possible to check for model bias on sensitive attributes, such as race and sex, even when the model is not trained on them?
Yes. Recently, Watson OpenScale delivered a ground-breaking feature called “Indirect Bias detection.” Use it to detect whether the model is exhibiting bias indirectly for sensitive attributes, even though the model is not trained on these attributes."
2695	Frequently asked questions	"Watson OpenScale
Is it possible to mitigate bias for regression-based models?
Yes. You can use Watson OpenScale to mitigate bias on regression-based models. No additional configuration is needed from you to use this feature. Bias mitigation for regression models is done out-of-box when the model exhibits bias."
2696	Frequently asked questions	"Watson OpenScale
What are the different methods of debiasing in Watson OpenScale?
You can use both Active Debiasing and Passive Debiasing for debiasing. For more information, see Debiasing options."
2697	Frequently asked questions	"Watson OpenScale
Configuring a model requires information about the location of the training data and the options are Cloud Object Storage and Db2. If the data is in Netezza, can Watson OpenScale use Netezza?
Use this Watson OpenScale Notebook to read the data from Netezza and generate the training statistics and also the drift detection model."
2698	Frequently asked questions	"Watson OpenScale
Why doesn't Watson OpenScale see the updates that were made to the model?
Watson OpenScale works on a deployment of a model, not on the model itself. You must create a new deployment and then configure this new deployment as a new subscription in Watson OpenScale. With this arrangement, you are able to compare the two versions of the model."
2699	Frequently asked questions	"Watson OpenScale
What are the various kinds of risks associated in using a machine learning model?
Multiple kinds of risks that are associated with machine learning models, such as any change in input data that is also known as Drift can cause the model to make inaccurate decisions, impacting business predictions. Training data can be cleaned to be free from bias but runtime data might induce biased behavior of model.  
Traditional statistical models are simpler to interpret and explain, but unable to explain the outcome of the machine learning model can pose a serious threat to the usage of the model."
2700	Frequently asked questions	"Watson OpenScale
Must I keep monitoring the Watson OpenScale dashboard to make sure that my models behave as expected?
No, you can set up email alerts for your production model deployments in Watson OpenScale. You receive email alerts whenever a risk evaluation test fails, and then you can come and check the issues and address them."
2701	Frequently asked questions	"Watson OpenScale
In Watson OpenScale, what data is used for Quality metrics computation?
Quality metrics are calculated that use manually labeled feedback data and monitored deployment responses for this data."
2702	Frequently asked questions	"Watson OpenScale
In Watson OpenScale, can the threshold be set for a metric other than 'Area under ROC' during configuration?
No, currently, the threshold can be set only for the 'Area under ROC' metric."
2703	Feature differences between watsonx deployments	"IBM watsonx as a Service and watsonx on Cloud Pak for Data software have some differences in features and implementation. IBM watsonx as a Service is a set of IBM Cloud services. Watsonx services on Cloud Pak for Data 4.8 are offered as software that you must install and maintain. Services that are available on both deployments also have differences in features on IBM watsonx as a Service compared to watsonx software on Cloud Pak for Data 4.8.  
Platform differences Common features across services Watson Studio Watson Machine Learning Watson OpenScale"
2704	Feature differences between watsonx deployments	"Platform differences
IBM watsonx as a Service and watsonx software on Cloud Pak for Data share a common code base, however, they differ in the following key ways:"
2705	Feature differences between watsonx deployments	"Common core functionality across services
The following core functionality that is provided with the platform is effectively the same for services on IBM watsonx as a Service and watsonx software on Cloud Pak for Data 4.8:  
Global search for assets across the platform The Platform assets catalog for sharing connections across the platform Role-based user management within collaborative workspaces across the platform Common infrastructure for assets and workspaces A services catalog for adding services View compute usage from the Administration menu  
The following table describes differences in core functionality across services between IBM watsonx as a Service and watsonx software on Cloud Pak for Data 4.8:"
2706	Feature differences between watsonx deployments	"Watson Studio
The following Watson Studio features are effectively the same on IBM watsonx as a Service and watsonx software on Cloud Pak for Data 4.8:  
Collaboration in projects and deployment spaces Accessing project assets programmatically Project import and export by using a project ZIP file Jupyter notebooks Job scheduling Data Refinery Watson Natural Language Processing for Python  
This table describes the feature differences between the Watson Studio service on the as-a-service and software deployment environments, differences between offering plans, and whether additional services are required. For more information about feature differences between offering plans on IBM watsonx, see Watson Studio offering plans."
2707	Feature differences between watsonx deployments	"Watson Machine Learning
The following Watson Machine Learning features are effectively the same on IBM watsonx as a Service and watsonx software on Cloud Pak for Data 4.8:  
Collaboration in projects and deployment spaces Deploy models Deploy functions Watson Machine Learning REST APIs Watson Machine Learning Python client Create online deployments Scale and update deployments Define and use custom components Use Federated Learning to train a common model with separate and secure data sources Monitor deployments across spaces Updated forms for testing online deployment Use nested pipelines AutoAI data imputation AutoAI fairness evaluation AutoAI time series supporting features  
This table describes the differences in features between the Watson Machine Learning service on the as-a-service and software deployment environments, differences between offering plans, and whether additional services are required. For details about functionality differences between offering plans on IBM watsonx, see Watson Machine Learning offering plans."
2708	Feature differences between watsonx deployments	"watsonx.governance
The following governance features are effectively the same on IBM watsonx as a Service and watsonx software on Cloud Pak for Data 4.8:  
Evaluate deployments for fairness Evaluate the quality of deployments Monitor deployments for drift View and compare model results in an Insights dashboard Add deployments from the machine learning provider of your choice Set alerts to trigger when evaluations fall below a specified threshold Evaluate deployments in a user interface or notebook Custom evaluations and metrics View details about evaluations in model factsheets  
This table describes the differences in features between the Watson OpenScale service on the as-a-service and software deployment environments, differences between offering plans, and whether additional services are required."
2709	Feature differences between watsonx deployments	"Learn more
Services for IBM watsonx Services for Cloud Pak for Data 4.8 Cloud deployment environment options for Cloud Pak for Data 4.8  
Parent topic: Overview of watsonx"
2710	Managing users and access	As the account owner or administrator, you add the people in your organization to the IBM Cloud account and then assign them access permissions using roles that provide access to the services that they need.
2711	Managing users and access	"User management on IBM Cloud
People who work in IBM watsonx must have a valid IBMid and be a member of the IBM Cloud account. Alternately, they must have a valid ID in a supported user registry. User management includes adding users to the account and then assigning appropriate roles to provide access to the services and actions that they need. See Adding users to the account."
2712	Managing users and access	"Access management using IBM Cloud Identity and Access Management (IAM)
You control the actions that a user can perform for a specific service by assigning permissions with IBM Cloud IAM. You create user access groups containing roles to provide permissions for users. You can also assign roles and permissions to individual users. If necessary, you can create custom roles to satisfy your business requirements."
2713	Managing users and access	"Learn more
Signing up for your organization's watsonx account Logging in to watsonx.ai through IBM Cloud App ID (beta) IBM Cloud docs: Assigning access to resources by using access groups IBM Cloud docs: Creating custom roles IBM Cloud docs: IAM access IBM Cloud docs: What is IBM Cloud Identity and Access Management IBM Cloud docs: Setting up access groups IBM Cloud docs: Best practices for organizing resources and assigning access  
Parent topic: Setting up the platform"
2714	What's new	"Check back each week to learn about new features and updates for IBM watsonx.ai and IBM watsonx.governance.  
Tip: Occasionally, you must take a specific action after an update. To see all required actions, search this page for “Action required”."
2715	What's new	"Week ending 19 April 2024
New Meta Llama 3 foundation models are now available
18 Apr 2024  
The following Llama 3 foundation models provided by Meta are available for inferencing from watsonx.ai:  
llama-3-8b-instruct llama-3-70b-instruct  
The new Llama 3 foundation models are instruction fine-tuned language models that can support various use cases.  
This latest release of Llama is trained with more tokens and applies new post-training procedures. The result is foundation models with better language comprehension, reasoning, code generation, and instruction-following capabilities.  
For more information, see Supported foundation models."
2716	What's new	"Week ending 19 April 2024
Introducing IBM embedding support for enhanced text matching and retrieval
18 Apr 2024  
You can now use the IBM embeddings API and IBM embedding models for transforming input text into vectors to more accurately compare and retrieve similar text.  
The following IBM Slate embedding models are available:  
slate.125m.english.rtrvr slate.30m.english.rtrvr  
For more information, see Text embedding generation.  
For pricing details, see Watson Machine Learning plans."
2717	What's new	"Week ending 19 April 2024
IBM watsonx.governance is included when you sign up for IBM watsonx.ai
18 Apr 2024  
If you sign up for watsonx.ai in the Dallas region, watsonx.governance is now included automatically. See Signing up for IBM watsonx as a Service."
2718	What's new	"Week ending 19 April 2024
Evaluate machine learning deployments in spaces
18 Apr 2024  
Configure watsonx.governance evaluations in your deployment spaces to gain insights about your machine learning model performance. For example, evaluate a deployment for bias or monitor a deployment for drift. When you configure evaluations, you can analyze evaluation results and model transaction records directly in your spaces.  
For more information, see Evaluating deployments in spaces."
2719	What's new	"Week ending 19 April 2024
A Korean-language foundation model is available in the Tokyo region
18 Apr 2024  
The llama2-13b-dpo-v7 foundation model provided by Minds & Company and based on the Llama 2 foundation model from Meta is available in the Tokyo region.  
The llama2-13b-dpo-v7 foundation model specializes in conversational tasks in Korean and English. You can also use the llama2-13b-dpo-v7 foundation model for general purpose tasks in the Korean language.  
For more information, see Supported foundation models."
2720	What's new	"Week ending 19 April 2024
A mixtral-8x7b-instruct-v01 foundation model is available for inferencing
18 Apr 2024  
The mixtral-8x7b-instruct-v01 foundation model from Mistral AI is available for inferencing from watsonx.ai. The mixtral-8x7b-instruct-v01 foundation model is a pretrained generative model that uses a sparse mixture-of-experts network to generate text more efficiently.  
You can use the mixtral-8x7b-instruct-v01 model for general-purpose tasks, including classification, summarization, code generation, language translation, and more. For more information, see Supported foundation models.  
The mixtral-8x7b-instruct-v01-q foundation model is deprecated and will be withdrawn on 23 May 2024. Revise any prompts that use this foundation model.  
Deprecation date: 19 April 2024 Withdrawal date: 23 May 2024 Alternative model: mixtral-8x7b-instruct-v01  
Inference requests that are submitted to the mixtral-8x7b-instruct-v01-q model by using the API continue to generate output, but include a warning message about the upcoming model withdrawal. Starting on 23 May 2024, API requests for inferencing the models will not generate output.  
For more information about deprecation and withdrawal, see Foundation model lifecycle."
2721	What's new	"Week ending 19 April 2024
A modification to the granite-20b-multilingual foundation model is introduced
18 Apr 2024  
The latest version of the granite-20b-multilingual is 1.1.0. The modification includes improvements that were gained by applying a novel AI alignment technique to the version 1.0 model. AI alignment involves using fine-tuning and reinforcement learning techniques to guide the model to return outputs that are as helpful, truthful, and transparent as possible.  
For more information about this foundation model, see Supported foundation models."
2722	What's new	"Week ending 12 April 2024
Prompt-tune the granite-13b-instruct-v2 foundation model
11 Apr 2024  
The Tuning Studio now supports tuning the granite-13b-instruct-v2 foundation model in addition to the flan-t5-xl-3b and llama-2-13b-chat foundation models. For more information, see Tuning a foundation model.  
The experiment configuration settings for tuning the granite-13b-instruct-v2 foundation model change to apply the best default values depending on your task. The tuning evaluation guidelines help you to analyze the experiment results and adjust experiment configuration settings based on your findings. For more information, see Evaluating the results of a tuning experiment."
2723	What's new	"Week ending 12 April 2024
An Arabic-language foundation model is available in the Frankfurt region
11 Apr 2024  
The jais-13b-chat foundation model provided by Inception, Mohamed bin Zayed University of Artificial Intelligence, and Cerebras Systems is available in the Frankfurt region.  
The jais-13b-chat foundation model specializes in conversational tasks in Arabic and English. You can also use the jais-13b-chat foundation model for general purpose tasks in the Arabic language, including language translation between Arabic and English.  
For more information, see Supported foundation models."
2724	What's new	"Week ending 12 April 2024
View the full text of a prompt in Prompt Lab
11 Apr 2024  
Now you can review the full prompt text that will be submitted to the foundation model, which is useful when your prompt includes prompt variables or when you're working in structured mode or chat mode.  
For more information, see Prompt Lab."
2725	What's new	"Week ending 12 April 2024
The deprecated Granite version 1 models are withdrawn
11 Apr 2024  
The following foundation models are now withdrawn:  
granite-13b-chat-v1 granite-13b-instruct-v1  
Revise any prompts that use these foundation models to use the IBM Granite v2 foundation models. For more information about foundation model deprecation and withdrawal, see Foundation model lifecycle."
2726	What's new	"Week ending 5 April 2024
Use pivot tables to display data aggregated in Decision Optimization experiments
5 Apr 2024  
You can now use pivot tables to display both input and output data aggregated in the Visualization view in Decision Optimization experiments. For more information, see Visualization widgets in Decision Optimization experiments."
2727	What's new	"Week ending 5 April 2024
New watsonx.ai tutorial and video
04 Apr 2024  
Try the new tutorial to see how to use watsonx.ai in an end-to-end use case from data preparation through prompt engineering."
2728	What's new	"Week ending 15 March 2024
The watsonx.ai API is available
14 Mar 2024  
The watsonx.ai API is generally available. Use the watsonx.ai API to work with foundation models programmatically. For more information, see the API reference.  
The API version is 2024-03-14.  
You can continue to use the Python library that is available for working with foundation models from a notebook. For more information, see Python library."
2729	What's new	"Week ending 15 March 2024
New foundation models are available in Dallas, Frankfurt, and Tokyo
14 Mar 2024  
The following foundation models are now available for inferencing from watsonx.ai:  
granite-20b-multilingual: A foundation model from the IBM Granite family that you can use for various generative tasks in English, German, Spanish, French, and Portuguese.  
codellama-34b-instruct-hf: A programmatic code generation model from Code Llama that is based on Llama 2 from Meta. You can use codellama-34b-instruct-hf to create prompts for generating code based on natural language inputs, and for completing and debugging code.  
For more information, see Supported foundation models."
2730	What's new	"Week ending 8 March 2024
The Tuning Studio is available in Frankfurt
7 Mar 2024  
The Tuning Studio is now available to users of paid plans in the Frankfurt region. Tuning Studio helps you to guide a foundation model to return useful output. You can tune both the flan-t5-xl-3b and llama-2-70b-chat foundation models when you use the Tuning Studio in Frankfurt.  
For more information, see Tuning Studio."
2731	What's new	"Week ending 8 March 2024
Prompt-tune the llama-2-13b-chat foundation model in the Tokyo region
7 Mar 2024  
The Tuning Studio now supports tuning the llama-2-13b-chat foundation model in the Tokyo region. First, engineer prompts for the larger llama-2-70b-chat model in the Prompt Lab to find effective prompt inputs for your use case. Then tune the smaller version of the Llama 2 model to generate comparable, if not better outputs with zero-shot prompts.  
For more information, see Tuning Studio."
2732	What's new	"Week ending 8 March 2024
Lower price for Mixtral8x7b model
5 Mar 2024  
The foundation model mixtral-8x7b-instruct-v01-q is reclassified from Class 2: $0.0018/Resource Unit to Class 1: $0.0006/Resource Unit, making it more cost effective to run inferencing tasks against this model. The reclassification applies to all regions where mixtral-8x7b-instruct-v01-q is available.  
For more information, see Supported foundation models.  
For pricing details, see Watson Machine Learning plans."
2733	What's new	"Week ending 8 March 2024
AI risk atlas is updated and enhanced
5 Mar 2024  
You can now find the following new and enhanced content in the AI risk atlas:  
A new category of non-technical risks that spans governance, legal compliance, and societal impact risks New examples for risks Clearer definitions of risks  
See AI risk atlas."
2734	What's new	"Week ending 8 March 2024
New use cases for watsonx
5 Mar 2024  
The watsonx uses cases are available to help you see how you can use our products, services, and tools:  
watsonx.ai use case: This use case covers how you can transform your business processes with AI-driven solutions by integrating machine learning and generative AI into your operational framework. watsonx.governance use case: This use case covers how you can erive responsible, transparent, and explainable AI workflows with an integrated system for tracking, monitoring, and retraining AI models.  
See watsonx use cases."
2735	What's new	"Week ending 1 March 2024
Chat mode is available in Prompt Lab
29 Feb 2024  
Chat mode in Prompt Lab is a simple chat interface that makes it easier to experiment with foundation models. Chat mode augments the already available structured and freeform modes that are useful when building few- or many-shot prompts for tasks such as extraction, summarization, and classification. Use Chat mode to simulate question-answering or conversational interactions for chatbot and virtual assistant use cases.  
For more information, see Prompt Lab."
2736	What's new	"Week ending 1 March 2024
A Japanese-language Granite model is available in the Tokyo region
29 Feb 2024  
The granite-8b-japanese foundation model provided by IBM is available from watsonx.ai in the Tokyo region. The granite-8b-japanese foundation model is based on the IBM Granite Instruct model and is trained to understand and generate Japanese text.  
You can use the granite-8b-japanese foundation model for general purpose tasks in the Japanese language, such as classification, extraction, question-answering, and for language translation between Japanese and English.  
For more information, see Supported foundation models."
2737	What's new	"Week ending 23 February 2024
Lower price for Granite-13b models
21 Feb 2024  
Granite-13b models are reclassified from Class 2: $0.0018/Resource Unit to Class 1: $0.0006/Resource Unit, making it more cost effective to run inferencing tasks against these models. The reclassification applies to the following models in all regions where they are available:  
granite-13b-chat-v2 granite-13b-chat-v1 granite-13b-instruct-v2 granite-13b-instruct-v1  
For more information on these models, see Supported foundation models.  
For pricing details, see Watson Machine Learning plans."
2738	What's new	"Week ending 16 February 2024
New shortcut to start working on common tasks
15 Feb 2024  
You can now start a common task in your project by clicking on a tile in the Start working section in the Overview tab. Use these shortcuts to start adding collaborators and data, and to experiment with and build models. Click View all to jump to a selection of tools."
2739	What's new	"Week ending 16 February 2024
New mixtral-8x7b-instruct-v01-q foundation model for general-purpose tasks
15 Feb 2024  
The mixtral-8x7b-instruct-v01-q foundation model provided by Mistral AI and quantized by IBM is available from watsonx.ai. The mixtral-8x7b-instruct-v01-q foundation model is a quantized version of the Mixtral 8x7B Instruct foundation model from Mistral AI.  
You can use this new model for general-purpose tasks, including classification, summarization, code generation, language translation, and more. For more information, see Supported foundation models.  
The following models are deprecated and will be withdrawn soon. Revise any prompts that use these foundation models to use another foundation model, such as mixtral-8x7b-instruct-v01-q.  
Inference requests that are submitted to these models by using the API continue to generate output, but include a warning message about the upcoming model withdrawal. When the withdrawal date is reached, API requests for inferencing the models will not generate output.  
For more information about deprecation and withdrawal, see Foundation model lifecycle."
2740	What's new	"Week ending 16 February 2024
A modification to the granite-13b-chat-v2 foundation model is available
15 Feb 2024  
The latest version of the granite-13b-chat-v2 is 2.1.0. The modification includes improvements that were gained by applying a novel AI alignment technique to the version 2.0.0 model. AI alignment involves using fine-tuning and reinforcement learning techniques to guide the model to return outputs that are as helpful, truthful, and transparent as possible. For more information, see the What is AI alignment? blog post from IBM Research."
2741	What's new	"Week ending 16 February 2024
New watsonx tutorial and video
15 Feb 2024  
Try the new watsonx.governance tutorial to help you learn how to evaluate a machine learning model for fairness, accuracy, drift, and explainability with Watson OpenScale."
2742	What's new	"Week ending 09 February 2024
More task-oriented Decision Optimization documentation
9 Feb 2024  
You can now more easily find the right information for creating and configuring Decision Optimization experiments. See Decision Optimization experiments and its subsections."
2743	What's new	"Week ending 09 February 2024
IBM Cloud Data Engine connection is deprecated
8 Feb 2022  
The IBM Cloud Data Engine connection is deprecated and will be discontinued in a future release. See Deprecation of Data Engine for important dates and details."
2744	What's new	"Week ending 09 February 2024
New Spark 3.4 environment for running Data Refinery flow jobs
9 Feb 2024  
When you select an environment for a Data Refinery flow job, you can now select Default Spark 3.4 & R 4.2, which includes enhancements from Spark.  
The Default Spark 3.3 & R 4.2 environment is deprecated and will be removed in a future update.  
Update your Data Refinery flow jobs to use the new Default Spark 3.4 & R 4.2 environment. For details, see Compute resource options for Data Refinery in projects."
2745	What's new	"Week ending 2 February 2024
Samples collection renamed to Resource hub
2 Feb 2024  
The Samples collection is renamed to Resource hub to better reflect the content. The Resource hub contains foundation models and sample projects, data sets, and notebooks. See Resource hub."
2746	What's new	"Week ending 2 February 2024
IBM Cloud Databases for DataStax connection is discontinued
2 Feb 2024  
The IBM Cloud Databases for DataStax connection has been removed from IBM watsonx.ai."
2747	What's new	"Week ending 2 February 2024
Dremio connection requires updates
2 Feb 2024  
Previously the Dremio connection used a JDBC driver. Now the connection uses a driver based on Arrow Flight.  
Action required Important: Update the connection properties. Different changes apply to a connection for a Dremio Software (on-prem) instance or a Dremio Cloud instance.  
The new default port number that is used by Flight is 32010. You can confirm the port number in the dremio.conf file. See Configuring via dremio.conf for information.  
Additionally, Dremio no longer supports connections with IBM Cloud Satellite.  
Log into Dremio and generate a personal access token. For instructions see Personal Access Tokens. In IBM watsonx in the Create connection: Dremio form, change the authentication type to Personal Access Token and add the token information. (The Username and password authentication can no longer be used to connect to a Dremio Cloud instance.) Select Port is SSL-enabled.  
If you use the default hostname for a Dremio Cloud instance, you need to change it:  
Change sql.dremio.cloud to data.dremio.cloud Change sql.eu.dremio.cloud to data.eu.dremio.cloud"
2748	What's new	"Week ending 2 February 2024
Prompt-tune the llama-2-13b-chat foundation model
1 Feb 2024  
The Tuning Studio now supports tuning the llama-2-13b-chat foundation model. First, engineer prompts for the larger llama-2-70b-chat model in the Prompt Lab to find effective prompt inputs for your use case. Then tune the smaller version of the Llama 2 model to generate comparable, if not better outputs with zero-shot prompts. The llama-2-13b-model is available for prompt tuning in the Dallas region. For more information, see Tuning Studio."
2749	What's new	"Week ending 26 January 2024
AutoAI supports ordered data for all experiments
25 Jan 2024  
You can now specify ordered data for all AutoAI experiments rather than just time series experiments. Specify if your training data is ordered sequentially, according to a row index. When input data is sequential, model performance is evaluated on newest records instead of a random sampling, and holdout data uses the last n records of the set rather than n random records. Sequential data is required for time series experiments but optional for classification and regression experiments."
2750	What's new	"Week ending 26 January 2024
Q&A with RAG accelerator
26 Jan 2024  
You can now implement a question and answer solution that uses retrieval augmented generation by importing a sample project. The sample project contains notebooks and other assets that convert documents from HTML or PDF to plain text, import document segments into an Elasticsearch vector index, deploy a Python function that queries the vector index, retrieve top N results, run LLM inference to generate an answer to the question, and check the answer for hallucinations.  
Try the Q&A with RAG accelerator."
2751	What's new	"Week ending 26 January 2024
Set to dark theme
25 Jan 2024  
You can now set your watsonx user interface to dark theme. Click your avatar and select Profile and settings to open your account profile. Then, set the Dark theme switch to on. Dark theme is not supported in RStudio and Jupyter notebooks. For information on managing your profile, see Managing your settings."
2752	What's new	"Week ending 26 January 2024
IBM watsonx.ai is available in the Tokyo region
25 Jan 2024  
Watsonx.ai is now generally available in the Tokyo data center and can be selected as the preferred region when signing-up. The Prompt Lab and foundation model inferencing are supported in the Tokyo region for these models:  
elyza-japanese-llama-2-7b-instruct flan-t5-xl-3b flan-t5-xxl-11b flan-ul2-20b granite-13b-chat-v2 granite-13b-instruct-v2 llama-2-70b-chat llama-2-13b-chat  
Also available from the Tokyo region:  
Prompt tuning the flan-t5-xl-3b foundation model with the Tuning Studio Generating tabular data with the Synthetic Data Generator to use for training models  
For more information on the supported models, see Supported foundation models available with watsonx.ai."
2753	What's new	"Week ending 26 January 2024
A Japanese-language Llama 2 model is available in the Tokyo region
25 Jan 2024  
The elyza-japanese-llama-2-7b-instruct foundation model provided by ELYZA, Inc is available from watsonx.ai instances in the Tokyo data center. The elyza-japanese-llama-2-7b-instruct model is a version of the Llama 2 model from Meta that was trained to understand and generate Japanese text.  
You can use this new model for general purpose tasks. It works well for Japanese-language classification and extraction and for translation between Japanese and English."
2754	What's new	"Week ending 12 January 2024
Support for IBM Runtime 22.2 deprecated in Watson Machine Learning
11 Jan 2024  
IBM Runtime 22.2 is deprecated and will be removed on 11 April 2024. Beginning 7 March 2024, you cannot create notebooks or custom environments by using the 22.2 runtimes. Also, you cannot train new models with software specifications that are based on the 22.2 runtime. Action required Update your assets and deployments to use IBM Runtime 23.1 before 7 March 2024.  
To learn more about migrating an asset to a supported framework and software specification, see Managing outdated software specifications or frameworks. To learn more about the notebook environment, see Compute resource options for the notebook editor in projects. To learn more about changing your environment, see Changing the environment of a notebook."
2755	What's new	"Week ending 12 January 2024
IBM Granite v1 foundation models are deprecated
11 Jan 2024  
The IBM Granite 13 billion-parameter v1 foundation models are deprecated and will be withdrawn on 11 April 2024. If you are using version 1 of the models, switch to using version 2 of the models instead.  
Inference requests that are submitted to the version 1 models by using the API continue to generate output, but include a warning message about the upcoming model withdrawal. Starting on 11 April 2024, API requests for inferencing the models will not generate output.  
For more information about IBM Granite foundation models, see Foundation models built by IBM. For more information about deprecation and withdrawal, see Foundation model lifecycle."
2756	What's new	"Week ending 15 December 2023
Create user API keys for jobs and other operations
15 Dec 2023  
Certain runtime operations in IBM watsonx, such as jobs and model training, require an API key as a credential for secure authorization. With user API keys, you can now generate and rotate an API key directly in IBM watsonx as needed to help ensure your operations run smoothly. The API keys are managed in IBM Cloud, but you can conveniently create and rotate them in IBM watsonx.  
The user API key is account-specific and is created from Profile and settings under your account profile.  
For more information, see Managing the user API key."
2757	What's new	"Week ending 15 December 2023
New watsonx tutorials and videos
15 Dec 2023  
Try the new watsonx.governance and watsonx.ai tutorials to help you learn how to tune a foundation model, and evaluate and track a prompt template.  
Find more watsonx.governance and watsonx.ai videos in the Video library."
2758	What's new	"Week ending 15 December 2023
New login session expiration and sign out due to inactivity
15 Dec 2023  
You are now signed out of IBM Cloud due to session expiration. Your session can expire due to login session expiration (24 hours by default) or inactivity (2 hours by default). You can change the default durations in the Access (IAM) settings in IBM Cloud. For more information, see Set the login session expiration."
2759	What's new	"Week ending 15 December 2023
IBM Cloud Databases for DataStax connector is deprecated
15 Dec 2023  
The IBM Cloud Databases for DataStax connector is deprecated and will be discontinued in a future release."
2760	What's new	"Week ending 08 December 2023
The Tuning Studio is available
7 Dec 2023  
The Tuning Studio helps you to guide a foundation model to return useful output. With the Tuning Studio, you can prompt tune the flan-t5-xl-3b foundation model to improve its performance on natural language processing tasks such as classification, summarization, and generation. Prompt tuning helps smaller, more computationally-efficient foundation models achieve results comparable to larger models in the same model family. By tuning and deploying a tuned version of a smaller model, you can reduce long-term inference costs. The Tuning Studio is available to users of paid plans in the Dallas region.  
For more information, see Tuning Studio. To get started, see Quick start: Tune a foundation model. To run a sample notebook, go to Tune a model to classify CFPB documents in watsonx."
2761	What's new	"Week ending 08 December 2023
New client properties in Db2 connections for workload management
8 Dec 2023  
You can now specify properties in the following fields for monitoring purposes: Application name, Client accounting information, Client hostname, and Client user. These fields are optional and are available for the following connections:  
IBM Db2 IBM Db2 for z/OS IBM Db2 Warehouse IBM Watson Query"
2762	What's new	"Week ending 1 December 2023
Watsonx.governance is available!
1 Dec 2023  
Watsonx.governance extends the governance capabilities of Watson OpenScale to evaluate foundation model assets as well as machine learning assets. For example, evaluate foundation model prompt templates for dimensions such as accuracy or to detect the presence of hateful and abusive speech. You can also define AI use cases to address business problems, then track prompt templates or model data in factsheets to support compliance and governance goals. Watsonx.governance plans and features are available only in the Dallas region.  
To view plan details, see watsonx.governance plans. For details on governance features, see watsonx.governance overview. To get started, see Provisioning and launching watsonx.governance."
2763	What's new	"Week ending 1 December 2023
Explore with the AI risk atlas
1 Dec 2023  
You can now explore some of the risks of working with generative AI, foundation models, and machine learning models. Read about risks for privacy, fairness, explainability, value alignment, and other areas. See AI risk atlas."
2764	What's new	"Week ending 1 December 2023
New versions of the IBM Granite models are available
30 Nov 2023  
The latest versions of the Granite models include these changes:  
granite-13b-chat-v2: Tuned to be better at question-answering, summarization, and generative tasks. With sufficient context, generates responses with the following improvements over the previous version:  
Generates longer, higher-quality responses with a professional tone Supports chain-of-thought responses Recognizes mentions of people and can detect tone and sentiment better Handles white spaces in input more gracefully  
Due to extensive changes, test and revise any prompts that were engineered for v1 before you switch to the latest version.  
granite-13b-instruct-v2: Tuned specifically for classification, extraction, and summarization tasks. The latest version differs from the previous version in the following ways:  
Returns more coherent answers of varied lengths and with a diverse vocabulary Recognizes mentions of people and can summarize longer inputs Handles white spaces in input more gracefully  
Engineered prompts that work well with v1 are likely to work well with v2 also, but be sure to test before you switch models.  
The latest versions of the Granite models are categorized as Class 2 models."
2765	What's new	"Week ending 1 December 2023
Some foundation models are now available at lower cost
30 Nov 2023  
Some popular foundation models were recategorized into lower-cost billing classes.  
The following foundation models changed from Class 3 to Class 2:  
granite-13b-chat-v1 granite-13b-instruct-v1 llama-2-70b  
The following foundation model changed from Class 2 to Class 1:  
llama-2-13b  
For more information about the billing classes, see Watson Machine Learning plans."
2766	What's new	"Week ending 1 December 2023
A new sample notebook is available: Introduction to RAG with Discovery
30 Nov 2023  
Use the Introduction to RAG with Discovery notebook to learn how to apply the retrieval-augmented generation pattern in IBM watsonx.ai with IBM Watson Discovery as the search component. For more information, see Introduction to RAG with Discovery."
2767	What's new	"Week ending 1 December 2023
Understand feature differences between watsonx as a service and software deployments
30 Nov 2023  
You can now compare the features and implementation of IBM watsonx as a Service and watsonx on Cloud Pak for Data software, version 4.8. See Feature differences between watsonx deployments."
2768	What's new	"Week ending 1 December 2023
Change to how stop sequences are handled
30 Nov 2023  
When a stop sequence, such as a newline character, is specified in the Prompt Lab, the model output text ends after the first occurrence of the stop sequence. The model output stops even if the occurrence comes at the beginning of the output. Previously, the stop sequence was ignored if it was specified at the start of the model output."
2769	What's new	"Week ending 10 November 2023
A smaller version of the Llama-2 Chat model is available
9 Nov 2023  
You can now choose between using the 13b or 70b versions of the Llama-2 Chat model. Consider these factors when you make your choice:  
Cost Performance  
The 13b version is a Class 2 model, which means it is cheaper to use than the 70b version. To compare benchmarks and other factors, such as carbon emissions for each model size, see the Model card."
2770	What's new	"Week ending 10 November 2023
Use prompt variables to build reusable prompts
Add flexibility to your prompts with prompt variables. Prompt variables function as placeholders in the static text of your prompt input that you can replace with text dynamically at inference time. You can save prompt variable names and default values in a prompt template asset to reuse yourself or share with collaborators in your project. For more information, see Building reusable prompts."
2771	What's new	"Week ending 10 November 2023
Announcing support for Python 3.10 and R4.2 frameworks and software specifications on runtime 23.1
9 Nov 2023  
Action required You can now use IBM Runtime 23.1, which includes the latest data science frameworks based on Python 3.10 and R 4.2, to run Watson Studio Jupyter notebooks and R scripts, train models, and run Watson Machine Learning deployments. Update your assets and deployments to use IBM Runtime 23.1 frameworks and software specifications.  
For information on the IBM Runtime 23.1 release and the included environments for Python 3.10 and R 4.2, see Changing notebook environments. For details on deployment frameworks, see Managing frameworks and software specifications."
2772	What's new	"Week ending 10 November 2023
Use Apache Spark 3.4 to run notebooks and scripts
Spark 3.4 with Python 3.10 and R 4.2 is now supported as a runtime for notebooks and RStudio scripts in projects. For details on available notebook environments, see Compute resource options for the notebook editor in projects and Compute resource options for RStudio in projects."
2773	What's new	"Week ending 27 October 2023
Use a Satellite Connector to connect to an on-prem database
26 Oct 2023  
Use the new Satellite Connector to connect to a database that is not accessible via the internet (for example, behind a firewall). Satellite Connector uses a lightweight Docker-based communication that creates secure and auditable communications from your on-prem environment back to IBM Cloud. For instructions, see Connecting to data behind a firewall."
2774	What's new	"Week ending 27 October 2023
Secure Gateway is deprecated
26 Oct 2023  
IBM Cloud announced the deprecation of Secure Gateway. For information, see the Overview and timeline.  
Action required If you currently have connections that are set up with Secure Gateway, plan to use an alternative communication method. In IBM watsonx, you can use the Satellite Connector as a replacement for Secure Gateway. See Connecting to data behind a firewall."
2775	What's new	"Week ending 20 October 2023
Maximum token sizes increased
16 Oct 2023  
Limits that were previously applied to the maximum number of tokens allowed in the output from foundation models are removed from paid plans. You can use larger maximum token values during prompt engineering from both the Prompt Lab and the Python library. The exact number of tokens allowed differs by model. For more information about token limits for paid and Lite plans, see Supported foundation models."
2776	What's new	"Week ending 13 October 2023
New notebooks in Samples
12 Oct 2023  
Two new notebooks are available that use a vector database from Elasticsearch in the retrieval phase of the retrieval-augmented generation pattern. The notebooks demonstrate how to find matches based on the semantic similarity between the indexed documents and the query text that is submitted from a user.  
Sample notebook: Use watsonx, Elasticsearch, and LangChain to answer questions (RAG) Sample notebook: Use watsonx, and Elasticsearch Python SDK to answer questions (RAG)"
2777	What's new	"Week ending 13 October 2023
Intermediate solutions in Decision Optimization
12 Oct 2023  
You can now choose to see a sample of intermediate solutions while a Decision Optimization experiment is running. This can be useful for debugging or to see how the solver is progressing. For large models that take longer to solve, with intermediate solutions you can now quickly and easily identify any potential problems with the solve, without having to wait for the solve to complete. You can configure the Intermediate solution delivery parameter in the Run configuration and select a frequency for these solutions. For more information, see Intermediate solutions and Run configuration parameters."
2778	What's new	"Week ending 13 October 2023
New Decision Optimization saved model dialog
When you save a model for deployment from the Decision Optimization user interface, you can now review the input and output schema, and more easily select the tables that you want to include. You can also add, modify or delete run configuration parameters, review the environment, and the model files used. All these items are displayed in the same Save as model for deployment dialog. For more information, see Deploying a Decision Optimization model by using the user interface."
2779	What's new	"Week ending 6 October 2023
Additional foundation models in Frankfurt
5 Oct 2023  
All foundation models that are available in the Dallas data center are now also available in the Frankfurt data center. The watsonx.ai Prompt Lab and foundation model inferencing are now supported in the Frankfurt region for these models:  
granite-13b-chat-v1 granite-13b-instruct-v1 llama-2-70b-chat gpt-neox-20b mt0-xxl-13b starcoder-15.5b  
For more information on these models, see Supported foundation models available with watsonx.ai.  
For pricing details, see Watson Machine Learning plans."
2780	What's new	"Week ending 6 October 2023
Control the placement of a new column in the Concatenate operation (Data Refinery)
6 Oct 2023  
You now have two options to specify the position of the new column that results from the Concatenate operation: As the right-most column in the data set or next to the original column.  
Previously, the new column was placed at the beginning of the data set.  
Important:  
Action required Edit the Concatenate operation in any of your existing Data Refinery flows to specify the new column position. Otherwise, the flow might fail.  
For information about Data Refinery operations, see GUI operations in Data Refinery."
2781	What's new	"Week ending 29 September 2023
IBM Granite foundation models for natural language generation
28 Sept 2023  
The first two models from the Granite family of IBM foundation models are now available in the Dallas region:  
granite-13b-chat-v1: General use model that is optimized for dialog use cases granite-13b-instruct-v1: General use model that is optimized for question answering  
Both models are 13B-parameter decoder models that can efficiently predict and generate language in English. They, like all models in the Granite family, are designed for business. Granite models are pretrained on multiple terabytes of data from both general-language sources, such as the public internet, and industry-specific data sources from the academic, scientific, legal, and financial fields.  
Try them out today in the Prompt Lab or run a sample notebook that uses the granite-13b-instruct-v1 model for sentiment analysis.  
Read the Building AI for business: IBM’s Granite foundation models blog post to learn more.  
For more information on these models, see Supported foundation models available with watsonx.ai. For a description of sample prompts, see Sample foundation model prompts for common tasks. For pricing details, see Watson Machine Learning plans."
2782	What's new	"Week ending 22 September 2023
Decision Optimization Java models
20 Sept 2023  
Decision Optimization Java models can now be deployed in Watson Machine Learning. By using the Java worker API, you can create optimization models with OPL, CPLEX, and CP Optimizer Java APIs. You can now easily create your models locally, package them and deploy them on Watson Machine Learning by using the boilerplate that is provided in the public Java worker GitHub. For more information, see Deploying Java models for Decision Optimization."
2783	What's new	"Week ending 22 September 2023
New notebooks in Resource hub
21 Sept 2023  
You can use the following new notebooks in Resource hub:  
Use watsonx and LangChain to answer questions using RAG Use watsonx and BigCode starcoder-15.5b to generate code"
2784	What's new	"Week ending 15 September 2023
Prompt engineering and synthetic data quick start tutorials
14 Sept 2023  
Try the new tutorials to help you learn how to:  
Prompt foundation models: There are usually multiple ways to prompt a foundation model for a successful result. In the Prompt Lab, you can experiment with prompting different foundation models, explore sample prompts, as well as save and share your best prompts. One way to improve the accuracy of generated output is to provide the needed facts as context in your prompt text using the retrieval-augmented generation pattern. Generate synthetic data: You can generate synthetic tabular data in watsonx.ai. The benefit to synthetic data is that you can procure the data on-demand, then customize to fit your use case, and produce it in large quantities."
2785	What's new	"Week ending 15 September 2023
Watsonx.ai Community
14 Sept 2023  
You can now join the watsonx.ai Community for AI architects and builders to learn, share ideas, and connect with others."
2786	What's new	"Week ending 8 September 2023
Generate synthetic tabular data with Synthetic Data Generator
7 Sept 2023  
Now available in the Dallas and Frankfurt regions, Synthetic Data Generator is a new graphical editor tool on watsonx.ai that you can use to generate tabular data to use for training models. Using visual flows and a statistical model, you can create synthetic data based on your existing data or a custom data schema. You can choose to mask your original data and export your synthetic data to a database or as a file.  
To get started, see Synthetic data."
2787	What's new	"Week ending 8 September 2023
Llama-2 Foundation Model for natural language generation and chat
7 Sept 2023  
The Llama-2 Foundation Model from Meta is now available in the Dallas region. Llama-2 Chat model is an auto-regressive language model that uses an optimized transformer architecture. The model is pretrained with publicly available online data, and then fine-tuned using reinforcement learning from human feedback. The model is intended for commercial and research use in English-language assistant-like chat scenarios.  
For more information on the Llama-2 model, see Supported foundation models available with watsonx.ai. For a description of sample prompts, see Sample foundation model prompts for common tasks. For pricing details for Llama-2, see Watson Machine Learning plans."
2788	What's new	"Week ending 8 September 2023
LangChain extension for the foundation models Python library
7 Sept 2023  
You can now use the LangChain framework with foundation models in watsonx.ai with the new LangChain extension for the foundation models Python library.  
This sample notebook demonstrates how to use the new extension: Sample notebook"
2789	What's new	"Week ending 8 September 2023
Introductory sample for the retrieval-augmented generation pattern
7 Sept 2023  
Retrieval-augmented generation is a simple, powerful technique for leveraging a knowledge base to get factually accurate output from foundation models.  
See: Introduction to retrieval-augmented generation"
2790	What's new	"Week ending 1 September 2023
Deprecation of comments in notebooks
31 Aug 2023  
As of today it is not possible to add comments to a notebook from the notebook action bar. Any existing comments were removed."
2791	What's new	"Week ending 1 September 2023
StarCoder Foundation Model for code generation and code translation
31 Aug 2023  
The StarCoder model from Hugging Face is now available in the Dallas region. Use StarCoder to create prompts for generating code or for transforming code from one programming language to another. One sample prompt demonstrates how to use StarCoder to generate Python code from a set of instruction. A second sample prompt demonstrates how to use StarCoder to transform code written in C++ to Python code.  
For more information on the StarCoder model, see Supported foundation models available with watsonx.ai. For a description of the sample prompts, see Sample foundation model prompts for common tasks."
2792	What's new	"Week ending 1 September 2023
IBM watsonx.ai is available in the Frankfurt region
31 Aug 2023  
Watsonx.ai is now generally available in the Frankfurt data center and can be selected as the preferred region when signing-up. The Prompt Lab and foundation model inferencing are supported in the Frankfurt region for these models:  
mpt-7b-instruct2  
flan-t5-xxl-11b  
flan-ul2-20b  
For more information on the supported models, see Supported foundation models available with watsonx.ai."
2793	What's new	"Week ending 25 August 2023
Additional cache enhancements available for Watson Pipelines
21 August 2023  
More options are available for customizing your pipeline flow settings. You can now exercise greater control over when the cache is used for pipeline runs. For details, see Managing default settings."
2794	What's new	"Week ending 18 August 2023
Plan name updates for Watson Machine Learning service
18 August 2023  
Starting immediately, plan names are updated for the IBM Watson Machine Learning service, as follows:  
The v2 Standard plan is now the Essentials plan. The plan is designed to give your organization the resources required to get started working with foundation models and machine learning assets.  
The v2 Professional plan is now the Standard plan. This plan provides resources designed to support most organizations through asset creation to productive use.  
Changes to the plan names do not change your terms of service. That is, if you are registered to use the v2 Standard plan, it will now be named Essentials, but all of the plan details will remain the same. Similarly, if you are registered to use the v2 Professional plan, there are no changes other than the plan name change to Standard.  
For details on what is included with each plan, see Watson Machine Learning plans. For pricing information, find your plan on the Watson Machine Learning plan page in the IBM Cloud catalog."
2795	What's new	"Week ending 11 August 2023
Deprecation of comments in notebooks
7 August 2023  
On 31 August 2023, you will no longer be able to add comments to a notebook from the notebook action bar. Any existing comments that were added that way will be removed."
2796	What's new	"Week ending 4 August 2023
Increased token limit for Lite plan
4 August 2023  
If you are using the Lite plan to test foundation models, the token limit for prompt input and output is now increased from 25,000 to 50,000 per account per month. This gives you more flexibility for exploring foundation models and experimenting with prompts.  
For details on watsonx.ai plans, see Watson Machine Learning plans. For details on working with prompts, see Engineer prompts with the Prompt Lab."
2797	What's new	"Week ending 4 August 2023
Custom text analytics template (SPSS Modeler)
4 August 2023  
For SPSS Modeler, you can now upload a custom text analytics template to a project. This provides you with more flexibility to capture and extract key concepts in a way that is unique to your context."
2798	What's new	"Week ending 28 July 2023
Foundation models Python library available
27 July 2023  
You can now prompt foundation models in watsonx.ai programmatically using a Python library.  
See: Foundation models Python library"
2799	What's new	"Week ending 14 July 2023
Control AI guardrails
14 July 2023  
You can now control whether AI guardrails are on or off in the Prompt Lab. AI guardrails remove potentially harmful text from both the input and output fields. Harmful text can include hate speech, abuse, and profanity. To prevent the removal of potentially harmful text, set the AI guardrails switch to off. See Hate speech, abuse, and profanity."
2800	What's new	"Week ending 14 July 2023
Microsoft Azure SQL Database connection supports Azure Active Directory authentication (Azure AD)
14 July 2023  
You can now select Active Directory for the Microsoft Azure SQL Database connection. Active Directory authentication is an alternative to SQL Server authentication. With this enhancement, administrators can centrally manage user permissions to Azure. For more information, see Microsoft Azure SQL Database connection."
2801	What's new	"Week ending 7 July 2023
Welcome to IBM watsonx.ai!
7 July 2023  
IBM watsonx.ai delivers all the tools that you need to work with machine learning and foundation models.  
Get started:  
Learn about watsonx.ai Learn about foundation models Engineer prompts with the Prompt Lab Take quick start tutorials Watson Natural Language Processing"
2802	What's new	"Week ending 7 July 2023
Try generative AI search and answer in this documentation
7 July 2023  
You can see generative AI in action by trying the new generative AI search and answer option in the watsonx.ai documentation. The answers are generated by a large language model running in watsonx.ai and based on the documentation content. This feature is only available when you are viewing the documentation while logged in to watsonx.ai.  
Enter a question in the documentation search field and click the Try generative AI search and answer icon (). The Generative AI search and answer pane opens and answers your question."
2803	Use cases	"When you implement the watsonx.ai and watsonx.governance use cases, you can solve the challenges of building machine learning models and generative AI, governing AI, and managing your AI lifecycles.  
The use cases in IBM watsonx as a Service provide these main capabilities for managing and automating your AI lifecycles:  
Collaborative innovation Collaborate with others to discover insights. Prepare data, analyze data, and build models with a set of integrated tools for all levels of experience.  
Governance and compliance Track and document the detailed history of AI models to help ensure compliance.  
Unified lifecycle Automate the building, testing, deploying, and monitoring of AI models.  
You can start with a use case and add another one as you need it. Explore each use case to learn about what you can accomplish and the tools you can use."
2804	Use cases	"Watsonx.ai
Transform your business processes with AI-driven solutions by integrating machine learning and generative AI into your operational framework. Watsonx.ai provides the processes and technologies to enable your enterprise to develop and deploy machine learning models and generative AI solutions.  
Services for this use case: Watson Studio and Watson Machine Learning  
See the watsonx.ai use case."
2805	Use cases	"Watsonx.governance
Drive responsible, transparent, and explainable AI workflows with an integrated system for tracking, monitoring, and retraining AI models. Watsonx.governance provides the processes and technologies to enable your enterprise to monitor, maintain, automate, and govern machine learning and generative AI models in production.  
Services for this use case: Watson Studio, Watson Machine Learning, and Watson OpenScale  
See the watsonx.governance use case."
2806	Use cases	"Learn more
Signing up for watsonx.ai and watsonx.governance  
watsonx.ai use case  
watsonx.governance use case  
Videos  
Parent topic: Overview of watsonx"
2807	Project collaborators	"Collaborators are the people you add to the project to work together. After you create a project, add collaborators to share knowledge and resources freely, shift workloads flexibly, and help one another complete jobs.  
You must have the Admin role in the project.  
You must belong to the project creator's IBM Cloud account.  
Add collaborators Add service IDs Change collaborator roles Remove a collaborator"
2808	Project collaborators	"Add collaborators
To add a collaborator as a Viewer or Editor of your project, they must either be:  
A member of the project creator's IBM Cloud account, or; A member of the same organization single sign-on (SAML federation on IBM Cloud).  
To add a collaborator as an Admin of your project, they must be a member of the project creator's IBM Cloud account.  
Watch this video to see how to add collaborators and grant them access to your projects.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
To add collaborators to your project:  
From your project, click the Access Control page on the Manage tab. Click Add collaborators then select Add users. Add the collaborators who you want to have the same access level: Choose the role for the collaborators and click Add: Add more collaborators with the same or different access levels. Click Add.  
Type email addresses into the Find users field. Copy multiple email addresses, separated by commas, and paste them into the Find users field.  
Viewer: View the project. Editor: Control project assets. Admin: Control project assets, collaborators, and settings.  
The invited users are added to your project immediately."
2809	Project collaborators	"Add service IDs
You can create service IDs in IBM Cloud to enable an application outside of IBM Cloud access to your IBM Cloud services. Because service IDs are not tied to a specific user, if a user happens to leave an organization and is deleted from the account, the service ID remains ensuring that your application or service stays up and running. See Creating and working with service IDs.  
To add a service ID to your project:  
From your project, select the Access Control page on the Manage tab. Click Add collaborators and select Add service IDs. In the Find service IDs field, search for the service name or description and select the one you want. Add other service IDs that you want to have the same access level. Select the access level. Click Add."
2810	Project collaborators	"Change collaborator roles
To change the role for a project collaborator or service ID:  
Go to the Access Control page on the Manage tab. In the row for the collaborator or service ID, click the edit icon next to the role name. Select the new role and click Save."
2811	Project collaborators	"Remove a collaborator
To remove a collaborator or service ID from a project, go to the Access Control page on the Manage tab. In the row for the collaborator or service ID, click the remove icon."
2812	Project collaborators	"Learn more
Collaborator permissions Setup additional account users  
Parent topic: Administering projects"
2813	Get help with watsonx	"You can get help with IBM watsonx through documentation, training, support, and community resources.  
Platform setup Training Community resources Samples Support"
2814	Get help with watsonx	"Help with platform setup
You must be the account owner or administrator for a billable IBM Cloud account to set up the IBM watsonx platform for your organization. To learn how to set up IBM watsonx, see Setting up the platform."
2815	Get help with watsonx	"Training
Start training your model by data preparation, analysis, and visualization. Learn how to build, deploy, and trust your models. Use the following tutorials and videos to get started with IBM watsonx:  
Quick start tutorials  
Video library  
IBM Developer site"
2816	Get help with watsonx	"Community resources
Share and gain knowledge using IBM Community and get the most out of our services.  
Explore blogs, forums, and other resources in these communities:  
watsonx.ai Community  
Data Science Community  
Watson Studio Community  
Find more blogs and forums on the following platforms:  
IBM Data and AI on Medium Watson Studio Stack Overflow"
2817	Get help with watsonx	"Samples
You can use sample projects, notebooks, and data sets to get started fast.  
Find samples in the following locations:  
Resource hub IBM Data Science assets in GitHub"
2818	Get help with watsonx	"Support
IBM Cloud provides you with three paid support options to customize your experience according to your business needs. Choose from Basic, Advanced, or Premium support plan. The level of support that you select determines the severity that you can assign to support cases and your level of access to the tools available in the Support Center.  
You can also go to IBM Cloud Support Center to open a support case, browse FAQs, or ask questions to IBM Chat Bot."
2819	Get help with watsonx	"Learn more
Known issues FAQ Browser support Language support"
2820	Collaborator security	IBM watsonx provides attribute-based access control to protect workspaces such as projects and catalogs. You control access to workspaces by assigning roles and by restricting collaborators.
2821	Collaborator security	"Collaborator roles
Everyone working in IBM watsonx is assigned a role that determines the workspaces that they can access and the tasks that they can perform. Collaborator roles control access to projects, deployment spaces, and catalogs using permissions specific to the role. Roles are assigned in IBM watsonx to provide Admin, Editor, or Viewer permissions.  
Users also have an IAM Platform access role for the Cloud account and they may also have an IAM Service access role for workspaces. To understand how the roles provide secure access, see Roles in IBM watsonx.  
To understand the permissions for each collaborator role, see Project collaborator roles.  
Parent topic: Security"
2822	Configuring fairness evaluations in Watson OpenScale	Watson OpenScale evaluate models for bias to ensure fair outcomes among different groups.
2823	Configuring fairness evaluations in Watson OpenScale	"Evaluating the model for fairness
You can use fairness evaluations to determine whether your model produces biased outcomes. The fairness evaluation checks when the model shows a tendency to provide a favorable (preferable) outcome more often for one group over another.  
The fairness evaluation generates a set of metrics every hour by default. You can generate these metrics on demand by clicking Evaluate fairness now or by using the Python client.  
When you test an evaluation in a pre-production environment, you evaluate fairness based on test data. Test data must have the same format and schema as the training data you used to train the model.  
In a production environment, you monitor feedback data, which is the actual data logged with payload logging. For proper monitoring, you must regularly log feedback data to Watson OpenScale. You can provide feedback data by clicking Upload feedback data on the Evaluations page of the Watson OpenScale Insights dashboard. You can also provide feedback data by using the Python client or REST API."
2824	Configuring fairness evaluations in Watson OpenScale	"Before you begin
Before configuring the fairness evaluation, you must provide model details and upload payload data to enable fairness evaluations.  
To configure fairness evaluations unstructured text and image models, you must provide payload data that contains meta fields, such as Gender, to calculate disparate impact. To calculate performance metrics for unstructured text and image models, you must also provide feedback data that contains meta fields with the correctly predicted outcomes.  
You must complete similar requirements when you configure fairness evaluations for indirect bias. When you configure fairness evaluations for unstructured text and image models, you don't have to provide training data."
2825	Configuring fairness evaluations in Watson OpenScale	"Configuring the evaluation
You can configure fairness evaluations manually or you can run a custom notebook to generate a configuration file. You can upload the configuration file to specify the settings for your evaluation.  
When you configure fairness evaluations manually, you can specify the reference group (value) that you expect to represent favorable outcomes. You can also select the corresponding model attributes (features) to monitor for bias (for example, Age or Sex), that will be compared against the reference group. Depending on your training data, you can also specify the minimum and maximum sample size that will be evaluated."
2826	Configuring fairness evaluations in Watson OpenScale	"Configuring the evaluation
Favorable and unfavorable outcomes
The output of the model is categorized as either favorable or unfavorable. For example, if the model is recommending whether a person gets a loan or not, then the Favorable outcome might be Loan Granted or Loan Partially Granted. The unfavorable outcome might be Loan Denied.  
The values that represent a favorable outcome are derived from the label column in the training data. By default the predictedLabel column is set as the prediction column. Favorable and unfavorable values must be specified by using the value of the prediction column as a string data type, such as 0 or 1 when you are uploading training data."
2827	Configuring fairness evaluations in Watson OpenScale	"Configuring the evaluation
Metrics
This section allows users to select all the metrics they want to configure. By default only Disparate impact metric is computed."
2828	Configuring fairness evaluations in Watson OpenScale	"Configuring the evaluation
Minimum sample size
The minimum sample size is set to delay the fairness evaluation until a minimum number of records are available in the evaluation data set. This function ensures that the sample size is not too small and skews results. By setting a minimum sample size, you prevent measuring fairness until a minimum number of records are available in the evaluation data set. This ensures that the sample size is not too small to skew results. When the fairness monitor runs, it uses the minimum sample size to decide the number of records that are evaluated."
2829	Configuring fairness evaluations in Watson OpenScale	"Configuring the evaluation
Features: Reference and monitored groups
The values of the features are specified as either a reference or monitored group. The monitored group represents the values that are most at risk for biased outcomes. For example, for the Sex feature, you can set Female and Non-binary as the monitored groups. For a numeric feature, such as Age, you can set [18-25] as the monitored group. All other values for the feature are then considered as the reference group, for example, Sex=Male or Age=[26,100]."
2830	Configuring fairness evaluations in Watson OpenScale	"Configuring thresholds for the monitor
To configure the model for fairness, follow these steps:  
Select a model deployment tile and click Configure monitors. Select Fairness in the Evaluations section of the Configure tab. For each configuration item, click Edit to specify the Favorable outcomes, Sample Size, and the features to evaluate."
2831	Configuring fairness evaluations in Watson OpenScale	"Configuring thresholds for the monitor
Features
The features are the model attributes that are evaluated to detect bias. For example, you can configure the fairness monitor to evaluate features such as Sex or Age for bias. Only features that are of categorical, numeric (integer), float, or double fairness data type are supported."
2832	Configuring fairness evaluations in Watson OpenScale	"Configuring thresholds for the monitor
Fairness alert threshold
The fairness alert threshold specifies an acceptable difference between the percentage of favorable outcomes for the monitored group and the percentage of favorable outcomes for the reference group. For example, if the percentage of favorable outcomes for a group in your model is 70% and the fairness threshold is set to 80%, then the fairness monitor detects bias in your model."
2833	Configuring fairness evaluations in Watson OpenScale	"Testing for indirect bias
If you select a field that is not a training feature, called an added field, Watson OpenScale will look for indirect bias by finding associated values in the training features. For example, the profession “student” may imply a younger individual even though the Age field was excluded from model training.For details on configuring the Fairness monitor to consider indirect bias, see Configuring the Fairness monitor for indirect bias."
2834	Configuring fairness evaluations in Watson OpenScale	"Mitigating bias
Watson OpenScale uses two types of debiasing: passive and active. Passive debiasing reveals bias, while active debiasing prevents you from carrying that bias forward by changing the model in real time for the current application. For details on interpreting results and mitigating bias in a model, see Reviewing results from a Fairness evaluation."
2835	Configuring fairness evaluations in Watson OpenScale	"Learn more
Fairness metrics overview Calculating fairness Configuring the fairness monitor for indirect bias  
Parent topic: Configuring model evaluations"
2836	Adding deployments for evaluations	"To enable evaluations in the Watson OpenScale service, you must add deployments that are hosted by your machine learning providers to the Watson OpenScale Insights dashboard. To add deployments, you can click Add to dashboard to open a wizard that guides you through a series of steps to provide model details.  
The following sections describe the steps that you must complete to add deployments in Watson OpenScale."
2837	Adding deployments for evaluations	"Select model location
You can choose from a list of available deployment spaces or machine learning providers to specify where your model is located.  
For Watson Machine Learning deployments, if you choose a deployment space that isn't associated with a machine learning provider, you must add a new provider on the Add new provider tab in the wizard before Watson OpenScale displays a list of deployments."
2838	Adding deployments for evaluations	"Select a deployed model
Select the deployment that you want to evaluate.  
If you select a custom machine learning provider, which doesn't provide a list of deployments, you must specify the deployment name and description. You can also specify a scoring endpoint."
2839	Adding deployments for evaluations	"Select a storage type
You must choose the location where you want to store your model transactions and evaluation results. You must also specify the type of data that your deployment uses and the type of algorithm that you use to build your model. For some model deployments, Watson OpenScale automatically discovers these details, depending on what metadata is available from your deployment. If you selected your deployment from a deployment list, the model endpoint is specified automatically. If you're adding a production deployment, you can import these settings from existing pre-production deployments."
2840	Adding deployments for evaluations	"Review model summary
Watson OpenScale displays a summary of the model details that are discovered from your deployment. If Watson OpenScale discovers all of the required details automatically, you can click Finish and start configuring evaluations. You can also edit the details to make changes.  
If Watson OpenScale does not automatically discover all the required model details, you must click Save and continue to continue the guided setup by manually providing details to enable evaluations.  
For more information, see Providing model details."
2841	Adding deployments for evaluations	"Next steps
Configuring model evaluations  
Parent topic: Preparing to evaluate a model"
2842	Managing feedback data for watsonx.governance	"You must provide feedback data to watsonx.governance to enable you to configure quality and generative AI quality evaluations and determine any changes in your model predictions.  
When you provide feedback data to watsonx.governance, you can regularly evaluate the accuracy of your model predictions."
2843	Managing feedback data for watsonx.governance	"Feedback logging
Watsonx.governance stores the feedback data that you provide as records in a feedback logging table.  
The feedback logging table contains the following columns when you evaluate prompt templates:  
Required columns: Optional columns:  
Prompt variable(s): Contains the values for the variables that are created for prompt templates reference_output: Contains the ground truth value  
_original_prediction: Contains the output that's generated by the foundation model"
2844	Managing feedback data for watsonx.governance	"Uploading feedback data
You can use a feedback logging endpoint to upload data for quality evaluations. You can also upload feedback data with a CSV file. For more information, see Sending model transactions."
2845	Managing feedback data for watsonx.governance	"Learn more
Feedback logging  
Parent topic: Managing data for model evaluations in Watson OpenScale"
2846	Configuring quality evaluations in watsonx.governance	"Watsonx.governance quality evaluations measure your model's ability to provide correct outcomes  
When you evaluate prompt templates, you can review a summary of quality evaluation results for the text classification task type.  
The summary displays scores and violations for metrics that are calculated with default settings.  
To configure quality evaluations with your own settings, you can set a minimum sample size and set threshold values for each metric. The minimum sample size indicates the minimum number of model transaction records that you want to evaluate and the threshold values create alerts when your metric scores violate your thresholds. The metric scores must be higher than the threshold values to avoid violations. Higher metric values indicate better scores."
2847	Configuring quality evaluations in watsonx.governance	"Supported quality metrics
When you enable quality evaluations in watsonx.governance, you can generate metrics that help you determine how well your foundation model predicts outcomes.  
Watsonx.governance supports the following quality metrics:  
Description: The proportion of correct predictions Default thresholds: Lower limit = 80% Problem types: Multiclass classification Chart values: Last value in the timeframe Metrics details available: Confusion matrix Understanding accuracy: Accuracy can mean different things depending on the type of algorithm;  
Multi-class classification: Accuracy measures the number of times any class was predicted correctly, normalized by the number of data points. For more details, see Multi-class classification in the Apache Spark documentation.  
Description: Weighted mean of class TPR with weights equal to class probability Default thresholds: Lower limit = 80% Problem type: Multiclass classification Chart values: Last value in the timeframe Metrics details available: Confusion matrix Do the math:  
The True positive rate is calculated by the following formula:  
Description: Weighted mean of class FPR with weights equal to class probability. For more details, see Multi-class classification in the Apache Spark documentation. Default thresholds: Lower limit = 80% Problem type: Multiclass classification Chart values: Last value in the timeframe Metrics details available: Confusion matrix Do the math:  
The Weighted False Positive Rate is the application of the FPR with weighted data.  
Description: Weighted mean of recall with weights equal to class probability Default thresholds: Lower limit = 80% Problem type: Multiclass classification Chart values: Last value in the timeframe Metrics details available: Confusion matrix Do the math:  
Weighted recall (wR) is defined as the number of true positives (Tp) over the number of true positives plus the number of false negatives (Fn) used with weighted data.  
Description: Weighted mean of precision with weights equal to class probability Default thresholds: Lower limit = 80% Problem type: Multiclass classification Chart values: Last value in the timeframe Metrics details available: Confusion matrix Do the math:  
Precision (P) is defined as the number of true positives (Tp) over the number of true positives plus the number of false positives (Fp)."
2848	Configuring quality evaluations in watsonx.governance	"Supported quality metrics
Precision (P) is defined as the number of true positives (Tp) over the number of true positives plus the number of false positives (Fp).  
Description: Weighted mean of F1-measure with weights equal to class probability Default thresholds: Lower limit = 80% Problem type: Multiclass classification Chart values: Last value in the timeframe Metrics details available: Confusion matrix Do the math:  
The Weighted F1-Measure is the result of using weighted data.  
Description: Measures the quality of binary and multiclass classifications by accounting for true and false positives and negatives. Balanced measure that can be used even if the classes are different sizes. A correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 and inverse prediction. Default thresholds: Lower limit = 80 Chart values: Last value in the timeframe Metrics details available: Confusion matrix  
Description: Measures the asymmetry of label distributions. If skewness is 0, the dataset is perfectly balanced, it if is less than -1 or greater than 1, the distribution is highly skewed, anything in between is moderately skewed. Default thresholds: Chart values: Last value in the timeframe  
Lower limit = -0.5 Upper limit = 0.5  
Parent topic: Configuring model evaluations"
2849	Reviewing model insights	"When you evaluate models in the Watson OpenScale service, you can analyze your results to gain insights about model performance.  
In Watson OpenScale, you can use the Insights dashboard to help you analyze your model evaluation results. The dashboard provides an overview of the deployments that you're evaluating, the types of evaluations that you've configured, and any metric threshold violation alerts. Each deployment tile provides a summary of the results from your last model evaluation. You can select a deployment tile to review evaluation results that provide more insights about your model.  
When you review evaluation results, Watson OpenScale provides details about how your model was measured by each metric over time. You can also review model transactions to reveal which features contributed to your model's predicted outcome for each transaction and understand what changes would result in a different outcome.  
Parent topic: Evaluating AI models with Watson OpenScale"
2850	Managing payload data for watsonx.governance	"You must provide payload data to configure drift v2 and generative AI quality evaluations in watsonx.governance.  
Payload data contains all of your model transactions. You can log payload data with watsonx.governance to enable evaluations. To log payload data, watsonx.governance must receive scoring requests."
2851	Managing payload data for watsonx.governance	"Logging payload data
When you send a scoring request, watsonx.governance processes your model transactions to enable model evaluations. Watsonx.governance scores the data and stores it as records in a payload logging table within the watsonx.governance data mart.  
The payload logging table contains the following columns when you evaluate prompt templates:  
Required columns: Optional columns:  
Prompt variable(s): Contains the values for the variables that are created for prompt templates generated_text: Contains the output that's generated by the foundation model  
input_token_count: Contains the number of tokens in the input text generated_token_count: Contains the number of tokens in the generated text prediction_probability: Contains the aggregate value of log probabilities of generated tokens that represent the winning output  
The table can also include timestamp and ID columns to store your data as scoring records.  
You can view your payload logging table by accessing the database that you specified for the data mart or by using the Watson OpenScale Python SDK as shown in the following example:"
2852	Managing payload data for watsonx.governance	"Sending payload data
If you are using IBM Watson Machine Learning as your machine learning provider, watsonx.governance automatically logs payload data when your model is scored.  
After you configure evaluations, you can also use a payload logging endpoint to send scoring requests to run on-demand evaluations. For production models, you can also upload payload data with a CSV file to send scoring requests. For more information see, Sending model transactions."
2853	Managing payload data for watsonx.governance	"Learn more
Payload logging  
Parent topic: Managing data for model evaluations in Watson OpenScale"
2854	Configuring drift v2 evaluations in watsonx.governance	"You can configure drift v2 evaluations with watsonx.governance to measure changes in your data over time to ensure consistent outcomes for your model. Use drift v2 evaluations to identify changes in your model output, the accuracy of your predictions, and the distribution of your input data.  
The following sections describe the steps that you must complete to configure drift v2 evaluations with watsonx.governance:"
2855	Configuring drift v2 evaluations in watsonx.governance	"Set sample size
Watsonx.governance uses sample sizes to understand how to process the number of transactions that are evaluated during evaluations. You must set a minimum sample size to indicate the lowest number of transactions that you want watsonx.governance to evaluate. You can also set a maximum sample size to indicate the maximum number of transactions that you want watsonx.governance to evaluate."
2856	Configuring drift v2 evaluations in watsonx.governance	"Configure baseline data
Watsonx.governance uses payload records to establish the baseline for drift v2 calculations. You must configure the number of records that you want to calculate as your baseline data."
2857	Configuring drift v2 evaluations in watsonx.governance	"Set drift thresholds
You must set threshold values for each metric to enable watsonx.governance to understand how to identify issues with your evaluation results. The values that you set create alerts on the evaluation summary page that appear when metric scores violate your thresholds. You must set the values between the range of 0 to 1. The metric scores must be lower than the threshold values to avoid violations."
2858	Configuring drift v2 evaluations in watsonx.governance	"Supported drift v2 metrics
When you enable drift v2 evaluations, you can view a summary of evaluation results with metrics for the type of model that you're evaluating.  
The following drift v2 metrics are supported by watsonx.governance:  
Watsonx.governance calculates output drift by measuring the change in the model confidence distribution.  
How it works: Watsonx.governance measures how much your model output changes from the time that you train the model. To evaluate prompt templates, watsonx.governance calculates output drift by measuring the change in distribution of prediction probabilities. The prediction probability is calculated by aggregating the log probabilities of the tokens from the model output. When you upload payload data with CSV files, you must include prediction_probability values or output drift cannot be calculated. For regression models, watsonx.governance calculates output drift by measuring the change in distribution of predictions on the training and payload data. For classification models, watsonx.governance calculates output drift for each class probability by measuring the change in distribution for class probabilities on the training and payload data. For multi-classification models, watsonx.governance also aggregates output drift for each class probability by measuring a weighted average.  
Do the math: Watsonx.governance uses the following formulas to calculate output drift:  
Total variation distance Overlap coefficient  
Applies to prompt template evaluations: Yes  
Task types:  
Text summarization Text classification Content generation Entity extraction Question answering  
Watsonx.governance calculates model quality drift by comparing the estimated runtime accuracy to the training accuracy to measure the drop in accuracy.  
How it works:  
Watsonx.governance builds its own drift detection model that processes your payload data when you configure drift v2 evaluations to predict whether your model generates accurate predictions without the ground truth. The drift detection model uses the input features and class probabilities from your model to create its own input features.  
Do the math:  
Watsonx.governance uses the following formula to calculate model quality drift:"
2859	Configuring drift v2 evaluations in watsonx.governance	"Supported drift v2 metrics
Do the math:  
Watsonx.governance uses the following formula to calculate model quality drift:  
Watsonx.governance calculates the accuracy of your model as the base_accuracy by measuring the fraction of correctly predicted transactions in your training data. During evaluations, your transactions are scored against the drift detection model to measure the amount of transactions that are likely predicted correctly by your model. These transactions are compared to the total number of transactions that watsonx.governance processes to calculate the predicted_accuracy. If the predicted_accuracy is less than the base_accuracy, watsonx.governance generates a model quality drift score.  
Applies to prompt template evaluations: No  
Watsonx.governance calculates feature drift by measuring the change in value distribution for important features.  
How it works:  
Watsonx.governance calculates drift for categorical and numeric features by measuring the probability distribution of continuous and discrete values. To identify discrete values for numeric features, watsonx.governance uses a binary logarithm to compare the number of distinct values of each feature to the total number of values of each feature. Watsonx.governance uses the following binary logarithm formula to identify discrete numeric features:  
If the distinct_values_count is less than the binary logarithm of the total_count, the feature is identified as discrete.  
Do the math:  
Watsonx.governance uses the following formulas to calculate feature drift:  
Jensen Shannon distance  
Total variation distance  
Overlap coefficient  
Applies to prompt template evaluations: No  
Prediction drift measures the change in distribution of the LLM predicted classes.  
Do the math:  
Watsonx.governance uses the Jensen Shannon distance formula to calculate prediction drift.  
Applies to prompt template evaluations: Yes  
Task types: Text classification  
Input metadata drift measures the change in distribution of the LLM input text metadata.  
How it works: Watsonx.governance calculates the following metadata with the LLM input text: Character count: Total number of characters in the input text Word count: Total number of words in the input text Sentence count: Total number of sentences in the input text Average word length: Average length of words in the input text Average sentence length: Average length of the sentences in the input text"
2860	Configuring drift v2 evaluations in watsonx.governance	"Supported drift v2 metrics
Watsonx.governance calculates input metadata drift by measuring the change in distribution of the metadata columns. The input token count column, if present in the payload, is also used to compute the input metadata drift. You can also choose to specify any meta fields while adding records to the payload table. These meta fields are also used to compute the input metadata drift. To identify discrete numeric input metadata columns, watsonx.governance uses the following binary logarithm formula:  
If the distinct_values_count is less than the binary logarithm of the total_count, the feature is identified as discrete.  
For discrete input metadata columns, watsonx.governance uses the Jensen Shannon distance formula to calculate input metadata drift.  
For continuous input metadata columns, watsonx.governance uses the total variation distance and overlap coefficient formulas to calculate input metadata drift.  
Applies to prompt template evaluations: Yes  
Task types:  
Text summarization Text classification Content generation Entity extraction Question answering  
Output metadata drift measures the change in distribution of the LLM output text metadata.  
How it works: Watsonx.governance calculates the following metadata with the LLM output text: Character count: Total number of characters in the output text Word count: Total number of words in the output text Sentence count: Total number of sentences in the output text Average word length: Average length of words in the output text Average sentence length: Average length of the sentences in the output text  
Watsonx.governance calculates output metadata drift by measuring the change in distribution of the metadata columns. The output token count column, if present in the payload, is also used to compute the output metadata drift. You can also choose to specify any meta fields while adding records to the payload table. These meta fields are also used to compute the output metadata drift. To identify discrete numeric output metadata columns, watsonx.governance uses the following binary logarithm formula:  
If the distinct_values_count is less than the binary logarithm of the total_count, the feature is identified as discrete.  
For discrete output metadata columns, watsonx.governance uses the Jensen Shannon distance formula to calculate input metadata drift.  
For continuous output metadata columns, watsonx.governance uses the total variation distance and overlap coefficient formulas to calculate output metadata drift:  
Applies to prompt template evaluations: Yes"
2861	Configuring drift v2 evaluations in watsonx.governance	"Supported drift v2 metrics
For continuous output metadata columns, watsonx.governance uses the total variation distance and overlap coefficient formulas to calculate output metadata drift:  
Applies to prompt template evaluations: Yes  
Task types:  
Text summarization Text classification Content generation Question answering  
Watsonx.governance uses the following formulas to calculate drift v2 evaluation metrics:"
2862	Configuring drift v2 evaluations in watsonx.governance	"Supported drift v2 metrics
Total variation distance
Total variation distance measures the maximum difference between the probabilities that two probability distributions, baseline (B) and production (P), assign to the same transaction as shown in the following formula:  
If the two distributions are equal, the total variation distance between them becomes 0.  
Watsonx.governance uses the following formula to calculate total variation distance:  
𝑥 is a series of equidistant samples that span the domain of that range from the combined miniumum of the baseline and production data to the combined maximum of the baseline and production data.  
is the difference between two consecutive 𝑥 samples.  
is the value of the density function for production data at a 𝑥 sample.  
is the value of the density function for baseline data for at a 𝑥 sample.  
The denominator represents the total area under the density function plots for production and baseline data. These summations are an approximation of the integrations over the domain space and both these terms should be 1 and total should be 2."
2863	Configuring drift v2 evaluations in watsonx.governance	"Supported drift v2 metrics
Overlap coefficient
Watsonx.governance calculates the overlap coefficient by measuring the total area of the intersection between two probability distributions. To measure dissimilarity between distributions, the intersection or the overlap area is subtracted from 1 to calculate the amount of drift. Watsonx.governance uses the following formula to calculate the overlap coefficient:  
𝑥 is a series of equidistant samples that span the domain of that range from the combined miniumum of the baseline and production data to the combined maximum of the baseline and production data.  
is the difference between two consecutive 𝑥 samples.  
is the value of the density function for production data at a 𝑥 sample.  
is the value of the density function for baseline data for at a 𝑥 sample."
2864	Configuring drift v2 evaluations in watsonx.governance	"Supported drift v2 metrics
Jensen Shannon distance
Jensen Shannon Distance is the normalized form of Kullback-Liebler (KL) Divergence that measures how much one probability distribution differs from the second probabillity distribution. Jensen Shannon Distance is a symmetrical score and always has a finite value.  
Watsonx.governance uses the following formula to calculate the Jensen Shannon distance for two probability distributions, baseline (B) and production (P):  
is the KL Divergence.  
Parent topic: Configuring model evaluations"
2865	Payload logging in watsonx.governance	"You can enable payload logging in watsonx.governance to configure model evaluations.  
To manage payload data for configuring drift v2, generative AI quality, and model health evaluations, watsonx.governance must log your payload data in the payload logging table.  
Generative AI quality evaluations use payload data to generate results for the following task types when you evaluate prompt templates:  
Text summarization Content generation Question answering  
Drift v2 and model health evaluations use payload data to generate results for the following task types when you evaluate prompt templates:  
Text classification Text summarization Content generation Entity extraction Question answering  
You can log your payload data with the payload logging endpoint or by uploading a CSV file. For more information, see Sending model transactions  
Parent topic: Managing payload data in Watson OpenScale"
2866	Configuring drift evaluations in Watson OpenScale	Watson OpenScale drift evaluations detect drops in accuracy and data consistency in a model. The model accuracy drops if there is an increase in transactions similar to those that the model did not evaluate correctly in the training data.
2867	Configuring drift evaluations in Watson OpenScale	"Drift evaluation examples
When configuring Drift in Watson OpenScale, you have to specify the tolerable accuracy drift magnitude. The drift is measured as the drop in accuracy as compared to the model accuracy at training time. For example, if the model accuracy at training time was 90% and at runtime the estimated accuracy of the model is 80%, then the model is said to have drifted by 10%. Depending on the use case, model owners will be willing to tolerate different amounts of drift. Hence Watson OpenScale allows you to specify the accuracy drift magnitude (called as Drift alert threshold) for each model being monitored in Watson OpenScale. If the drift for a model drops below the specified threshold, an alert will be generated.  
If you use IBM Watson Machine Learning and your data does not exceed 500 MB, you can train your model online by using Watson OpenScale. Otherwise, you must use a notebook to train the model."
2868	Configuring drift evaluations in Watson OpenScale	"Before you begin
You must configure drift detection before it can analyze your model. You can train your drift detection model online by using the user interface or by running code inside a notebook. Drift configuration is supported for structured data only. The classification models support both data and accuracy drift, regression models support only data drift.  
These are the requirements for configuring the Drift monitor:  
The Machine Learning Provider must be Watson Machine Learning The training data size must be less than 500MB The training data must be hosted in IBM Cloud Object Storage/Db2.  
To upload the training data and set the Model details for drift detection:  
Click Upload training data and upload a file with the labeled data.  
For details, see Providing model details.  
Throughout this process, IBM Watson OpenScale analyzes your model and makes recommendations based on the most logical outcome. For drift detection to work properly, the data type of your prediction column in the training data must match the data type of the same column in the payload data. Assign matching string or numeric types to the prediction and label columns. To confirm data types, click Model details > Model output details > Edit. These selections ensure that you have accurate information for the following configuration steps. If for some reason you must change data types, you must redeploy the evaluation to effect the changes.  
On the successive pages of the Drift tab, you must provide the following information:"
2869	Configuring drift evaluations in Watson OpenScale	"Before you begin
Alert threshold
Required only for classification type models: IBM Watson OpenScale tracks the degree of change in model accuracy when compared to accuracy at training time. The alert threshold, which must be at least 5%, indicates the degree of tolerance for change over time."
2870	Configuring drift evaluations in Watson OpenScale	"Before you begin
Sample size
By setting a minimum sample size, you prevent measuring drift until a minimum number of records are available in the evaluation data set. This setting ensures that the sample size is not too small to skew results. Every time drift checking runs, it uses the minimum sample size to decide the number of records on which it does the computation."
2871	Configuring drift evaluations in Watson OpenScale	"Steps to configure drift evaluation
If you use IBM Watson Machine Learning, you have the option of using the Watson OpenScale user interface to configure drift detection.  
To start the configuration process, from the Drift tab, in the Drift model box, click the Edit icon. Use the Train in Watson OpenScale option.  
Follow the prompts and enter required information. When you finish, a summary of your selections is presented for review. If you want to change anything, click the Edit icon for that section. Otherwise, save your work."
2872	Configuring drift evaluations in Watson OpenScale	"Steps to configure drift without retraining
Reconfigure the drift evaluation without retraining the drift model to update parameters without more processing. You update the minimum sample size and threshold to produce more data on the currently trained model without incurring more processing costs. It is one way to avoid intensive CPU usage when the underlying data is stable and you want to view drift magnitude with different thresholds.  
Note: Your drift model requires retraining only when training data or schema changes.  
To start the configuration process, from the Drift tab, in the Drift threshold box or Sample size box, click Edit . Update the current setting and save it."
2873	Configuring drift evaluations in Watson OpenScale	"Steps to configure drift by using a notebook
Use a notebook to configure drift in the following circumstances:  
You do not want to share the training data with Watson OpenScale. You do not have a means to share the training data on Db2 or IBM Cloud Object Storage, which are the only two training data locations that are supported by Watson OpenScale.  
This option is useful if the training data is not stored in Db2 or IBM Cloud Object Storage. Using a notebook, you must read the training data into a dataframe. The specialized notebook that you can download from Watson OpenScale then creates a specialized output that you can upload to Watson OpenScale.  
To generate the drift detection model, you can run the cell that installs the ibm-wos-utils>=4.5.0 package and sci-kit learn version 1.0.2. Scikit-learn version 1.0.2 is required to build the model.  
Create a notebook to generate the drift detection model. Use the sample notebook that is available in Watson OpenScale. The drift detection model is converted into a .tar.gz file for you.  
To start the configuration process, from the Drift tab, in the Drift model box, click Edit . Use the Train in a data science notebook option. You can drag your compressed drift detection model to the drop zone.  
Follow the prompts and enter required information. When you finish, a summary of your selections is presented for review. If you want to change anything, click Edit for that section. Otherwise, save your work."
2874	Configuring drift evaluations in Watson OpenScale	"Learn more
Drift metrics  
Parent topic: Configuring model evaluations"
2875	Configuring Watson OpenScale with manual setup	You can use the manual setup to configure Watson OpenScale. With manual setup, you can use existing assets, such as databases and deployment spaces and connect them to Watson OpenScale. You also have the flexibility to choose the environment type (pre-production or production) for your deployment. Unlike the auto setup, the manual setup does not install sample assets to demonstrate Watson OpenScale capabilities.
2876	Configuring Watson OpenScale with manual setup	"Running the manual setup
Follow these steps to start the manual setup:  
The System setup page opens. To finish the manual setup, you must complete the steps that are described in the following sections."
2877	Configuring Watson OpenScale with manual setup	"Running the manual setup
Setting up a machine learning provider
Watson OpenScale connects to deployed models stored in a machine learning environment, including pre-production and production environments. The following machine learning service providers are available in Watson OpenScale:  
You can also use a custom service environment.  
Follow these steps to connect Watson OpenScale to a machine learning provider:  
In the Machine learning providers section, click Add machine learning provider. Optional: To change the default name, click the Edit icon beside Machine learning providers. Optional: To enter a description, click the Edit icon beside Description. To enter connection information, click the Edit icon beside Connection. Choose a Service provider and specify the connection details. Click Save."
2878	Configuring Watson OpenScale with manual setup	"Running the manual setup
Managing users and roles
You must add the users that you want to have access to your Watson OpenScale evaluations and assign roles to determine which tasks they can complete. For more information,"
2879	Configuring Watson OpenScale with manual setup	"Learn more
Configuring Watson OpenScale with advanced setup  
Configuring Watson OpenScale with auto setup  
Parent topic: Setup options for Watson OpenScale"
2880	Configuring generative AI quality evaluations	"You can use watsonx.governance generative AI quality evaluations to measure how well your foundation model performs tasks.  
When you evaluate prompt templates, you can review a summary of generative AI quality evaluation results for the following task types:  
Text summarization Content generation Entity extraction Question answering  
The summary displays scores and violations for metrics that are calculated with default settings.  
To configure generative AI quality evaluations with your own settings, you can set a minimum sample size and set threshold values for each metric as shown in the following example:  
The minimum sample size indicates the minimum number of model transaction records that you want to evaluate and the threshold values create alerts when your metric scores violate your thresholds. The metric scores must be higher than the lower threshold values to avoid violations. Higher metric values indicate better scores."
2881	Configuring generative AI quality evaluations	"Supported generative AI quality metrics
The following generative AI quality metrics are supported by  watsonx.governance:  
ROUGE is a set of metrics that assess how well a generated summary or translation compares to one or more reference summaries or translations. The generative AI quality evaluation calculates the rouge1, rouge2, and rougeLSum metrics.  
Task types:  
Text summarization Content generation Question answering Entity extraction  
Parameters:  
Use stemmer: If true, users Porter stemmer to strip word suffixes. Defaults to false.  
Thresholds:  
Lower bound: 0.8 Upper boud: 1.0  
SARI compares the predicted simplified sentences against the reference and the source sentences and explicitly measures the goodness of words that are added, deleted, and kept by the system.  
Task types:  
Text summarization  
Thresholds:  
Lower bound: 0 Upper bound: 100  
METEOR is calculated with the harmonic mean of precision and recall to capture how well-ordered the matched words in machine translations are in relation to human-produced reference translations.  
Task types:  
Text summarization Content generation  
Parameters:  
Alpha: Controls relative weights of precision and recall Beta: Controls shape of penalty as a function of fragmentation. Gamma: The relative weight assigned to fragmentation penalty.  
Thresholds:  
Lower bound: 0 Upper bound: 1  
Text quality evaluates the output of a model against SuperGLUE datasets by measuring the F1 score, precision, and recall against the model predictions and its ground truth data. It is calculated by normalizing the input strings and checking the number of similar tokens between the predictions and references.  
Task types:  
Text summarization Content generation  
Thresholds:  
Lower bound: 0.8 Upper bound: 1  
BLEU evaluates the quality of machine-translated text when translated from one natural language to another by comparing individual translated segments to a set of reference translations.  
Task types:  
Text summarization Content generation Question answering  
Parameters:  
Max order: Maximum n-gram order to use when completing BLEU score Smooth: Whether or not to apply Lin et al. 2004 smoothing  
Thresholds:  
Lower bound: 0.8 Upper bound: 1"
2882	Configuring generative AI quality evaluations	"Supported generative AI quality metrics
Thresholds:  
Lower bound: 0.8 Upper bound: 1  
Sentence similarity determines how similar two texts are by converting input texts into vectors that capture semantic information and calculating their similarity. It measures Jaccard similarity and Cosine similarity.  
Task types: Text summarization  
Thresholds:  
Lower limit: 0.8 Upper limit: 1  
PII measures if the provided content contains any personally identifiable information in the input and output data by using the  Watson Natural Language Processing Entity extraction model.  
Task types:  
Text summarization Content generation Question answering  
Thresholds:  
Upper limit: 0  
HAP measures if there is any toxic content in the input data provided to the model, and also any toxic content in the model generated output.  
Task types:  
Text summarization Content generation Question answering  
Thesholds  
Upper limit: 0  
The readability score determines the readability, complexity, and grade level of the model's output.  
Task types:  
Text summarization Content generation  
Thresholds:  
Lower limit: 60  
Exact match returns the rate at which the input predicted strings exactly match their references.  
Task types:  
Question answering Entity extraction  
Parameters:  
Regexes to ignore: Regex expressions of characters to ignore when calculating the exact matches. Ignore case: If True, turns everything to lowercase so that capitalization differences are ignored. Ignore punctuation: If True, removes punctuation before comparing strings. Ignore numbers: If True, removes all digits before comparing strings.  
Thresholds:  
Lower limit: 0.8 Upper limit: 1  
Multi-label/class metrics measure model performance for multi-label/multi-class predictions.  
Metrics: Task types: Entity extraction Thresholds:  
Micro F1 score Macro F1 score Micro precision Macro precision Micro recall Macro recall  
Lower limit: 0.8 Upper limit: 1  
Parent topic: Configuring model evaluations"
2883	Providing model details	"To configure model evaluations, you must provide details about your model to enable the Watson OpenScale service to understand how your model is set up.  
Watson OpenScale provides different methods that you can use to provide model details for evaluations. The method that you use depends on how you want to configure evaluations and the type of deployments that you want to evaluate."
2884	Providing model details	"Providing model details in Watson OpenScale
When you add deployments, Watson OpenScale can automatically detect all of the required model details. If Watson OpenScale doesn't detect all of the required model details, you must manually provide model details.  
The following sections describe how you can provide model details in Watson OpenScale:"
2885	Providing model details	"Providing model details in Watson OpenScale
Select a configuration method
For structured data models, you must provide model details by following guided steps or running a notebook to generate a configuration package that you can upload. If the deployment that you selected does not have a scoring endpoint, you must upload a configuration package."
2886	Providing model details	"Providing model details in Watson OpenScale
Provide a sample transaction
For image and unstructured text models, Watson OpenScale does not require training data and you must manually provide a sample transaction to specify your model output and input."
2887	Providing model details	"Providing model details in Watson OpenScale
Specify training data
If Watson OpenScale doesn't detect your training data details when you add a deployment, you can upload a CSV file to specify training data or connect to training data that is stored in a database or cloud storage. To connect to training data, you must select the location and specify connection details. If your training data details are detected when you add a deployment, the Database or cloud storage option is preselected and Watson OpenScale specifies the location and connection details for you."
2888	Providing model details	"Providing model details in Watson OpenScale
Select the feature and label columns
Watson OpenScale displays a list of columns that are available in your training data. You must select the features that you used to train the model and specify a column as the Label/Target column that contains the expected or accurate class label for each record. After you select the feature and label columns, Watson OpenScale uses your training data, and automatically sends a scoring request to your deployment to validate your model output and your deployment status."
2889	Providing model details	"Providing model details in Watson OpenScale
Select model output
Select a prediction column and a prediction probability column. The prediction column contains the prediction that your deployment generates and the prediction probability column contains the model's confidence in the prediction. Watson OpenScale might preselect expected columns based on the metadata that it identifies from your model deployment. You can choose to change these selections. The data type of the prediction column must match the data type of the label column. If the data types don't match, Watson OpenScale evaluations might not work properly.  
Parent topic: Preparing to evaluate a model"
2890	Watson OpenScale fairness metrics	"When you configure fairness evaluations in IBM Watson OpenScale, you can generate a set of metrics to evaluate the fairness of your model. You can use the fairness metrics to determine whether your model produces biased outcomes.  
You can view the results of your fairness evaluations on the Insights dashboard in Watson OpenScale. To view results, you can select a model deployment tile and click the arrow in the Fairness evaluation section to display a summary of fairness metrics from your last evaluation. For more information, see Reviewing fairness results.  
Fairness metrics are calculated with the payload data that you provide to Watson OpenScale. For more information, see Managing payload data."
2891	Watson OpenScale fairness metrics	"Supported fairness metrics
Watson OpenScale supports the following fairness metrics:  
How it works: When you view the details of a model deployment, the Fairness section of the model summary that is displayed, provides the fairness scores for different groups that are described as metrics. The fairness scores are calculated with the disparate impact formula.  
Uses the confusion matrix to measure performance: No  
Do the math:  
The num_positives value represents the number of individuals in the group who received a positive outcome, and the num_instances value represents the total number of individuals in the group. The privileged=False label specifies unprivileged groups and the privileged=True label specifies privileged groups. In Watson OpenScale, the positive outcomes are designated as the favorable outcomes, and the negative outcomes are designated as the unfavorable outcomes. The privileged group is designated as the reference group, and the unprivileged group is designated as the monitored group.  
The calculation produces a percentage that specifies how often the rate that the unprivileged group receives the positive outcome is the same rate that the privileged group receives the positive outcome. For example, if a credit risk model assigns the “no risk” prediction to 80% of unprivileged applicants and to 100% of privileged applicants, that model has a disparate impact of 80%.  
Supported fairness details  
Watson OpenScale supports the following details for fairness metrics:  
The favorable percentages for each of the groups Fairness averages for all the fairness groups Distribution of the data for each of the monitored groups Distribution of payload data  
Description: Fairness metric that describes the fairness for the model predictions. It is the difference between the ratio of favorable outcomes in monitored and reference groups  
Under 0: Higher benefits for the monitored group. At 0: Both groups have equal benefit. Over 0 Implies higher benefit for the reference group.  
Uses the confusion matrix to measure performance: Yes  
Do the math:  
Description: Returns the difference in false negative rates for the monitored and reference groups  
At 0: Both groups have equal benefit.  
Uses the confusion matrix to measure performance: Yes  
Do the math:  
The following formula is used for calculating false negative rate (FNR):  
The following formula is used for calculating false negative rate difference:"
2892	Watson OpenScale fairness metrics	"Supported fairness metrics
Uses the confusion matrix to measure performance: Yes  
Do the math:  
The following formula is used for calculating false negative rate (FNR):  
The following formula is used for calculating false negative rate difference:  
The false positive rate difference gives the percentage of negative transactions that were incorrectly scored as positive by your model in Watson OpenScale.  
Description: Returns the ratio of false positive rate for the monitored group and reference groups.  
At 0: Both groups have equal odds.  
Uses the confusion matrix to measure performance: Yes  
Do the math:  
The following formula is used for calculating false positive rate (FPR):  
The following formula is used for calculating false positive rate difference:  
Description: Returns the difference in false discovery rate for the monitored and reference groups.  
At 0: Both groups have equal odds.  
Uses the confusion matrix to measure performance: Yes  
Do the math:  
The following formula is used for calculating the false discovery rate (FDR):  
The following formula is used for calculating the false discovery rate difference:  
Description: Returns the difference in false omission rate for the monitored and reference groups  
At 0: Both groups have equal odds.  
Uses the confusion matrix to measure performance: Yes  
Do the math:  
The following formula is used for calculating the false omission rate (FOR):  
The following formula is used for the false omission rate difference:  
Description: Returns the difference in error rate for the monitored and reference groups.  
At 0: Both groups have equal odds.  
Uses the confusion matrix to measure performance: Yes  
Do the math:  
The following formula is used for calculating the the error rate (ER):  
The following formula is used for calculating the error rate difference:  
Description: Returns the difference in error rate for the monitored and reference groups.  
At 0: Both groups have equal odds.  
Uses the confusion matrix to measure performance: Yes  
Do the math:  
The following formula is used for calculating false positive rate (FPR):  
The following formula is used for calculating true positive rate (TPR):  
The following formula is used for calculating average odds difference:"
2893	Watson OpenScale fairness metrics	"Supported fairness metrics
The following formula is used for calculating false positive rate (FPR):  
The following formula is used for calculating true positive rate (TPR):  
The following formula is used for calculating average odds difference:  
Description: Returns the average of the absolute difference in false positive rate and true positive rate for the monitored and reference groups.  
At 0: Both groups have equal odds.  
Uses the confusion matrix to measure performance: Yes  
Do the math:  
The following formula is used for calculating false positive rate (FPR):  
The following formula is used for calculating true positive rate (TPR):  
The following formula is used for calculating average absolute odds difference:"
2894	Watson OpenScale fairness metrics	"Supported fairness metrics
Measure Performance with Confusion Matrix
Watson OpenScale uses a confusion matrix to measure performance. The confusion matrix categorizes positive and negative predictions into four quadrants that represent the measurement of actual and predicted values as shown in the following example:  
The true negative (TN) quadrant represents values that are actually negative and predicted as negative and the true positive (TP) quadrant represents values that are actually positive and predicted as positive. The false positive (FP) quadrant represents values that are actually negative but are predicted as positive and the the false negative (FN) quadrant represents values that are actually positive but predicted as negative.  
Note: Watson OpenScale doesn't support performance measures for regression models.  
Parent topic: Configuring fairness evaluations"
2895	Sending model transactions in watsonx.governance	"You must send model transactions from your deployment to watsonx.governance to enable model evaluations.  
To generate accurate results for your model evaluations constantly, watsonx.governance must continue to receive new data from your deployment. watsonx.governance provides different methods that you can use to send transactions for model evaluations."
2896	Sending model transactions in watsonx.governance	"Importing data
When you review evaluation results in watsonx.governance, you can import data by selecting Evaluate now in the Actions menu to import payload and feedback data for your model evaluations.  
For pre-production models, you must upload a CSV file that contains examples of input and output data. To run evaluations with imported data, you must map prompt variables to the associated columns in your CSV file and select Upload and evaluate as shown in the following example:  
For production models, you can select Upload payload data or Upload feedback data in the Import test data window to upload a CSV file as shown in the following example:  
The CSV file must contain labeled columns that match the columns in your payload and feedback schemas. When your upload completes successfully, you can select Evaluate now to run your evaluations with your imported data."
2897	Sending model transactions in watsonx.governance	"Using endpoints
For production models, Watson OpenScale supports endpoints that you can use to provide data in formats that enable evaluations. You can use the payload logging endpoint to send scoring requests for drift evaluations and use the feedback logging endpoint to provide feedback data for quality evaluations. For more information about the data formats, see Managing data for model evaluations.  
Parent topic: Managing data for model evaluations in Watson OpenScale"
2898	Configuring Watson OpenScale with automatic setup	The automatic setup option for Watson OpenScale sets up a machine learning environment, a database, and a sample model for you. Follow the steps in the guided tour to learn how to evaluate the sample model in Watson OpenScale. After the setup is complete, you can add your own model to the dashboard.
2899	Configuring Watson OpenScale with automatic setup	"Sample model
The automatic setup uses the sample data set German Credit Risk to demonstrate key features of Watson OpenScale."
2900	Configuring Watson OpenScale with automatic setup	"Sample model
Overview of the sample data
The German Credit Risk sample data provides a collection of records for bank customers who were used to train the sample model. It contains 20 attributes for each loan applicant. The sample models provisioned as part of the automatic setup are trained to predict level of credit risk for new customers. Two of the attributes considered for the prediction - sex and age - can be tested for bias to make sure that outcomes are consistent regarding gender or age of customers.  
To evaluate the outcomes, results are divided into groups. The Reference groups are the groups that are considered most likely to have positive outcomes. In this case, the Reference groups are male customers and customers over the age of 25. The Monitored groups are the groups that you want to review to ensure that the results do not differ greatly from the results for the monitored groups. In this case, the Monitored groups are females and customers aged 19 - 25."
2901	Configuring Watson OpenScale with automatic setup	"Guided tour highlights
The guided tour demonstrates these features:  
Introduction to the user interface (UI): The four main areas of the UI include Insights, Explanations, Configuration, and Support. Monitoring and viewing results for the German credit risk model: Use predefined monitors to evaluate your model for fairness, quality, and drift. You can also use custom monitors for model evaluation. Exploring Fairness monitor: Use the Fairness monitor to looks for biased outcomes from your model. If a fairness issue is found, an alert is triggered based on configurable thresholds. Exploring data sets: Toggle between balanced, payload, training, and debiased data sets to see how they affect the fairness score of your model. Introduction to transactions: Review transactions from the payload data set for group bias and individual bias. Explaining model outcomes: Understand the features that led to the model prediction to build trust in the model. Additionally, learn how to change feature values to receive more favorable model outcomes. Exploring Drift monitor: Use the Drift monitor to determine if the processing of data in the model is causing a drop in accuracy. Reviewing transactions: Review the transactions list to investigate the drop in accuracy."
2902	Configuring Watson OpenScale with automatic setup	"Touring a specific page
To use the automatic setup guided tour for a specific page, follow these steps:  
Open the page for which you would like to follow the guided tour. Open the Support tab and select Tour this page."
2903	Configuring Watson OpenScale with automatic setup	"Resetting the tour
To reset the automatic setup tour, open the Support tab and select Reset auto setup."
2904	Configuring Watson OpenScale with automatic setup	"Learn more
Configuring Watson OpenScale with manual setup  
Configuring Watson OpenScale with advanced setup  
Parent topic: Setup options for Watson OpenScale"
2905	Drop in data consistency in Watson OpenScale drift metrics	As your data changes over time, the ability of your model to make accurate predictions might deteriorate. The Watson OpenScale drop in data consistency metric calculates the percentage of transactions at run time that are significantly different than the transactions in the training data.
2906	Drop in data consistency in Watson OpenScale drift metrics	"How it works
Watson OpenScale analyzes each transaction for data inconsistency by comparing the run-time transactions with the patterns of the transactions in the training data. If a transaction violates one or more of the training data patterns, the transaction is identified as inconsistent. To calculate the drop in data consistency, Watson OpenScale divides the total number of transactions by the number of transactions that are identified as inconsistent. For example, if 10 transactions are identified as inconsistent from a set of 100 transactions, then the drop in data consistency is 10%."
2907	Drop in data consistency in Watson OpenScale drift metrics	"Do the math
To identify data inconsistency, Watson OpenScale generates a schema when you configure drift detection by creating a constraints.json file to specify the rules that your input data must follow. Watson OpenScale uses the schema to evaluate your data for drift by identifying outliers that do not fit within the constraints that are specified. The schema is specified as a JSON object with columns and constraints arrays that describe the training data as shown in the following example:"
2908	Drop in data consistency in Watson OpenScale drift metrics	"Do the math
Columns
Watson OpenScale specifies values for the name, dtype, count,sparse, and skip_learning keys to describe a column.  
The name and dtype keys describe the label and the data type for a column. The following values that are specified with the dtype key describe the data type:  
categorical numeric_discrete numeric_continuous  
The data type that is specified determines if more statistical properties are described with keys, such as min, max, and mean. For example, when the numeric_discrete or the numeric_continuous data type is specified, properties are described as shown in the following example:  
The count key specifies the number of rows for a column. Watson OpenScale specifies Boolean values to describe the sparse and skip_learning keys for a column. The sparse key specifies whether a column is sparse and the skip_learning key specifies whether a column skips learning any of the rules that are described in the schema. A column is sparse if the 25th and 75th percentiles have the same value."
2909	Drop in data consistency in Watson OpenScale drift metrics	"Do the math
Constraints
The name key specifies the constraint type. The following values are specified to describe the constraint type:  
categorical_distribution_constraint numeric_range_constraint numeric_distribution_constraint catnum_range_constraint catnum_distribution_constraint catcat_distribution_constraint  
The id key identifies constraints with a universally unique identifier (UUID). The kind key specifies whether the constraint is a single_column or two-column constraint.  
The columns key specifies an array of column names. When Watson OpenScale specifies a single_column constraint with the kind key, the array contains a value that correlates with the column that you want to describe. When Watson OpenScale specifies a two-column constraint with the kind key, the array contains values that correlate with columns that contain related data.  
The content key specifies attributes that describe the statistical characteristics of your data. The constraint type that is specified with the name key determines which attribute is specified in the content key as shown in the following table:  
The following sections provide examples of how each constraint type is specified:  
Categorical distribution constraint Numeric range constraint Numeric distribution constraint Categorical- categorical distribution constraint Categorical- numeric range constraint Categorical- numeric distribution constraint  
In the training data, the CheckingStatus column contains four values that are specified with the frequency_distribution attribute. The frequency_distribution attribute specifies the frequency counts with values for categories, such as 0_to_200. If Watson OpenScale finds records in the payload data that specifies values that are different than the frequency_distribution attribute values, the records are identified as drift.  
The LoanAmount column contains minimum and maximum values that are specified with the ranges attribute to set a range for the training data. The ranges attribute specifies the high-density regions of the column. Any ranges that rarely occur in the training data aren't included. If Watson OpenScale finds records in the payload data that does not fit within the range and a pre-defined buffer, the records are identified as drift.  
The LoanAmount column contains values that are specified with the distribution attribute to set a normal distribution for the training data. If Watson OpenScale finds records in the payload data that does not fit within the normal distribution, the records are identified as drift. The distributions that Watson OpenScale tries to fit within are uniform, exponential, or normal distributions. If Watson OpenScale doesn't find records that fit within these distributions, this constraint is not learned."
2910	Drop in data consistency in Watson OpenScale drift metrics	"Do the math
Constraints
For the CheckingStatus and CreditHistory columns, the rare_combinations attributes specifies a combination of values that rarely occur in the training data. If Watson OpenScale finds records in the payload data that contains the combination, the records are identified as drift.  
The ranges attribute specifies minimum and maximum values for the CheckingStatus and LoanAmount columns that set a range for the training data. If Watson OpenScale finds records in the payload data that don't contain LoanAmount and CheckingStatus column values that fit within the range and a pre-defined buffer, the records are identified as drift.  
The LoanAmount and CheckingStatus columns contain values that are specified with the distribution attribute to set a normal distribution for the training data. If Watson OpenScale finds records in the payload data that don't contain LoanAmount and CheckingStatus column values that fit within the normal distribution, the records are identified as drift.  
Note:  
To mitigate drift after it is detected by Watson OpenScale, you must build a new version of the model that fixes the problem. A good place to start is with the data points that are highlighted as reasons for the drift. Introduce the new data to the predictive model after you manually label the drifted transactions and use them to retrain the model."
2911	Drop in data consistency in Watson OpenScale drift metrics	"Learn more
Reviewing drift results  
Parent topic: Drift detection overview"
2912	Configuring explainability in Watson OpenScale	"You can configure explainability in Watson OpenScale to reveal which features contribute to the model's predicted outcome for a transaction and predict what changes would result in a different outcome.  
In the Explainability section of your model configuration page, configure explainability to analyze the factors that influence your model outcomes. You can choose to configure local explanations to analyze the impact of factors for specific model transactions and configure global explanations to analyze general factors that impact model outcomes."
2913	Configuring explainability in Watson OpenScale	"Configure general settings
On the General settings tab, you can configure explainability settings manually or you can run a custom notebook to generate an explainability archive. You can upload the archive to specify the settings for your evaluation.  
If you configure the settings manually, you must specify the explanation methods that you want to use. The methods that you select determines the type of insights that Watson OpenScale provides for explainability. By enabling Global explanation, users can choose either SHAP (Shapley Additive explanations) or LIME (Local Interpretable Model-Agnostic explanations) as the global explanation method. For more information, see Explaining transactions. If you do not provide training data, you will need to upload an explainability archive.  
You can also choose to specify controllable features and enable language support. Controllable features are features that can be changed and have a significant impact on your model outcomes. Watson OpenScale analyzes the controllable features that you specify to identify changes that might produce different outcomes.  
If you enable language support, Watson OpenScale can analyze languages that are not space-delimited to determine explainability. You can configure Watson OpenScale to automatically detect supported languages or you can manually specify any supported languages that you want analyzed. You can't configure language support for structured and image models."
2914	Configuring explainability in Watson OpenScale	"Configure SHAP explanation
If you use SHAP as the local explanation method or enable SHAP global explanation, you must specify settings that determine how SHAP explanations are measured on the SHAP tab. To configure common settings, you must specify the number of perturbations that the model generates for each local explanation and select an option for using background data. Watson OpenScale uses background data to determine the influence of features on outcomes for global and local explanations.  
If you enable SHAP global explanation, you must also configure settings for global explanation. You must specify the sample size of model transactions that is used to generate ongoing explanations and a schedule that determines when the explanations are generated. You must also specify a global explanation stability threshold and select an option that specifies how Watson OpenScale generates a baseline global explanation. These settings are used to calculate global explanation stability."
2915	Configuring explainability in Watson OpenScale	"Configure SHAP explanation
Limitations
When you configure settings for SHAP global explanations, Watson OpenScale has the following limitations: Watson OpenScale does not support equal signs (=) in column names in your data. The equal sign might cause an error. Explainability is not supported for SPSS multiclass models that return only the winning class probability.  
The sample size that you use to configure explanations can affect the number of explanations that Watson OpenScale can generate during specific time periods. If you attempt to generate multiple explanations for large sample sizes, Watson OpenScale might fail to process your transactions. If you configure explanations for multiple Watson OpenScale subscriptions, you must specify the default values for the sample size and number of perturbations settings when your deployment contains 20 features or less."
2916	Configuring explainability in Watson OpenScale	"Learn more
Global explanation stability  
Explaining model transactions  
Parent topic: Evaluating AI models with Watson OpenScale"
2917	Reviewing evaluation results	"When you configure evaluations in the Watson OpenScale service, you can analyze evaluation results to gain insights about your model performance. A dashboard provides the tools for reviewing performance details, sharing information about alerts, or printing reports.  
Some of the details you can review from the dashboard include:  
Review quality results to see a confusion matrix that helps you determine whether your deployed model analyzed your transactions correctly. View drift results to see the transactions that are responsible for a drop in accuracy, a drop in data consistency, or both. Inspect model health evaluation results, where you can see a summary of the metrics that are generated during your last evaluation with scorecard tiles that correlate with different dimensions.  
To view results in the Insights dashboard:  
In Watson Openscale, click the Activity icon to open the Insights Dashboard.  
Select the deployment model tile you want to view results. Watson OpenScale displays the results from your last evaluation.  
Click the arrow in an evaluation section to view data visualizations of evaluation results within the timeframe and Date range settings that you specify. The last evaluation for the timeframe that you select is also displayed during the associated data range.  
Use the Actions menu to view details about your model by selecting any of the following analysis options:  
All evaluations: For pre-production models, display a history of your evaluations to understand how your results change over time. Compare: Compare models with a matrix chart that highlights key metrics to help you determine which version of a model is ready for production or which models might need more training. View model information: View details about your model to understand how your deployment environment is set up. Download report PDF: Generate a model summary report that provides which gives you all of the metrics and the explanation for why they were scored the way they were. Set up alert: Send alerts about threshold violations to an email address.  
You can also use the Actions menu to manage data for model evaluations. For more information, see Sending model transactions.  
With time series charts, Watson OpenScale displays aggregated evaluations as data points that you can select to view results for a specific time. The timestamp of each datapoint that displays when you hover on time series charts does not match the timestamp of the latest evaluation due to the default Watson OpenScale aggregation behaviour."
2918	Reviewing evaluation results	"Analyzing results
The following sections describe how you can analyze results from your Watson OpenScale model evaluations:  
To help you review fairness results, Watson OpenScale provides calculations for the following types of data sets:  
Balanced: Balanced calculation includes the scoring request that is received for the selected hour. The calculation also includes more records from previous hours if the minimum number of records that are required for evaluation was not met. Includes more perturbed and synthesized records that are used to test the model's response when the value of the monitored feature changes. Payload: The actual scoring requests that are received by the model for the selected hour. Training: The training data records that are used to train the model. Debiased: The output of the debiasing algorithm after processing the runtime and perturbed data.  
With the chart, you can observe the groups that experience bias and see the percentage of expected outcomes for these groups. You can also see the percentage of expected outcomes for reference groups, which is the average of expected outcomes across all reference groups. The charts indicates the presence of bias by comparing the ratio of the percentage of expected outcomes for monitored groups in a data range to the percentage of outcomes for reference groups.  
The chart also shows the distribution of the reference and monitored values for each distinct value of the attribute in the data from the payload table that was analyzed to identify bias. The distribution of the payload data is shown for each distinct value of the attributes. You can use this data to correlate the amount of bias with the amount of data that is received by the model. You can also see the percentage of groups with expected outcomes to identify sources of bias that skewed results and led to increases in the percentage of expected outcomes for reference groups.  
To help you review quality results, Watson OpenScale displays a confusion matrix to help you determine whether your deployed model analyzed your transactions incorrectly. For binary classification models, the records are classified as false positives or false negatives and as incorrect class assignments for multi-class models. For binary classification problems, IBM Watson OpenScale assigns the target category to either the positive or negative level. In the confusion matrix, the label for the positive category is located in the second row or column."
2919	Reviewing evaluation results	"Analyzing results
For drift evaluations, you can view the transactions that are responsible for a drop in accuracy, a drop in data consistency, or both. You can also view the number of transactions that are identified and the features of your model that are responsible for reduced accuracy or data consistency.  
For more information, see Reviewing drift transactions.  
When you review drift v2 evaluation results, Watson OpenScale displays collapsible tiles that you can open to view different details about the metrics. You can view the history of how each metric score changes over time with a time series chart or view details how the scores output and feature drifts are calculated. You can also view details about each feature to understand how they contribute to the scores that Watson OpenScale generates.  
When you review model health evaluation results, Watson OpenScale provides a summary of the metrics that are generated during your last evaluation with scorecard tiles that correlate with different dimensions. For metrics with multiple dimensions, you can click a dropdown menu on the tiles to select the metric that you want to analyze. To analyze how your metrics change over time, you can click the collapsible tiles for each category to view timeseries charts.  
For more information, see Model health evaluation metrics.  
Parent topic: Getting insights with Watson OpenScale"
2920	Microsoft Azure ML Studio frameworks	"You can use Microsoft Azure ML Studio with the IBM Watson OpenScale service to log payload or feedback data, and to measure performance accuracy, runtime bias detection, explainability, and auto-debias results for deployed models.  
IBM Watson OpenScale fully supports the following Microsoft Azure Machine Learning Studio frameworks:  
Table 1. Framework support details"
2921	Microsoft Azure ML Studio frameworks	"Adding Microsoft Azure ML Studio to Watson OpenScale
You can configure Watson OpenScale to work with Microsoft Azure ML Studio by using one of the following methods:  
For the first time that you add a machine learning provider to Watson OpenScale, you can use the configuration interface. For more information, see Specifying a Microsoft Azure ML Studio instance. You can also add your machine learning provider by using the Python SDK. You must use this method if you want to have more than one provider. For more information on performing this programmatically, see Add your Microsoft Azure machine learning engine."
2922	Microsoft Azure ML Studio frameworks	"Sample Notebooks
The following Notebook shows how to work with Microsoft Azure ML Studio:  
Data mart creation, model deployment monitoring and data analysis"
2923	Microsoft Azure ML Studio frameworks	"Explore further
Monitor Azure machine learning with Watson OpenScale Consume an Azure Machine Learning model that is deployed as a web service"
2924	Microsoft Azure ML Studio frameworks	"Specifying a Microsoft Azure ML Studio instance
Your first step in the Watson OpenScale tool is to specify a Microsoft Azure ML Studio instance. Your Azure ML Studio instance is where you store your AI models and deployments.  
You can also add your machine learning provider by using the Python SDK. For more information, see Add your Microsoft Azure machine learning engine."
2925	Microsoft Azure ML Studio frameworks	"Connect your Azure ML Studio instance
Watson OpenScale connects to AI models and deployments in an Azure ML Studio instance. To connect your service to Watson OpenScale, go to the Configure tab, add a machine learning provider, and click the Edit icon. In addition to a name and description and whether the environment is a Pre-production or Production, you must provide the following information:  
Client ID: The actual string value of your client ID, which verifies who you are and authenticates and authorizes calls that you make to Azure Studio. Client Secret: The actual string value of the secret, which verifies who you are and authenticates and authorizes calls that you make to Azure Studio. Tenant: Your tenant ID corresponds to your organization and is a dedicated instance of Azure AD. To find the tenant ID, hover over your account name to get the directory and tenant ID, or select Azure Active Directory > Properties > Directory ID in the Azure portal. Subscription ID: Subscription credentials that uniquely identify your Microsoft Azure subscription. The subscription IDforms part of the URI for every service call. See How to: Use the portal to create an Azure AD application and service principal that can access resources for instructions about how to get your Microsoft Azure credentials."
2926	Microsoft Azure ML Studio frameworks	"Payload logging with the Microsoft Azure Machine Learning Studio engine
Add your Microsoft Azure machine learning engine
A non-IBM Watson Machine Learning engine is bound as Custom, meaning that this is just metadata; there is no direct integration with the non-IBM Watson Machine Learning service.  
To see your service subscription, run the following command:"
2927	Microsoft Azure ML Studio frameworks	"Payload logging with the Microsoft Azure Machine Learning Studio engine
Add Microsoft Azure ML Studio subscription
Add subscription by adapting the following code sample:  
To get the subscription list, run the following code:"
2928	Microsoft Azure ML Studio frameworks	"Payload logging with the Microsoft Azure Machine Learning Studio engine
Enable payload logging
To enable payload logging, run the following code:  
To get the logging details, run the following command:"
2929	Microsoft Azure ML Studio frameworks	"Payload logging with the Microsoft Azure Machine Learning Studio engine
Scoring and payload logging
Score your model. For a full example, see the Working with Azure Machine Learning Studio Engine Notebook.  
To store the request and response in the payload logging table, use the following code:  
For languages other than Python, you can also log payload by using a REST API.  
Parent topic: Supported machine learning engines, frameworks, and models"
2930	Glossary	"Learn the terms and concepts that are used in the Watson OpenScale service for evaluating machine learning models.  
Acceptable fairness The percentage of favorable outcomes that a monitored group must receive to meet the fairness threshold. It is calculated by multiplying perfect equality by the fairness threshold.  
Alert A notification that a performance metric is outside of the acceptable range specified by configured monitors.  
Balanced data set A data set that includes the scoring requests received by the model for the selected hour and the perturbed records.  
Baseline data Previous data that is collected before intervention or modification. This data serves as the foundation to which future data collected is compared to.  
Batch deployment Processes the input data from a file, data connection, or connected data in a storage bucket, and writes the output to a selected destination. A method to deploy models that processes input data from a file and writes the output to a file.  
Batch processing If OpenScale needs to be used to monitor with deployment involving huge payload/feedback data, then batch processing is suggested.  
Bias When a machine learning model produces a result for a monitored person, group, or thing that is considered to be unfair when compared to a reference result. Can be caused by a problem with the training data for a model. The Fairness monitor can detect bias that falls under a threshold you set. Related term: Debiasing.  
Cloud Object Storage A service offered by IBM for storing and accessing data. If Cloud Object Storage is the repository for machine learning assets, the associated service credentials must be used to connect to the assets from OpenScale. See also: Resource ID, API key.  
Confidence score The probability that a machine learning model's prediction is correct. A higher score indicates a higher probability that the predicted outcome matches the actual outcome.  
Contrastive explanation Explanations that indicate the minimal set of feature column value changes to change the model prediction. This is computed for a single data point.  
Data mart Workspace where all the OpenScale related metadata gets saved. Behind the scenes, it is connected to a database persistence layer where metadata gets saved.  
Debiased transactions The transactions for which debiased outcome is generated.  
Debiasing When the Fairness monitor detects bias. When a monitored group receives biased outcomes, take steps to mitigate the bias automatically or manually."
2931	Glossary	"Debiased transactions The transactions for which debiased outcome is generated.  
Debiasing When the Fairness monitor detects bias. When a monitored group receives biased outcomes, take steps to mitigate the bias automatically or manually.  
Deployment You deploy a model to make an endpoint available so you can input new data (the request) to the model and get a score, or response. A model deployment can be in a pre-production environment for testing, or a production environment for actual usage.  
Drift When model accuracy declines over time. Can be caused by a change in model input data that leads to model performance deterioration. To monitor for draft, alerts can be created for when the model accuracy drops below a specified acceptable threshold.  
Evaluation The process of using metrics to assess a machine learning model and measure how well the model performs (in areas such as fairness and accuracy). Monitors can assess a model for areas important to goals.  
Explanation An insight into the evaluation of a particular measurement of a model. In OpenScale, an explanation helps understand results and also experiments with what-if scenarios to help address issues.  
Fairness Determine whether a model produces biased outcomes that favor a monitored group over a reference group. The fairness evaluation checks when the model shows a tendency to provide a favorable/preferable outcome more often for one group over another. Typical categories to monitor are age, sex, and race.  
Features List of dataset column names (feature columns) used to train a machine learning model. Example: In a model that predicts whether a person qualifies for a loan, the features for employment status and credit history might be given greater weight than zip code.  
Feedback data Labeled data that matches the schema and structure of the data used to train a machine learning model (including the target) but that was not used for training. This data is already known or actual data used by the Quality monitor to measure the accuracy of a deployed model. Determines whether predictions are accurate when measured against the known outcome.  
Global explanation Explains model's prediction on a sample of data.  
Headless subscription A subscription that has a realtime deployment behind the scenes. Through headless subscription, user can monitor the deployment by using the data (Payload/Feedback) being supplied to the deployment without supplying any scoring URL."
2932	Glossary	"Headless subscription A subscription that has a realtime deployment behind the scenes. Through headless subscription, user can monitor the deployment by using the data (Payload/Feedback) being supplied to the deployment without supplying any scoring URL.  
Labeled data Data that is labeled in a uniform manner for the machine learning algorithms to recognize during model training. Example: A table of data with labeled columns is typical for supervised machine learning. Images can also be labeled for use in a machine learning problem.  
Local explanation Explains a model's prediction by using specific, individual examples.  
Meta-fields Specialized data that is unique between products.  
Monitor Each feature that OpenScale provides is called monitor. Example: Fairness, drift, quality, explainability.  
Monitored group When evaluating fairness, the monitored group represents the values that are most at risk for biased outcomes. Example: In the sex feature, Female and Nonbinary can be set as monitored groups.  
Online deployment Method of accessing a deployment through an API endpoint that provides a real-time score or solution on new data.  
Payload data Any real-time data supplied to a model. Consists of requests to a model (input) and responses from a model (output).  
Payload logging Persisting payload data.  
Perfect equality The percentage of favorable outcomes delivered to all reference groups. For the balanced and debiased data sets, the calculation includes monitored group transactions that were altered to become reference group transactions.  
Perturbations Data points that are simulated around real data points during the computation of different metrics that are associated with monitors—such as fairness, explainability.  
Pre-production space An environment that is used to readily test the data for model validations.  
Prediction column The variable that a supervised machine learning model (trained with labeled data) predicts when presented with new data. See also: Target.  
Probability The confidence with which a model predicts the output. Applicable for classification models.  
Production space A deployment space used for operationalizing machine learning models. Deployments from a production space are evaluated for comparison of actual performance against specified metrics.  
Quality A monitor that evaluates how well a model predicts accurate outcomes based on the evaluation of feedback data. It uses a set of standard data science metrics to evaluate how well the model predicts outcomes that match the actual outcomes in the labeled data set.  
Records Transactions on which monitors are evaluated."
2933	Glossary	"Records Transactions on which monitors are evaluated.  
Reference group When evaluating fairness, the reference group represents the values that are least at risk for biased outcomes. Example: For the Age feature, you can set 30-55 as the reference group and compare results for other cohorts to that group.  
Relative weight The relative weight that a feature has on predicting the target variable. A higher weight indicates more importance. Knowing the relative weight helps explain the model results.  
Resource ID The unique identifier for a resource stored in Cloud Object Storage. To obtain:  
Open https://cloud.ibm.com/resources Find and expand the resource (such as a storage service) Copy the value for Resource ID without the quotation marks  
Response time The time taken to process a scoring request by the model deployment  
Runtime data Data obtained from running a model's lifecycle.  
Scoring endpoint The HTTPS endpoint that users can call to receive the scoring output of a deployed model.  
Scoring request The input to a deployment. See also: Payload.  
Scoring In a model inference, the action of sending request to model and getting a response.  
Self-managed Model transactions stored in your own data warehouse and evaluated by your own Spark analytics engine.  
Service credentials The access IDs required to connect to IBM Cloud resources.  
Service Provider A machine learning providers (typically a model engine: WML, AWS, Azure, Custom) which hosts the deployments.  
Subscription A deployment getting monitored at OpenScale level. There is a 1-1 mapping between deployment and subscription.  
System-managed Model transactions stored in the OpenScale database and evaluated using OpenScale computing resources.  
Target The feature or column of a data set that the trained model predicts. The model is trained by using pre-existing data to learn patterns and discover relationships between the features of the data set and the target. See also: Prediction column.  
Threshold When monitors are configured to evaluate a machine learning model. A benchmark for an acceptable range of outcomes is established. When the outcome falls under the configured threshold, an alert is triggered assess and remedy the situation.  
Training data Data used to teach and train a model's learning algorithm.  
Transactions The records for machine learning model evaluations that are stored in the payload logging table."
2934	Glossary	"Training data Data used to teach and train a model's learning algorithm.  
Transactions The records for machine learning model evaluations that are stored in the payload logging table.  
Unlabeled data Data that is not associated with labels that identify characteristics, classifications, and properties. Unstructured data that is not labeled in a uniform manner. Example: Email or unlabeled images are typical of unlabeled data. Unlabeled data can be used in unsupervised machine learning.  
User ID The id of the user associated with the scoring request  
Parent topic: Evaluating AI models with Watson OpenScale"
2935	Managing data for model evaluations in watsonx.governance	"To enable model evaluations in watsonx.governance, you must prepare your data for logging to generate insights.  
You must provide your model data to watsonx.governance in a format that it supports to enable model evaluations. Watsonx.governance processes your model transactions and logs the data in the watsonx.governance data mart. The data mart is the logging database that stores the data that is used for model evaluations. The following sections describe the different types of data that watsonx.governance logs for model evaluations:"
2936	Managing data for model evaluations in watsonx.governance	"Payload data
Payload data contains the input and output transactions for your deployment. watsonx.governance must receive payload data from your model that it stores in a payload logging table. The table includes timestamp and ID columns to identify each scoring request that you send to watsonx.governance as shown in the following example:  
You must send scoring requests to provide watsonx.governance with a log of your model transactions. For more information, see Managing payload data."
2937	Managing data for model evaluations in watsonx.governance	"Feedback data
Feedback data is labeled data that matches the structure of training data and includes known model outcomes that are compared to your model predictions to measure the accuracy of your model. Watsonx.governance uses feedback data to enable you to configure quality evaluations. You must upload feedback data regularly to watsonx.governance to continuously measure the accuracy of your model predictions. For more information, see Managing feedback data."
2938	Managing data for model evaluations in watsonx.governance	"Learn more
Managing payload data Managing feedback data Sending model transactions"
2939	Configure asset deployments using JSON configuration files	"Import a JSON file in the Watson OpenScale service to create and configure all deployments of your asset for monitoring purposes. Export the configuration file to configure other assets and their deployments.  
Note:  
You can also import and export configurations to Watson OpenScale by using the import subscription and export subscription API methods."
2940	Configure asset deployments using JSON configuration files	"Load the JSON file content as a Python dictionary
For this example, the file sagemaker_native_multiclass_breast-cancer_all_monitors_sub_configuration.json defines configuration data for a model that predicts cancer type.  
To load the file in Python, run the following command:  
The file contains configuration data. Refer to the following example. See the notebook for a complete example of the configuration content."
2941	Configure asset deployments using JSON configuration files	"Import from the configuration file
Now, run the call to add and configure the asset deployment for the sample breast cancer prediction model deployment.  
The binding_uid parameter is optional if only one ML engine is bound."
2942	Configure asset deployments using JSON configuration files	"Exporting to the configuration file
You can also export the configuration file as JSON:"
2943	Configure asset deployments using JSON configuration files	"Results
The asset deployment is created and configured for use by Watson OpenScale.  
Parent topic: Preparing to evaluate a model"
2944	Custom machine learning frameworks	"You can use your custom machine learning framework to complete payload logging, feedback logging, and to measure performance accuracy, runtime bias detection, explainability, drift detection, and auto-debias function in the IBM Watson OpenScale service. The custom machine learning framework must have equivalency to IBM Watson Machine Learning.  
IBM Watson OpenScale fully supports the following custom machine learning frameworks:  
Table 1. Framework support details  
For a model that is not equivalent to IBM Watson Machine Learning, you must create a wrapper for the custom model that exposes the required REST API endpoints. You must also and bridge the input/output between Watson OpenScale and the actual custom machine learning engine."
2945	Custom machine learning frameworks	"When is a custom machine learning engine the best choice for me?
A custom machine learning engine is the best choice when the following situations are true:  
You are not using any immediately available products to serve your machine learning models. You have a system to serve your models and no direct support exists in Watson OpenScale for that function. The serving engine that you are using from a 3rd-party supplier is not supported by Watson OpenScale yet. In this case, consider developing a custom machine learning engine as a wrapper to your original or native deployments."
2946	Custom machine learning frameworks	"How it works
The following image shows the custom environment support:  
You can also reference the following links:  
Watson OpenScale payload logging API  
Python client SDK  
Python SDK for IBM Watson OpenScale  
{ ""fields"": [ ""name"", ""age"", ""position"" ], ""values"": [ [ ""john"", 33, ""engineer"" ], [ ""mike"", 23, ""student"" ] ] { ""fields"": [ ""name"", ""age"", ""position"", ""prediction"", ""probability"" ], ""labels"": [ ""personal"", ""camping"" ], ""values"": [ [ ""john"", 33, ""engineer"", ""personal"", [ 0.6744664422398081, 0.3255335577601919 ] ], [ ""mike"", 23, ""student"" ""camping"", [ 0.2794765664946941, 0.7205234335053059 ] ] ] }  
Input criteria for model to support monitors  
In the following example, your model takes a feature vector, which is essentially a collection of named fields and their values, as an input.  
The “age” field can be evaluated for fairness.  
If the input is a tensor or matrix, which is transformed from the input feature space, that model cannot be handled by the Watson OpenScale platform in the current release. By extension, deep learning models with text or image inputs cannot be handled for bias detection and mitigation.  
Additionally, training data must be loaded to support Explainability.  
For explainability on text, the full text should be one of the features. Explainability on images for a Custom model is not supported in the current release.  
Output criteria for model to support monitors  
Your model outputs the input feature vector alongside the prediction probabilities of various classes in that model.  
In this example, ""personal” and “camping” are the possible classes, and the scores in each scoring output are assigned to both classes. If the prediction probabilities are missing, bias detection works, but auto-debias does not.  
You can access the scoring output from a live scoring endpoint that Watson OpenScale can call with the REST API. For CUSTOMML, SageMaker, and IBM Watson Machine Learning, Watson OpenScale directly connects to the native scoring endpoints."
2947	Custom machine learning frameworks	"Custom machine learning engine
A custom machine learning engine provides the infrastructure and hosting capabilities for machine learning models and web applications. Custom machine learning engines that are supported by Watson OpenScale must conform to the following requirements:  
Expose two types of REST API endpoints:  
discovery endpoint (GET list of deployments and details) scoring endpoints (online and real-time scoring)  
All endpoints need to be compatible with the swagger specification to be supported.  
Input payload and output to or from the deployment must be compliant with the JSON file format that is described in the specification.  
To see the REST API endpoints specification, see Watson OpenScale REST API."
2948	Custom machine learning frameworks	"Adding a custom machine learning engine to Watson OpenScale
You can configure Watson OpenScale to work with a custom machine learning provider by using one of the following methods:  
You can use the configuration interface to add your first custom machine learning provider to Watson OpenScale. For more information, see Specifying a custom machine learning instance. You can also add your machine learning provider by using the Python SDK. You must use this method if you want to have more than one provider. For more information, see Add your custom machine learning engine."
2949	Custom machine learning frameworks	"Explore further
Use a custom machine learning monitor to create a way to interact with other services:  
Monitor custom machine learning engine with Watson OpenScale"
2950	Custom machine learning frameworks	"Specifying a Custom ML service instance
Your first step in the Watson OpenScale tool is to specify a service instance. Your service instance is where you store your AI models and deployments."
2951	Custom machine learning frameworks	"Connect your Custom service instance
Watson OpenScale connects to AI models and deployments in a service instance. You can connect a custom service. To connect your service to Watson OpenScale, go to the Configure tab, add a machine learning provider, and click the Edit icon. In addition to a name, description and specifying the Pre-production or Production environment type, you must provide the following information that is specific to this type of service instance:  
Username Password API endpoint that uses the format https://host:port, such as https://custom-serve-engine.example.net:8443  
Choose whether to connect to your deployments by requesting a list or by entering individual scoring endpoints."
2952	Custom machine learning frameworks	"Connect your Custom service instance
Requesting the list of deployments
If you selected the Request the list of deployments tile, enter your credentials and API Endpoint, then save your configuration."
2953	Custom machine learning frameworks	"Connect your Custom service instance
Providing individual scoring endpoints
If you selected the Enter individual scoring endpoints tile, enter your credentials for the API Endpoint, then save your configuration.  
You are now ready to select deployed models and configure your monitors. Watson OpenScale lists your deployed models on the Insights dashboard where you can click Add to dashboard. Select the deployments that you want to monitor and click Configure.  
For more information, see Configure monitors."
2954	Custom machine learning frameworks	"Custom machine learning engine examples
Use the following ideas to set up your own custom machine learning engine."
2955	Custom machine learning frameworks	"Python and flask
You can use Python and flask to serve scikit-learn model.  
To generate the drift detection model, you must use scikit-learn version 0.20.2 in the notebook.  
The app can be deployed locally for testing purposes and as an application on IBM Cloud."
2956	Custom machine learning frameworks	"Node.js
You can also find an example of a custom machine learning engine that is written in Node.js here."
2957	Custom machine learning frameworks	"End2end code pattern
Code pattern showing end2end example of custom engine deployment and integration with Watson OpenScale."
2958	Custom machine learning frameworks	"Payload logging with the Custom machine learning engine
To configure payload logging for a non-IBM Watson Machine Learning or custom machine learning engine, you must bind the ML engine as custom."
2959	Custom machine learning frameworks	"Payload logging with the Custom machine learning engine
Add your Custom machine learning engine
A non-Watson Machine Learning engine is added as custom by using metadata and no direct integration with the non-IBM Watson Machine Learning service exists. You can add more than one machine learning engine to Watson OpenScale by using the wos_client.service_providers.add method.  
You can see your service provider with the following command:"
2960	Custom machine learning frameworks	"Payload logging with the Custom machine learning engine
Configure security with an API key
To configure security for your custom machine learning engine, Watson OpenScale supports IBM Cloud and IBM Cloud Pak for Data as authentication providers. You can use the https://iam.cloud.ibm.com/identity/token URL to generate an IAM token for IBM Cloud and use the https://<$hostname>/icp4d-api/v1/authorize URL to generate a token for Cloud Pak for Data.  
You can use the POST /v1/deployments/{deployment_id}/online request to implement your scoring API in the following formats:  
Request  
Response"
2961	Custom machine learning frameworks	"Payload logging with the Custom machine learning engine
Add Custom subscription
To add a custom subscription, run the following command:  
To get the subscription list, run the following command:"
2962	Custom machine learning frameworks	"Payload logging with the Custom machine learning engine
Enable payload logging
To enable payload logging in subscription, run the following command:  
To get logging details, run the following command:"
2963	Custom machine learning frameworks	"Payload logging with the Custom machine learning engine
Scoring and payload logging
records_list = [PayloadRecord(request=request_data, response=response_data, response_time=response_time), PayloadRecord(request=request_data, response=response_data, response_time=response_time)] subscription.payload_logging.store(records=records_list)  
Score your model.  
Store the request and response in the payload logging table  
For languages other than Python, you can also log payload by using a REST API.  
Parent topic: Supported machine learning engines, frameworks, and models"
2964	Watson OpenScale performance metrics	"Use performance monitoring to know the velocity of data records processed by your deployment.You enable performance monitoring when you select the deployment to be tracked and monitored.  
Performance metrics are calculated based on the following information:  
scoring payload data  
For proper monitoring purpose, log in every scoring request in Watson OpenScale as well. Payload data logging is automated for IBM Watson Machine Learning engines. For other machine learning engines, the payload data can be provided either by using the Python client or the REST API. Performance monitoring does not create any additional scoring requests on the monitored deployment.  
You can review performance metrics value over time on the Insights dashboard:"
2965	Watson OpenScale performance metrics	"Supported performance metrics
The following performance metrics are supported:  
Throughput"
2966	Watson OpenScale performance metrics	"Next steps
Throughput  
Parent topic: Configuring model evaluations"
2967	Creating custom evaluations and metrics in Watson OpenScale	"To create custom evaluations, select a set of custom metrics to quantitatively track your model deployment and business application. You can define these custom metrics and use them alongside the standard metrics monitored by Watson OpenScale: drift, fairness, and quality.  
You can use one of the following methods to manage custom evaluations and metrics:  
Managing custom metrics with the Python SDK Managing custom metrics in Watson OpenScale"
2968	Creating custom evaluations and metrics in Watson OpenScale	"Managing custom metrics with the Python SDK
To manage custom metrics with the Python SDK, you must perform the following tasks:  
Register custom monitor with metrics definition. Enable custom monitor. Store metric values.  
The following advanced tutorial shows how to do this:  
Working with IBM Watson Machine Learning  
You can disable and enable again custom monitoring at any time. You can remove custom monitor if you do not need it anymore.  
For more information, see the Python SDK documentation."
2969	Creating custom evaluations and metrics in Watson OpenScale	"Managing custom metrics with the Python SDK
Step 1: Register custom monitor with metrics definition.
Before you can start by using custom metrics, you must register the custom monitor, which is the processor that tracks the metrics. You also must define the metrics themselves.  
Use the get_definition(monitor_name) method to import the Metric and Tag objects. Use the metrics method to define the metrics, which require name, thresholds, and type values. Use the tags method to define metadata.  
The following code is from the working sample notebook that was previously mentioned:  
To check how you're doing, run the client.data_mart.monitors.list() command to see whether your newly created monitor and metrics are configured properly.  
You can also get the monitor ID by running the following command:  
For a more detailed look, run the following command:"
2970	Creating custom evaluations and metrics in Watson OpenScale	"Managing custom metrics with the Python SDK
Step 2: Enable custom monitor.
Next, you must enable the custom monitor for subscription. This activates the monitor and sets the thresholds.  
Use the target method to import the Threshold object. Use the thresholds method to set the metric lower_limit value. Supply the metric_id value as one of the parameters. If you don't remember, you can always use the custom_monitor_details command to get the details as shown in the previous example.  
The following code is from the working sample notebook that was previously mentioned:  
To check on your configuration details, use the subscription.monitoring.get_details(monitor_uid=monitor_uid) command."
2971	Creating custom evaluations and metrics in Watson OpenScale	"Managing custom metrics with the Python SDK
Step 3: Store metric values.
You must store, or save, your custom metrics to the region where your Watson OpenScale instance exists.  
Use the metrics method to set which metrics you are storing. Use the subscription.monitoring.store_metrics method to commit the metrics.  
The following code is from the working sample notebook that was previously mentioned:  
To list all custom monitors, run the following command:"
2972	Creating custom evaluations and metrics in Watson OpenScale	"Managing custom metrics in Watson OpenScale
Step 1: Add metric groups
On the Configure tab, click Add metric group. If you want to configure a metric group manually, click Configure new group. a. Specify a name and a description for the metric group. The length of the name that you specify must be less than or equal to 48 characters. b. Click the Edit icon on the Input parameters tile and the specify the details for your input parameters. The parameter name that you specify must match the parameter name that is specified in the metric API. c. If the parameter is required to configure your custom monitor, select the Required parameter checkbox. d. Click Add. After you add the input parameters, click Next. e. Select the model types that your evaluation supports and click Next. f. If you don't want to specify an evaluation schedule, click Save. g. If you want to specify an evaluation schedule, click the toggle. You must specify the interval for the evaluation schedule and click Save. h. Click Add metric and specify the metric details. Click Save. If you want to configure a metric group by using a JSON file, click Import from file. Upload a JSON file and click Import."
2973	Creating custom evaluations and metrics in Watson OpenScale	"Managing custom metrics in Watson OpenScale
Step 2: Add metric endpoints
In the Metric endpoints section, click Add metric endpoint. Specify a name and a description for the metric endpoint. Click the Edit icon on the Connection tile and specify the connection details. Click Next. Select the metric groups that you want associate with the metric endpoint and click Save."
2974	Creating custom evaluations and metrics in Watson OpenScale	"Managing custom metrics in Watson OpenScale
Step 3: Configure custom monitors
On the Insights Dashboard page, select Configure monitors on a model deployment tile. In the Evaluations section, select the name of the metric group that you added. Select the Edit icon on the Metric endpoint tile. Select a metric endpoint and click Next. If you don't want to use a metric endpoint, select None. Use the toggles to specify the metrics that you want to use to evaluate the model and provide threshold values. Click Next. Specify values for the input parameters. If you selected JSON as the data type for the metric group, add the JSON data. Click Next.  
You can now evaluate models with a custom monitor."
2975	Creating custom evaluations and metrics in Watson OpenScale	"Accessing and visualizing custom metrics
To access and visualize custom metrics, you can use programmatic interface. The following advanced tutorial shows how to do this:  
Working with IBM Watson Machine Learning  
For more information, see the Python SDK documentation.  
Visualization of your custom metrics appears on the Watson OpenScale Dashboard."
2976	Creating custom evaluations and metrics in Watson OpenScale	"Learn more
Reviewing evaluation results  
Parent topic: Configuring model evaluations"
2977	Drop in accuracy in Watson OpenScale drift metrics	Watson OpenScale drift evaluations estimate the drop in accuracy of your model at run time when compared to the training data. The model accuracy drops if there is an increase in transactions similar to those that the model did not evaluate correctly in the training data.
2978	Drop in accuracy in Watson OpenScale drift metrics	"How it works
The drift monitor works differently in pre-production and production environments.  
In pre-production environments, when you upload labeled test data, the data is added to the feedback and payload tables. The labeled data is added as an annotation in the payload table. Accuracy is calculated with the labeled data column and the prediction column from the payload table.  
In production environments, Watson OpenScale creates a drift detection model by looking at the data that was used to train and test the model. For example, if the model has an accuracy of 90% on the test data, it means that it provides incorrect predictions on 10% of the test data. Watson OpenScale builds a binary classification model that accepts a data point and predicts whether that data point is similar to the data that the model either incorrectly (10%) or accurately (90%) predicted.  
After Watson OpenScale creates the drift detection model, at run time, it scores this model by using all of the data that the client model receives. For example, if the client model received 1000 records in the past 3 hours, Watson OpenScale runs the drift detection model on those same 1000 data points. It calculates how many of the records are similar to the 10% of records on which the model made an error when training. If 200 of these records are similar to the 10%, then it implies that the model accuracy is likely to be 80%. Because the model accuracy at training time was 90%, it means that there is an accuracy drift of 10% in the model.  
To mitigate drift after it is detected by Watson OpenScale, you must build a new version of the model that fixes the problem. A good place to start is with the data points that are highlighted as reasons for the drift. Introduce the new data to the predictive model after you manually label the drifted transactions and use them to retrain the model."
2979	Drop in accuracy in Watson OpenScale drift metrics	"Do the math
The drop in accuracy metric is calculated for structured binary and multi-class classification models only. Watson OpenScale analyzes each transaction to estimate if the model prediction is accurate. If the model prediction is inaccurate, the transaction is marked as drifted. The estimated accuracy is then calculated as the fraction of nondrifted transactions to the total number of transactions analyzed. The base accuracy is the accuracy of the model on the test data. Watson OpenScale calculates the extent of the drift in accuracy as the difference between Base accuracy and Estimated accuracy. Further, Watson OpenScale analyzes all the drifted transactions; and then, groups transactions based on the similarity of each feature's contribution to the drift in accuracy. In each cluster, Watson OpenScale also estimates the important features that contributed to the drift in accuracy and classifies their feature impact as large, some, and small."
2980	Drop in accuracy in Watson OpenScale drift metrics	"Learn more
Reviewing drift results  
Parent topic: Drift detection overview"
2981	IBM Watson Machine Learning frameworks	"You can use IBM Watson Machine Learning to perform payload logging, feedback logging, and to measure performance accuracy, runtime bias detection, drift detection, explainability, and auto-debias function in the IBM Watson OpenScale service.  
IBM Watson OpenScale fully supports the following IBM Watson Machine Learning frameworks:  
Table 1. Framework support details  
1To learn more about AutoAI, see AutoAI implementation details. For models where the training data is in Cloud Object Storage, there is no support for fairness attributes of type Boolean. However, if the training data is in Db2, Watson OpenScale supports fairness attributes that are Boolean type. When using the AutoAI option, Watson OpenScale does not support models when the model prediction is a binary data type. You must change such models so that the data type of their prediction is a string data type.  
2Fairness and drift metrics are not supported for unstructured (image or text) data types.  
3Keras support does not include support for fairness.  
4Explainability is supported if your model / framework outputs prediction probabilities.  
5To generate the drift detection model, you must use scikit-learn version 0.24.1 in notebooks.  
6For XGBoost binary and multiple class models, you must update the model to return prediction probability in the form of numerical values for binary models and a list of probabilities per class for multi-class models. Support for the XGBoost framework has the following limitations for classification problems: For binary classification, Watson OpenScale supports the binary:logistic logistic regression function with an output as a probability of True. For multiclass classification, Watson OpenScale supports the multi:softprob function where the result contains the predicted probability of each data point belonging to each class."
2982	IBM Watson Machine Learning frameworks	"AutoAI models and training data
AutoAI automatically prepares data, applies algorithms, or estimators, and builds model pipelines that are best suited for your data and use case. Watson OpenScale requires access to the training data to analyze the model.  
Because Watson OpenScale cannot detect the training data location for an AutoAI model like it can for a regular model, you must explicitly provide the needed details to access the training data location:  
For the online path, where you manually configuring monitors, you must provide the database details from which training data can be accessed. For the custom notebook path, where you upload training data distribution, you can use the JSON file that is produced by running the notebook.  
For more information, see Provide model details."
2983	IBM Watson Machine Learning frameworks	"Specifying an IBM Watson Machine Learning service instance
Your first step in the Watson OpenScale tool is to specify an IBM Watson Machine Learning instance. Your Machine Learning instance is where you store your AI models and deployments."
2984	IBM Watson Machine Learning frameworks	"Prerequisites
You should have provisioned an IBM Watson Machine Learning instance in the same account or cluster where the Watson OpenScale service instance is present. If you have provisioned a IBM Watson Machine Learning instance in some other account or cluster, then you cannot configure that instance with automatic payload logging with Watson OpenScale."
2985	IBM Watson Machine Learning frameworks	"Connect your Machine Learning service instance
Watson OpenScale connects to AI models and deployments in an IBM Watson Machine Learning instance. To connect your service to Watson OpenScale, go to the Configure tab, add a machine learning provider, and click the Edit icon. In addition to a name and description and whether this is a Pre-production or Production environment type, you must provide the following information that is specific to this type of service instance:  
If you have an instance of IBM Watson Machine Learning, Watson OpenScale detects it, along with the configuration information.  
Parent topic: Supported machine learning engines, frameworks, and models"
2986	Global explanation stability in Watson OpenScale explainability metrics	Global explanation stability gives the degree of consistency in global explanation over time in Watson OpenScale.
2987	Global explanation stability in Watson OpenScale explainability metrics	"How it works
Watson OpenScale generates a global explanation with the baseline data that you provide when you configure explainability evaluations. Global explanations identify the features that have the most impact on the behavior of your model. When Watson OpenScale generates new global explanations, each explanation is compared to the baseline global explanation to calculate global explanation stability. Global explanation stability uses the normalized discounted cumulative gain (NDGC) formula to determine the similarity between new global explanations and the baseline global explanation."
2988	Global explanation stability in Watson OpenScale explainability metrics	"Global explanation stability at a glance
Description: Higher values indicate higher uniformity with the baseline explanation  
At 0: The explanations are very different. At 1: The explanations are very similar."
2989	Global explanation stability in Watson OpenScale explainability metrics	"Do the math
The following formula is used for calculating global explanation stability:"
2990	Global explanation stability in Watson OpenScale explainability metrics	"Learn more
Explaining model transactions  
Parents topic: Configuring explainability in Watson OpenScale"
2991	Setting up watsonx.governance	"You can set up watsonx.governance to monitor model assets in your IBM watsonx projects or deployment spaces. To set up watsonx.governance, you can manage users and roles for your organization to control access to your projects or deployment spaces.  
To set up watsonx.governance, complete the following tasks:  
Creating access policies Managing users and roles"
2992	Setting up watsonx.governance	"Creating access policies
You can complete the following steps to invite users to an IBM Cloud account that has a watsonx.governance instance installed and assign service access.  
From the IBM Cloud homepage, click Manage > Access (IAM). From the IAM dashboard, click Users and select Invite user. Complete the following fields: If you have multiple instances, you must find the data mart ID to specify the instance that you want to assign users access to. You can use one of the following methods to find the data mart ID: Select the Reader role in the Service access list. Assign access to users.  
How do you want to assign access? : Access policy. Which service do you want to assign access to? : watsonx.governance and click Next. How do you want to scope the access : Select the scope of access for users and click Next.  
If you select Specific resources, select an attribute type and specify a value for each condition that you add. If you select Service instance in the Attribute type list, specify your instance in the Value field.  
On the Insights dashboard, click a model deployment tile and go to Actions > View model information to find the data mart ID. On the Insights dashboard, click the navigation menu on a model deployment tile and select Configure monitors. Then, go to the Endpoints tab and find the data mart ID in the Integration details section of the Model information tab.  
If you are assigning access to new users, click Add, and then click Invite in the Access summary pane. If you are assigning access to existing users, click Add, and then click Assign in the Access summary pane."
2993	Setting up watsonx.governance	"IBM watsonx.governance users and roles
You can assign roles to watsonx.governance users to collaborate on model evaluations in projects and deployment spaces.  
The following table lists permissions for roles that you can assign for access to evaluations. The Operator and Viewer roles are equivalent.  
Parent topic: Setting up the platform for administrators"
2994	Calculating fairness in Watson OpenScale	"Understand the concepts that Watson OpenScale uses to calculate fairness evaluations  
How bias is computed Balanced data and perfect equality Calculating perfect equality Converting the data type of a prediction column Interpreting a fairness score that is greater than 100 percent"
2995	Calculating fairness in Watson OpenScale	"How bias is computed
The Watson OpenScale algorithm for the fairness monitor computes bias on an hourly basis by using the last N records that are present in the payload logging table and the value of N is specified when you configure the fairness monitor. The algorithm applies a method called perturbation to evaluate differences in expected outcomes in the data.  
The perturbation changes the values of the feature from the reference group to the monitored group, or vice-versa. The perturbed data is then sent to the model to evaluate its behavior. The algorithm looks at the last N records in the payload table, and the behavior of the model on the perturbed data, to decide whether the model results indicate the presence of bias.  
A model is biased if the percentage of favorable outcomes for the monitored group is less than the percentage of favorable outcomes for the reference group, by a threshold value you specify when you configure the fairness monitor.  
Note that fairness values can be more than 100%. This calculation means that the monitored group received more favorable outcomes than the reference group. In addition, if no new scoring requests are sent, then the fairness value remains constant."
2996	Calculating fairness in Watson OpenScale	"Balanced data and perfect equality
For balanced data sets, the following concepts apply:  
To determine the perfect equality value, reference group transactions are synthesized by changing the monitored feature value of every monitored group transaction to all reference group values. These new synthesized transactions are added to the set of reference group transactions and evaluated by the model.  
For example, if the monitored feature is SEX and the monitored group is FEMALE, all FEMALE transactions are duplicated as MALE transactions. Other features values remain unchanged. These new synthesized MALE transactions are added to the set of original MALE reference group transactions.  
The percentage of favorable outcomes is determined from the new reference group. This percentage represents perfect fairness for the monitored group. The monitored group transactions are also synthesized by changing the reference feature value of every reference group transaction to the monitored group value. These new synthesized transactions are added to the set of monitored group transactions and evaluated by the model.  
If the monitored feature is SEX and the monitored group is FEMALE, all MALE transactions are duplicated as FEMALE transactions. Other features values remain unchanged. These new synthesized FEMALE transactions are added to the set of original FEMALE monitored group transactions."
2997	Calculating fairness in Watson OpenScale	"Balanced data and perfect equality
Calculating perfect equality
The following mathematical formula is used for calculating perfect equality:  
For example, if the monitored feature is SEX and the monitored group is FEMALE, the following formula shows the equation for perfect equality:  
When you configure fairness evaluations in IBM Watson OpenScale, you can generate a set of metrics to evaluate the fairness of your model. You can use the fairness metrics to determine if your model produces biased outcomes."
2998	Calculating fairness in Watson OpenScale	"Converting the data type of a prediction column
For fairness monitoring, the prediction column allows only an integer numerical value even though the prediction label is categorical. Conversion of the prediction column data type is possible.  
For example, the training data might have class labels such as “Loan Denied”, “Loan Granted”. The prediction value that is returned by IBM Watson Machine Learning scoring end point has values such as “0.0”, “1.0"". The scoring end point also has an optional column that contains the text representation of prediction. For example, if prediction=1.0, the predictionLabel column might have a value “Loan Granted”. If such a column is available, when you configure the favorable and unfavorable outcome for the model, specify the string values “Loan Granted” and “Loan Denied”. If such a column is not available, then you need to specify the integer and double values of 1.0, 0.0 for the favorable, and unfavorable classes.  
IBM Watson Machine Learning has a concept of output schema that defines the schema of the output of IBM Watson Machine Learning scoring end point and the role for the different columns. The roles are used to identify which column contains the prediction value, which column contains the prediction probability, and the class label value, etc. The output schema is automatically set for models that are created by using model builder. It can also be set by using the IBM Watson Machine Learning Python client. Users can use the output schema to define a column that contains the string representation of the prediction. Set the modeling_role for the column to ‘decoded-target’. The documentation for the IBM Watson Machine Learning Python client is available at: https://ibm.github.io/watson-machine-learning-sdk/core_api.html#repository. Search for “OUTPUT_DATA_SCHEMA” to understand the output schema. The API call to use is the store_model call that accepts the OUTPUT_DATA_SCHEMA as a parameter."
2999	Calculating fairness in Watson OpenScale	"Interpreting a fairness score that is greater than 100 percent
Depending on your fairness configuration, your fairness score can exceed 100 percent. It means that your monitored group is getting relatively more “fair” outcomes as compared to the reference group. Technically, it means that the model is unfair in the opposite direction."
3000	Calculating fairness in Watson OpenScale	"Learn more
Configuring the Fairness monitor for indirect bias  
Reviewing model insights with Watson OpenScale  
Parent topic: Configuring the Fairness monitor"
3001	Evaluating prompt templates in projects	"You can evaluate prompt templates in projects with watsonx.governance to measure the performance of foundation model tasks and understand how your model generates responses.  
With watsonx.governance, you can evaluate prompt templates in projects to measure how effectively your foundation models generate responses for the following task types:  
Classification Summarization Generation Question answering Entity extraction"
3002	Evaluating prompt templates in projects	"Before you begin
You must have access to a project to evaluate prompt templates. For more information, see Setting up watsonx.governance.  
To run evaluations, you must log in and switch to a watsonx account that has watsonx.governance and watsonx.ai instances that are installed and open a project. You must be assigned the Admin or Editor roles for the account to open projects.  
In your project, you must use the watsonx.ai Prompt Lab to create and save a prompt template. You must specify variables when you create prompt templates to enable evaluations. The Try section in the Prompt Lab must contain at least one variable.  
Watch this video to see how to evaluate a prompt template in a project.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
The following sections describe how to evaluate prompt templates in projects and review your evaluation results."
3003	Evaluating prompt templates in projects	"Running evaluations
To run prompt template evaluations, you can click Evaluate when you open a saved prompt template on the Assets tab in watsonx.governance to open the Evaluate prompt template wizard. You can run evaluations only if you are assigned the Admin or Editor roles for your project."
3004	Evaluating prompt templates in projects	"Running evaluations
Select dimensions
The Evaluate prompt template wizard displays the dimensions that are available to evaluate for the task type that is associated with your prompt. You can expand the dimensions to view the list of metrics that are used to evaluate the dimensions that you select.  
Watsonx.governance automatically configures evaluations for each dimension with default settings. To configure evaluations with different settings, you can select Advanced settings to set minimum sample sizes and threshold values for each metric as shown in the following example:"
3005	Evaluating prompt templates in projects	"Running evaluations
Select test data
You must upload a CSV file that contains test data with reference columns and columns for each prompt variable. When the upload completes, you must also map prompt variables to the associated columns from your test data."
3006	Evaluating prompt templates in projects	"Running evaluations
Review and evaluate
Before you run your prompt template evaluation, you can review the selections for the prompt task type, the uploaded test data, and the type of evaluation that runs."
3007	Evaluating prompt templates in projects	"Reviewing evaluation results
When your evaluation completes, you can review a summary of your evaluation results on the Evaluate tab in watsonx.governance to gain insights about your model performance. The summary provides an overview of metric scores and violations of default score thresholds for your prompt template evaluations.  
If you are assigned the Viewer role for your project, you can select Evaluate from the asset list on the Assets tab to view evaluation results.  
To analyze results, you can click the arrow next to your prompt template evaluation to view data visualizations of your results over time. You can also analyze results from the model health evaluation that is run by default during prompt template evaluations to understand how efficiently your model processes your data.  
The Actions menu also provides the following options to help you analyze your results:  
Evaluate now: Run evaluation with a different test data set All evaluations: Display a history of your evaluations to understand how your results change over time. Configure monitors: Configure evaluation thresholds and sample sizes. View model information: View details about your model to understand how your deployment environment is set up.  
If you track prompt templates, you can review evaluation results to gain insights about your model performance throughout the AI lifecycle.  
Parent topic: Evaluating AI models with Watson OpenScale."
3008	Configuring fairness evaluations for indirect bias in Watson OpenScale	"Under certain conditions, you can configure Watson OpenScale fairness evaluations to consider indirect bias as well as direct bias for a model deployment.  
Indirect bias occurs when one feature in a data set can be used to stand in for another. For example, in a data set where race is not a known feature, a feature such as postal-code can often track closely to race. Evaluating the postal-code feature for bias is a way of detecting indirect bias. In another example, customer purchasing history might correspond closely with sex. Thus, even a predictive model that does not contain any of the typical protected attributes such as race, age, or sex might indicate biased results.  
Watson OpenScale analyzes indirect bias when the following conditions are met:  
To find correlations, the data set must be sufficiently large (more than 4000 records). The training data must include the meta fields. You must train the model on a subset of data fields. These additional fields, the meta fields, are for determining indirect bias. (Include the meta fields, but don't use them in model training.) Payload logging must contain meta fields and be run before the fairness monitor is configured. You must use this method to upload the meta fields to the Watson OpenScale service. Payload logging for indirect bias requires two types of input: 1) training features with values and 2) meta fields with values. When you configure the fairness monitor, select the additional fields to monitor."
3009	Configuring fairness evaluations for indirect bias in Watson OpenScale	"Typical workflow for indirect bias
However, you can determine indirect bias for preproduction and production models the models require different columns. The test data that is used to evaluate preproduction models and the feedback data that is used to evaluate either preproduction or production models differ on the use of meta columns. Meta columns are required for the test data for preproduction and cannot be included in the feedback data that is used for preproduction or production models. A typical workflow, might include the following steps:  
Create training data that contains both feature columns and meta columns. The meta columns contain data that is not used to train the model. In Watson OpenScale, configure the fairness monitor with the meta columns. During preproduction, upload test data that contains both the feature columns and the meta columns. This test data must be uploaded by using the Import test data CSV option. During pre-production, you might interate on different versions of the model while using the indirect bias measures to ensure that your final model is free of bias. After you send the model to production, the feedback data should not have any of the meta columns, only the feature columns that were used to train the model."
3010	Configuring fairness evaluations for indirect bias in Watson OpenScale	"Sample JSON payload file with meta fields
The following sample file shows a JSON payload with the fields and values that are used to train the model. The meta fields and values that are used for the indirect bias analysis are also included. The meta fields are not used to train the model, instead they are reserved for a different kind of analysis that attempts to correlate them to bias in the model. Although the meta fields can be any type of data, they are usually protected attributes, such as sex, race, or age.  
Meta values must be in the format of an array of arrays:"
3011	Configuring fairness evaluations for indirect bias in Watson OpenScale	"Configuring the Watson OpenScale service for indirect bias
When you set up the fairness monitor, select the fields to monitor. Include both training features and fields that are excluded from model training. If you select a field that is excluded from model training, Watson OpenScale finds correlations between values in that field and values in the training features. The correlated features are used as proxies for the fields that were excluded from model training.  
Some fields are training features. Others fields that are not training features are identified as meta fields. For the selected meta fields, Watson OpenScale checks for indirect bias."
3012	Configuring fairness evaluations for indirect bias in Watson OpenScale	"Learn more
Debiasing options For a notebook for indirect bias, Watson OpenScale Indirect Bias detection  
Parent topic: Configuring model evaluations"
3013	Evaluating prompt templates in deployment spaces	"You can evaluate prompt templates in deployment spaces to measure the performance of foundation model tasks and understand how your model generates responses.  
With watsonx.governance, you can evaluate prompt templates in deployment spaces to measure how effectively your foundation models generate responses for the following task types:  
Classification Summarization Generation Question answering Entity extraction  
Prompt templates are saved prompt inputs for foundation models. You can evaluate prompt template deployments in pre-production and production spaces."
3014	Evaluating prompt templates in deployment spaces	"Before you begin
You must have access to a deployment space to evaluate prompt templates. For more information, see Setting up watsonx.governance.  
To run evaluations, you must log in and switch to a watsonx account that has watsonx.governance and watsonx.ai instances that are installed and open a deployment space. You must be assigned the Admin or Editor roles for the account to open deployment spaces.  
In your project, you must also create and save a prompt template and promote a prompt template to a deployment space. You must specify at least one variable when you create prompt templates to enable evaluations.  
The following sections describe how to evaluate prompt templates in deployment spaces and review your evaluation results:  
Evaluating prompt templates in pre-production spaces Evaluating prompt templates in production spaces"
3015	Evaluating prompt templates in deployment spaces	"Evaluating prompt templates in pre-production spaces
Activate evaluation
To run prompt template evaluations, you can click Activate on the Evaluations tab when you open a deployment to open the Evaluate prompt template wizard. You can run evaluations only if you are assigned the Admin or Editor roles for your deployment space."
3016	Evaluating prompt templates in deployment spaces	"Evaluating prompt templates in pre-production spaces
Select dimensions
The Evaluate prompt template wizard displays the dimensions that are available to evaluate for the task type that is associated with your prompt. You can expand the dimensions to view the list of metrics that are used to evaluate the dimensions that you select.  
Watsonx.governance automatically configures evaluations for each dimension with default settings. To configure evaluations with different settings, you can select Advanced settings to set minimum sample sizes and threshold values for each metric as shown in the following example:"
3017	Evaluating prompt templates in deployment spaces	"Evaluating prompt templates in pre-production spaces
Select test data
You must upload a CSV file that contains test data with reference columns and columns for each prompt variable. When the upload completes, you must also map prompt variables to the associated columns from your test data."
3018	Evaluating prompt templates in deployment spaces	"Evaluating prompt templates in pre-production spaces
Review and evaluate
You can review the selections for the prompt task type, the uploaded test data, and the type of evaluation that runs. You must select Evaluate to run the evaluation."
3019	Evaluating prompt templates in deployment spaces	"Evaluating prompt templates in pre-production spaces
Reviewing evaluation results
When your evaluation finishes, you can review a summary of your evaluation results on the Evaluations tab in watsonx.governance to gain insights about your model performance. The summary provides an overview of metric scores and violations of default score thresholds for your prompt template evaluations.  
To analyze results, you can click the arrow next to your prompt template evaluation to view data visualizations of your results over time. You can also analyze results from the model health evaluation that is run by default during prompt template evaluations to understand how efficiently your model processes your data.  
The Actions menu also provides the following options to help you analyze your results:  
Evaluate now: Run evaluation with a different test data set All evaluations: Display a history of your evaluations to understand how your results change over time. Configure monitors: Configure evaluation thresholds and sample sizes. View model information: View details about your model to understand how your deployment environment is set up.  
If you track your prompt templates, you can review evaluation results to gain insights about your model performance throughout the AI lifecycle."
3020	Evaluating prompt templates in deployment spaces	"Evaluating prompt templates in production spaces
Activate evaluation
To run prompt template evaluations, you can click Activate on the Evaluations tab when you open a deployment to open the Evaluate prompt template wizard. You can run evaluations only if you are assigned the Admin or Editor roles for your deployment space."
3021	Evaluating prompt templates in deployment spaces	"Evaluating prompt templates in production spaces
Select dimensions
The Evaluate prompt template wizard displays the dimensions that are available to evaluate for the task type that is associated with your prompt. You can provide a label column name for the reference output that you specify in your feedback data. You can also expand the dimensions to view the list of metrics that are used to evaluate the dimensions that you select.  
Watsonx.governance automatically configures evaluations for each dimension with default settings. To configure evaluations with different settings, you can select Advanced settings to set minimum sample sizes and threshold values for each metric as shown in the following example:"
3022	Evaluating prompt templates in deployment spaces	"Evaluating prompt templates in production spaces
Review and evaluate
You can review the selections for the prompt task type and the type of evaluation that runs. You can also select View payload schema or View feedback schema to validate that your column names match the prompt variable names in the prompt template. You must select Activate to run the evaluation.  
To generate evaluation results, select Evaluate now in the Actions menu to open the Import test data window when the evaluation summary page displays."
3023	Evaluating prompt templates in deployment spaces	"Evaluating prompt templates in production spaces
Import test data
In the Import test data window, you can select Upload payload data or Upload feedback data to upload a CSV file that contains labeled columns that match the columns in your payload and feedback schemas.  
When your upload completes successfully, you can select Evaluate now to run your evaluation."
3024	Evaluating prompt templates in deployment spaces	"Evaluating prompt templates in production spaces
Reviewing evaluation results
When your evaluation finishes, you can review a summary of your evaluation results on the Evaluations tab in watsonx.governance to gain insights about your model performance. The summary provides an overview of metric scores and violations of default score thresholds for your prompt template evaluations.  
To analyze results, you can click the arrow next to your prompt template evaluation to view data visualizations of your results over time. You can also analyze results from the model health evaluation that is run by default during prompt template evaluations to understand how efficiently your model processes your data.  
The Actions menu also provides the following options to help you analyze your results:  
Evaluate now: Run evaluation with a different test data set Configure monitors: Configure evaluation thresholds and sample sizes. View model information: View details about your model to understand how your deployment environment is set up.  
If you track your prompt templates, you can review evaluation results to gain insights about your model performance throughout the AI lifecycle."
3025	Frequently asked questions	Find answers to frequently asked questions about the Watson OpenScale service.
3026	Frequently asked questions	"General
To run Watson OpenScale on my own servers, how much computer processing power is required? Why does Watson OpenScale need access to training data? Is there a command-line tool to use? What version of Python can I use with Watson OpenScale? Which browsers can I use to run Watson OpenScale?"
3027	Frequently asked questions	"Data training questions
Configuring a model requires information about the location of the training data and the options are Cloud Object Storage and Db2. If the data is in Netezza, can Watson OpenScale use Netezza? Why doesn't Watson OpenScale see the updates that were made to the model? Must I keep monitoring the Watson OpenScale dashboard to make sure that my models behave as expected?"
3028	Frequently asked questions	"Risk, bias, fairness, and explainability
What are the various kinds of risks associated in using a machine learning model? How is model bias mitigated by using Watson OpenScale? Is it possible to check for model bias on sensitive attributes, such as race and sex, even when the model is not trained on them? Can I configure model fairness through an API? What are the types of explanations shown in Watson OpenScale? What is what-if analysis in Watson OpenScale? In Watson OpenScale, for which models is Local/LIME explanation supported? In Watson OpenScale, for which models is contrastive explanation and what-if analysis supported? What are controllable features in Watson OpenScale explainability configuration?"
3029	Frequently asked questions	"Drift
What are the different kinds of drift that IBM Watson OpenScale detects? Does Watson OpenScale detect drift in accuracy and drift in data? Why should one be concerned about model accuracy drift or data drift? How is drop in accuracy that is, model accuracy drift calculated in IBM Watson OpenScale? How is the drop in data consistency calculated in IBM Watson OpenScale? Can Watson OpenScale detect drift in my model that is trained on text corpus? Can Watson OpenScale detect drift in my model that is trained on image data? Can Watson OpenScale detect drift in my Python function that is deployed on IBM Watson Machine Learning?"
3030	Frequently asked questions	"Other
In a pre-production environment that uses Watson OpenScale, after the model is evaluated for risk and approved for usage, do I must reconfigure all the monitors again in production environment? In Watson OpenScale, can I compare my model deployments in pre-production with a benchmark model to see how good or bad it is? In Watson OpenScale, what data is used for Quality metrics computation? In Watson OpenScale, can the threshold be set for a metric other than 'Area under ROC' during configuration? In Watson OpenScale, why are some of the configuration tabs disabled?"
3031	Frequently asked questions	"Other
To run Watson OpenScale on my own servers, how much computer processing power is required?
There are specific guidelines for hardware configuration for three-node and six-node configurations. Your IBM Technical Sales team can also help you with sizing your specific configuration. Because Watson OpenScale runs as an add-on to IBM Cloud Pak for Data, you need to consider the requirements for both software products."
3032	Frequently asked questions	"Other
Why does Watson OpenScale need access to training data?
You must either provide Watson OpenScale access to training data that is stored in Db2 or IBM Cloud Object Storage, or you must run a Notebook to access the training data.  
Watson OpenScale needs access to your training data for the following reasons:  
To generate Local Interpretable Model-Agnostic Explanations (LIME) and Contrastive explanations: To create explanations, access to statistics, such as median value, standard deviation, and distinct values from the training data is required. To display training data statistics: To populate the bias details page, Watson OpenScale must have training data from which to generate statistics. To build a drift detection model: The Drift monitor uses training data to create and calibrate drift detection.  
In the Notebook-based approach, you are expected to upload the statistics and other information when you configure a deployment in Watson OpenScale. Watson OpenScale no longer has access to the training data outside of the Notebook, which is run in your environment. It has access only to the information uploaded during the configuration."
3033	Frequently asked questions	"Other
Is there a command-line tool to use?
Yes! There is a ModelOps CLI tool, whose official name is the Watson OpenScale CLI model operations tool. Use it to run tasks related to the lifecycle management of machine learning models."
3034	Frequently asked questions	"Other
What version of Python can I use with Watson OpenScale?
Because Watson OpenScale is independent of your model-creation process, it supports whatever Python versions your machine learning provider supports. The Watson OpenScale Python client is a Python library that works directly with the Watson OpenScale service on IBM Cloud. For the most up-to-date version information, see the Requirements section. You can use the Python client, instead of the Watson OpenScale client UI, to directly configure a logging database, bind your machine learning engine, and select and monitor deployments. For examples of using the Python client in this way, see the Watson OpenScale sample Notebooks."
3035	Frequently asked questions	"Other
Which browsers can I use to run Watson OpenScale?
The Watson OpenScale service tooling requires the same level of browser software as is required by IBM Cloud. See the IBM Cloud Prerequisites topic for details."
3036	Frequently asked questions	"Other
Configuring a model requires information about the location of the training data and the options are Cloud Object Storage and Db2. If the data is in Netezza, can Watson OpenScale use Netezza?
Use this Watson OpenScale Notebook to read the data from Netezza and generate the training statistics and also the drift detection model."
3037	Frequently asked questions	"Other
Why doesn't Watson OpenScale see the updates that were made to the model?
Watson OpenScale works on a deployment of a model, not on the model itself. You must create a new deployment and then configure this new deployment as a new subscription in Watson OpenScale. With this arrangement, you are able to compare the two versions of the model."
3038	Frequently asked questions	"Other
How is model bias mitigated by using Watson OpenScale?
The debiasing capability in Watson OpenScale is enterprise grade. It is robust, scalable and can handle a wide variety of models. Debiasing in Watson OpenScale consists of a two-step process.  
Learning Phase: Learning customer model behavior to understand when it acts in a biased manner. Application Phase: Identifying whether the customer’s model acts in a biased manner on a specific data point and, if needed, fixing the bias. For more information, see Reviewing debiased transactions."
3039	Frequently asked questions	"Other
Is it possible to check for model bias on sensitive attributes, such as race and sex, even when the model is not trained on them?
Yes. Recently, Watson OpenScale delivered a ground-breaking feature called “Indirect Bias detection.” Use it to detect whether the model is exhibiting bias indirectly for sensitive attributes, even though the model is not trained on these attributes. For more information, see Reviewing debiased transactions."
3040	Frequently asked questions	"Other
Can I configure model fairness through an API?
Yes, it is possible with the Watson OpenScale SDK. For more information, see IBM Watson OpenScale Python SDK documentation."
3041	Frequently asked questions	"Other
What are the various kinds of risks associated in using a machine learning model?
Multiple kinds of risks that are associated with machine learning models, such as any change in input data (also known as drift) can cause the model to make inaccurate decisions, impacting business predictions. Training data can be cleaned to be free from bias but runtime data might induce biased behavior of the model.  
Traditional statistical models are simpler to interpret and explain, but unable to explain the outcome of the machine learning model can pose a serious threat to the usage of the model."
3042	Frequently asked questions	"Other
Does Watson OpenScale detect drift in accuracy and drift in data?
Watson OpenScale detects both drift in accuracy and drift in data:  
Drift in accuracy estimates the drop in accuracy of the model at run time. Model accuracy drops when there is an increase in transactions that are similar to those that the model did not evaluate correctly in the training data. This type of drift is calculated for structured binary and multi-class classification models only. Drift in data estimates the drop in consistency of the data at runtime as compared to the characteristics of the data at training time."
3043	Frequently asked questions	"Other
What are the types of explanations shown in Watson OpenScale?
Watson OpenScale provides Local explanation based on LIME, contrastive explanation, and SHAP explanations. For more information, see Viewing explainability."
3044	Frequently asked questions	"Other
What is what-if analysis in Watson OpenScale?
The explanations UI also provides the ability to test what-if scenarios. For example, the user can change the feature values of the input datapoint and check its impact on the model prediction and probability."
3045	Frequently asked questions	"Other
Which models support LIME explanations?
The following types of models support LIME explanations in Watson OpenScale:  
Structured regression and classification models Unstructured text and image classification models"
3046	Frequently asked questions	"Other
Which models support contrastive explanations and what-if analysis?
Contrastive explanations and what-if analyses are supported for models that use structured data and classification problems only."
3047	Frequently asked questions	"Other
What are controllable features in Watson OpenScale explainability configuration?
Using controllable features some features of the input data point can be locked, so that they do not change when the contrastive explanation is generated and also they cannot be changed in what if analysis. The features that should not be changed should be set as noncontrollable or NO in the explainability configuration."
3048	Frequently asked questions	"Other
Must I keep monitoring the Watson OpenScale dashboard to make sure that my models behave as expected?
No, you can set up email alerts for your production model deployments in Watson OpenScale. Email alerts are sent whenever a risk evaluation test fails, and then you can come and check the issues and address them."
3049	Frequently asked questions	"Other
In a pre-production environment that uses Watson OpenScale after the model is evaluated for risk and approved for usage, do I have to reconfigure all the monitors again in production environment?
No, Watson OpenScale provides a way to copy the configuration of pre-production subscription to production subscription. For more information, see Manage model risk."
3050	Frequently asked questions	"Other
In Watson OpenScale, can I compare my model deployments in pre-production with a benchmark model to see how good or bad it is?
Yes, Watson OpenScale provides you with the option to compare two model deployments or subscriptions. You can see a side-by-side comparison of the behavior of the two models on each of the monitors configured. To compare go to the model summary page on Watson OpenScale dashboard and select Actions -> Compare. For more information, see Reviewing evaluation results."
3051	Frequently asked questions	"Other
In Watson OpenScale, what data is used for Quality metrics computation?
Quality metrics are calculated that use manually labeled feedback data and monitored deployment responses for this data."
3052	Frequently asked questions	"Other
In Watson OpenScale, can the threshold be set for a metric other than 'Area under ROC' during configuration?
No, currently, the threshold can be set only for the 'Area under ROC' metric."
3053	Frequently asked questions	"Other
In Watson OpenScale, why are some of the configuration tabs disabled?
Some conditions enable particular tabs. You can see the reason why that tab is not enabled, by hovering your mouse over the circle icon on the tab."
3054	Frequently asked questions	"Other
Why should one be concerned about model accuracy drift or data drift?
A drop in either model accuracy or data consistency leads to a negative impact on the business outcomes that are associated with the model and must be addressed by retraining the model."
3055	Frequently asked questions	"Other
What are the different kinds of drift that IBM Watson OpenScale detects?
Watson OpenScale detects both drift in model accuracy and drift in data."
3056	Frequently asked questions	"Other
How is drop in accuracy that is, model accuracy drift calculated in Watson OpenScale?
Watson OpenScale learns the behavior of the model by creating a proxy model, also known as a drift detection model. It looks at the training data and how the model is making predictions on the training data.  
For more information, see Drift detection."
3057	Frequently asked questions	"Other
How is the drop in data consistency calculated in IBM Watson OpenScale?
IBM Watson OpenScale learns single and two-column constraints or boundaries on the training data at the time of configuration. It then analyzes all payload transactions to determine which transactions are causing drop in data consistency. For more information, see Drift in data."
3058	Frequently asked questions	"Other
Can Watson OpenScale detect drift in my model that is trained on text corpus?
Watson OpenScale cannot detect drift in text-based models as of now."
3059	Frequently asked questions	"Other
Can Watson OpenScale detect drift in my model that is trained on image data?
Watson OpenScale cannot detect drift in image-based models as of now."
3060	Frequently asked questions	"Other
Can Watson OpenScale detect drift in my Python function that is deployed on IBM Watson Machine Learning?
Watson OpenScale can detect drift for Python functions that are trained on structured datasets. The python functions that were trained on text or image datasets are not supported. The scoring output for the python functions must include prediction columns and probability columns that contain all of the class probabilities for classification problems.  
Parent topic: Evaluating AI models with Watson OpenScale"
3061	IBM watsonx.governance offering plan options	"The watsonx.governance service enables responsible, transparent, and explainable AI.  
The available plans depend on the region where you are provisioning the service from the IBM Cloud catalog.  
In the Dallas region, provision a watsonx.governance plan. If you provision watsonx.ai in Dallas, watsonx.governance is provisioned as well. In the Frankfurt region, provision an Watson OpenScale plan. Watsonx.governance plans are not available in the Tokyo region."
3062	IBM watsonx.governance offering plan options	"IBM watsonx.governance plans (Dallas only)
Watsonx.governance offers a free Lite plan and a paid Essentials plan.  
With watsonx.governance you can:  
Evaluate machine learning models for dimensions such as fairness, quality, or drift. Define AI use cases in a collaborative, open way to define a business problem and track the solution. Capture the details for machine learning models, in each stage of their lifecycle, and store the data in factsheets within an associated AI use case. Maintain collections of AI uses cases in inventories, where you can manage access.  
For Large Language Models in watsonx.ai, you can also:  
Evaluate prompt templates across multiple dimensions such as quality, Personally Identifable Information (PII) in prompt input and outputs, and Abuse or Profanity in Prompt input and ouput. Monitor metrics for Large Language Model performance. Automatically capture metadata in a Factsheet from development to deployment, for each stage in the lifecycle."
3063	IBM watsonx.governance offering plan options	"IBM watsonx.governance plans (Dallas only)
IBM watsonx.governance Lite plan features
Lite plan features include:  
Maximum of 200 resource units 1 resource unit per predictive model evaluation 1 resource unit per foundational model evaluation 1 resource unit per global explanation, with a maximum of 500 local explanations 1 resource unit per 500 local explanations Maximum of 1,000 records per evaluation Limit of 3 rows per use case Limit of 3 use cases Limit of 1 inventory"
3064	IBM watsonx.governance offering plan options	"IBM watsonx.governance plans (Dallas only)
IBM watsonx.governance Essential plan features
Essential plan features include:  
Maximum of 500 inventories 1 resource unit per predictive model evaluation 1 resource unit per foundational model evaluation 1 resource unit per global explanation, with a maximum of 500 local explanations 1 resource unit per 500 local explanations Maximum of 50,000 records per evaluation"
3065	IBM watsonx.governance offering plan options	"IBM watsonx.governance plans (Dallas only)
Billing rates
For the Essentials plan, billing is based on a flat rate of $0.60 per resource unit consumed. For billing details, see the IBM Cloud service page for watsonx.governance."
3066	IBM watsonx.governance offering plan options	"Monitoring resource usage
You can track RU usage for assets you own or collaborate on in a project or space. If you are an account owner or administrator, you can track resource usage for an entire account."
3067	IBM watsonx.governance offering plan options	"Monitoring resource usage
Tracking resource usage in a project or space
To monitor resource consumption in a project or space:  
Navigate to the Manage tab for a project or space. Click Resources to review a summary of resource consumption for assets in the project or space, or to review resource consumption details for particular assets."
3068	IBM watsonx.governance offering plan options	"Monitoring resource usage
Tracking resource usage for an account
You can track the runtime usage for an account on the Environment Runtimes page if you are the IBM Cloud account owner or administrator or the Watson Machine Learning service owner. For details, see Monitoring resource."
3069	IBM watsonx.governance offering plan options	"Next steps
Provisioning and launching the watsonx.governance service  
Parent topic: watsonx.governance"
3070	Evaluating AI models	"If you are using the Watson OpenScale or watsonx.governance service, you can track and measures outcomes from your AI models to help ensure that they are compliant with business processes no matter where your models are built or running.  
Enterprises use model evaluations as part of AI governance strategies to ensure that models in deployment environments meet established compliance standards regardless of the tools and frameworks that are used to build and run the models. This approach ensures that AI models are free from bias, can be easily explained and understood by business users, and are auditable in business transactions. You can evaluate machine learning models or evaluate prompt templates for foundation models, depending on which service that you use.  
You can use watsonx.governance to evaluate machine learning models and prompt templates in projects and spaces for foundation models."
3071	Evaluating AI models	"Learn more
Setup options for Watson OpenScale Glossary FAQs Supported machine learning engines, frameworks, and models Preparing to evaluate models Managing data for model evaluations Evaluating prompt templates in projects Evaluating prompt templates in deployment spaces Configuring model evaluations Configuring explainability Reviewing model insights with Watson OpenScale Model risk management and model governance APIs, SDKs, and tutorials  
Parent topic: Governing AI assets"
3072	Amazon SageMaker frameworks	"You can use Amazon SageMaker to log payload and feedback data, and to measure performance accuracy, bias detection, explainability, and auto-debias function in the IBM Watson OpenScale service.  
IBM Watson OpenScale fully supports the following Amazon SageMaker frameworks:  
Table 1. Framework support details  
1Support for regression models does not include drift magnitude."
3073	Amazon SageMaker frameworks	"Adding Amazon SageMaker to Watson OpenScale
You can configure Watson OpenScale to work with Amazon SageMaker by using one of the following methods:  
The first time that you add a machine learning provider to Watson OpenScale, you can use the configuration interface. For more information, see Specifying an Amazon SageMaker instance. You can also add your machine learning provider by using the Python SDK. You must use this method if you want to have more than one provider. For more information, see Add your Amazon SageMaker machine learning engine."
3074	Amazon SageMaker frameworks	"Sample Notebooks
The following Notebooks show how to work with Amazon SageMaker:  
Creation and deployment of credit risk prediction model Data mart creation, model deployment monitoring and data analysis"
3075	Amazon SageMaker frameworks	"Specifying an Amazon SageMaker ML service instance
Your first step in the Watson OpenScale tool is to specify an Amazon SageMaker service instance. Your Amazon SageMaker service instance is where you store your AI models and deployments."
3076	Amazon SageMaker frameworks	"Connect your Amazon SageMaker service instance
Watson OpenScale connects to AI models and deployments in an Amazon SageMaker service instance. To connect your service to Watson OpenScale, go to the Configure tab, add a machine learning provider, and click the Edit icon. In addition to a name and description and whether the environment Pre-production or Production, you must provide the following information that is specific to this type of service instance:  
Access Key ID: Your AWS access key ID, aws_access_key_id, which verifies who you are and authenticates and authorizes calls that you make to AWS. Secret Access Key: Your AWS secret access key, aws_secret_access_key, which is required to verify who you are and to authenticate and authorize calls that you make to AWS. Region: Enter the region where your Access Key ID was created. Keys are stored and used in the region in which they were created and cannot be transferred to another region.  
You are now ready to select deployed models and configure your monitors. Watson OpenScale lists your deployed models on the Insights dashboard where you can click Add to dashboard. Select the deployments that you want to monitor and click Configure.  
For more information, see Configure monitors."
3077	Amazon SageMaker frameworks	"Add your Amazon SageMaker machine learning engine
A non-IBM Watson Machine Learning engine is bound as Custom by using metadata. No direct integration with the non-IBM Watson Machine Learning service is possible.  
To see your service subscription, run the following code:"
3078	Amazon SageMaker frameworks	"Add Amazon SageMaker ML subscription
To add the subscription, run the following code:  
To get the subscription list, run the following command:"
3079	Amazon SageMaker frameworks	"Enable payload logging
To enable payload logging, run the following command:  
To get logging details, run the following command:"
3080	Amazon SageMaker frameworks	"Scoring and payload logging
Score your model. For a full example, see the Working with SageMaker machine learning Engine Notebook.  
To store the request and response in the payload logging table, run the following code:  
For languages other than Python, you can also log payload by using a REST API.  
Parent topic: Supported machine learning engines, frameworks, and models"
3081	Feedback logging	"You can enable feedback logging in watsonx.governance to configure model evaluations.  
To manage feedback data for configuring quality and generative AI quality evaluations, watsonx.governance must log your feedback data in the feedback logging table.  
Generative AI quality evaluations use feedback data to generate results for the following task types when you evaluate prompt templates:  
Text summarization Content generation Question answering Entity extraction  
Quality evaluations use feedback data to generate results for text classification tasks."
3082	Creating credentials for Watson OpenScale	"To access the IBM Watson OpenScale REST APIs, you must have an API key and a data mart ID, also known as a service instance. The Platform API key gives an individual user the ability to access resources in the IBM Cloud.  
For enterprise accounts, an administrator can create the data mart, invite users into the account, and give those users access to a specific Watson OpenScale data mart. A user can then create their own API key, and can access the same Watson OpenScale data mart without conflict or security risk."
3083	Creating credentials for Watson OpenScale	"Creating the API key
To create an IBM Cloud API key, complete the following steps:  
Log in to IBM Cloud. Select Manage > Access (IAM) > IBM Cloud API Keys Click Create an IBM Cloud API key. Give your key a name and description and click Create."
3084	Creating credentials for Watson OpenScale	"Finding service IDs:
Find your data mart, deployment, subscription, or binding IDs, on the Watson OpenScale Payload logging page, which is shown when you select Configure monitors for a deployment.  
Click the model deployment tile. Click Configure monitors . Click Endpoints. On the Watson OpenScale Model information tab, in the Integration details pane, find an ID, such as the Datamart ID."
3085	Creating credentials for Watson OpenScale	"Creating service instance credentials by using the command console
To create credentials for Watson OpenScale, complete the following steps by using the IBM Cloud command console:  
ibmcloud login --sso ibmcloud iam api-key-create 'my_key' Name my_key Created At 2018-10-09T14:04+0000 API Key Tg4Gxxxxxxxxxxxxxxxxx_xxxxxxxxxxxxxxxxxQU-nE Locked false UUID ApiKey-xxxxxxxxx-afd7-xxxxx-b0e1-xxxxxxxxxxx ibmcloud target -g myResourceGroup ibmcloud resource service-instance '<Your_Watson_OpenScale_instance_name>' Name: AI OpenScale-my_instance ID: crn:v1:ibmcloud:public:aiopenscale:us-south:a/c2f2xxxxxxxxxxxx867:: GUID: 03daxxxx-xxxx-xxxx-xxxx-xxxxxxxx38a7 Location: us-south Service Name: aiopenscale Service Plan Name: lite Resource Group Name: Default State: active Type: service_instance Sub Type: Tags: Created at: 2018-09-17T13:58:43Z Updated at:  
Retrieve your API key by running the following command:  
The following information displays:  
Verify the Resource Group that you are using in your IBM Cloud account.  
Go to the Dashboard. From the navigation menu, click Resource list. From the Group column, click the Filter by group or organization drop-down selection and set the Default checkbox.  
If you are not using the Default resource group, then run the following command to get your credential for Watson OpenScale:  
Where myResourceGroup is the name of the resource group that is associated with your Watson OpenScale instance.  
Retrieve your Watson OpenScale instance ID by running the following command:  
If you are using the IBM Cloud command console on Windows, replace the single quotation marks (') in the preceding commands with double quotation marks ("").  
The following information displays:  
The GUID value is your Watson OpenScale instance ID."
3086	Creating credentials for Watson OpenScale	"Next steps
Defining the input and output schema by using the Python Client or REST API  
Parent topic:"
3087	Reviewing debiased transactions	"The Watson OpenScale service can detect direct and indirect bias and uses two types of debiasing: active and passive. Passive debiasing reveals bias, while active debiasing prevents bias by changing the model in real time.  
The algorithm applies a method called perturbation to evaluate differences in expected outcomes in the data. For more information on how bias is computed, see Calculating fairness.  
When you evaluate deployments for fairness, Watson OpenScale detects both direct and indirect bias bias of the transactions from the payload logging table."
3088	Reviewing debiased transactions	"Passive debiasing
Passive debiasing is the work that Watson OpenScale does by itself, automatically, every hour. It is considered passive because it happens without user intervention. When Watson OpenScale checks for bias, it also debiases the data. It analyzes the behavior of the model, and identifies the data where the model acts in a biased manner.  
Watson OpenScale then builds a machine learning model to predict whether the model is likely to act in a biased manner on a given, new data point. Watson OpenScale then analyzes the data that is received by the model, on an hourly basis, and finds the data points that cause bias. For such data points, the fairness attribute is perturbed from minority to majority, and majority to minority, and the perturbed data is sent to the original model for prediction. The prediction of the perturbed record, along with the original prediction is used to calculate the bias.  
Watson OpenScale debiases the identified biased records that the model receives in the past hour. It also computes the fairness for the debiased output, and displays it in the Debiased model tab."
3089	Reviewing debiased transactions	"Active debiasing
Active debiasing is a way for you to request and bring debiased results into your application through the REST API endpoint. You can actively invoke Watson OpenScale to know the debiased prediction of your model so that you can run your application in a no bias way. In active debiasing, you can use a debiasing REST API endpoint from your application. This REST API endpoint internally calls your model, and checks its behavior.  
If Watson OpenScale detects that the model is acting in a biased manner, it perturbs the data, and sends it back to the original model. After internal analysis on the perturbed data point, if Watson OpenScale detects that the model is behaving in a biased manner on the data point, then the output of the original model on the perturbed data is returned as the debiased prediction.  
If Watson OpenScale determines that the original model is not acting in a biased manner, then Watson OpenScale returns the original model's prediction as the debiased prediction. Thus, by using this REST API endpoint, you can ensure that your application does not base decisions on biased output."
3090	Reviewing debiased transactions	"Reviewing data for debiased values
When the fairness evaluation runs, Watson OpenScale stores the debiased values in the payload logging table of the model deployment. All scoring transactions done through this endpoint are automatically debiased, as applicable. You can access the debiased scoring endpoint just as you would the normal scoring endpoint for your deployed model. In addition to returning the response of your deployed model, it also returns the debiased_prediction and debiased_probability columns.  
The debiased_prediction column contains the debiased prediction value.  
The debiased_probability column represents the probability of the debiased prediction. This array of double values represents the probability of the debiased prediction that belongs to one of the prediction classes."
3091	Reviewing debiased transactions	"Reviewing data for debiased values
Enabling the debiasing parameter
Debiasing is disabled by default when you configure new deployments in Watson OpenScale. You can also set the perform_debias parameter to true in the parameters section of the Python SDK or specify the PERFORM_DEBIASING pod-level environment label to enable debiasingm, as shown in the following example:  
When you patch the monitor instance, the fairness monitor runs debiasing during the next evaluation.  
For more information, see the Watson OpenScale Python SDK documentation."
3092	Reviewing debiased transactions	"Reviewing debiased transactions
You can use the debiased transactions endpoint to review debiased transactions for fairness evaluations. For more information, see Sending model transactions in Watson OpenScale.  
Note: Ideally, you would directly call the debias endpoint from your production application instead of calling the scoring endpoint from the machine learning provider.  
Because the debias endpoint deals with runtime bias, it continues to run background checks for the scoring data from the payload logging table. It also keeps updating the bias mitigation model, which debiases the scoring requests. In this way, Watson OpenScale is always up to date with the incoming data, and with its behavior to detect and mitigate bias.  
You can configure a fairness threshold in Watson OpenScale to indicate when data is acceptable and unbiased."
3093	Reviewing debiased transactions	"Reviewing debiased transactions
Mitigate bias with a new version of the model:
You must build a new version of the model that fixes the problem. Watson OpenScale stores biased records in the manual labeling table. These biased records must be manually labeled and then the model is retrained through the additional data to build a new version of the model that is unbiased."
3094	Reviewing debiased transactions	"Reviewing debiased transactions
Extract a list of the individual biased records:
Connect to the manual labeling table and read the records by using standard SQL queries.  
Note: Ideally, you would directly call the debias endpoint from your production application instead of calling the scoring endpoint from the machine learning provider.  
Parent topic: Reviewing model transactions"
3095	Reviewing model transactions	"You can analyze model transactions in the Watson OpenScale service to understand how your model predicts outcomes. For example, you can review the predicted biased outcomes in payload transactions, or analyze a drop in accuracy, to get a better understanding of how your deployment is performing.  
Watson OpenScale provides the following different methods to help you analyze your model transactions:  
When you review results from fairness evaluations, you can select View payload transactions to view a list of all of the model transactions that Watson OpenScale processes to predict any biased outcomes for each monitored attribute.  
When you review results from drift evaluations, you can click the Number of transactions link on a transactions summary tile to view the transactions that are responsible for drift. The Transactions page specifies reasons for the drop in accuracy or drop in data consistency and provides a recommendation to help fix the drift of your model.  
When you review evaluation results, you can review performance to understand how your model processes scoring requests. Performance evaluations are enabled by default when you add deployments. To evaluate performance, Watson OpenScale calculates the total number of transaction records and the average number of transaction records per minute that your model processes.  
When you review evaluation results, you can use the Predictions by confidence chart to analyze the transactions that are sent to your deployment in the date range that you select by reviewing prediction classes and confidence distribution in each class. You can also use the Chart builder to create custom visualizations to help you better understand model predictions and inputs at run time.  
The chart builder displays the output of the model’s prediction against the features or data ranges that you specify. For example, you can use the chart builder to see the split in predicted classes or confidence scores for different ranges of an attribute. You can also analyze the transactions that are sent to your deployment during a data range by creating a custom chart and choosing between featues, prediction classes, and confidence.  
When you review any transactions on the Transactions page in Watson OpenScale, you can choose to view explanations that provide details about how the predictions for each transaction are determined. For more information, see Explaining model transactions.  
Note:  
If your data includes a feature name containing a period (.), rendering a chart will fail with this error: Failed to get data"
3096	Reviewing model transactions	"Learn more
Explaining model transactions  
Parent topic: Reviewing model insights with Watson OpenScale"
3097	Watson OpenScale offering plan options	"The Watson OpenScale enables responsible, transparent, and explainable AI.  
With Watson OpenScale you can:  
Evaluate machine learning models for dimensions such as fairness, quality, or drift. Explore transactions to gain insights about your model."
3098	Watson OpenScale offering plan options	"Watson OpenScale legacy offering plans
Important:  
The legacy offering plan for Watson OpenScale is available only in the Frankfurt region. In the Dallas region, the watsonx.governance plans are available instead."
3099	Watson OpenScale offering plan options	"Watson OpenScale legacy offering plans
Watson OpenScale Standard v2 plan
Watson OpenScale offers a Standard v2 plan that charge users on a per model basis.  
There are no restrictions or limitations on payload data, feedback rows, or explanations under the Standard v2 instance."
3100	Watson OpenScale offering plan options	"Watson OpenScale legacy offering plans
Regional limitations
Watson OpenScale is not available in some regions. See Regional availability for services and features for more details.  
Note:  
The regional availability for every service can also be found in the IBM watsonx catalog."
3101	Watson OpenScale offering plan options	"Watson OpenScale legacy offering plans
Quota limits
To avoid performance issues and manage resources efficiently, Watson OpenScale sets the following quota limits:  
Every asset in Watson OpenScale has a hard limitation of 10000 instances of the asset per service instance."
3102	Watson OpenScale offering plan options	"Watson OpenScale legacy offering plans
PostgreSQL databases for Watson OpenScale
You can use a PostgreSQL database for your Watson OpenScale instance. PostgreSQL is a powerful, open source object-relational database that is highly customizable and compliant with many security standards.  
If your model processes personally identifiable information (PII), use a PostgreSQL database for your model. PostgreSQL is compliant with:  
GDPR HIPAA PCI-DSS SOC 1 Type 2 SOC 2 Type 2 ISO 27001 ISO 27017 ISO 27018 ISO 27701"
3103	Watson OpenScale offering plan options	"Next steps
Managing the Watson OpenScale service  
Parent topic: watsonx.governance"
3104	Throughput	Throughput measures the average scoring requests per minute in a specific timeframe.
3105	Throughput	"Throughput at a glance
Description: Average scoring requests per minute in a specific timeframe Default thresholds: Not applicable Default recommendation: Not applicable Problem type: All Chart values: Average value in the timeframe Metrics details available: None  
Throughput is not supported for the IBM Watson Machine Learning models with automatic payload logging."
3106	Throughput	"Interpreting the display
The throughput chart displays the average number of requests per minute. You can use controls to choose the timeframe, such as daily, weekly, or monthly:  
Parent topic: Performance metrics overview"
3107	Supported machine learning providers	"The Watson OpenScale service supports Watson Machine Learning as well as many third-party machine learning providers.  
Use one of these supported machine learning providers to perform payload logging, feedback logging, and to measure performance accuracy, runtime bias detection, explainability, and auto-debias function as part of your model evaluation.  
Watson Machine Learning Azure ML Studio Azure ML Service AWS SageMaker Custom (The custom machine learning framework must have equivalent functionality to Watson Machine Learning.)"
3108	Supported machine learning providers	"Support for multiple machine learning engines
Watson OpenScale supports multiple machine learning engines within a single instance. You can provision them through the Watson OpenScale dashboard configuration or the Python SDK."
3109	Supported machine learning providers	"Adding providers using the Watson OpenScale dashboard
After you open Watson OpenScale, from the Configure tab, click Add machine learning provider. Select the provider you want to add. Enter the required information, such as credentials, and click Save."
3110	Supported machine learning providers	"Adding providers using the Watson OpenScale dashboard
Changing or updating details for machine learning providers
Click the tile menu icon and then click View & edit details."
3111	Supported machine learning providers	"Adding machine learning providers by using the Python SDK
You can add more than one machine learning engine to Watson OpenScale by using the Python API wos_client.service_providers.add method."
3112	Supported machine learning providers	"Adding machine learning providers by using the Python SDK
IBM Watson Machine Learning
To add the IBM Watson Machine Learning machine learning engine, run the following command:"
3113	Supported machine learning providers	"Adding machine learning providers by using the Python SDK
Microsoft Azure ML Studio
To add the Azure ML Studio machine learning engine, run the following command:"
3114	Supported machine learning providers	"Adding machine learning providers by using the Python SDK
Amazon Sagemaker
To add the AWS Sagemaker machine learning engine, run the following command:"
3115	Supported machine learning providers	"Adding machine learning providers by using the Python SDK
Microsoft Azure ML Service
To add the Azure ML Service machine learning engine, run the following command:"
3116	Supported machine learning providers	"Adding machine learning providers by using the Python SDK
Producing a list of machine learning providers
To view a list of all the bindings, run the list method:  
client.service_providers.list()  
For information about specific machine learning engines, see the following topics:  
Add your Custom machine learning engine. Add your Microsoft Azure machine learning studio engine Add your Microsoft Azure machine learning service engine Add your Amazon SageMaker machine learning engine  
For a coding example, see the Watson OpenScale sample notebooks.  
Parent topic: Evaluating AI models with Watson OpenScale"
3117	Explaining model transactions	"The Watson OpenScale service provides explanations for your model transactions to help you understand how predictions are determined.  
You can analyze local explanations to understand the impact of factors for specific transactions or analyze global explanations to understand the general factors that impact model outcomes. The type of model that you configure determines the type of explanation that you can use to analyze your transactions.  
Watson OpenScale supports explanations for structured, image, and unstructured text models. Structured models can use binary, multiclass, or regression classification problems. Image or unstructured text models can use binary or multiclass classification problems.  
When you configure explainability in Watson OpenScale, you can use Local Interpretable Model-Agnostic Explanations (LIME), contrastive explanations, or Shapley Additive explanations (SHAP) to analyze transactions.  
LIME identifies which features are most important for a specific data point by analyzing up to 5000 other close-by data points. In an ideal setting, the features with high importance in LIME are the features that are most important for that specific data point.  
Contrastive explanations calculate how many values need to change to change the prediction or maintain the same prediction. The factors that need the maximum change are considered more important, so the features with the highest importance in contrastive explanations are the features where the model is least sensitive. For contrastive explanations, Watson OpenScale displays the maximum changes for the same outcome and the minimum changes for a changed outcome. These categories are also known as pertinent positive and pertinent negative values. These values help explain the behavior of the model in the vicinity of the data point for which an explanation is generated."
3118	Explaining model transactions	"SHAP is a game-theoretic approach that explains the output of machine learning models. It connects optimal credit allocation with local explanations by using Shapley values and their related extensions. SHAP assigns each model feature an importance value for a particular prediction, which is called a Shapley value. The Shapley value is the average marginal contribution of a feature value across all possible groups of features. The SHAP values of the input features are the sums of the difference between baseline or expected model output and the current model output for the prediction that is being explained. The baseline model output can be based on the summary of the training data or any subset of data that explanations must be generated for. The Shapley values of a set of transactions can be combined to get global explanations that provide an overview of which features of a model are most important.  
Note:  
Watson OpenScale only supports global explanations for online subscriptions. SHAP explanations only support tabular data."
3119	Explaining model transactions	"Analyzing local explanations
Watson OpenScale provides different methods that you can use to view local explanations.  
When you review evaluation results, you can select the Number of explanations link to open the Select an explanation window.  
In the Action column select Explain to display the Transaction details page. The Transactions details provides different explanations, depending on which explanation methods and model types that you use.  
For categorical models, on the Explain tab, the Transaction details page provides an analysis of the features that influenced the outcome of the transaction with the local explanation method that you use. For SHAP, you can select the background data that you use in the Background data menu to regenerate the explanation.  
On the Inspect tab, Watson OpenScale generates advanced contrastive explanations for binary classification models that you can use to experiment with features by changing the values to see whether the outcome changes.  
You can also view different explanations for the following type of transactions:  
Explaining image transactions Explaining unstructured text transactions Explaining tabular transactions"
3120	Explaining model transactions	"Analyzing local explanations
Explaining image transactions
For image models, you can view which parts of an image contribute positively and negatively to the predicted outcome. In the following example, the image in the positive panel shows the parts which impacted positively to the prediction. The image in the negative panel shows the parts of images that had a negative impact on the outcome.  
You can also use the following notebooks to generate explanations for image models:  
Tutorial on generating an explanation for an image-based multiclass classification model Tutorial on generating an explanation for an image-based binary classification model"
3121	Explaining model transactions	"Analyzing local explanations
Explaining unstructured text transactions
For unstructured text models, you can view which keywords had a positive or a negative impact on the model prediction. Unstructured text models explain the importance of words or tokens.  
The explanation also shows the position of the identified keywords in the original text that was fed as input to the model. To change the language, select a different language from the list. The explanation runs again by using a different tokenizer.  
You can also use the following notebook to generate explanations for unstructured text models:  
Tutorial on generating an explanation for a text-based model"
3122	Explaining model transactions	"Analyzing local explanations
Explaining tabular transactions
For tabular classification models, you can view the top three features that positively influence the model prediction and the top three features that negatively influence the prediction.  
To view local explanations, you can also select the Explain a transaction tab to open the Recent transactions page. The page displays all of the transactions that are processed by your model."
3123	Explaining model transactions	"Analyzing global explanations
If you enable the SHAP global explanation method when you configure explainability, you can view details for the global explanation stability metric on your dashboard. The global explanation metric calculates the degree of consistency in the global explanation over time.  
When you review evaluation results for explainability, you can view the following details:  
Feature influence: The most important features in descending order of the average absolute SHAP values Distribution: The SHAP values for distribution of each feature Comparison: The change in features influence between current global and baseline global explanation  
You can also test what-if scenarios by adjusting model transaction values to determine how different changes can affect your outcomes.  
Parent topic: Reviewing model transactions"
3124	Metrics computation with the Python SDK	"The Watson OpenScale service supports the computation of the following fairness metrics and explanation algorithms:  
Note:  
The following metrics and algorithms can be computed in a notebook runtime environment or offloaded as Spark jobs against IBM Analytics Engine.  
You can use the FairScore transformer as a post-processing bias mitigation technique. This technique transforms probability estimates or the scores of probabilistic binary classification models regarding fairness goals. To use FairScore Transformer in Watson OpenScale, you must train a Fair score transformer.  
The individual fairness post-processor is a post-processing transformer algorithm that transforms individual scores to achieve individual fairness. You can use it with the Python SDK to support multi-class text classification. You must train this algorithm before you can use it to transform model outputs.  
You can use the input reduction algorithm to calculate the minimum set of features that you must specify to keep model predictions consistent. The algorithm excludes the features that do not affect model predictions.  
The likelihood compensation (LC) is a framework for explaining the deviations of the prediction of a black box model from the ground truth. With test data and the predict function of a black box model, LC can identify the anomalies in the test data and explain what caused the sample to become an anomaly. The LC explanation is provided as deltas, which when added to the original test data or anomaly, converges the prediction of the model to the ground truth. LC provides local explanations and is supported only for regression models.  
LIME identifies which features are most important for a specific data point by analyzing up to 5000 other close-by data points. In an ideal setting, the features with high importance in LIME are the features that are most important for that specific data point.  
You can use the mean individual disparity to verify whether your model generates similar predictions or scores for similar samples. This metric calculates the difference in probability estimates of multi-class classification models for similar samples.  
You can use the multidimensional subset scanning algorithm as a general bias scan method. This method detects and identifies which subgroups of features have statistically significant predictive bias for a probabilistic binary classifier. This algorithm helps you decide which features are the protected attributes and which values of these features are the privileged group for monitor evaluations.  
You can use the following performance measure metrics to evaluate models with a confusion matrix that is calculated using ground truth data and model predictions from sample data:"
3125	Metrics computation with the Python SDK	"You can use the following performance measure metrics to evaluate models with a confusion matrix that is calculated using ground truth data and model predictions from sample data:  
average_odds_difference average_abs_odds_difference error_rate_difference error_rate_ratio false_negative_rate_difference false_negative_rate_ratio false_positive_rate_difference false_positive_rate_ratio false_discovery_rate_difference false_discovery_rate_ratio false_omission_rate_difference false_omission_rate_ratio  
The protected attribute extraction algorithm transforms text data sets to structured data sets. The algorithm tokenizes the text data, compares the data to patterns that you specify, and extracts the protected attribute from the text to create structured data. You can use this structured data to detect bias against the protected attribute with a Watson OpenScale bias detection algorithm. The protected attribute extraction algorithm only supports gender as a protected attribute.  
The protected attribute perturbation algorithm generates counterfactual statements by identifying protected attribute patterns in text data sets. It also tokenizes the text and perturbs the keywords in the text data to generate statements. You can use the original and perturbed data sets to detect bias against the protect attribute with a Watson OpenScale bias detection algorithm. The protected attribute perturbation algorithm only supports gender as a protected attribute.  
The protodash explainer identifies input data from a reference set that need explanations. This method minimizes the maximum mean discrepancy (MMD) between the reference datapoints and a number of instances that are selected from the training data. To help you better understand your model predictions, the training data instances mimic a similar distribution as the reference datapoints.  
Note: Protodash explainer is supported only for structured classification models.  
SHAP is a game-theoretic approach that explains the output of machine learning models. It connects optimal credit allocation with local explanations by using Shapley values and their related extensions.  
SHAP assigns each model feature an importance value for a particular prediction, which is called a Shapley value. The Shapley value is the average marginal contribution of a feature value across all possible groups of features. The SHAP values of the input features are the sums of the difference between baseline or expected model output and the current model output for the prediction that is being explained. The baseline model output can be based on the summary of the training data or any subset of data that explanations must be generated for."
3126	Metrics computation with the Python SDK	"The Shapley values of a set of transactions can be combined to get global explanations that provide an overview of which features of a model are most important.  
The SED is fairness metric that you can use to describe fairness for your model predictions. SED quantifies the differential in the probability of favorable and unfavorable outcomes between intersecting groups that are divided by features. All intersecting groups are equal, so there are no unprivileged or privileged groups. This calculation produces a SED value that is the minimum ratio of Dirichlet smoothed probability for favorable and unfavorable outcomes between intersecting groups in the data set. The value is in the range 0-1, excluding 0 and 1, and a larger value specifies a better outcome.  
Statistical parity difference is a fairness metric that you can use to describe fairness for your model predictions. It is the difference between the ratio of favorable outcomes in unprivileged and privileged groups. This metric can be computed from either the input data set or the output of the data set from a classifier or predicted data set. A value of 0 implies that both groups receive equal benefit. A value less than 0 implies higher benefit for the privileged group. A value greater than 0 implies higher benefit for the unprivileged group.  
You can compute these metrics and algorithms with Watson OpenScale Python SDK version 3.0.14 or later. For more information, see the Watson OpenScale Python SDK documentation.  
You can also use sample notebooks to compute fairness metrics and explainability.  
Parent topic: APIs, SDKs, and tutorials"
3127	Configuring model evaluations	Configure watsonx.governance evaluations to generate insights about your model performance.
3128	Configuring model evaluations	"Configuring model evaluations with Watson OpenScale
If you're using the Watson OpenScale service, you can configure the following types of evaluations:  
Quality Evaluates how well your model predicts correct outcomes that match labeled test data. Fairness Evaluates whether your model produces biased outcomes that provide favorable results for one group over another. Drift Evaluates how your model changes in accuracy and data consistency by comparing recent transactions to your training data. Drift v2 Evaluates changes in your model output, the accuracy of your predictions, and the distribution of your input data. Model health Evaluates how efficiently your model deployment processes your transactions.  
You can also create custom evaluations and metrics to generate a greater variety of insights about your model performance.  
Each evaluation generates metrics that you can analyze to gain insights about your model performance. For more information see, Reviewing evaluation results."
3129	Configuring model evaluations	"Configuring model evaluations with watsonx.governance
If you're using the watsonx.governance service, you can configure the following types of evaluations:  
Quality Evaluates how well your model predicts correct outcomes that match labeled test data. Drift v2 Evaluates changes in your model output, the accuracy of your predictions, and the distribution of your input data Generative AI quality Measures how well your foundation model performs tasks Model health Evaluates how efficiently your model deployment processes your transactions.  
Parent topic: Evaluating AI models with Watson OpenScale"
3130	Managing training data for Watson OpenScale	"You must connect Watson OpenScale to your training data to configure model evaluations and explainability.  
Watson OpenScale uses training data to calculate the metrics for your model evaluations and explainability methods. To configure model evaluations and explainability methods in Watson OpenScale, you must prepare and store your training data."
3131	Managing training data for Watson OpenScale	"Preparing training data
The format of your training data can determine the results of your model evaluations. To enable model evaluations, you must prepare to store your training data in a format that Watson OpenScale can process. Your training data must contain labeled feature columns and a prediction column as shown in the following example:  
Watson OpenScale uses the training data that you provide to create a training data schema to ensure that your training data corresponds with the format that it understands. The schema specifies the feature columns that you provide in your training data and the type of data that the columns contain. The following example shows a training data schema for the German Credit Risk dataset:"
3132	Managing training data for Watson OpenScale	"Storing training data
You can also store your data in Cloud Object Storage.  
If you want to keep the details of your training data location private, you can also connect Watson OpenScale to your training data by running a custom notebook to upload the configuration file that it generates."
3133	Managing training data for Watson OpenScale	"Next steps
You must connect your training data to Watson OpenScale so that it understands how to process your model."
3134	Managing training data for Watson OpenScale	"Learn more
Sending model transactions  
Parent topic: Managing data for model evaluations in Watson OpenScale"
3135	Integrating 3rd-party ML engines	"The IBM Watson OpenScale service integrates with external machine learning service engines, such as Microsoft Azure ML Studio, Microsoft Azure ML Service, and Amazon SageMaker.  
You can integrate Watson OpenScale with 3rd-party engines in the following ways:  
Engine binding level  
Ability to list deployments and get deployment details.  
Deployment subscription level  
Watson OpenScale needs to be able to score subscribed deployment in the proper format, such as the IBM Watson Machine Learning format and receive the output in the same compatible format. Watson OpenScale must be configured to process both input and output formats.  
Payload logging  
Each input and output to the deployed model triggered by a user’s application must be stored in a payload store. The format of the payload records follows the same specification as mentioned in the preceding section on deployment subscription levels.  
Watson OpenScale uses those records to calculate bias, explanations, and others. It is not possible for Watson OpenScale to automatically store transactions that run on the user site, which is outside the Watson OpenScale system. This method is one of the ways that Watson OpenScale safeguards proprietary information. Use the Watson OpenScale Rest API or Python SDK to work with secure data."
3136	Integrating 3rd-party ML engines	"Steps to implement this solution
Learn about custom machine learning engines. Set up payload logging. Set up a custom machine learning engine by using one of these Custom machine learning engine examples.  
Parent topic: Supported machine learning engines, frameworks, and models"
3137	Setup options for Watson OpenScale	Choose a path for configuring Watson OpenScale according to your preference and level of expertise.
3138	Setup options for Watson OpenScale	"Automatic setup
The automatic setup option is a guided, no-code experience that sets up and configures a machine learning environment, a database, and a sample model. Follow the steps in the tour to learn to evaluate the sample model in Watson OpenScale. After the setup is complete, you can configure the service with your own database and add your own models to the dashboard. See Automatic setup."
3139	Setup options for Watson OpenScale	"Manual setup
Prepare Watson OpenScale by connecting to a database, setting up machine learning providers, specifying the environment type (pre-production or production), and optionally adding integrated services. See Manual setup."
3140	Setup options for Watson OpenScale	"Advanced setup
For data scientists who prefer working in Notebooks, learn how to use the OpenScale REST APIs or Python SDK to provision and configure the Watson OpenScale service. See Advanced setup.  
For more information, see APIs, SDKs, and tutorials.  
Parent topic: Evaluating AI models with Watson OpenScale"
3141	Watson OpenScale drift metrics	"When you configure drift evaluations in Watson OpenScale, you can generate metrics that help you determine how well your model predicts outcomes over time.  
You can view the results of your drift evaluations on the Insights dashboard in Watson OpenScale. To view results, you can select a model deployment tile and click the arrow in the Drift evaluation section to display a summary of drift metrics from your last evaluation. For more information, see Reviewing drift results.  
Watson OpenScale calculates drift metrics by analyzing the behavior of your model and building its own model to predict whether your model generates accurate predictions for data points. This drift detection model processes payload data to identify the number of records that your model makes inaccurate predictions for and generates the predicted accuracy of your model.  
Drift is supported for structured data only and does not support Python functions."
3142	Watson OpenScale drift metrics	"Supported drift metrics
The following drift metrics are supported by Watson OpenScale:  
Drop in accuracy Drop in data consistency"
3143	Watson OpenScale drift metrics	"Learn more
Reviewing model insights  
Parent topic: Configuring drift detection"
3144	Configuring Watson OpenScale with advanced setup	"Use the advanced setup option to run a Python notebook that uses sample data to demonstrate how to configure model evaluations for Watson OpenScale.  
Use a notebook to build, train, and deploy a model and configure Watson OpenScale to monitor the model deployment. The notebook also provides historical data that generates insights that you can view in Watson OpenScale. The notebook uses the German Credit risk model that is also used for the auto setup option to provide sample data. For more information, see the Overview of the sample data."
3145	Configuring Watson OpenScale with advanced setup	"Before you begin
To use the advanced setup option, you must select the Default Spark 3.3.x & Python 3.9 or the Runtime 22.2 on Python 3.10 runtime environment when you create a Jupyter Notebook in the notebook editor. The runtime environments require service credentials for the following services:  
Watson OpenScale IBM Watson Machine Learning Db2 Warehouse  
Watson Studio, Watson Machine Learning, Watson OpenScale, and other supplemental services are not available by default. An administrator must install these services on the IBM Cloud Pak for Data platform. To determine whether a service is installed, open the Services catalog and check whether the service is enabled."
3146	Configuring Watson OpenScale with advanced setup	"Running the advanced setup
Download the Watson OpenScale and Watson ML Engine.ipynb file from Github. You can sign in and clone the watson-openscale-samples repository to download the file or click Copy raw contents to paste the file content into a new IPYNB file. Open Watson Studio and select one of your projects. From the Assets tab, click New asset > Work with data and models in Python or R notebooks. Select the From file tab and specify a notebook name. Select the Default Spark 3.3.x & Python 3.9 or the Runtime 22.2 on Python 3.10 runtime environment from the Select runtime menu. Upload the IPYNB file and click Create. The Working with Watson Machine Learning notebook loads and opens in your project.  
Now you can follow the steps in the notebook to run the advanced setup and deploy the German Credit risk model to your Watson OpenScale service instance."
3147	Configuring Watson OpenScale with advanced setup	"Learn more
Configuring Watson OpenScale with manual setup Configuring Watson OpenScale with auto setup  
Parent topic: Watson OpenScale"
3148	APIs, SDKs, and tutorials	You can use APIs, SDKs, and notebook tutorials to evaluate your model in more ways with the Watson OpenScale service. Use the model evaluation and prompt setup APIs to prepare for model evaluations, the Python SDK to customize metrics for model monitoring, and the tutorials to demonstrate tasks with Jupyter Notebooks.
3149	APIs, SDKs, and tutorials	"Model evaluation API
You can use the Watson OpenScale v2 API to prepare and manage data for traditional model evaluations with the Watson OpenScale service."
3150	APIs, SDKs, and tutorials	"Prompt setup API
You can use the prompt setup API to configure artifacts for evaluating prompt templates in deployment spaces and projects with the watsonx.governance service. For more information, see Python SDK."
3151	APIs, SDKs, and tutorials	"Python SDK
The Watson OpenScale Python SDK is a Python library that performs model evaluations. You can use the Python SDK to directly configure the data mart database, add your machine learning engine, or select and monitor deployments. For more information, see Metrics computation with the Python SDK."
3152	APIs, SDKs, and tutorials	"Learn more
Parent topic: Evaluating AI models with Watson OpenScale"
3153	Preparing to evaluate models	"Prepare for evaluations in the Watson OpenScale servcie by adding your deployments and providing details about your models.  
After you set up Watson OpenScale, you must prepare for model evaluations. To prepare for model evaluations you must add deployments to the Watson OpenScale Insights dashboard. The deployments that you add must be connected to a machine learning provider. If you're using custom or external providers, you must also provide model details to connect Watson OpenScale to your training data and specify how your data is structured.  
You can also prepare to evaluate deployments with a set of JSON configuration templates. For details, see Configure asset deployments using JSON configuration files.  
Parent topic: Evaluating AI models with Watson OpenScale."
3154	Microsoft Azure ML Service frameworks	"You can use Microsoft Azure ML Service to perform payload logging, feedback logging, and to measure performance accuracy, runtime bias detection, explainability, and auto-debias function in the IBM Watson OpenScale service.  
IBM Watson OpenScale fully supports the following Microsoft Azure Machine Learning Service frameworks:  
Table 1. Framework support details  
To generate the drift detection model, you must use scikit-learn version 0.20.2 in Notebooks."
3155	Microsoft Azure ML Service frameworks	"Adding Microsoft Azure ML Service to Watson OpenScale
You can configure Watson OpenScale to work with Microsoft Azure ML Service by using one of the following methods:  
For the first time that you are adding a machine learning provider to Watson OpenScale, you can use the configuration interface. For more information, see Specifying a Microsoft Azure ML Service instance. You can also add your machine learning provider by using the Python SDK. You must use this method if you want to have more than one provider. For more information, see Add your Microsoft Azure machine learning engine.  
Watson OpenScale calls various REST endpoints that are needed to interact with the Azure ML Service. To do this, you must bind the Azure Machine Learning Service Watson OpenScale.  
Create an Azure Active Directory Service Principal. Specify the credential details when you add the Azure ML Service service binding, either through the UI or the Watson OpenScale Python SDK."
3156	Microsoft Azure ML Service frameworks	"Requirements for JSON request and response files
For Watson OpenScale to work with Azure ML Service, the web service deployments you create must meet certain requirements. The web service deployments that you create must accept JSON requests and return JSON responses, according to the following requirements."
3157	Microsoft Azure ML Service frameworks	"Requirements for JSON request and response files
Required web service JSON request format
The REST API request body must be a JSON document that contains one JSON array of JSON objects The JSON array must be named ""input"". Each JSON object can include only simple key-value pairs, where the values can be a string, a number, true, false, or null The values cannot be a JSON object or array Each JSON object in the array must all have the same keys (and hence number of keys) specified, regardless of whether there is a non-null value available  
The following sample JSON file meets the preceding requirements and can be used as a template for creating your own JSON request files:"
3158	Microsoft Azure ML Service frameworks	"Requirements for JSON request and response files
Required web service JSON response format
Make note of the following items when you create a JSON response file:  
{ ""output"": [ { ""Scored Probabilities"": [""Risk"" probability,""No Risk"" probability ] }, { ""Scored Probabilities"": [""Risk"" probability,""No Risk"" probability ] } ]  
The REST API response body must be a JSON document that contains one JSON array of JSON objects  
The JSON array must be named ""output"".  
Each JSON object can include only key-value pairs, where the values can be a string, a number, true, false, null, or an array that does not contain any other JSON objects or arrays  
The values cannot be a JSON object  
Each JSON object in the array must all have the same keys (and number of keys) specified, regardless of whether there is a non-null value available  
For classification models: the web service must return an array of probabilities for each class and the ordering of the probabilities must be consistent for each JSON object in the array  
Example: suppose you have a binary classification model that predicts credit risk, where the classes are Risk or No Risk For every result returned back in the ""output"" array, the objects must contain a key-value pair that includes the probabilities in fixed order, in the form:  
To be consistent with Azure ML visual tools that are used in both Azure ML Studio and Service, use the following key names:  
the key name ""Scored Labels"" for the output key that denotes the predicted value of the model the key name ""Scored Probabilities"" for the output key that denotes an array of probabilities for each class  
The following sample JSON file meets the preceding requirements and can be used as a template for creating your own JSON response files:"
3159	Microsoft Azure ML Service frameworks	"Sample Notebooks
The following Notebooks show how to work with Microsoft Azure ML Service:  
Model creation with Azure Service and deployment Data mart creation, deployment wrapper for payload logging, model deployment monitoring and data analysis"
3160	Microsoft Azure ML Service frameworks	"Specifying a Microsoft Azure ML Service instance
Your first step in the Watson OpenScale tool is to specify a Microsoft Azure ML Service instance. Your Azure ML Service instance is where you store your AI models and deployments.  
Watson OpenScale connects to AI models and deployments in an Azure ML Service instance. To connect your service to Watson OpenScale, go to the Configure tab, add a machine learning provider, and click the Edit icon. In addition to a name and description and whether the environment is Pre-production or Production, you must provide the following information:  
Client ID: The actual string value of your client ID, which verifies who you are and authenticates and authorizes calls that you make to Azure Service. Client Secret: The actual string value of the secret, which verifies who you are and authenticates and authorizes calls that you make to Azure Service. Tenant: Your tenant ID corresponds to your organization and is a dedicated instance of Azure AD. To find the tenant ID, hover over your account name to get the directory and tenant ID, or select Azure Active Directory > Properties > Directory ID in the Azure portal. Subscription ID: Subscription credentials that uniquely identify your Microsoft Azure subscription. The subscription IDforms part of the URI for every service call.  
See How to: Use the portal to create an Azure AD application and service principal that can access resources for instructions about how to get your Microsoft Azure credentials.  
You are now ready to select deployed models and configure your monitors. Watson OpenScale lists your deployed models on the Insights dashboard where you can click Add to dashboard. Select the deployments that you want to monitor and click Configure.  
For more information, see Configure monitors."
3161	Microsoft Azure ML Service frameworks	"Payload logging with the Microsoft Azure ML Service engine
Add your Microsoft Azure ML Service engine
A non-IBM Watson Machine Learning engine is bound as Custom, and consists of metadata. There is no direct integration with the non-IBM Watson Machine Learning service.  
You can see your service binding with the following command:  
The sample output:"
3162	Microsoft Azure ML Service frameworks	"Payload logging with the Microsoft Azure ML Service engine
Add Microsoft Azure ML Service subscription
Add subscription  
Get subscription list"
3163	Microsoft Azure ML Service frameworks	"Payload logging with the Microsoft Azure ML Service engine
Enable payload logging
Enable payload logging in subscription  
Get logging details"
3164	Microsoft Azure ML Service frameworks	"Payload logging with the Microsoft Azure ML Service engine
Scoring and payload logging
Score your model. For a full example, see the Working with Azure Machine Learning Service Engine Notebook.  
Store the request and response in the payload logging table:  
For languages other than Python, you can also log payload by using a REST API.  
Parent topic: Supported machine learning engines, frameworks, and models"
3165	Configuring IBM watsonx.governance model health monitor evaluations	"You can configure model health monitor evaluations in Watsonx.governance to help you understand your model behavior and performance. You can use model health metrics to determine how efficiently your model deployment processes your transactions.  
To configure model health monitor evaluations, you can set threshold values for each metric as shown in the following example:"
3166	Configuring IBM watsonx.governance model health monitor evaluations	"Supported model health metrics
The following metric categories for model health evaluations are supported by watsonx.governance. Each category contains metrics that provide details about your model performance:  
Watsonx.governance calculates the number of scoring requests that your model deployment receives during model health evaluations. This metric category is supported for traditional machine learning models and foundation models.  
Watsonx.governance calculates the total, average, minimum, maximum, and median number of transaction records that are processed across scoring requests during model health evaluations. This metric category is supported for traditional machine learning models and foundation models.  
Watsonx.governance calculates the number of tokens that are processed across scoring requests for your model deployment. This metric category is supported for foundation models only.  
Watsonx.governance calculates the following metrics to measure token count during evaluations:  
Input token count: Calculates the total, average, minimum, maximum, and median input token count across multiple scoring requests during evaluations Output token count: Calculates the total, average, minimum, maximum, and median output token count across scoring requests during evaluations  
Watsonx.governance calculates latency by tracking the time that it takes to process scoring requests and transaction records per millisecond (ms). Throughput is calculated by tracking the number of scoring requests and transaction records that are processed per second.  
To calculate throughput and latency, watsonx.governance uses the response_time value from your scoring requests to track the time that your model deployment takes to process scoring requests.  
For Watson Machine Learning deployments, Watson OpenScale automatically detects the response_time value when you configure evaluations.  
For external and custom deployments, you must specify the response_time value when you send scoring requests to calculate throughput and latency as shown in the following example from the Watson OpenScale Python SDK:  
Watsonx.governance calculates the following metrics to measure thoughput and latency during evaluations:  
API latency: Time taken (in ms) to process a scoring request by your model deployment. API throughput: Number of scoring requests processed by your model deployment per second Record latency: Time taken (in ms) to process a record by your model deployment Record throughput: Number of records processed by your model deployment per second  
This metric category is supported for traditional machine learning models and foundation models."
3167	Configuring IBM watsonx.governance model health monitor evaluations	"Supported model health metrics
This metric category is supported for traditional machine learning models and foundation models.  
Watsonx.governance calculates the number of users that send scoring requests to your model deployments. This metric category is supported for traditional machine learning models and foundation models. To calculate the number of users, watsonx.governance uses the user_id from scoring requests to identify the users that send the scoring requests that your model receives.  
For external and custom deployments, you must specify the user_id value when you send scoring requests to calculate the number of users as shown in the following example from the Watson OpenScale Python SDK:  
When you view a summary of the Users metric in watsonx.governance, you can use the real-time view to see the total number of users and the aggregated views to see the average number of users.  
Watsonx.governance calculates the total, average, minimum, maximum, and median payload size of the transaction records that your model deployment processes across scoring requests in kilobytes (KB). Watsonx.governance does not support payload size metrics for image models. This metric category is supported for traditional machine learning models only."
3168	Accessing data in AWS through access points from a notebook	"In IBM watsonx you can access data stored in AWS S3 buckets through access points from a notebook.  
Run the notebook in an environment in IBM watsonx. Create an internet-enabled access point to connect to the S3 bucket."
3169	Accessing data in AWS through access points from a notebook	"Connecting to AWS S3 data through an internet-enabled access point
You can access data in an AWS S3 bucket through an internet-enabled access point in any AWS region.  
To access S3 data through an internet-enabled access point:  
Create an access point for your S3 bucket. See Creating access points.  
Set the network origin to Internet.  
After the access point is created, make a note of the Amazon resource name (ARN) for the access point. Example: ARN: arn:aws:s3:us-east-1:675068711478:accesspoint/cust-data-bucket-internet-ap. You will need to enter the ARN in your notebook."
3170	Accessing data in AWS through access points from a notebook	"Accessing AWS S3 data from your notebook
The following sample code snippet shows you how to access AWS data from your notebook by using an access point:  
Parent topic: Loading and accessing data in a notebook"
3171	Exporting synthetic data	"Using Synthetic Data Generator, you can export synthetic data to remote data sources using connections or write data to a project (Delimited or SAV).  
Double-click the node to open its properties. Various options are available, described as follows. After running the node, you can find the data at the export location you specified."
3172	Exporting synthetic data	"Exporting to a project
Under Export to, select This project and then select the project path. For File type, select either Delimited or SAV."
3173	Exporting synthetic data	"Exporting to a connection
Under Export to, select Save to a connection to open the Asset Browser and then select the connection to export to. For a list of supported data sources, see Creating synthetic data from imported data."
3174	Exporting synthetic data	"Setting the field delimiter, quote character, and decimal symbol
Different countries use different symbols to separate the integer part from the fractional part of a number and to separate fields in data. For example, you might use a comma instead of a period to separate the integer part from the fractional part of numbers. And, rather than using commas to separate fields in your data, you might use colons or tabs. With a Data Asset import or export node, you can specify these symbols and other options. Double-click the node to open its properties and specify data formats as desired."
3175	Creating synthetic data from production data	"Using the Synthetic Data Generator graphical editor flow tool, you can generate a structured synthetic data set based on your production data. You can import data, anonymize, mimic (to generate synthetic data), export, and review your data.  
Before you can use mimic and mask to create synthetic data, you need to create a task.  
1. The Generate synthetic tabular data flow window opens. Select use case Leverage your existing data. Click Next.  
2. Select Import data. You can also drag-and-drop a data file into your project. You can also select data from a project. For more information, see Importing data.  
3. Once you have imported your data, you can use the Synthetic Data Generator graphical flow editor tool to anonymize your production data, masking the data. You can disguise column names, column values, or both, when working with data that is to be included in a model downstream of the node. For example, you can use bank customer data and hide marital status.  
4. You can then use the Synthetic Data Generator tool to mimic your production data. This will generate synthetic data, based on your production data, using a set of candidate statistical distributions to modify each column in your data.  
5. You can export your synthetic data and review it. For more information, see Exporting synthetic data."
3176	Creating synthetic data from production data	"Using differential privacy
Differential privacy protects user data from being traced back to individual users. The parameters involved are known as the privacy budget. This is a metric of privacy loss based on adding or removing one entry in a data set.  
To implement differential privacy in your synthetic data created from production data:  
1. Select the Mimic node. Select Edit.  
2. Scroll down and select Privacy. In the Privacy section, turn on Enable differential privacy. This will ensure that no sensitive data specific to any individual is exposed in the synthetic output. You can control the level of privacy protection by adjusting the privacy budget (epsilon) and leakage (delta) parameters.  
3. Adjust the Privacy budget (epsilon). The privacy budget allows you to tune the level of privacy protection required in your synthetic output. A smaller value provides greater privacy protection, with some loss in accuracy. A larger value provides greater accuracy, with less privacy protection.  
4. Adjust the Privacy leakage probability (delta). Delta is usually referred to as the maximum allowable probability of a privacy leakage. Delta should be less than or equal to 1/n*n, where n = sample size. The smaller the delta is, the better the privacy is preserved.  
5. Generate a Random seed. When differential privacy is enabled, this random seed value will enable you to reproduce your differentially private synthetic output. When differential privacy is disabled, the random seed value can be adjusted in the Generate node.  
6. Manually adjust the Column bounds (optional). Column bounds are automatically applied, but you can manually adjust these bounds to restrict the range of values used for fitting. You can only select numeric columns.  
7. After updating the Privacy options, select Save.  
8. Select Run all.  
Note that parameters that are based on the synthetically generated dataset where differential privacy has been enabled will differ from the parameters in your original dataset."
3177	Creating synthetic data from production data	"Learn more
Creating synthetic data from a custom data schema"
3178	Creating Synthetic data	"Use the graphical flow editor tool Synthetic Data Generator to generate synthetic tabular data based on production data or a custom data schema using visual flows and modeling algorithms.  
To create synthetic data, the first option is to use the Synthetic Data Generator graphical flow editor tool to mask and mimic production data, and then to load the result into a different location.  
The second option is to use the Synthetic Data Generator graphical flow editor to generate synthetic data from a custom data schema using visual flows and modeling algorithms.  
This image shows an overview of the Synthetic Data Generator graphical flow editor.  
Data format Learn more about Creating synthetic data from imported data."
3179	Creating Synthetic data	"Prerequisites
Before you can create synthetic data, you need to create a project."
3180	Creating Synthetic data	"Create synthetic data
1. Access the Synthetic Data Generator tool from within a project. To select a new asset, open a tool, and create an asset, click New asset.  
2. Select All > Prepare Data > Generate synthetic tabular data from the What do you want to do? window.  
3. The Generate synthetic tabular data window opens. Add a name for the asset and a description (optional). Click Create. The flow will open and it might take a minute to create a new session for the flow.  
4. The Welcome to Synthetic Data Generator wizard opens. You can choose to get started as a first time or experienced user.  
5. If you choose to get started as a first time user, the Generate synthetic tabular data flow window opens."
3181	Creating Synthetic data	"Learn more
Creating synthetic data from production data Creating synthetic data from a custom data schema Try the Generate synthetic tabular data tutorial"
3182	Creating synthetic data from a custom data schema	"Using the Synthetic Data Generator graphical editor flow tool, you can generate a structured synthetic data set based on meta data, automatically or with user-specified statistical distributions. You can define the data within each table column, their distributions, and any correlations. You can then export and review your synthetic data.  
Before you can use generate to create synthetic data, you need to create a task.  
1. The Generate synthetic tabular data flow window opens. Select use case Create from custom data schema. Click Next.  
2. Select Generate options. You can use the Synthetic Data Generator graphical editor flow tool to specify the number of rows and add columns. You can define properties and specify fields, storage types, statistical distributions, and distribution parameters. Click Next.  
3. Select Export data to select the export file name and type. For more information, see Exporting data. Click Next.  
4. Select Review to check your selection and make any updates before generating your synthetic data. Click Save and run."
3183	Creating synthetic data from a custom data schema	"Learn more
Creating synthetic data from production data"
3184	Creating synthetic data from imported data	"Supported data sources for Synthetic Data Generator.  
Using Synthetic Data Generator, you can connect to your data no matter where it lives, using either connectors or data files."
3185	Creating synthetic data from imported data	"Data size
The Synthetic Data Generator environment can import up to ~2.5GB of data."
3186	Creating synthetic data from imported data	"Connectors
The following table lists the data sources that you can connect to using Synthetic Data Generator."
3187	Creating synthetic data from imported data	"Data files
In addition to using data from remote data sources or integrated databases, you can use data from files. You can work with data from the following types of files using Synthetic Data Generator."
3188	Interactive code templates in Data Refinery	"Data Refinery provides interactive templates for you to code operations, functions, and logical operators. Access the templates from the command-line text box at the top of the page. The templates include interactive assistance to help you with the syntax options.  
Important: Support is for the operations and functions in the user interface. If you insert other operations or functions from an open source library, the Data Refinery flow might fail. See the command-line help and be sure to use the list of operations or functions from the templates. Use the examples in the templates to further customize the syntax as needed.  
Operations Functions Logical operators"
3189	Interactive code templates in Data Refinery	"Operations
arrange
arrange(`<column>`) Sort rows, in ascending order, by the specified columns.  
arrange(desc(`<column>`)) Sort rows, in descending order, by the specified column.  
arrange(`<column>`, `<column>`) Sort rows, in ascending order, by each specified, successive column, keeping the order from the prior sort intact."
3190	Interactive code templates in Data Refinery	"Operations
count
count() Total the data by group.  
count(`<column>`) Group the data by the specified column and return the number of rows with unique values (for string values) or return the total for each group (for numeric values).  
count(`<column>`, wt=`<column>`) Group the data by the specified column and return the number of rows with unique values (for string values) or return the total for each group (for numeric values) in the specified weight column.  
count(`<column>`, wt=<func>(`<column>`)) Group the data by the specified column and return the result of the function applied to the specified weight column.  
count(`<column>`, wt=<func>(`<column>`), sort = <logical>) Group the data by the specified column and return the result of the function applied to the specified weight column, sorted or not."
3191	Interactive code templates in Data Refinery	"Operations
distinct
distinct() Keep distinct, unique rows based on all columns or on specified columns."
3192	Interactive code templates in Data Refinery	"Operations
filter
filter(`<column>` <logicalOperator> provide_value) Keep rows that meet the specified condition and filter out all other rows. For the Boolean column type, provide_value should be uppercase TRUE or FALSE.  
filter(`<column>`== <logical>) Keep rows that meet the specified filter conditions based on logical value TRUE or FALSE.  
filter(<func>(`<column>`) <logicalOperator> provide_value) Keep rows that meet the specified condition and filter out all other rows. The condition can apply a function to a column on the left side of the operator.  
filter(`<column>` <logicalOperator> <func(column)>) Keep rows that meet the specified condition and filter out all other rows. The condition can apply a function to a column on the right side of the operator.  
filter(<logicalfunc(column)>) Keep rows that meet the specified condition and filter out all other rows. The condition can apply a logical function to a column.  
filter(`<column>` <logicalOperator> provide_value <andor> `<column>` <logicalOperator> provide_value ) Keep rows that meet the specified conditions and filter out all other rows."
3193	Interactive code templates in Data Refinery	"Operations
group_by
group_by(`<column>`) Group the data based on the specified column.  
group_by(desc(`<column>`)) Group the data, in descending order, based on the specified column."
3194	Interactive code templates in Data Refinery	"Operations
mutate
mutate(provide_new_column = `<column>`) Add a new column and keep existing columns.  
mutate(provide_new_column = <func(column)>) Add a new column by using the specified expression, which applies a function to a column. Keep existing columns.  
mutate(provide_new_column = case_when(`<column>` <operator> provide_value_or_column_to_compare ~ provide_value_or_column_to_replace, `<column>` <operator> provide_value_or_column_to_compare ~ provide_value_or_column_to_replace, TRUE ~ provide_default_value_or_column)) Add a new column by using the specified conditional expression.  
mutate(provide_new_column = `<column>` <operator> `<column>`) Add a new column by using the specified expression, which performs a calculation with existing columns. Keep existing columns.  
mutate(provide_new_column = coalesce(`<column>`, `<column>`)) Add a new column by using the specified expression, which replaces missing values in the new column with values from another, specified column. As an alternative to specifying another column, you can specify a value, a function on a column, or a function on a value. Keep existing columns.  
mutate(provide_new_column = if_else(`<column>` <logicalOperator> provide_value, provide_value_for_true, provide_value_for_false)) Add a new column by using the specified conditional expression. Keep existing columns.  
mutate(provide_new_column = `<column>`, provide_new_column = `<column>`) Add multiple new columns and keep existing columns.  
mutate(provide_new_column = n()) Count the values in the groups. Ensure grouping is done already using group_by. Keep existing columns."
3195	Interactive code templates in Data Refinery	"Operations
mutate_all
mutate_all(funs(<func>)) Apply the specified function to all of the columns and overwrite the existing values in those columns. Specify whether to remove missing values.  
mutate_all(funs(. <operator> provide_value)) Apply the specified operator to all of the columns and overwrite the existing values in those columns.  
mutate_all(funs(""provide_value"" = . <operator> provide_value)) Apply the specified operator to all of the columns and create new columns to hold the results. Give the new columns names that end with the specified value."
3196	Interactive code templates in Data Refinery	"Operations
mutate_at
mutate_at(vars(`<column>`), funs(<func>)) Apply functions to the specified columns."
3197	Interactive code templates in Data Refinery	"Operations
mutate_if
mutate_if(<predicateFunc>, <func>) Apply functions to the columns that meet the specified condition.  
mutate_if(<predicateFunc>, funs( . <operator> provide_value)) Apply the specified operator to the columns that meet the specified condition.  
mutate_if(<predicateFunc>, funs(<func>)) Apply functions to the columns that meet the specified condition. Specify whether to remove missing values."
3198	Interactive code templates in Data Refinery	"Operations
rename
rename(provide_new_column = `<column>`) Rename the specified column."
3199	Interactive code templates in Data Refinery	"Operations
sample_frac
sample_frac(provide_number_between_0_and_1, weight=`<column>`,replace=<logical>) Generate a random sample based on a percentage of the data. weight is optional and is the ratio of probability the row will be chosen. Provide a numeric column. replace is optional and its Default is FALSE."
3200	Interactive code templates in Data Refinery	"Operations
sample_n
sample_n(provide_number_of_rows,weight=`<column>`,replace=<logical>) Generate a random sample of data based on a number of rows. weight is optional and is the ratio of probability the row will be chosen. Provide a numeric column. replace is optional and its default is FALSE."
3201	Interactive code templates in Data Refinery	"Operations
select
select(`<column>`) Keep the specified column.  
select(-`<column>`) Remove the specified column.  
select(starts_with(""provide_text_value"")) Keep columns with names that start with the specified value.  
select(ends_with(""provide_text_value"")) Keep columns with names that end with the specified value.  
select(contains(""provide_text_value"")) Keep columns with names that contain the specified value.  
select(matches (""provide_text_value"")) Keep columns with names that match the specified value. The specified value can be text or a regular expression.  
select(`<column>`:`<column>`) Keep the columns in the specified range. Specify the range as from one column to another column.  
select(`<column>`, everything()) Keep all of the columns, but make the specified column the first column.  
select(`<column>`, `<column>`) Keep the specified columns."
3202	Interactive code templates in Data Refinery	"Operations
select_if
select_if(<predicateFunc>) Keep columns that meet the specified condition. Supported functions include:  
contains ends_with matches num_range starts_with"
3203	Interactive code templates in Data Refinery	"Operations
summarize
summarize(provide_new_column = <func>(`<column>`)) Apply aggregate functions to the specified columns to reduce multiple column values to a single value. Be sure to group the column data first by using the group_by operation."
3204	Interactive code templates in Data Refinery	"Operations
summarize_all
summarize_all(<func>) Apply an aggregate function to all of the columns to reduce multiple column values to a single value. Specify whether to remove missing values. Be sure to group the column data first by using the group_by operation.  
summarize_all(funs(<func>)) Apply multiple aggregate functions to all of the columns to reduce multiple column values to a single value. Create new columns to hold the results. Specify whether to remove missing values. Be sure to group the column data first by using the group_by operation."
3205	Interactive code templates in Data Refinery	"Operations
summarize_if
summarize_if(<predicate_conditions>,...) Apply aggregate functions to columns that meet the specified conditions to reduce multiple column values to a single value. Specify whether to remove missing values. Be sure to group the column data first by using the group_by operation. Supported functions include:  
count max mean min standard deviation sum"
3206	Interactive code templates in Data Refinery	"Operations
tally
tally() Counts the number of rows (for string columns) or totals the data (for numeric values) by group. Be sure to group the column data first by using the group_by operation.  
tally(wt=`<column>`) Counts the number of rows (for string columns) or totals the data (for numeric columns) by group for the weighted column.  
tally( wt=<func>(`<column>`), sort = <logical>) Applies a function to the specified weighted column and returns the result, by group, sorted or not."
3207	Interactive code templates in Data Refinery	"Operations
top_n
top_n(provide_value) Select the top or bottom N rows (by value) in each group. Specify a positive integer to select the top N rows; specify a negative integer to select the bottom N rows.  
top_n(provide_value, `<column>`) Select the top or bottom N rows (by value) in each group, based on the specified column. Specify a positive integer to select the top N rows; specify a negative integer to select the bottom N rows.  
If duplicate rows affect the count, use the Remove duplicates GUI operation prior to using the top_n() operation."
3208	Interactive code templates in Data Refinery	"Operations
transmute
transmute(<new_or_existing_column> = `<column>`) Add a new column or overwrite an existing one by using the specified expression. Keep only columns that are mentioned in the expression.  
transmute(<new_or_existing_column> = <func(column)>) Add a new column or overwrite an existing one by applying a function to the specified column. Keep only columns that are mentioned in the expression.  
transmute(<new_or_existing_column> = `<column>` <operator> `<column>`) Add a new column or overwrite an existing one by applying an operator to the specified column. Keep only columns that are mentioned in the expression.  
transmute(<new_or_existing_column> = `<column>`, <new_or_existing_column> = `<column>`) Add multiple new columns. Keep only columns that are mentioned in the expression.  
transmute(<new_or_existing_column> = if_else( provide_value, provide_value_for_true, provide_value_for_false)) Add a new column or overwrite an existing one by using the specified conditional expressions. Keep only columns that are mentioned in the expressions."
3209	Interactive code templates in Data Refinery	"Operations
ungroup
ungroup() Ungroup the data."
3210	Interactive code templates in Data Refinery	"Functions
Aggregate
mean min n sd sum"
3211	Interactive code templates in Data Refinery	"Functions
Logical
is.na"
3212	Interactive code templates in Data Refinery	"Functions
Numerical
abs coalesce cut exp floor"
3213	Interactive code templates in Data Refinery	"Functions
Text
c coalesce paste tolower toupper"
3214	Interactive code templates in Data Refinery	"Functions
Type
as.character as.double as.integer as.logical"
3215	Interactive code templates in Data Refinery	"Logical operators
< <= >= > between != == %in%  
Parent topic: Refining data"
3216	Supported data sources for Data Refinery	Data Refinery supports the following data sources in connections.
3217	Supported data sources for Data Refinery	"IBM services
IBM Cloud Data Engine (Supports source connections only) IBM Cloud Databases for MongoDB (Supports source connections only) IBM Cloud Object Storage IBM Cloudant IBM Cognos Analytics (Supports source connections only) IBM Data Virtualization Manager for z/OS IBM Db2 IBM Db2 Big SQL IBM Db2 on Cloud IBM Db2 Warehouse IBM Planning Analytics (Supports source connections only)  
IBM watsonx.data"
3218	Supported data sources for Data Refinery	"Third-party services
Amazon RDS for MySQL Amazon RDS for Oracle Amazon RDS for PostgreSQL Amazon Redshift Amazon S3 Apache Cassandra Apache Derby Apache HDFS Apache Hive (Supports source connections only) Box Cloudera Impala (Supports source connections only) Dremio Dropbox Elasticsearch FTP Generic S3 Google BigQuery Google Cloud Storage HTTP (Supports source connections only) MariaDB Microsoft Azure Blob Storage Microsoft Azure Cosmos DB Microsoft SQL Server MongoDB (Supports source connections only) OData (Supports source connections only) Oracle PostgreSQL Presto (Supports source connections only) SAP OData (Supports source connections only) SingleStoreDB Snowflake  
Parent topic: Refining data"
3219	Visualizing your data in Data Refinery	"Visualizing information in graphical ways gives you insights into your data. You can add steps to your Data Refinery flow while you visualize you data and see the changes. By exploring data from different perspectives with visualizations, you can identify patterns, connections, and relationships within that data as well as quickly understand large amounts of information.  
You can also visualize your data with these same charts in an SPSS Modeler flow. Use the Charts node, which is available under the Graphs section on the node palette. Double-click the Charts node to open the properties pane. Then click Launch Chart Builder to open the chart builder and create one or more chart definitions to associate with the node.  
To visualize your data:  
From Data Refinery, click the Visualizations tab. Start with a chart or select columns:  
Click any of the available charts. Then, add columns in the DETAILS pane that opens on the left side of the page. Select the columns that you want to work with. Suggested charts are indicated with a dot next to the chart name. Click a chart to visualize your data.  
Important: Available chart types are ordered from most relevant to least relevant, based on the selected columns. If there are no columns in the data set with a data type that is supported for a chart type, that chart will not be available. If a column's data type is not supported for a chart, that column is not available for selection for that chart. Dots next to the charts' names suggest the best charts for your data."
3220	Visualizing your data in Data Refinery	"Charts
The following charts are included:  
3D charts display data in a 3-D coordinate system by drawing each column as a cuboid to create a 3D effect.  
Bar charts are handy for displaying and comparing categories of data side by side. The bars can be in any order. You can also arrange them from high to low or from low to high.  
Box plot charts compare distributions between many groups or data sets. They display the variation in groups of data: the spread and skew of that data and the outliers.  
Bubble charts display each category in the groups as a bubble.  
Candlestick charts are a type of financial chart that displays price movements of a security, derivative, or currency.  
Circle packing charts display hierarchical data as a set of nested areas.  
Customized charts give you the ability to render charts based on JSON input.  
Dual Y-axes charts use two Y-axis variables to show relationships between data.  
Error bars indicate the error or uncertainty in a value. They give a general idea of how precise a value is or conversely, how far a value might be from the true value.  
Evaluation charts are combination charts that measure the quality of a binary classifier. You need three columns for input: actual (target) value, predict value, and confidence (0 or 1). Move the slider in the Cutoff chart to dynamically update the other charts. The ROC and other charts are standard measurements of the classifier.  
Heat map charts display data as color to convey activity levels or density. Typically low values are displayed as cooler colors and high values are displayed as warmer colors.  
Histogram charts show the frequency distribution of data.  
Line charts show trends in data over time by calculating a summary statistic for one column for each value of another column and then drawing a line that connects the values.  
Map charts show geographic point data, so you can compare values and show categories across geographical regions.  
Math curve charts display a group of curves based on equations that you enter. You do not use a data set with this chart. Instead, you use it to compare the results with the data set in another chart, like the scatter plot chart."
3221	Visualizing your data in Data Refinery	"Charts
Multi-charts display up to four combinations of Bar, Line, Pie, and Scatter plot charts. You can show the same kind of chart more than once with different data. For example, two pie charts with data from different columns.  
Multi-series charts display data from multiple data sets or multiple columns as a series of points that are connected by straight lines or bars.  
Parallel coordinate charts display and compare rows of data (called profiles) to find similarities. Each row is a line and the value in each column of the row is represented by a point on that line.  
Pie charts show proportion. Each value in a series is displayed as a proportional slice of the pie. The pie represents the total sum of the values.  
Population pyramid charts show the frequency distribution of a variable across categories. They are typically used to show changes in demographic data.  
Quantile-quantile (Q-Q) plot charts compare the expected distribution values with the observed values by plotting their quantiles.  
Radar charts integrate three or more quantitative variables that are represented on axes (radii) into a single radial figure. Data is plotted on each axis and joined to adjacent axes by connecting lines. Radar charts are useful to show correlations and compare categorized data.  
Relationship charts show how columns of data relate to one another and what the strength of that relationship is by using varying types of lines.  
Scatter matrix charts map columns against each other and display their scatter plots and correlation. Use to compare multiple columns and how strong their correlation is with one another.  
Scatter plot charts show correlation (how much one variable is affected by another) by displaying and comparing the values in two columns.  
Sunburst charts are similar to layered pie charts, in which different proportions of different categories are shown at once on multiple levels.  
Theme river charts use a specialized flow graph that shows changes over time.  
Time plot charts illustrate data points at successive intervals of time.  
t-SNE charts help you visualize high-dimensional data sets. They're useful for embedding high-dimensional data into a space of two or three dimensions, which can then be visualized in a scatter plot."
3222	Visualizing your data in Data Refinery	"Charts
t-SNE charts help you visualize high-dimensional data sets. They're useful for embedding high-dimensional data into a space of two or three dimensions, which can then be visualized in a scatter plot.  
Tree charts display hierarchical data, categorically splitting into different branches. Use to sort different data sets under different categories. The Tree chart consists of a root node, line connections called branches that represent the relationships and connections between the members, and leaf nodes that do not have child nodes.  
Treemap charts display hierarchical data as a set of nested areas. Use to compare sizes between groups and single elements that are nested in the groups.  
Word cloud charts display how frequently words appear in text by making the size of each word proportional to its frequency."
3223	Visualizing your data in Data Refinery	"Actions
You can take any of the following actions:  
Start over: Clears the visualization and the DETAILS pane, and returns you to the starting page for visualizations  
Specify whether to display the field value or the field label. This option applies only to SPSS Modeler when you define labels. For example, if you have a ""Gender"" field and you have defined a label as female with the value 0, and then the label male for value 1. If there is no label defined, the value is displayed.  
Download visualization:  
Download chart image: Download a PNG file that contains an image of the current chart.  
Download chart details: Download a JSON file that contains the details for the current chart.  
Set global preferences that apply to all charts"
3224	Visualizing your data in Data Refinery	"Chart actions
Available chart actions depend on the chart. Chart actions include:  
Zoom  
Restore: View the chart at normal scale  
Select data: Highlight data in the Data tab that you select in the chart  
Clear selection: Remove highlighting from the data in the Data tab"
3225	Visualizing your data in Data Refinery	"Learn more
Data Visualization – How to Pick the Right Chart Type?  
Parent topic: Refining data"
3226	Refining data	"To refine data, you take it from one location, cleanse and shape it, and then load the result into a different location. You can cleanse and shape tabular data with a graphical flow editor tool called Data Refinery.  
When you cleanse data, you fix or remove data that is incorrect, incomplete, improperly formatted, or duplicated. When you shape data, you customize it by filtering, sorting, combining or removing columns.  
You create a Data Refinery flow as a set of ordered operations on data. Data Refinery includes a graphical interface to profile your data to validate it and over 20 customizable charts that give you insights into your data.  
Prerequisites Source file limitations Target file limitations Data set previews Refine your data"
3227	Refining data	"Prerequisites
Before you can refine data, you need a project that uses Cloud Object Storage. You can use the sandbox project or create a new project.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
If you have data in cloud or on-premises data sources, you'll need to add connections to those sources and you'll need to add data assets from each connection. If you want to be able to save refined data to cloud or on-premises data sources, create connections for this purpose as well. Source connections can be used only to read data; target connections can be used only to load (save) data. When you create a target connection, be sure to use credentials that have Write permission or you won't be able to save your Data Refinery flow output to the target.  
Video disclaimer: Some minor steps and graphical elements in this video might differ from your platform.  
This video provides a visual method to learn the concepts and tasks in this documentation."
3228	Refining data	"Source file limitations
CSV files
Be sure that CSV files are correctly formatted and conform to the following rules:  
Two consecutive commas in a row indicate an empty column. If a row ends with a comma, an additional column is created."
3229	Refining data	"Source file limitations
White-space characters are considered as part of the data
If your data includes columns that contain white space (blank) characters, Data Refinery considers those white-space characters as part of the data, even though you can't see them in the grid. Some database tools might pad character strings with white-space characters to make all the data in a column the same length and this change affects the results of Data Refinery operations that compare data."
3230	Refining data	"Source file limitations
Column names
Be sure that column names conform to the following rules:  
Duplicate column names are not allowed. Column names must be unique within the data set. Column names are not case-sensitive. A data set that includes a column name ""Sales"" and another column name ""sales"" will not work. The column names are not reserved words in the R programming language. The column names are not numbers. A workaround is to enclose the column names in double quotation marks ("""")."
3231	Refining data	"Source file limitations
Data sets with columns with the ""Other"" data type are not supported in Data Refinery flows
If your data set contains columns that have data types that are identified as ""Other"" in the Watson Studio preview, the columns will show as the String data type in Data Refinery. However, if you try to use the data in a Data Refinery flow, the job for the Data Refinery flow will fail. An example of a data type that shows as ""Other"" in the preview is the Db2 DECFLOAT data type."
3232	Refining data	"Target file limitations
The following limitation applies if you save Data Refinery flow output (the target data set) to a file:  
You can't change the file format if the file is an existing data asset."
3233	Refining data	"Data set previews
Data Refinery provides support for large data sets, which can be time-consuming and unwieldy to refine. To enable you to work quickly and efficiently, it operates on a subset of rows in the data set while you interactively refine the data. When you run a job for the Data Refinery flow, it operates on the entire data set."
3234	Refining data	"Refine your data
The following video shows you how to refine data.  
Video disclaimer: Some minor steps and graphical elements in this video might differ from your platform.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
1. Access Data Refinery from within a project. Click New asset > Prepare and visualize data. Then select the data that you want to work with. Alternatively, from the Assets tab of a project, open a file ( supported formats) to preview it, and then click Prepare data.  
Note that different asset types can have duplicate names. However, you can't add an asset type with the same name multiple times.  
2. Use steps to apply operations that cleanse, shape, and enrich your data. Browse operation categories or search for a specific operation, then let the UI guide you. You can enter R code in the command line and let autocomplete assist you in getting the correct syntax. As you apply operations to a data set, Data Refinery keeps track of them and builds a Data Refinery flow. For each operation that you apply, Data Refinery adds a step.  
Data tab  
If your data contains non-string data types, the Convert column type GUI operation is automatically applied as the first step in the Data Refinery flow when you open a file in Data Refinery. Data types are automatically converted to inferred data types, such as Integer, Date, or Boolean. You can undo or edit this step.  
3. Click the Profile tab to validate your data throughout the data refinement process.  
Profile tab  
4. Click the Visualizations tab to visualize the data in charts. Uncover patterns, trends, and correlations within your data.  
Visualizations tab  
5. Refine the sample data set to suit your needs.  
6. Click Save and create a job or Save and view jobs in the toolbar to run the Data Refinery flow on the entire data set. Select the runtime and add a one-time or repeating schedule. For information about jobs, see Creating jobs in Data Refinery.  
For the actions that you can do as you refine your data, see Managing Data Refinery flows."
3235	Refining data	"Next step
Analyze your data and build models"
3236	Refining data	"Learn more
Manage Data Refinery flows  
Quick start: Refine data  
Parent topic: Preparing data"
3237	Managing Data Refinery flows	"A Data Refinery flow is an ordered set of steps to cleanse, shape, and enhance data. As you refine your data by applying operations to a data set, you dynamically build a customized Data Refinery flow that you can modify in real time and save for future use.  
These are actions that you can do while you refine your data:  
Working with the Data Refinery flow  
Save a Data Refinery flow Run or schedule a job for Data Refinery flow Rename a Data Refinery flow  
Steps  
Undo or redo a step Edit, duplicate, insert, or delete a step View the Data Refinery flow steps in a ""snapshot view"" Export the Data Refinery flow data to a CSV file  
Working with the data sets  
Change the source of a Data Refinery flow Edit the sample size Edit the source properties Change the target of a Data Refinery flow Edit the target properties Change the name of the Data Refinery flow target  
Actions on the project page  
Reopen a Data Refinery flow to continue working Duplicate a Data Refinery flow Delete a Data Refinery flow Promote a Data Refinery flow to a space"
3238	Managing Data Refinery flows	"Working with the Data Refinery flow
Save a Data Refinery flow
Save a Data Refinery flow by clicking the Save Data Refinery flow icon in the Data Refinery toolbar. Data Refinery flows are saved to the project that you're working in. Save a Data Refinery flow so that you can continue refining a data set later.  
The default output of the Data Refinery flow is saved as a data asset source-file-name_shaped.csv. For example, if the source file is mydata.csv, the default name and output for the Data Refinery flow is mydata_csv_shaped. You can edit the name and add an extension by changing the target of a Data Refinery flow."
3239	Managing Data Refinery flows	"Working with the Data Refinery flow
Run or schedule a job for a Data Refinery flow
Data Refinery supports large data sets, which can be time-consuming and unwieldy to refine. So that you can work quickly and efficiently, Data Refinery operates on a sample subset of rows in the data set. The sample size is 1 MB or 10,000 rows, whichever comes first. When you run a job for the Data Refinery flow, the entire data set is processed. When you run the job, you select the runtime and you can add a one-time or repeating schedule.  
In Data Refinery, from the Data Refinery toolbar click the Jobs icon , and then select Save and create a job or Save and view jobs.  
After you save a Data Refinery flow, you can also create a job for it from the Project page. Go to the Assets tab, select the Data Refinery flow, choose New job from the overflow menu ().  
You must have the Admin or Editor role to view the job details or to edit or run the job. With the Viewer role for the project, you can view only the job details.  
For more information about jobs, see Creating jobs in Data Refinery."
3240	Managing Data Refinery flows	"Working with the Data Refinery flow
Rename a Data Refinery flow
On the Data Refinery toolbar, open the Info pane . Or open the Flow settings and go to the General tab."
3241	Managing Data Refinery flows	"Steps
Undo or redo a step
Click the undo () icon or the redo () icon on the toolbar."
3242	Managing Data Refinery flows	"Steps
Edit, duplicate, insert, or delete a step
In the Steps pane, click the overflow menu () on the step for the operation that you want to change. Select the action (Edit, Duplicate, Insert step before, Insert step after, or Delete).  
If you select Edit, Data Refinery goes into edit mode and either displays the operation to be edited on the command line or in the Operation pane. Apply the edited operation.  
If you select Duplicate, the duplicated step is inserted after the selected step.  
Note:  
The Duplicate action is not available for the Join or Union operations.  
Data Refinery updates the Data Refinery flow to reflect the changes and reruns all the operations."
3243	Managing Data Refinery flows	"Steps
View the Data Refinery flow steps in a ""snapshot view""
To see what your data looked like at any point in time, click a previous step to put Data Refinery into snapshot view. For example, if you click Data source, you see what your data looked like before you started refining it. Click any operation step to see what your data looked like after that operation was applied. To leave snapshot view, click Viewing step x of y or click the same step that you selected to get into snapshot view."
3244	Managing Data Refinery flows	"Steps
Export the Data Refinery flow data to a CSV file
Click Export () on the toolbar to export the data at the current step in your Data Refinery flow to a CSV file without saving or running a Data Refinery flow job. Use this option, for example, if you want quick output of a Data Refinery flow that is in progress. When you export the data, a CSV file is created and downloaded to your computer's Downloads folder (or the user-specified download location) at the current step in the Data Refinery flow. If you are in snapshot view, the output of the CSV file is at the step that you clicked. If you are viewing a sample (subset) of the data, only the sample data will be in the output."
3245	Managing Data Refinery flows	"Working with the data sets
Change the source of a Data Refinery flow
Change the source of a Data Refinery flow. Run the same Data Refinery flow but with a different source data set. There are two ways that you can change the source:  
In the Steps pane: Click the overflow menu () next to Data source, select Edit, and then choose a different source data set.  
In the Flow settings: You can use this method if you want to change more than one data source in the same place. For example, for a Join or a Union operation. On the toolbar, open the Flow settings . Go to the Source data sets tab and click the overflow menu () next to the data source. Select Replace data source, and then choose a different source data set.  
For best results, the new data set should have a schema that is compatible to the original data set (for example, column names, number of columns, and data types). If the new data set has a different schema, operations that won't work with the schema will show errors. You can edit or delete the operations, or change the source to one that has a more compatible schema."
3246	Managing Data Refinery flows	"Working with the data sets
Edit the sample size
When you run the job for the Data Refinery flow, the operations are performed on the full data set. However, when you apply the operations interactively in Data Refinery, depending on the size of the data set, you view only a sample of the data.  
Increase the sample size to see results that will be closer to the results of the Data Refinery flow job, but be aware that it might take longer to view the results in Data Refinery. The maximum is a top-row count of 10,000 rows or 1 MB, whichever comes first. Decrease the sample size to view faster results. Depending on the size of the data and the number and complexity of the operations, you might want to experiment with the sample size to see what works best for the data set.  
On the toolbar, open the Flow settings . Go to the Source data sets tab and click the overflow menu () next to the data source, and select Edit sample."
3247	Managing Data Refinery flows	"Working with the data sets
Edit the source properties
The available properties depend on the data source. Different properties are available for data assets and for data from different kinds of connections. Change the file format only if the inferred file format is incorrect. If you change the file format, the source is read with the new format, but the source file remains unchanged. Changing the format source properties might be an iterative process. Inspect your data after you apply an option.  
On the toolbar, open the Flow settings . Go to the Source data sets tab and click the overflow menu () next to the data source, and select Edit format.  
Important: Use caution if you edit the source properties. Incorrect selections might produce unexpected results when the data is read or impair the Data Refinery flow job. Inspect the results of the Data Refinery flow carefully."
3248	Managing Data Refinery flows	"Working with the data sets
Change the target of a Data Refinery flow
By default, the target of the Data Refinery is saved as a data asset in the project that you're working in.  
To change the target location, open Flow settings from the toolbar. Go to the Target data set tab, click Select target, and select a different target location."
3249	Managing Data Refinery flows	"Working with the data sets
Edit the target properties
The available properties depend on the data source. Different properties are available for data assets and for data from different kinds of connections.  
To change the target data set's properties, open the Flow settings from the toolbar. Go to the Target data set tab, and click Edit properties.  
The name of the target data set is included in the fields that you can change when you edit the target properties.  
By default, the target of the Data Refinery is saved as a data asset source-file-name_shaped.csv in the project. For example, if the source is mydata.csv, the default name and output for the Data Refinery flow is the data asset mydata_csv_shaped.  
Different properties and naming conventions apply to a target data set from a connection. For example, if the data set is in Cloud Object Storage, the data set is identified in the Bucket and File name fields. If the data set is in a Db2 database, the data set is identified in the Schema name and Table name fields.  
Important: Use caution if you edit the target properties. Incorrect selections might produce unexpected results or impair the Data Refinery flow job. Inspect the results of the Data Refinery flow carefully."
3250	Managing Data Refinery flows	"Actions on the project page
Reopen a Data Refinery flow to continue working
To reopen a Data Refinery flow and continue refining your data, go to the project’s Assets tab. Under Asset types, expand Flows, click Data Refinery flow. Click the Data Refinery flow name."
3251	Managing Data Refinery flows	"Actions on the project page
Duplicate a Data Refinery flow
To create a copy of a Data Refinery flow, go to the project's Assets tab, expand Flows, click Data Refinery flow. Select the Data Refinery flow, and then select Duplicate from the overflow menu (). The Data Refinery flow is added to the Data Refinery flows list as ""original-name copy 1""."
3252	Managing Data Refinery flows	"Actions on the project page
Delete a Data Refinery flow
To delete a Data Refinery flow, go to the project's Assets tab, expand Flows, click Data Refinery flow. Select the Data Refinery flow, and then select Delete from the overflow menu ()."
3253	Managing Data Refinery flows	"Actions on the project page
Promote a Data Refinery flow to a space
Deployment spaces are used to manage a set of related assets in a separate environment from your projects. You use a space to prepare data for a deployment job for Watson Machine Learning. You can promote Data Refinery flows from multiple projects to a single space. Complete the steps in the Data Refinery flow before you promote it because the Data Refinery flow is not editable in a space.  
To promote a Data Refinery flow to a space, go to the project's Assets tab, expand Flows, click Data Refinery flow. Select the Data Refinery flow. Click the overflow menu () for the Data Refinery flow, and then select Promote. The source file for the Data Refinery flow and any other dependent data will be promoted as well.  
To create or run a job for the Data Refinery flow in a space, go the space’s Assets tab, scroll down to the Data Refinery flow, and select New job () from the overflow menu (). If you've already created the job, go to the Jobs tab to edit the job or view the job run details. The shaped output of the Data Refinery flow job will be available on the space’s Assets tab. You must have the Admin or Editor role to view the job details or to edit or run the job. With the Viewer role for the project, you can only view the job details. You can use the shaped output as input data for a job in Watson Machine Learning.  
Restriction:  
When you promote a Data Refinery flow from a project to a space and the target of the Data Refinery flow is a connected data asset, you must manually promote the connected data asset. This action ensures that the connected data asset's data is updated when you run the Data Refinery flow job in the space. Otherwise, a successful run of the Data Refinery flow job will create a new data asset in the space.  
For information about spaces, see Deployment spaces.  
Parent topic: Refining data"
3254	Adding data to Data Refinery	"After you create a project and you create connections or you add data assets to the project, you can then add data to Data Refinery and start prepping that data for analysis.  
You can add data to Data Refinery in one of several ways:  
Select Prepare data from the overflow menu () of a data asset in the All assets list for the project Preview a data asset in the project and then click Prepare data Navigate to Data Refinery first and then add data to it"
3255	Adding data to Data Refinery	"Navigate to Data Refinery
Access Data Refinery from within a project. Click the Assets tab.  
Click New asset > Prepare and visualize data.  
Select the data that you want to work with from Data assets or from Connections.  
From Data assets:  
Select a data file (the selection includes data files that were already shaped with Data Refinery) Select a connected data asset  
From Connections:  
Select a connection and file Select a connection, folder, and file Select a connection, schema, and table or view  
Data Refinery supports these file types: Avro, CSV, delimited text files, JSON, Microsoft Excel (xls and xlsx formats. First sheet only, except for connections and connected data assets.), Parquet, SAS with the ""sas7bdat"" extension (read only), TSV (read only)  
Data Refinery operates on a sample subset of rows in the data set. The sample size is 1 MB or 10,000 rows, whichever comes first. However, when you run a job for the Data Refinery flow, the entire data set is processed. If the Data Refinery flow fails with a large data asset, see workarounds in Troubleshooting Data Refinery.  
Data connections marked with a key icon () are locked. If you are authorized to access the data source, you are asked to enter your personal credentials the first time you select it. This one-time step permanently unlocks the connection for you. After you have unlocked the connection, the key icon is no longer displayed. See Adding connections to projects.  
Click Add to load the data into Data Refinery."
3256	Adding data to Data Refinery	"Next steps
Refine your data Validate your data Use visualizations to gain insights into your data  
Parent topic: Refining data"
3257		"Data Refinery supports the following categories of GUI operations.  
CLEANSE COMPUTE ORGANIZE NATURAL LANGUAGE  
Click New step to select a GUI operation.  
A subset of the operations is available from each column’s overflow menu (). You can rename a column by clicking the Edit icon in the column header."
3258		"CLEANSE
Convert column type When you open a file in Data Refinery, the Convert column type operation is automatically applied as the first step if it detects any nonstring data types in the data. Data types are automatically converted to inferred data types. To change the automatic conversion for a selected column, click the overflow menu () for the step and select Edit. As with any other operation, you can undo the step. The Convert column type operation is reapplied every time that you open the file in Data Refinery. Automatic conversion is applied as needed for file-based data sources only. (It does not apply to a data source from a database connection.)  
To confirm what data type each column's data was converted to, click Edit from the overflow menu () to view the data types. The information includes the format for date or timestamp data.  
If the data is converted to an Integer or to a Decimal data type, you can specify the decimal symbol and the thousands grouping symbol for all applicable columns. Strings that are converted to the Decimal data type use a dot for the decimal symbol and a comma for the thousands grouping symbol. Alternatively, you can select comma for the decimal symbol and dot or a custom symbol for the thousands grouping symbol. The decimal symbol and the thousands grouping symbol cannot be the same.  
The source data is read from left to right until a terminator or an unrecognized character is encountered. For example, if you are converting string data 12,834 to Decimal and you do not specify what to do with the comma (,), the data will be truncated to 12. Similarly, if the source data has multiple dots (.), and you select dot for the decimal symbol, the first dot is used as the decimal separator and the digits following the second dot are truncated. A source string of 1.834.230,000 is converted to a value of 1.834.  
The Convert column type operation automatically converts these date and timestamp formats:  
Date: ymd, ydm Timestamp: ymdHMS, ymdHM, ydmHMS, ydmHM  
Date and Timestamp strings must use four digits for the year.  
You can manually apply the Convert column type operation to change the data type of a column at any point in the Data Refinery flow. You can create a new column to hold the result of this operation or you can overwrite the existing column."
3259		"CLEANSE
You can manually apply the Convert column type operation to change the data type of a column at any point in the Data Refinery flow. You can create a new column to hold the result of this operation or you can overwrite the existing column.  
Tip: A column's data type determines the operations that you can use. Changing the data type can affect which operations are relevant for that column.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript  
The Convert column type operation automatically converted the first column from String to Integer. Let's change the data types of the other three columns. To change the data type of european column from string to decimal, select the column and then edit the Convert column type operation step. To change the data type of european column from string to decimal, select the column and then edit the Convert column type operation step. Select Decimal. The column uses the comma delimiter so select Comma (,) for the decimal symbol. Select the next column, DATETIME. Select Timestamp and a format. Click Apply. The columns are now Integer, Decimal, Date, and Timestamp data types The Convert column type step in the Steps panel is updated.  
Convert column value to missing Convert values in the selected column to missing values if they match values in the specified column or they match a specified value.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript"
3260		"CLEANSE
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript  
The Convert column value to missing operation converts the values in a selected column to missing values if they match the values in a specified column or if they match a specified value. A missing value is equivalent to an SQL NULL, which is a field with no value. It is different from a zero value or a value that contains spaces. You can use the Convert column value to missing operation when you think that the data would be better represented as missing values. For example, when you want to use missing values in a Replace missing values operation or in a Filter operation. Let's use the Convert column value to missing operation to change values to missing based on a matched value. Notice that the DESC column has many rows with the value CANCELLED ORDER. Let's convert the CANCELLED ORDER strings to missing values. The Convert column value to missing operation is under the CLEANSE category. Type the string to replace with missing values. The values that were formerly CANCELLED ORDER are now missing values.  
Extract date or time value Extract a selected portion of a date or time value from a column with a date or timestamp data type.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript  
The Extract date or time value operation extracts a selected portion of a date or time value from a column that is a date or timestamp data type. The DATE column is a String data type. First, let's use the Convert column type operation to convert it to the Date data type. Select the Convert column type operation from the DATE column's menu. Select Date. Select a Date format. The DATE column is now a date data type. The ISO Date format is used when the String data type was converted to the Date data type. For example, the string 01/08/2018 was converted to the date 2018-01-08. Now we can extract the year portion of the date into a new column. The Extract date or time value operation is under the CLEANSE category. Select Year for the portion of the date to extract, and type YEAR for the new column name. The year portion of the DATE column is in the new column, YEAR. The Steps panel displays the Extract date or time value operation."
3261		"CLEANSE
Filter Filter rows by the selected columns. Keep rows with the selected column values; filter out all other rows.  
For these string Filter operators, do not enclose the value in quotation marks. If the value contains quotation marks, escape them with a slash character. For example: \""text\"":  
Contains Does not contain Starts with Does not start with End with Does not end with  
Folowing are the operators for numeric, string, and Boolean (logical), and date and timestamp columns:  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript  
Use the Filter operation to filter rows by the selected columns. You can apply multiple conditions in one Filter operation. Use a regular expression to filter out all the rows except those where the string in the Emp ID column starts with 8. Filter the rows by two states abbreviations. Click Apply. Only the rows where Emp ID starts with 8 and State is AR or TX are in the table. The rows are now filtered by AR and PA. The Filter step in the Steps panel is updated.  
Remove column Remove the selected column.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript  
Use the Remove column operation to quickly remove a column from a data asset. The quickest way to remove a column is from the column's menu. The name of the removed column is in the Steps panel. Remove another column. The name of the removed column is in the Steps panel.  
Remove duplicates Remove rows with duplicate column values.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript  
The Remove duplicates operation removes rows that have duplicate column values. The data set has 43 rows. Many of the rows in the APPLYCODE column have duplicate values. We want to reduce the data set to the rows where each value in the APPLYCODE column occurs only once. Select the Remove duplicates operation from the APPLYCODE column's menu. The Remove duplicates operation removed each occurrence of a duplicate value starting from the top row. The data set is now 4 rows.  
Remove empty rows Remove rows that have a blank or missing value for the selected column.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript"
3262		"CLEANSE
Remove empty rows Remove rows that have a blank or missing value for the selected column.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript  
The Remove empty rows operation removes rows that have a blank or missing value for the selected column. A missing value is equivalent to an SQL NULL, which is a field with no value. It is different from a zero value or a value that contains spaces. The data set has 43 rows. Many of the rows in the TRACK column have missing values. We want to reduce the data set to the rows that have a value in the TRACK column. Select the Remove empty rows operation from the TRACK column's menu. The Remove empty rows operation removed each row that had a blank or missing value in the TRACK column. The data set is now 21 rows.  
Replace missing values Replace missing values in the column with a specified value or with the value from a specified column in the same row.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript  
The Replace missing values operation replaces missing values in a column with a specified value or with the value from a specified column in the same row. The STATE column has many rows with empty values. We want to replace those empty values with a string. The Replace missing values operation is under the CLEANSE category. For the State column, replace the missing values with the string Incomplete. The missing values now have the value Incomplete. The Steps panel displays the Replace missing values operation.  
Replace substring Replace the specified substring with the specified text.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript  
The Replace substring operation replaces a substring with text that you specify. The DECLINE column has many rows that include the string BANC. We want to replace this string with BANK. The Replace substring operation is under the CLEANSE category. Type the string to replace and the replacement string. All occurrences of the string BANC have been replaced with BANK. The Steps panel displays the Replace substring operation.  
Substitute Obscure sensitive information from view by substituting a random string of characters for the actual data in the selected column.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript"
3263		"CLEANSE
Substitute Obscure sensitive information from view by substituting a random string of characters for the actual data in the selected column.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript  
The Substitute operation obscures sensitive information by substituting a random string of characters for the data in the selected column. The quickest way to substitute the data in a column is to select Substitute from the column's menu. The Substitute operation shows in the Steps panel. Substitute values in another column. The second Substitute operation shows in the Steps panel."
3264		"CLEANSE
Text
You can apply text operations only to string columns. You can create a new column to hold the result of an operation or you can overwrite the existing column.  
Text > Collapse spaces Collapse multiple, consecutive spaces in the text to a single space.  
Text > Concatenate string Link together any string to the text. You can prepend the string to the text, append the string to the text, or both.  
Text > Lowercase Convert the text to lowercase.  
Text > Number of characters Return the number of characters in the text.  
Text > Pad characters Pad the text with the specified string. Specify whether to pad the text on the left, right, or both the left and right.  
Text > Substring Create substrings from the text that start at the specified position and have the specified length.  
Text > Title case Convert the text to title case.  
Text > Trim quotes Remove single or double quotation marks from the text.  
Text > Trim spaces Remove leading, trailing, and extra spaces from the text.  
Text > Uppercase Convert the text to uppercase.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript  
You can apply a Text operation to string columns. Create a new column for the result or overwrite the existing column. First, concatenate a string to the values in the WORD column. Available Text operations. Concatenate the string to the right side, append with a space, and type up. The values in the WORD column are appended with a space and the word up. The Text operation displays in the Steps panel. Next, pad the values in the ANIMAL column with a string. Pad the values in the ANIMAL column with ampersand (&) symbols to the right for a minimum of 7 characters. The values in the ANIMAL column are padded with the & symbol so that each string is at least seven characters. Notice that the opossum, pangolin, platypus, and hedgehog values do not have a padding character because those strings were already seven or more characters long. Next, use Substring to remove the t character from the ID column. Select Position 2 to start the new string at that position. Select Length 4 for a four-character length string. The initial t character in the ID column is removed in the NEW-ID column."
3265		"COMPUTE
Calculate Perform a calculation with another column or with a specified value. The operators are:  
Addition Division Exponentiation Is between two numbers Is equal to Is greater than Is greater than or equal to Is less than Is less than or equal to Is not equal to Modulus Multiplication Subtraction  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript  
The Calculate operation performs a calculation, such as addition or subtraction, with another column or with a specified value. Select the column to begin. Available calculations Now select the second column for the Addition calculation. And apply the change. The id column is updated, and the Steps panel shows the completed operation. You can also access the operations from the column's menu. This time, select Is between two numbers. Specify the range, and create a new column for the results. The new column displays in the table and the new calculate operation displays in the Steps panel. This time, select Is equal to to compare two columns, and create a new column for the results. The new column displays in the table and the new calculate operation displays in the Steps panel."
3266		"COMPUTE
Math
You can apply math operations only to numeric columns. You can create a new column to hold the result of an operation or you can overwrite the existing column.  
Math > Absolute value Get the absolute value of a number. Example: The absolute value of both 4 and -4 is 4.  
Math > Arc cosine Get the arc cosine of an angle.  
Math > Ceiling Get the nearest integer of greater value, also known as the ceiling of the number. Examples: The ceiling of 2.31 is 3. The ceiling of -2.31 is -2.  
Math > Exponent Get a number raised to the power of the column value.  
Math > Floor Get the nearest integer of lesser value, also known as the floor of the number. Example: The floor of 2.31 is 2. The floor of -2.31 is -3.  
Math > Round Get the whole number nearest to the column value. If the column value is a whole number, return it.  
Math > Square root Get the square root of the column value.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript  
Apply a Math operation to the values in a column. Create a new column for the results or overwrite the existing column. Available Math operations Apply Absolute value to the column's values. Create new column for results. The new column is added to the table, and the Math operation displays in the Steps panel. You can also access the operation from the column's menu. Apply Round to the ANGLE column's values. Create a new column for results. The new column is added to the table, and the new Math operation displays in the Steps panel."
3267		"ORGANIZE
Aggregate Apply summary calculations to the values of one or more columns. Each aggregation creates a new column. Optionally, select Group by columns to group the new column by another column that defines a characteristic of the group, for example, a department or an ID. You can group by multiple columns. You can combine multiple aggregations in a single operation.  
The available aggregate operations depend on the data type.  
Numeric data:  
Count unique values Minimum Maximum Sum Standard deviation Mean  
String data:  
Combine row values Count unique values  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript  
The Aggregate operation applies summary calculations to the values of one or more columns. Each aggregation creates a new column. Available aggregations depend on whether the data is numeric or string data. The available operators depend on the column's data type. Available operators for numeric data. With the UniqueCarrier text column selected, you can see the available operators for string data. We will count how many unique values are in the UniqueCarrier column. This aggregation will show how many airlines are in the data set. We have 22 airlines in the new Airlines column. The other columns are deleted. The Aggregate operation displays in the Steps panel. Let's start over to show an aggregation on numeric data. Show the average (mean value) of the arrival delays. The average value of all the arrival delays is in the new MeanArrDelay column. The other columns are deleted. You can also group the aggregated column by another column that defines a characteristic of the group. Let's edit the Aggregate step by adding a Group by selection so we can see the average of arrival delays by airline. Group the results by the UniqueCarrier column. The average arrival delays are now grouped by airline. The Steps panel displays the Aggregate operation.  
Concatenate Concatenate the values of two or more columns.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript"
3268		"ORGANIZE
Concatenate Concatenate the values of two or more columns.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript  
The Concatenate operation concatenates the values of two or more columns. The Concatenate operation is under the ORGANIZE category. Select the columns to concatenate. Select a separator to use between the concatenated values. Type a name for the column for the concatenated values. The new column can display as the right-most column in the data set, or next to the original column. Keep the original columns, and apply the changes. The new DATE column shows the concatenated values from the other three columns with a semicolon separator. The Concatenate operation displays in the Steps panel. The DATE column is a String data type. Let's use the Convert column type operation to convert it to the Date data type. Select the Convert column type operation from the DATE column's menu. Select Date. Select a date format and create a new column for the result. Place the new column next to the original column, and apply the changes. The new column displays with the converted date format. The Convert column type operation displays in the Steps panel. The ISO Date format is used when the String data type was converted to the Date data type. For example, the string 2004;2;3 was converted to the date 2004-02-03.  
Conditional replace Replace the values in a column based on conditions.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript"
3269		"ORGANIZE
Conditional replace Replace the values in a column based on conditions.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript  
Use the Conditional replace operation to replace the values in a column based on conditions. First, let's specify conditions to replace data in the CODE string column and create a new column for the results. Available condition operators for string data. Add the first condition - CONDITION 1: CODE Is equal to value C replace with COMPLETE. Add a second condition - CONDITION 2: CODE Is equal to value I replace with INCOMPLETE. Specify what to do with any values that do not meet the conditions. Here we will enter two double quotation marks to indicate an empty string. Create a new column for the results. The new column, STATUS, shows the conditional replacements from the CODE column. The Conditional replace operation shows in the Steps panel. Next, let's specify conditions to replace data in the INPUT integer column and create a new column for the results. Available condition operators for numeric data. Add the first condition - CONDITION 1: INPUT Is less than or equal to value 3 replace with value LOW. Add a second condition - CONDITION 2: INPUT Is in values 4,5,6 replace with value MED. Add a third condition - CONDITION 3: INPUT Is greater than or equal to value 7 replace with value HIGH. Specify what to do with any values that do not meet the conditions. Create a new column for the results. The new column, RATING, shows the conditional replacements from the INPUT column. The Conditional replace operation shows in the Steps panel.  
Join Combine data from two data sets based on a comparison of the values in specified key columns. Specify the type of join to perform, select the columns (join keys) in both data sets that you want to compare, and select the columns that you want in the resulting data set."
3270		"ORGANIZE
The join key columns in both data sets need to be compatible data types. If the Join operation is the first step that you add, check whether the Convert column type operation automatically converted the data type of the join key columns in the first data set when you opened the file in Data Refinery. Also, depending where the Join operation is in the Data Refinery flow, you can use the Convert column type operation to ensure that the join key columns' data types match. Click a previous step in Steps panel to see the snapshot view of the step.  
The join types include:  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript  
The customers.csv data set contains information about your company's customers, and the sales.csv data set contains information about your company's sales representatives. The data sets share the SALESREP_ID column. The customers.csv data set is open in Data Refinery. The Join operation can combine the data from these two data sets based on a comparison of the values in the SALESREP_ID column. You want to do an inner join to return only the rows in each data set that match in the other data set. You can add a custom suffix to append to columns that exist in both data sets to see the source data set for that column. Select the sales.csv data set to join with the customers.csv data set. For the join key, begin typing the column name to see a filtered list. The SALESREP_ID column links the two data sets. Next, select the columns to include. Duplicate columns will display the suffix appended. Now apply the changes. The Join operation displays in the Steps panel. Now, the data set is enriched with the columns from the customers.csv and sales.csv data sets.  
Rename column Rename the selected column.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript  
Use the Rename column operation to quickly rename a column. The fastest way to rename a column is to edit the column's name in the table. Edit the name and press Enter on your keyboard. The Rename column step shows the old name and the new name. Now rename another column. The Steps panel shows the BANKS column was renamed to DOGS. Now rename the last column. The Steps panel shows the RATIOS column was renamed to BIRDS."
3271		"ORGANIZE
Sample Generate a subset of your data by using one of the following methods. Sampling steps from UI operations apply only when the flow is run.  
Random sample: Each data record of the subset has an equal probability of being chosen. Stratified sample: Divide the data into one or more subgroups called strata. Then generate one random sample that contains data from each subgroup.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript"
3272		"ORGANIZE
The Sample operation generates a subset of your data. Use the Sample operation when you have a large amount of data and you want to work on a representative sample for faster prototyping. The Sample operation is in the ORGANIZE category. Choose one of two methods to create a sample. With a random sample, each row has an equal probability to be included in the sample data. You can choose a random sample by number of rows or by percentage of data. A stratified sample builds on a random sample. As with a random sample, you specify the amount of data in the sample (rows or percentage). With a stratified sample, you divide the data into one or more subgroups called strata. Then you generate one random sample that contains customized data from each subgroup. For Method, if you choose Auto, you select one column for the strata. If you choose Manual, you specify one or more strata and for each strata you specify filter conditions that define the rows in each strata. In this airline data example, we'll create two strata. One strata defines 50% of the output to have New York City destination airports and the second the strata defines the remaining 50% to have a specified flight distance. In Specify details for this strata box, enter the percentage of the sample that will represent the conditions that you will specify in this first strata. The strata percentages must total 100%. Available operators for string data. 50% of the sample will have New York City area destination airports. Click Save to save the first strata. The first strata, identified as Strata0, has one condition. In this strata, 50% of sample must meet the condition. In Specify details for this strata box, enter the percentage of the sample that will represent the conditions that you will specify in the second strata. Available operators for numeric data. 50% of the sample will be for flights with a distance greater than 500. Click Save to save the second strata. The second strata, identified as Strata1, has one condition. In this strata, 50% of the sample must meet the condition. If you use multiple strata, the Sample operation internally applies a Filter operation with an OR condition on the strata. Depending on the data, the conditions, and the size of the sample, the results of using one strata with multiple conditions might differ"
3273		"ORGANIZE
you use multiple strata, the Sample operation internally applies a Filter operation with an OR condition on the strata. Depending on the data, the conditions, and the size of the sample, the results of using one strata with multiple conditions might differ from using multiple strata. Unlike the other Data Refinery operations, the Sample operation changes the data set only after you create and run a job for the Data Refinery flow. The Sample step shows in the Steps panel. The data set is over 10000 rows. Save and create a job for the Data Refinery flow. The new asset file is added to the project for the output of the Data Refinery flow. View the output file. There are 10 rows (50% of the sample) with New York City airports in the Dest column, but 17 rows in the Distance column with values greater than 500. These results are because the strata were applied with an OR condition and there was overlapping data for the conditions specified in first strata where the rows that were filtered by Dest containing New York City airports had Distance values greater than 500. The output file in Data Refinery shows the reduced size."
3274		"ORGANIZE
Sort ascending Sort all the rows in the table by the selected column in ascending order.  
Sort descending Sort all the rows in the table by the selected column in descending order.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript  
Quickly sort all the rows in a data set by sorting the rows in a selected column. The fastest way to sort columns is from the column's menu. You can sort the rows in ascending or descending order. Sort ascending. The order of all the rows in the table is updated by the Sort operation of the first column. The Sort operation shows in the Steps panel. Sort descending. The order of all the rows in the table is changed by the Sort operation of the second column. The second Sort operation shows in the Steps panel. Sort ascending. The order of all the rows in the table is changed by the Sort operation of the third column. The third Sort operation shows in the Steps panel.  
Split column Split the column by non-alphanumeric characters, position, pattern, or text.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript"
3275		"ORGANIZE
Split column Split the column by non-alphanumeric characters, position, pattern, or text.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript  
The Split column operation splits one column into two or more columns based on non-alphanumeric characters, text, pattern, or position. To begin, let's split the YMD column into YEAR, MONTH, and DAY columns. The Split column operation is in the ORGANIZE category. First, select the YMD column to split. The tabs offer four choices for ways to split the column. DEFAULT uses any non-alphanumeric character that's in the column values to split the column. In TEXT, you select a character or enter text to split the column. In PATTERN, you enter a regular expression based on R syntax to determine where to split the column. In POSITION, you specify at what position to split the column. We want to split the YMD column by the asterisk (*), which is a non-alphanumeric character, so we'll select the DEFAULT tab. Split the YMD column into three new columns - YEAR, MONTH, and DAY. The three new columns, YEAR, MONTH, and DAY, are added to the data set. The Split column operation shows in the Steps panel. Next split the FLIGHT column into two columns - One for the airline code and one for the flight number. Because airline codes are two characters, we can split the column by position. Click the POSITION tab, and then type 2 in the Positions box. Split the FLIGHT column into two new columns - AIRLINE and FLTNMBR. The two new columns, AIRLINE and FLIGHTNBR, are added to the data set. The Split column operation shows in the Steps panel.  
Union Combine the rows from two data sets that share the same schema and filter out the duplicates. If you select Allow a different number of columns and allow duplicate values, the operation is a UNION ALL command.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript"
3276		"ORGANIZE
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript  
The Union operation combines the rows from two data sets that share the same schema. This data set has four columns and six rows. The data types from left to right are String, String, Decimal, String. When the data set was loaded into Data Refinery, the AUTOMATIC Convert column type operation automatically converted the PRICE column to the Decimal data type. The columns in the second data set must be compatible to the data types in this data set. Select the data set to combine with the current data set. When you preview the new data set, you see that it also has four columns. However, the PRICE column is a String data type. Before you apply the Union operation, you need to delete the AUTOMATIC Convert column type step so that the PRICE column is the same data type as the PRICE column in the new data set (String). The PRICE column is now string data. Now repeat the union operation. The new data set is added to the current data set. The data set is increased to 12 rows. The Union operation shows in the Steps panel. Now add a data set that has a different number of columns. The matching columns must still be compatible data types. Select the data set to combine with the current data set. When you preview the new data set, you see that it has one more column than the original data set. The fifth column is TYPE. Select Allow a different number of columns and allow duplicate values. Apply the Union operation. The new data set is added to the current data set. The data set is increased to 18 rows. The additional column, TYPE, is added to the data set. The Union operation shows in the Steps panel.  
Tip for the Union operation: If you receive an error about incompatible schemas, check if the automatic Convert column type operation changed the data types of the first data set. Delete the Convert column type step and try again."
3277		"NATURAL LANGUAGE
Remove stop words Remove common words of the English language, such as “the” or “and.” Stop words usually have little semantic value for text analytics algorithms and models. Remove the stop words to reduce the data volume and to improve the quality of the data that you use to train machine learning models.  
Optional: To confirm which words were removed, apply the Tokenize operation (by words) on the selected column, and then view the statistics for the words in the Profile tab. You can undo the Tokenize step later in the Data Refinery flow.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript  
The Remove stop words operation removes common words of the English language from the data set. Stop words usually have little semantic value for text analytics algorithms and models. Remove the stop words to reduce the data volume and to improve the data quality. The Remove stop words operation removes these words: a, an, and, are, as, at, be, but, by, for, from, if, in, into, is, it, no, not, of, on, or, such, that, the, their, then, there, these, they, this, to, was, will, with. The Remove stop words operation is under the NATURAL LANGUAGE category. Select the STRING column. Click Apply to remove the stop words. The stop words are removed from the STRING column. The Remove stop words operation shows in the Steps panel.  
Tokenize Break up English text into words, sentences, paragraphs, lines, characters, or by regular expression.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Video transcript  
The Tokenize operation breaks up English text into words, sentences, paragraphs, lines, characters, or by regular expression. The Tokenize operation is under the NATURAL LANGUAGE category. Select the STRING column. Available tokenize options. Create a new column with the name WORDS. The Tokenize operation has taken the words from the STRING column and created a new column, WORDS, with a row for each word. The Tokenize operation shows in the Steps panel.  
Parent topic: Refining data"
3278	Troubleshooting Data Refinery	"Use this information to resolve questions about using Data Refinery.  
Cannot refine data from an Excel data asset Data Refinery flow job fails with a large data asset"
3279	Troubleshooting Data Refinery	"Cannot refine data from an Excel data asset
The Data Refinery flow might fail if it cannot read the data. Confirm the format of the Excel file. By default, the first line of the file is treated as the header. You can change this setting in the Flow settings . Go to the Source data sets tab and click the overflow menu () next to the data source, and select Edit format. You can also specify the first line property, which designates which row is the first row in the data set to be read. Changing these properties affects how the data is displayed in Data Refinery as well as the Data Refinery job run and flow output."
3280	Troubleshooting Data Refinery	"Data Refinery flow job fails with a large data asset
If your Data Refinery flow job fails with a large data asset, try these troubleshooting tips to fix the problem:  
Instead of using a project data asset as the target of the Data Refinery flow (default), use Cloud storage. For example, IBM Cloud Object Storage, Amazon S3, or Google Cloud Storage. Select a Spark & R environment for the Data Refinery flow job or create a new Spark & R environment template."
3281	Validating your data in Data Refinery	"At any time after you've added data to Data Refinery, you can validate your data. Typically, you'll want to do this at multiple points in the refinement process.  
To validate your data:  
From Data Refinery, click the Profile tab.  
Review the metrics for each column.  
Take appropriate actions, as described in the following sections, depending on what you learn."
3282	Validating your data in Data Refinery	"Frequency
Frequency is the number of times that a value, or a value in a specified range, occurs. Each frequency distribution (bar) shows the count of unique values in a column.  
Review the frequency distribution to find anomalies in your data. If you want to cleanse your data of those anomalies, simply remove the values.  
For Integer and Date/Time columns, you can customize the number of bins (groupings) that you want to see. In the default multi-column view, the maximum is 20. If you expand the frequency chart row, the maximum is 50."
3283	Validating your data in Data Refinery	"Statistics
Statistics are a collection of quantitative data. The statistics for each column show the minimum, maximum, mean, and number of unique values in that column.  
Depending on a column's data type, the statistics for each column will vary slightly. For example, statistics for a column of data type integer have minimum, maximum, and mean values while statistics for a column of data type string have minimum length, maximum length, and mean length values.  
Parent topic: Refining data"
3284	IBM Cloud Databases for MySQL connection	"To access your data in IBM Cloud Databases for MySQL, create a connection asset for it.  
IBM Cloud Databases for MySQL extends the capabilities of MySQL by offering an auto-scaling deployment system managed on IBM Cloud that delivers high availability, redundancy, and automated backups. IBM Cloud Databases for MySQL was formerly known as IBM Cloud Compose for MySQL."
3285	IBM Cloud Databases for MySQL connection	"Create a connection to IBM Cloud Databases for MySQL
To create the connection asset, you need these connection details:  
Database name Hostname or IP address Port number Username and password"
3286	IBM Cloud Databases for MySQL connection	"Create a connection to IBM Cloud Databases for MySQL
Next step: Add data assets from the connection
See Add data from a connection in a project."
3287	IBM Cloud Databases for MySQL connection	"Where you can use this connection
You can use IBM Cloud Databases for MySQL connections in the following workspaces and tools:  
Projects  
SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3288	IBM Cloud Databases for MySQL connection	"IBM Cloud Databases for MySQL setup
IBM Cloud Databases for MySQL"
3289	IBM Cloud Databases for MySQL connection	"Restriction
For SPSS Modeler, you can use this connection only to import data. You cannot export data to this connection or to an IBM Cloud Databases for MySQL connected data asset."
3290	IBM Cloud Databases for MySQL connection	"Learn more
IBM Cloud Databases for MySQL Help  
Parent topic: Supported connections"
3291	IBM Cloud Object Storage (infrastructure) connection	"To access your data in IBM Cloud Object Storage (infrastructure), create a connection asset for it.  
The Cloud Object Storage (infrastructure) connection is for object storage that was formerly on SoftLayer. SoftLayer was replaced by IBM Cloud. You cannot provision a new instance for Cloud Object Storage (infrastructure). This connection is for users who set up an earlier instance on SoftLayer."
3292	IBM Cloud Object Storage (infrastructure) connection	"Create a connection to Cloud Object Storage (infrastructure)
To create the connection asset, you need this information."
3293	IBM Cloud Object Storage (infrastructure) connection	"Create a connection to Cloud Object Storage (infrastructure)
Required connection values
The Login URL is required, plus one of the following values for authentication:  
Access Key and Secret Key Credentials If you plan to use the S3 API, you must enter an Access Key."
3294	IBM Cloud Object Storage (infrastructure) connection	"Create a connection to Cloud Object Storage (infrastructure)
Connection values in the Cloud Object Storage Resource list
The values for these fields are found in the Cloud Object Storage Resource list.  
To find the Login URL:  
Go to the Cloud Object Storage Resource list at https://cloud.ibm.com/resources. Expand the Storage resource. Click the Cloud Object Storage service. From the menu, select Endpoints. Copy the value of the public endpoint that is in the same region as the bucket that you want to use.  
To find the values for Access key and the Secret Key:  
Go to the Cloud Object Storage Resource list at https://cloud.ibm.com/resources. Expand the Storage resource. Click the Cloud Object Storage service, and then click the Service credentials tab. Expand the Key name that you want to use. Copy the values without the quotation marks:  
Access Key: access_key_id  
Secret Key: secret_access_key  
Note: Alternatively, you can use the contents of the JSON file in Credentials to copy the values for the Access Key and Secret Key.  
To find the Credentials:  
Go to the Cloud Object Storage Resource list at https://cloud.ibm.com/resources. Expand the Storage resource. Click the Cloud Object Storage service, and then click the Service credentials tab. Expand the Key name that you want to use. Copy the entire JSON file. Include the opening and closing braces { } symbols.  
For Certificates (Optional) Enter the self-signed SSL certificate that was created by a tool such as OpenSSL."
3295	IBM Cloud Object Storage (infrastructure) connection	"Create a connection to Cloud Object Storage (infrastructure)
Next step: Add data assets from the connection
See Add data from a connection in a project."
3296	IBM Cloud Object Storage (infrastructure) connection	"Where you can use this connection
Projects  
Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3297	IBM Cloud Object Storage (infrastructure) connection	"Supported file types
The Cloud Object Storage (infrastructure) connection supports these file types:  Avro, CSV, Delimited text, Excel, JSON, ORC, Parquet, SAS, SAV, SHP, and XML."
3298	IBM Cloud Object Storage (infrastructure) connection	"Learn more
Related connection: IBM Cloud Object Storage connection  
Parent topic: Supported connections"
3299	Controlling access to Cloud Object Storage buckets	"A bucket is a logical abstraction that provides a container for data. Buckets in Cloud Object Storage are created in IBM Cloud. Within a Cloud Object Storage instance, you can use policies to restrict users' access to buckets.  
Here's how it works:  
In this illustration, two credentials are associated with a Cloud Object Storage instance. Each of the credentials references an IAM service ID in which policies are defined to control which bucket that service ID can access. By using a specific credential when you add a Cloud Object Storage connection to a project, only the buckets accessible to the service ID associated with that credential are visible.  
To create connections that restrict users' access to buckets, follow these steps.  
First, in IBM Cloud:  
Create a Cloud Object Storage instance and several buckets  
Create a service credential and Service ID for each combination of buckets that you want users to be able to access  
Verify that the service IDs were created  
Edit the policies of each service ID to provide access to the appropriate buckets  
Copy values from each of the service credentials that you created  
Copy the endpoint  
Then, in your project:  
Add Cloud Object Storage connections that use the service credentials that you created  
Test users' access to buckets"
3300	Controlling access to Cloud Object Storage buckets	"Step 1: Create a Cloud Object Storage instance and several buckets
From the IBM Cloud catalog, search for Object Storage, then create a Cloud Object Storage instance.  
Select Buckets in the navigation pane.  
Create as many buckets as you need.  
For example, create three buckets: dept1-bucket, dept2-bucket, and dept3-bucket."
3301	Controlling access to Cloud Object Storage buckets	"Step 2: Create a service credential and Service ID for each combination of buckets that you want users to be able to access
Select Service credentials in the navigation pane.  
Click New Credential.  
In the Add new credential dialog, provide a name for the credential and select the appropriate access role.  
Within the Select Service ID field, click Create New Service ID.  
Enter a name for the new service ID. We recommend using the same or a similar name to that of the credential for easy identification.  
Click Add.  
Repeat steps 2 to 6 for each credential that you want to create.  
For example, create three credentials: cos-all-access, dept1-dept2-buckets-only, and dept2-dept3-buckets-only."
3302	Controlling access to Cloud Object Storage buckets	"Step 3: Verify that the service IDs were created
In the IBM Cloud page header, click Manage > Access (IAM).  
Select Service IDs in the navigation pane.  
Confirm that the service IDs you created in steps 2d and 2e are visible."
3303	Controlling access to Cloud Object Storage buckets	"Step 4: Edit the policies of each service ID to provide access to the appropriate buckets
Open each service ID in turn.  
On the Access policies tab, select Edit from the Actions menu to view the policy.  
If necessary, edit the policy to provide access to the appropriate buckets.  
If needed, create one or more new policies.  
Remove the existing, default policy which provides access to all of the buckets in the Cloud Object Storage instance.  
Click Assign access.  
For Resource type, specify ""bucket"".  
For Resource ID, specify a bucket name.  
In the Select roles section, select Viewer from the ""Assign platform access roles"" list and select Writer from the ""Assign service access roles"" list."
3304	Controlling access to Cloud Object Storage buckets	"Step 4: Edit the policies of each service ID to provide access to the appropriate buckets
Example 1
By default, the policy for the cos-all-access service ID provides Writer access to the Cloud Object Storage instance.  
Because you want this service ID and the corresponding credential to provide users with access to all of the buckets, no edits are required."
3305	Controlling access to Cloud Object Storage buckets	"Step 4: Edit the policies of each service ID to provide access to the appropriate buckets
Example 2
By default, the policy for the ""dept1-dept2-buckets-only"" service ID provides Writer access to the Cloud Object Storage instance. Because you want this service ID and the corresponding credential to provide users with access only to the dept1-bucket and dept2-bucket buckets, remove the default policy and create two access policies, one for dept1-bucket and one for dept2-bucket."
3306	Controlling access to Cloud Object Storage buckets	"Step 5: Copy values from each of the service credentials that you created
Return to your IBM Cloud Dashboard and select Cloud Object Storage from the Storage list.  
Select Service credentials in the navigation pane.  
Click the View credentials action for one of the service IDs that you created in step 2.  
Copy the ""apikey"" value and the ""resource_instance_id"" value to a temporary location, such as a desktop note.  
Repeat steps 3 and 4 for each credential."
3307	Controlling access to Cloud Object Storage buckets	"Step 6: Copy the Endpoint
Select Endpoint in the navigation pane.  
Copy the URL of the endpoint that you want to connect to. Save the value to a temporary location, such as a desktop note."
3308	Controlling access to Cloud Object Storage buckets	"Step 7: Add Cloud Object Storage connections that use the service credentials that you created
Return to your project on the Assets tab, and click New asset > Connect to a data source..  
On the New connection page, click Cloud Object Storage.  
Name the new connection and enter the login URL (from the Endpoints page) as well as the ""apikey"" and ""resource_instance_id"" values that you copied in step 5 from one of the service credentials.  
Repeat steps 3 to 5 for each service credential.  
The connections will be visible in the Data assets section of the project."
3309	Controlling access to Cloud Object Storage buckets	"Test users' access to buckets
Going forward, when you add a data asset from a Cloud Object Storage connection to a project, you'll see only the buckets that the policies allow you to access. To test this:  
From a project, click New asset > Connected data. Or from a catalog, click Add to project > Connected data.  
In the Connection source section, click Select source.  
On the Select connection source page, you can see the Cloud Object Storage connections that you created.  
Select one of the Cloud Object Storage connections to see that only the buckets accessible to the service ID associated with that bucket's credential are visible.  
Parent topic: Adding connections to projects"
3310	Creating and managing jobs in a project	"You create jobs to run assets or files in tools, such as Data Refinery flows, SPSS Modeler flows, Notebooks, and scripts, in a project.  
When you create a job you define the properties for the job, such as the name, definition, environment runtime, schedule and notification specifications on different pages. You can run a job immediately or wait for the job to run at the next scheduled interval.  
Each time a job is started, a job run is created, which you can monitor and use to compare with the job run history of previous runs. You can view detailed information about each job run, job state changes, and job failures in the job run log.  
How you create a job depends on the asset or file."
3311	Creating and managing jobs in a project	"Creating jobs from the Assets page
You can create a job to run an asset from the project's Assets page.  
Restriction:  
You cannot run a job by using an API key from a service ID.  
To create jobs for a listed asset from the Assets page of a project:  
An API key is generated when you create a scheduled job, and future runs will use this API key. If you didn't create a scheduled job but choose to modify one, an API key is generated for you when you modify the job and future runs will use this API key.  
Select the asset from the section for your asset type and choose New job from the menu icon with the lists of options () at the end of the table row.  
Define the job details by entering a name and a description (optional).  
If you can select Setting, specify the settings that you want for the job.  
If you can select Configure, choose an environment runtime for the job. Depending on the asset type, you can optionally configure more settings, for example environment variables or script arguments.  
To avoid accumulating too many finished job runs and job run artifacts, set how long to retain finished job runs and job run artifacts like logs or notebook results. You can either select the number of days to retain the job runs or the last number of job runs to keep.  
On the Schedule page, you can optionally add a one-time or repeating schedule.  
If you select the Repeat option and unit of Minutes with the value of n, the job runs at the start of the hour, and then at every multiple of n. For example, if you specify a value of 11 it will run at 0, 11, 22, 33, 44 and 55 minutes of each hour.  
If you also select the Start of Schedule option, the job starts to run at the first multiple of n of the hour that occurs after the time that you provide in the Start Time field. For example, if you enter 10:24 for the Start of Time value, and you select Repeat and set the job to repeat every 14 minutes, then your job will run at 10:42, 10:56, 11:00, 11:14. 11:28, 11:42, 11:56, and so on."
3312	Creating and managing jobs in a project	"Creating jobs from the Assets page
You can't change the time zone; you must set your job schedule in your web browser's time zone. The schedule will be translated to the time zone of the compute node where your job runs.  
If you exclude certain weekdays, the job might not run as you would expect. The reason might be due to a discrepancy between the time zone of the user who creates the schedule, and the time zone of the compute node where the job runs.  
Note: Your scheduled job can appear differently if your web browser’s time zone is set to your local time zone following Daylight Savings Time (DST). For example, your scheduled job appears at 3:00PM Eastern Standard Time Zone (EST) daily corresponding to 8:00PM Coordinated Universal Time (UTC). When your local time zone changes to Eastern Daylight Time Zone (EDT), your scheduled job continues to run at 8:00PM (UTC) which will now appear as 4:00PM (EDT) daily.  
(Optional): Select to see notifications for the job. You can select the type of alerts to receive.  
Review the job settings. Then, create the job and run it immediately, or create the job and run it later."
3313	Creating and managing jobs in a project	"Managing jobs
You can view all of the jobs that exist for your project from the project's Jobs page. With Admin or Editor role for the project, you can view and edit the job details. You can run jobs manually and you can delete jobs. With Viewer role for the project, you can only view the job details. You can't run or delete jobs with Viewer role.  
To view the details of a specific job, click the job. From the job's details page, you can:  
View the runs for that job and the status of each run. If a run failed, you can select the run and view the log tail or download the entire log file to help you troubleshoot the run. A failed run might be related to a temporary connection or environment problem. Try running the job again. If the job still fails, you can send the log to Customer Support. Edit job settings by clicking Edit job, for example to change schedule settings or to pick another environment template. Run the job manually by clicking the Run icon from the job's action bar. You can start a scheduled job based on the schedule and on demand. Delete the job by clicking the Delete from the job's action bar."
3314	Creating and managing jobs in a project	"Managing jobs
Viewing and editing jobs in a tool
You can view and edit job settings associated with an asset directly in the following tools:  
Data Refinery DataStage Match 360 Notebook editor or viewer Pipelines  
In the tool, click the Jobs icon from the toolbar and select Save and view jobs. This action lists the jobs that exist for the asset. Select a job to see its details. You can change job settings by clicking Edit job."
3315	Creating and managing jobs in a project	"Learn more
Creating jobs in Data Refinery Creating jobs in the Notebook editor or Notebook viewer Creating jobs for Pipelines  
Parent topic: Working in projects"
3316	Amazon RDS for MySQL connection	"To access your data in Amazon RDS for MySQL, create a connection asset for it.  
Amazon RDS for MySQL is a MySQL relational database that runs on the Amazon Relational Database Service (RDS)."
3317	Amazon RDS for MySQL connection	"Supported versions
MySQL database versions 5.6 through 8.0"
3318	Amazon RDS for MySQL connection	"Create a connection to Amazon RDS for MySQL
To create the connection asset, you need these connection details:  
Database name Hostname or IP address Port number Username and password SSL certificate (if required by the database server)"
3319	Amazon RDS for MySQL connection	"Create a connection to Amazon RDS for MySQL
Next step: Add data assets from the connection
See Add data from a connection in a project."
3320	Amazon RDS for MySQL connection	"Where you can use this connection
You can use Amazon RDS for MySQL connections in the following workspaces and tools:  
Projects  
Data Refinery Notebooks. Click Read data on the Code snippets pane to get the connection credentials and load the data into a data structure. See Load data from data source connections. Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3321	Amazon RDS for MySQL connection	"Amazon RDS for MySQL setup
For setup instructions, see these topics:  
Creating an Amazon RDS DB Instance Connecting to a DB Instance Running the MySQL Database Engine"
3322	Amazon RDS for MySQL connection	"Amazon RDS for MySQL setup
Running SQL statements
To ensure that your SQL statements run correctly, refer to the Amazon RDS for MySQL documentation for the correct syntax."
3323	Amazon RDS for MySQL connection	"Learn more
Amazon RDS for MySQL  
Parent topic: Supported connections"
3324	Tableau connection	"To access your data in Tableau, you must create a connection asset for it.  
Tableau is an interactive data visualization platform."
3325	Tableau connection	"Supported products
Tableau Server 2020.3.3 and Tableau Cloud"
3326	Tableau connection	"Create a connection to Tableau
To create the connection asset, you need the following connection details:  
Hostname or IP address Port number Site: The name of the Tableau site to use For Authentication method, you need either a username and password or an Access token (with Access token name and Access token secret). SSL certificate (if required by the database server)"
3327	Tableau connection	"Create a connection to Tableau
Next step: Add data assets from the connection
See Add data from a connection in a project."
3328	Tableau connection	"Where you can use this connection
You can use Tableau connections in the following workspaces and tools:  
Projects  
SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3329	Tableau connection	"Tableau setup
Get Started with Tableau Server on Linux Get Started with Tableau Server on Windows Get Started with Tableau Cloud"
3330	Tableau connection	"Restriction
You can use this connection only for source data. You cannot write to data or export data with this connection."
3331	Tableau connection	"Restriction
Running SQL statements
To ensure that your SQL statements run correctly, refer to the Run Initial SQL for the correct syntax."
3332	Tableau connection	"Learn more
Tableau SSL for Tableau Server on Linux SSL for Tableau Server on Windows Security in Tableau Cloud  
Parent topic: Supported connections"
3333	Box connection	"To access your data in Box, create a connection asset for it.  
The Box platform is a cloud content management and file sharing service."
3334	Box connection	"Prerequisite: Create a custom app in Box
Before you create a connection to Box, you create a custom app in the Box Developer Console. You can create an app for application-level access that users can use to share files, or you can create an app for enterprise-wide access to all user accounts. With enterprise-wide access, users do not need to share files and folders with the application.  
Go to the Box Developer Console, and follow the wizard to create a Custom App. For the Authentication Method, select OAuth 2.0 with JWT (Server Authentication). Make the following selections in the Configuration page. Otherwise, keep the default settings. If you selected an App + Enterprise Access, under Advanced Features, select both of these check boxes: Submit the app client ID to the Box enterprise administrator for authorization: Go to your application in the Box Developer Console and select the General link from the left sidebar in your application. Scroll down to the App Authorization section.  
Select one of two choices for App Access Level: Under Add and Manage Public Keys, click Generate a Public/Private Keypair. This selection requires that two-factor authentication is enabled on the Box account, but you can disable it afterward. The generated key pair produces a config (*_config.json) file for you to download. You will need the information in this file to create the connection in your project.  
Keep the default App Access Only selection to allow access where users share files. Select App + Enterprise Access to create an app with enterprise-wide access to all user accounts.  
Make API calls using the as-user header Generate user access tokens"
3335	Box connection	"Prerequisite: Create a custom app in Box
Create the Box connection
Enter the values from the downloaded config file for these settings:  
Client ID Client Secret Enterprise ID Private Key (Replace each \n with a newline) Private Key Password (The passphrase value in the config file) Public Key (The publicKeyID value in the config file)"
3336	Box connection	"Prerequisite: Create a custom app in Box
Enterprise-wide app
If you configured an enterprise-wide access app, enter the username of the Box user account in the Username field."
3337	Box connection	"Prerequisite: Create a custom app in Box
Application-level app
Users must explicitly share their files with the app's email address in order for the app to access the files.  
{} { ""login_name"": ""AutomationUser_123467_aBcDEFg12h@boxdevedition.com"" }  
Make a REST call to the connection to find out the app email address. For example:  
PUT https://api.dataplatform.cloud.ibm.com/v2/connections/{connection_id}/actions/get_user_info?project_id={project_id}  
Request body:  
Returns:  
Share the files and folders in Box that you want accessible from Watson Studio with the login name that was returned by the REST call."
3338	Box connection	"Prerequisite: Create a custom app in Box
Next step: Add data assets from the connection
See Add data from a connection in a project."
3339	Box connection	"Where you can use this connection
You can use the Box connection in the following workspaces and tools:  
Projects  
Data Refinery Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3340	Box connection	"Limitation
If you have thousands of files in a Box folder, the connection might not be able to retrieve the files before a time-out. Jobs or profiling that use the Box files might not work.  
Workaround: Reorganize the file hierarchy in Box so that there are fewer files in the same folder."
3341	Box connection	"Supported file types
The Box connection supports these file types:  Avro, CSV, Delimited text, Excel, JSON, ORC, Parquet, SAS, SAV, SHP, and XML."
3342	Box connection	"Learn more
Managing custom apps  
Parent topic: Supported connections"
3343	Salesforce.com connection	"To access your data in Salesforce.com, create a connection asset for it.  
Salesforce.com is a cloud-based software company which provides customer relationship management (CRM). The Salesforce.com connection supports the standard SQL query language to select, insert, update, and delete data from Salesforce.com products and other supported products that use the Salesforce API."
3344	Salesforce.com connection	"Other supported products that use the Salesforce API
Salesforce AppExchange FinancialForce Service Cloud ServiceMax Veeva CRM"
3345	Salesforce.com connection	"Create a connection to Salesforce.com
To create the connection asset, you need these connection details:  
The username to access the Salesforce.com server. The password and security token to access the Salesforce.com server. In the Password field, append your security token to the end of your password. For example, MypasswordMyAccessToken. For information about access tokens, see Reset Your Security Token. The Salesforce.com server name. The default is login.salesforce.com."
3346	Salesforce.com connection	"Create a connection to Salesforce.com
Next step: Add data assets from the connection
See Add data from a connection in a project."
3347	Salesforce.com connection	"Where you can use this connection
You can use Salesforce.com connections in the following workspaces and tools:  
Projects  
SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3348	Salesforce.com connection	"Restriction
You can only use this connection for source data. You cannot write to data or export data with this connection."
3349	Salesforce.com connection	"Known issue
The following objects in the SFORCE schema are not supported: APPTABMEMBER, CONTENTDOCUMENTLINK, CONTENTFOLDERITEM, CONTENTFOLDERMEMBER, DATACLOUDADDRESS, DATACLOUDCOMPANY, DATACLOUDCONTACT, DATACLOUDANDBCOMPANY, DATASTATISTICS, ENTITYPARTICLE, EVENTBUSSUBSCRIBER, FIELDDEFINITION, FLEXQUEUEITEM, ICONDEFINITION, IDEACOMMENT, LISTVIEWCHARINSTANCE, LOGINEVENT, OUTGOINGEMAIL, OUTGOINGEMAILRELATION, OWNERCHANGEOPTIONINFO, PICKLISTVALUEINFO, PLATFORMACTION, RECORDACTIONHISTORY, RELATIONSHIPDOMAIN, RELATIONSHIPINFO, SEARCHLAYOUT, SITEDETAIL, USERAPPMENUITEM, USERENTITYACCESS, USERFIELDACCESS, USERRECORDACCESS, VOTE."
3350	Salesforce.com connection	"Learn more
Get Started with Salesforce Salesforce editions with API access  
Parent topic: Supported connections"
3351	Adding data from a connection to a project	"A connected data asset is a pointer to data that is accessed through a connection to an external data source. You create a connected data asset by specifying a connection, any intermediate structures or paths, and a relational table or view, a set of partitioned data files, or a file. When you access a connected data asset, the data is dynamically retrieved from the data source.  
You can also add a connected folder asset that is accessed through a connection in the same way. See Adding a connected folder asset to a project.  
Partitioned data assets have previews and profiles like relational tables. However, you cannot yet shape and cleanse partitioned data assets with the Data Refinery tool.  
To add a data asset from a connection to a project:  
From the project page, click the Assets tab, and then click Import assets > Connected data.  
Select an existing connection asset as the source of the data. If you don't have any connection assets, cancel and go to New asset > Connect to a data source, and create a connection asset.  
Select the data you want. You can select multiple connected data assets from the same connection. Click Import. For partitioned data, select the folder that contains the files. If the files are recognized as partitioned data, you see the message This folder contains a partitioned data set.  
Type a name and description.  
Click Create. The asset appears on the project Assets page.  
When you click on the asset name, you can see this information about connected assets:  
The asset name and description The tags for the asset The name of the person who created the asset The size of the data The date when the asset was added to the project The date when the asset was last modified A preview of relational data A profile of relational data  
Watch this video to see how to create a connection and add connected data to a project.  
Video disclaimer: Some minor steps and graphical elements in this video might differ from your platform.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Transcript  
Synchronize transcript with video"
3352	Adding data from a connection to a project	"Video disclaimer: Some minor steps and graphical elements in this video might differ from your platform.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Transcript  
Synchronize transcript with video  
Video transcript Time Transcript 00:00 This video shows you how to set up a connection to a data source and add connected data to a Watson Studio project. 00:08 If you have data stored in a data source, you can set up a connection to that data source from any project. 00:16 From here, you can add different elements to the project. 00:20 In this case, you want to add a connection. 00:24 You can create a new connection to an IBM service, such as IBM Db2 and Cloud Object Storage, or to a service from third parties, such as Amazon, Microsoft or Apache. 00:39 And you can filter the list based on compatible services. 00:45 You can also add a connection that was created at the platform level, which can be used across projects and catalogs. 00:54 Or you can create a connection to one of your provisioned IBM Cloud services. 00:59 In this case, select the provisioned IBM Cloud service for Db2 Warehouse on Cloud. 01:08 If the credentials are not prepopulated, you can get the credentials for the instance from the IBM Cloud service launch page. 01:17 First, test the connection and then create the connection. 01:25 The new connection now displays in the list of data assets. 01:30 Next, add connected data assets to this project. 01:37 Select the source - in this case, it's the Db2 Warehouse on Cloud connection just created. 01:43 Then select the schema and table. 01:50 You can see that this will add a reference to the data within this connection and include it in the target project. 01:58 Provide a name and a description and click ""Create"". 02:06 The data now displays in the list of data assets. 02:09 Open the data set to get a preview; and from here you can move directly into refining the data. 02:17 Find more videos in the Cloud Pak for Data as a Service documentation."
3353	Adding data from a connection to a project	"Next steps
Refine the data Analyze the data or build models"
3354	Adding data from a connection to a project	"Learn more
Connected folder assets Connection assets  
Parent topic:  
Adding data to a project"
3355	Adding platform connections	"You can add connections to the Platform assets catalog to share them across your organization. All collaborators in the Platform assets catalog can see the connections in the catalog. However, only users with the credentials for the data source can use a platform connection in a project to create a connected data asset.  
Editor Admin  
If you're not a collaborator in the Platform assets catalog, ask someone who is a collaborator to add you or tell you who has the Admin role in the catalog. You create connections to these types of data sources:  
IBM Cloud services Other cloud services On-premises databases  
See Connectors for a full list of data sources. Watch this video to see how to add platform connections.  
Video disclaimer: Some minor steps and graphical elements in this video might differ from your platform.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
To create a platform connection:  
From the main menu, choose Data > Platform connections. Click New connection. Choose a data source. If necessary, enter the connection information required for your data source. Typically, you need to provide information like the host, port number, username, and password. If prompted, specify whether you want to use personal or shared credentials. You cannot change this option after you create the connection. The credentials type for the connection, either Personal or Shared, is set by the account owner on the Account page. The default setting is Shared. To connect to a database that is not externalized to the internet (for example, behind a firewall), see Connecting to data behind a firewall. Click Create. The connection appears on the Connections page. You can edit the connection by clicking the connection name.  
Personal: With personal credentials, each user must specify their own credentials to access the connection. Each user's credentials are saved but are not shared with any other users. Use personal credentials instead of shared credentials to protect credentials. For example, if you use personal credentials and another user changes the connection properties (such as the hostname or port number), the credentials are invalidated to prevent malicious redirection. Shared: With shared credentials, all users access the connection with the credentials that you provide.  
Alternatively, you can create a connection in a project and then publish it to the Platform assets catalog.  
To publish a connection from a project to the Platform assets catalog:"
3356	Adding platform connections	"Alternatively, you can create a connection in a project and then publish it to the Platform assets catalog.  
To publish a connection from a project to the Platform assets catalog:  
Locate the connection in the project's Assets tab in the Data assets section. From the Actions menu , select Publish to catalog. Select Platform assets catalog and click Publish."
3357	Adding platform connections	"Next step
Add a connection to a project"
3358	Adding platform connections	"Learn more
Connectors Set the credentials for connections  
Parent topic: Preparing data"
3359	Leaving a project	You can leave a project from within the project or from the Projects page.
3360	Leaving a project	"Restrictions
If you are the only collaborator in the project with the Admin role, you must assign the Admin role to another collaborator before you can leave the project."
3361	Leaving a project	"Leaving a project from within the project
To leave a project from within the project:  
Open the project. On the Manage tab, go to the General page. In the Danger zone section, click Leave project. Click Leave."
3362	Leaving a project	"Leaving multiple projects
To leave one or more projects from the Projects page:  
Select View all projects from the navigation menu. Select one or more projects to leave. Click Leave. Click Leave to confirm.  
Parent topic: Projects"
3363	SingleStoreDB connection	"To access your data in SingleStoreDB, create a connection asset for it.  
SingleStoreDB is a fast, distributed, and highly scalable cloud-based SQL database. You can use SingleStoreDB to power real-time and data-intensive applications.  
Use SingleStoreDB and watsonx.ai for generative AI applications. Benefits include semantic search, fast ingest, and low-latency response times for foundation models and traditional machine learning."
3364	SingleStoreDB connection	"Create a connection to SingleStoreDB
To create the connection asset, you need these connection details:  
Database name Hostname or IP address Port number Username and password SSL certificate (if required by the database server)"
3365	SingleStoreDB connection	"Create a connection to SingleStoreDB
Next step: Add data assets from the connection
See Add data from a connection in a project."
3366	SingleStoreDB connection	"Where you can use this connection
You can use the SingleStoreDB connection in the following workspaces and tools:  
Projects  
Data Refinery Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3367	SingleStoreDB connection	"SingleStoreDB setup
To set up SingleStoreDB, see Getting Started with SingleStoreDB Cloud."
3368	SingleStoreDB connection	"SingleStoreDB setup
Running SQL statements
To ensure that your SQL statements run correctly, refer to the SingleStore Docs SQL Reference for the correct syntax."
3369	SingleStoreDB connection	"Learn more
SingleStoreDB Cloud SingleStoreDB with IBM for information about the IBM partnership with SingleStoreDB that provides a single source of procurement, support, and security.  
Parent topic: Supported connections"
3370	Oracle connection	"To access your data in Oracle, create a connection asset for it.  
Oracle is a multi-model database management system."
3371	Oracle connection	"Supported versions
Oracle Database 19c and 21c"
3372	Oracle connection	"Create a connection to Oracle
To create the connection asset, you need the following connection details:  
Service name or Database (SID)  
Hostname or IP address  
Port number  
SSL certificate (if required by the database server)  
Alternate servers: A list of alternate database servers to use for failover for new or lost connections. Syntax: (servername1[:port1][;property=value[;...]][,servername2[:port2][;property=value[;...]]]...) The server name (servername1, servername2, and so on) is required for each alternate server entry. The port number (port1, port2, and so on) and the connection properties (property=value) are optional for each alternate server entry. If the port is unspecified, the port number of the primary server is used. If the port number of the primary server is not specified, the default port number 1521 is used. The optional connection properties are the ServiceName and SID.  
Metadata discovery: The setting determines whether comments on columns (remarks) and aliases for schema objects such as tables or views (synonyms) are retrieved when assets are added by using this connection.  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3373	Oracle connection	"Create a connection to Oracle
Next step: Add data assets from the connection
See Add data from a connection in a project."
3374	Oracle connection	"Where you can use this connection
You can use Oracle connections in the following workspaces and tools:  
Projects  
Data Refinery Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3375	Oracle connection	"Oracle setup
Oracle installation"
3376	Oracle connection	"Oracle setup
Running SQL statements
To ensure that your SQL statements run correctly, refer to the Oracle Supported SQL Syntax and Functions for the correct syntax."
3377	Oracle connection	"Learn more
Oracle product documentation  
Parent topic: Supported connections"
3378	SAP IQ connection	"To access your data in SAP IQ, create a connection asset for it.  
SAP IQ is a column-based, petabyte scale, relational database software system used for business intelligence, data warehousing, and data marts. SAP IQ was formerly Sybase IQ."
3379	SAP IQ connection	"Create a connection to SAP IQ
To create the connection asset, you need these connection details:  
Database name Hostname or IP address Port number Username and password SSL certificate (if required by the database server)  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3380	SAP IQ connection	"Create a connection to SAP IQ
Next step: Add data assets from the connection
See Add data from a connection in a project."
3381	SAP IQ connection	"Where you can use this connection
You can use SAP IQ connections in the following workspaces and tools:  
Projects  
SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3382	SAP IQ connection	"SAP IQ setup
Get Started with SAP IQ"
3383	SAP IQ connection	"Restriction
You can use this connection only for source data. You cannot write to data or export data with this connection."
3384	SAP IQ connection	"Restriction
Running SQL statements
To ensure that your SQL statements run correctly, refer to the SAP IQ SQL Reference for the correct syntax."
3385	SAP IQ connection	"Learn more
SAP IQ technical information  
Parent topic: Supported connections"
3386	SAP OData connection	"To access your data in SAP OData, create a connection asset for it.  
Use the SAP OData connection to extract data from a SAP system through its exposed OData services."
3387	SAP OData connection	"Supported SAP OData products
The SAP OData connection is supported on SAP products that support the OData protocol version 2. Example products are S4/HANA (on premises or cloud), ERP, and CRM."
3388	SAP OData connection	"Create a connection to SAP OData
To create the connection asset, you need these connection details:  
Credentials type:  
API Key Basic None  
Encryption: SSL certificate (if required by the database server)  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3389	SAP OData connection	"Create a connection to SAP OData
Next step: Add data assets from the connection
See Add data from a connection in a project."
3390	SAP OData connection	"Where you can use this connection
You can use the SAP OData connection in the following workspaces and tools:  
Projects  
Data Refinery SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3391	SAP OData connection	"SAP OData setup
See Prerequisites for using the SAP ODATA Connector for the SAP Gateway setup instructions."
3392	SAP OData connection	"Restrictions
For Data Refinery, you can use this connection only as a source. You cannot use this connection as a target connection or as a target connected data asset. For SPSS Modeler, you cannot create new entity sets.  
Parent topic: Supported connections"
3393	Greenplum connection	"To access your data in Greenplum, create a connection asset for it.  
Greenplum is a massively parallel processing (MPP) database server that supports next generation data warehousing and large-scale analytics processing."
3394	Greenplum connection	"Supported versions
Greenplum 3.2+"
3395	Greenplum connection	"Create a connection to Greenplum
To create the connection asset, you need the following connection details:  
Username and password SSL certificate (if required by the database server)  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3396	Greenplum connection	"Create a connection to Greenplum
Next step: Add data assets from the connection
See Add data from a connection in a project."
3397	Greenplum connection	"Where you can use this connection
You can use Greenplum connections in the following workspaces and tools:  
Projects  
SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3398	Greenplum connection	"Greenplum setup
Greenplum Database Installation Guide"
3399	Greenplum connection	"Restriction
For SPSS Modeler, you can use this connection only to import data. You cannot export data to this connection or to a Greenplum connected data asset."
3400	Greenplum connection	"Learn more
Greenplum database  
Parent topic: Supported connections"
3401	Apache Hive connection	"To access your data in Apache Hive, create a connection asset for it.  
Apache Hive is a data warehouse software project that provides data query and analysis and is built on top of Apache Hadoop."
3402	Apache Hive connection	"Supported versions
Apache Hive 1.0.x, 1.1.x, 1.2.x. 2.0.x, 2.1.x, 3.0.x, 3.1.x."
3403	Apache Hive connection	"Create a connection to Apache Hive
To create the connection asset, you need the following connection details:  
Database name Hostname or IP address Port number HTTP path (Optional): The path of the endpoint such as the gateway, default, or hive if the server is configured for the HTTP transport mode.  
If required by the database server, the SSL certificate  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3404	Apache Hive connection	"Create a connection to Apache Hive
Next step: Add data assets from the connection
See Add data from a connection in a project."
3405	Apache Hive connection	"Where you can use this connection
You can use the Apache Hive connection in the following workspaces and tools:  
Data Refinery Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3406	Apache Hive connection	"Apache Hive setup
Apache Hive installation and configuration"
3407	Apache Hive connection	"Restriction
Running SQL statements
To ensure that your SQL statements run correctly, refer to the SQL Operations in the Apache Hive documentation for the correct syntax."
3408	Apache Hive connection	"Learn more
Apache Hive documentation  
Parent topic: Supported connections"
3409	Adding very large objects to a project's Cloud Object Storage	"The amount of data you can load to a project's Cloud Object Storage at any one time depends on where you load the data from. If you are loading the data in the product UI, the limit is 5 GB. To add larger objects to a project's Cloud Object Storage, you can use an API or an FTP client.  
The Cloud Object Storage API An FTP client The IBM Cloud Object Storage Python SDK (in case you can't use an FTP client)"
3410	Adding very large objects to a project's Cloud Object Storage	"Load data in multiple parts by using the Cloud Object Storage API
With the Cloud Object Storage API, you can load data objects as large as 5 GB in a single PUT, and objects as large as 10 TB by loading the data into object storage as a set of parts which can be loaded independently in any order and in parallel. After all of the parts have been loaded, they are presented as a single object in Cloud Object Storage.  
You can load files with these formats and mime types in multiple parts:  
application/xml application/pdf text/plain; charset=utf-8  
To load a data object in multiple parts:  
Initiate a multipart upload:  
The value for bucket-name can be found on your project's General page on the Manage tab. Click Manage in IBM Cloud on the Watson Studio for the endpoint value. The value for the bearer token is retrieved via the IBM Cloud IAM API.  
Load the parts by specifying arbitrary sequential part numbers and an UploadId for the object:  
Replacecontent-type with application/xml, application/pdf or text/plain; charset=utf-8.  
Complete the multipart load:  
Add your file to the project as an asset. From the Assets page of your project, click the Import assets button. Then from the Project files page, select the file you uploaded to your Cloud Object Storage and click Import."
3411	Adding very large objects to a project's Cloud Object Storage	"Next steps
Refining the data Analyzing the data and building models"
3412	Adding very large objects to a project's Cloud Object Storage	"Learn more
Storing large objects in Cloud Object Storage  
Parent topic: Adding connections to projects"
3413	PostgreSQL connection	"To access your data in PostgreSQL, create a connection asset for it.  
PostgreSQL is an open source and customizable object-relational database."
3414	PostgreSQL connection	"Supported versions
PostgreSQL 15.0 and later PostgreSQL 14.0 and later PostgreSQL 13.0 and later PostgreSQL 12.0 and later PostgreSQL 11.0 and later PostgreSQL 10.1 and later PostgreSQL 9.6 and later"
3415	PostgreSQL connection	"Create a connection to PostgreSQL
To create the connection asset, you need the following connection details:  
Database name Hostname or IP address Port number Username and password SSL certificate (if required by the database server)  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3416	PostgreSQL connection	"Create a connection to PostgreSQL
Next step: Add data assets from the connection
See Add data from a connection in a project."
3417	PostgreSQL connection	"Where you can use this connection
You can use PostgreSQL connections in the following workspaces and tools:  
Projects  
Data Refinery Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3418	PostgreSQL connection	"PostgreSQL setup
PostgreSQL installation"
3419	PostgreSQL connection	"PostgreSQL setup
Running SQL statements
To ensure that your SQL statements run correctly, refer to the SQL Syntax in the PostgreSQL documentation."
3420	PostgreSQL connection	"Learn more
PostgreSQL documentation  
Parent topic: Supported connections"
3421	IBM Cloud Databases for MongoDB connection	"To access your data in IBM Cloud Databases for MongoDB, create a connection asset for it.  
IBM Cloud Databases for MongoDB is a MongoDB database that is managed by IBM Cloud. It uses a JSON document store with a rich query and aggregation framework."
3422	IBM Cloud Databases for MongoDB connection	"Supported editions
MongoDB Community Edition MongoDB Enterprise Edition"
3423	IBM Cloud Databases for MongoDB connection	"Create a connection to IBM Cloud Databases for MongoDB
To create the connection asset, you need these connection details:  
Database name Hostname or IP address Port number Authentication database: The name of the database in which the user was created. Username and password SSL certificate (if required by the database server)"
3424	IBM Cloud Databases for MongoDB connection	"Create a connection to IBM Cloud Databases for MongoDB
Next step: Add data assets from the connection
See Add data from a connection in a project."
3425	IBM Cloud Databases for MongoDB connection	"Where you can use this connection
You can use IBM Cloud Databases for MongoDB connections in the following workspaces and tools:  
Projects  
Data Refinery Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3426	IBM Cloud Databases for MongoDB connection	"IBM Cloud Databases for MongoDB setup
Getting Started Tutorial"
3427	IBM Cloud Databases for MongoDB connection	"Restrictions
You can only use this connection for source data. You cannot write to data or export data with this connection. MongoDB Query Language (MQL) is not supported.  
Related connection: MongoDB connection  
Parent topic: Supported connections"
3428	IBM watsonx.data connection	"To access your data in a database in IBM watsonx.data, create a connection asset for it.  
IBM watsonx.data is an open, hybrid, and governed data lakehouse that is optimized by a query engine for all data and AI workloads."
3429	IBM watsonx.data connection	"Prerequisite
Set up an instance of watsonx.data. You can connect to an instance of watsonx.data on Cloud Pak for Data, IBM Cloud, or stand-alone watsonx.data:  
Installing watsonx.data on Cloud Pak for Data  
Getting started with watsonx.data on IBM Cloud  
Installing stand-alone watsonx.data"
3430	IBM watsonx.data connection	"Create a connection to watsonx.data
Connection details
To create the connection asset, you need these details for the watsonx.data instance:  
Select Connect to watsonx.data on Cloud Pak for Data if you are connecting to an instance of watsonx.data that is deployed on Cloud Pak for Data.  
Hostname or IP address: Find this information in the URL of the watsonx.data web console. https://<hostname-or-IPaddress>/watsonx-data/#/home?instanceId=<instance-id>  
Port: The default port number is 443. If the connection has a different port, you can find this number in the URL of the watsonx.data web console.  
Instance ID: Find this value in the watsonx.data console navigation menu. Click the Instance details link.  
Instance name:  
Cloud Pak for Data: Find the instance name in the Cloud Pak for Data web client home page at Services > Instances. IBM Cloud or stand-alone watsonx.data: The default value is lh-default.  
CRN: Cloud resource name: This field is needed only for connections to IBM Cloud. Find this information in the IBM Cloud resource list for the watsonx.data service."
3431	IBM watsonx.data connection	"Create a connection to watsonx.data
Credentials
Username and password for the watsonx.data instance. The same credentials are used for the engine.  
For watsonx.data that is deployed on Cloud Pak for Data, use the username and password that is used to access Cloud Pak for Data. For watsonx.data that is deployed on IBM Cloud, the default username is ibmlhapikey_<cloud-account-email-address>. For example: ibmlhapikey_username@example.com. The password is the user's API key."
3432	IBM watsonx.data connection	"Create a connection to watsonx.data
Certificates
By default, SSL is enabled is selected. This setting is recommended for increased security. If you do not use SSL, the data might be subject to vulnerabilities such as data leakage. Although the database that is hosted in watsonx.data can also have an SSL certificate, the connection goes through the engine.  
If SSL is enabled on a watsonx.data instance on Cloud Pak for Data and the certificate is a self-signed certificate, you must enter the certificate in the SSL certificate field.  
For an instance on IBM Cloud, the certificate is optional.  
The SSL certificate must be in PEM format for both a watsonx.data instance on Cloud Pak for Data or IBM Cloud.  
Ask your watsonx.data administrator if SSL is set up. To obtain the certificate of an IBM watsonx.data instance on Cloud Pak for Data, run this command:  
openssl s_client -showcerts -connect <cpd_hostname>:<cpd_port>  
For example: openssl s_client -showcerts -connect cpd.myserver.example.com:443"
3433	IBM watsonx.data connection	"Create a connection to watsonx.data
Engine connection details
Engine's hostname or IP address:  
watsonx.data instance on Cloud Pak for Data: watsonx.data instance on IBM Cloud: Find this information in the watsonx.data web console. Open the Infrastructure manager and go to the Details page. Under Engine details, the hostname or IP address is the value in the Host field before the colon (:).  
Expose the secure route to Presto server. This task is done by the administrator. See Exposing secure route to Presto server. In the watsonx.data console, open the Infrastructure manager and go to the Details page. Under Engine details, the hostname or IP address is the value of the External host.  
Engine ID: Find this information in the watsonx.data web console. Go to the Infrastructure manager and click the engine's name.  
Engine's port:  
watsonx.data instance on Cloud Pak for Data: The port number is determined by which port is exposed by the administrator. See Exposing secure route to Presto server. The default port number is 443. watsonx.data instance on IBM Cloud: Find this information in the watsonx.data web console. Open the Infrastructure manager and go to the Details page. Under Engine details, the port number is the value in the Host field after the colon (:)."
3434	IBM watsonx.data connection	"Create a connection to watsonx.data
Next step: Add data assets from the connection
See Add data from a connection in a project."
3435	IBM watsonx.data connection	"Where you can use this connection
You can use the watsonx.data connection in the following workspaces and tools:  
Projects  
Data Refinery Decision Optimization  
Catalogs  
Platform assets catalog"
3436	IBM watsonx.data connection	"Restrictions
Only the Presto engine is supported. You can use this connection only for source data. You cannot write to data or export data with this connection."
3437	IBM watsonx.data connection	"watsonx.data web console
Cloud Pak for Data IBM Cloud Stand-alone watsonx.data"
3438	IBM watsonx.data connection	"Learn more
watsonx.data on Cloud Pak for Data watsonx.data on IBM Cloud Stand-alone watsonx.data  
Related connection: Presto connection  
Parent topic: Supported connections"
3439	Amazon RDS for PostgreSQL connection	"To access your data in Amazon RDS for PostgreSQL, create a connection asset for it.  
Amazon RDS for PostgreSQL is a PostgreSQL relational database that runs on the Amazon Relational Database Service (RDS)."
3440	Amazon RDS for PostgreSQL connection	"Supported versions
PostgreSQL database versions 9.4, 9.5, 9.6, 10, 11 and 12"
3441	Amazon RDS for PostgreSQL connection	"Create a connection to Amazon RDS for PostgreSQL
To create the connection asset, you need these connection details:  
Database name Hostname or IP address Port number Username and password SSL certificate (if required by the database server)"
3442	Amazon RDS for PostgreSQL connection	"Create a connection to Amazon RDS for PostgreSQL
Next step: Add data assets from the connection
See Add data from a connection in a project."
3443	Amazon RDS for PostgreSQL connection	"Where you can use this connection
You can use Amazon RDS for PostgreSQL connections in the following workspaces and tools:  
Projects  
Data Refinery Decision Optimization Notebooks. Click Read data on the Code snippets pane to get the connection credentials and load the data into a data structure. See Load data from data source connections. SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3444	Amazon RDS for PostgreSQL connection	"Amazon RDS for PostgreSQL setup
For setup instructions, see these topics:  
Creating an Amazon RDS DB Instance Connecting to a DB Instance Running the PostgreSQL Database Engine"
3445	Amazon RDS for PostgreSQL connection	"Amazon RDS for PostgreSQL setup
Running SQL statements
To ensure that your SQL statements run correctly, refer to the Amazon RDS for PostgreSQL documentation for the correct syntax."
3446	Amazon RDS for PostgreSQL connection	"Learn more
Amazon RDS for PostgreSQL  
Parent topic: Supported connections"
3447	IBM Db2 on Cloud connection	"To access your data in IBM Db2 on Cloud, create a connection asset for it.  
Db2 on Cloud is an SQL database that is managed by IBM Cloud and is provisioned for you in the cloud."
3448	IBM Db2 on Cloud connection	"Create a connection to Db2 on Cloud
To create the connection asset, you need the following connection details:  
Database name Hostname or IP address Port number Username and password  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3449	IBM Db2 on Cloud connection	"Create a connection to Db2 on Cloud
Next step: Add data assets from the connection
See Add data from a connection in a project."
3450	IBM Db2 on Cloud connection	"Where you can use this connection
You can use Db2 on Cloud connections in the following workspaces and tools:  
Projects  
Data Refinery Decision Optimization Notebooks. Click Read data on the Code snippets pane to get the connection credentials and load the data into a data structure. For more information, see Load data from data source connections. SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3451	IBM Db2 on Cloud connection	"Db2 on Cloud setup
Getting started with Db2 on Cloud"
3452	IBM Db2 on Cloud connection	"Db2 on Cloud setup
Running SQL statements
To ensure that your SQL statements run correctly, refer to the Structured Query Language (SQL) topic in the Db2 on Cloud documentation for the correct syntax."
3453	IBM Db2 on Cloud connection	"Learn more
Db2 on Cloud documentation SSL connectivity  
Parent topic: Supported connections"
3454	Google Cloud Storage connection	"To access your data in Google Cloud Storage, create a connection asset for it.  
Google Cloud Storage is an online file storage web service for storing and accessing data on Google Cloud Platform Infrastructure."
3455	Google Cloud Storage connection	"Create a connection to Google Cloud Storage
To create the connection asset, choose an authentication method. Choices include an authentication with or without workload identity federation.  
Without workload identity federation  
Account key (full JSON snippet): The contents of the Google service account key JSON file Client ID, Client secret, Access token, and Refresh token  
With workload identity federation You use an external identity provider (IdP) for authentication. An external identity provider uses Identity and Access Management (IAM) instead of service account keys. IAM provides increased security and centralized management. You can use workload identity federation authentication with an access token or with a token URL.  
You can configure a Google BigQuery connection for workload identity federation with any identity provider that complies with the OpenID Connect (OIDC) specification and that satisfies the Google Cloud requirements that are described in Prepare your external IdP. The requirements include:  
The identity provider must support OpenID Connect 1.0. The identity provider's OIDC metadata and JWKS endpoints must be publicly accessible over the internet. Google Cloud uses these endpoints to download your identity provider's key set and uses that key set to validate tokens. The identity provider is configured so that your workload can obtain ID tokens that meet these criteria:  
Tokens are signed with the RS256 or ES256 algorithm. Tokens contain an aud claim.  
For examples of the workload identity federation configuration steps for Amazon Web Services (AWS) and Microsoft Azure, see Workload identity federation examples.  
//iam.googleapis.com/projects/PROJECT_NUMBER/locations/global/workloadIdentityPools/POOL_ID/providers/PROVIDER_ID  
Access token: An access token from the identity provider to connect to BigQuery.  
Security Token Service audience: The security token service audience that contains the project ID, pool ID, and provider ID. Use this format:  
For more information, see Authenticate a workload by using the REST API.  
Service account email: The email address of the Google service account to be impersonated. For more information, see Create a service account for the external workload.  
Service account token lifetime (optional): The lifetime in seconds of the service account access token. The default lifetime of a service account access token is one hour. For more information, see URL-sourced credentials."
3456	Google Cloud Storage connection	"Create a connection to Google Cloud Storage
Service account token lifetime (optional): The lifetime in seconds of the service account access token. The default lifetime of a service account access token is one hour. For more information, see URL-sourced credentials.  
Token format: Text or JSON with the Token field name for the name of the field in the JSON response that contains the token.  
Token field name: The name of the field in the JSON response that contains the token. This field appears only when the Token format is JSON.  
Token type: AWS Signature Version 4 request, Google OAuth 2.0 access token, ID token, JSON Web Token (JWT), or SAML 2.0.  
//iam.googleapis.com/projects/PROJECT_NUMBER/locations/global/workloadIdentityPools/POOL_ID/providers/PROVIDER_ID  
Security Token Service audience: The security token service audience that contains the project ID, pool ID, and provider ID. Use this format:  
For more information, see Authenticate a workload using the REST API.  
Service account email: The email address of the Google service account to be impersonated. For more information, see Create a service account for the external workload.  
Service account token lifetime (optional): The lifetime in seconds of the service account access token. The default lifetime of a service account access token is one hour. For more information, see URL-sourced credentials.  
Token URL: The URL to retrieve a token.  
HTTP method: HTTP method to use for the token URL request: GET, POST, or PUT.  
Request body (for POST or PUT methods): The body of the HTTP request to retrieve a token.  
HTTP headers: HTTP headers for the token URL request in JSON or as a JSON body. Use format: ""Key1""=""Value1"",""Key2""=""Value2"".  
Token format: Text or JSON with the Token field name for the name of the field in the JSON response that contains the token.  
Token field name: The name of the field in the JSON response that contains the token. This field appears only when the Token format is JSON.  
Token type: AWS Signature Version 4 request, Google OAuth 2.0 access token, ID token, JSON Web Token (JWT), or SAML 2.0."
3457	Google Cloud Storage connection	"Create a connection to Google Cloud Storage
Other properties
Project ID (optional) The ID of the Google project."
3458	Google Cloud Storage connection	"Create a connection to Google Cloud Storage
Next step: Add data assets from the connection
See Add data from a connection in a project."
3459	Google Cloud Storage connection	"Where you can use this connection
You can use Google Cloud Storage connections in the following workspaces and tools:  
Projects  
Data Refinery Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3460	Google Cloud Storage connection	"Supported file types
The Google Cloud Storage connection supports these file types:  Avro, CSV, Delimited text, Excel, JSON, ORC, Parquet, SAS, SAV, SHP, and XML."
3461	Google Cloud Storage connection	"Learn more
Workload identity federation examples Google Cloud Storage documentation  
Parent topic: Supported connections"
3462	Setting up temporary credentials or a Role ARN for Amazon S3	"Instead of adding another IAM user to your Amazon S3 account, you can grant them access with temporary security credentials and a Session token. Or, you can create a Role ARN (Amazon Resource Name) and then grant permission to that role to access the account. The trusted user can then use the role.  
You can assign role policies to the temporary credentials to limit the permissions. For example, you can assign read-only access or access to a particular S3 bucket.  
Prerequisite: You must be the IAM owner of the Amazon S3 account.  
You can set up one of the following authentication combinations:  
Access key, Secret key, and Session token Access key, Secret key, Role ARN, Role session name, and optional Duration seconds Access key, Secret key, Role ARN, Role session name, External ID, and optional Duration seconds"
3463	Setting up temporary credentials or a Role ARN for Amazon S3	"Access key, Secret key, and Session token
Use the AWS Security Token Service (AWS STS) operations in the AWS API to obtain temporary security credentials. These credentials consist of an Access key, a Secret key, and a Session token that expires within a configurable amount of time. For instructions, see the AWS documentation: Requesting temporary security credentials."
3464	Setting up temporary credentials or a Role ARN for Amazon S3	"Access key, Secret key, Role ARN, Role session name, and optional Duration seconds
If someone else has their own S3 account, you can create a temporary role for that person to access your S3 account. Create the role either with the AWS Management Console or the AWS CLI. See Creating a role to delegate permissions to an IAM user  
The Role ARN is the Amazon Resource Name for connection's role. The Role session name identifies the session to S3 administrators. For example, your IAM username. The Duration seconds parameter is optional. The minimum is 15 minutes. The maximum is 36 hours, the default is 1 hour. The duration seconds timer starts every time that the connection is established.  
You then provide values for the Access key, Secret key, Role ARN, Role session name, and optional Duration seconds to the user who will create the connection."
3465	Setting up temporary credentials or a Role ARN for Amazon S3	"Access key, Secret key, Role ARN, Role session name, External ID, and optional Duration seconds
If someone else has their own S3 account, you can create a temporary role for that person to access your S3 account. With this combination, the External ID is a unique string that you specify and that the user must enter for extra security. First, create the role either with the AWS Management Console or the AWS CLI. See Creating a role to delegate permissions to an IAM user. To create the External ID, see How to use an external ID when granting access to your AWS resources to a third party.  
You then provide the values for the Access key, Secret key, Role ARN, Role session name, External ID, and optional Duration seconds to the user who will create the connection."
3466	Setting up temporary credentials or a Role ARN for Amazon S3	"Learn more
Amazon Resource Names (ARNs)  
Parent topic: Amazon S3 connection"
3467	Generic S3 connection	To access your data from a storage service that is compatible with the Amazon S3 API, create a connection asset for it.
3468	Generic S3 connection	"Create a Generic S3 connection
To create the connection asset, you need these connection details:  
Endpoint URL: The endpoint URL to access to S3 Bucket(optional): The name of the bucket that contains the files Region (optional): S3 region. Specify a region that matches the regional endpoint. Access key: The access key (username) that authorizes access to S3 Secret key: The password associated with the Access key ID that authorizes access to S3 The SSL certificate of the trusted host. The certificate is required when the host certificate is not signed by a known certificate authority.  
Disable chunked encoding: Select if the storage does not support chunked encoding. Enable global bucket access: Consult the documentation for your S3 data source for whether to select this property. Enable path style access: Consult the documentation for your S3 data source for whether to select this property."
3469	Generic S3 connection	"Create a Generic S3 connection
Next step: Add data assets from the connection
See Add data from a connection in a project."
3470	Generic S3 connection	"Where you can use this connection
You can use the Generic S3 connection in the following workspaces and tools:  
Projects  
Data Refinery Decision Optimization  
Catalogs  
Platform assets catalog"
3471	Generic S3 connection	"Generic S3 connection setup
For setup information, consult the documentation of the S3-compatible data source that you are connecting to."
3472	Generic S3 connection	"Supported file types
The Generic S3 connection supports these file types:  Avro, CSV, delimited text, Excel, JSON, ORC, Parquet, SAS, SAV, SHP, and XML.  
Related connection: Amazon S3 connection  
Parent topic: Supported connections"
3473	Teradata connection	"To access your data in Teradata, you must create a connection asset for it.  
Teradata provides database and analytics-related services and products."
3474	Teradata connection	"Supported versions
Teradata databases 15.10, 16.10, 17.00, 17.10, and 17.20"
3475	Teradata connection	"Create a connection to Teradata
To create the connection asset, you need the following connection details:  
Database name  
Hostname or IP address  
Port  
Client character set: IMPORTANT: Do not enter a value unless you are instructed by IBM support. The character set value overrides the Teradata JDBC drivers normal mapping of the Teradata session character sets. Data corruption can occur if you specify the wrong character set. If no value is specified, UTF16 is used.  
Query band expression: Semicolon-separated list of name-value pairs to use in the generated query band statement for the session. A query band is a set of user-defined parameters that can be set on a session, a transaction, or both to identify the originating source of a query. After you define a query band, it is passed to the Teradata database as a list of name=value pairs in a single quoted string. For example, 'ProjectName=dstage1'.  
Authentication method: Select the security mechanism to use to authenticate the user:  
TD2 (Teradata Method 2): Use the Teradata security mechanism. LDAP: Use an LDAP security mechanism for external authentication.  
Username and password  
SSL certificate (if required by the database server)  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3476	Teradata connection	"Create a connection to Teradata
Next step: Add data assets from the connection
See Add data from a connection in a project."
3477	Teradata connection	"Where you can use this connection
You can use Teradata connections in the following workspaces and tools:  
Projects  
Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3478	Teradata connection	"Where you can use this connection
Running SQL statements
To ensure that your SQL statements run correctly, refer to the Teradata SQL documentation for the correct syntax."
3479	Teradata connection	"Learn more
Teradata documentation Teradata Community  
Parent topic: Supported connections  
Teradata JDBC Driver 17.00.00.03 Copyright (C) 2024 by Teradata. All rights reserved. IBM provides embedded usage of the Teradata JDBC Driver under license from Teradata solely for use as part of the IBM Watson service offering."
3480	Microsoft Azure Data Lake Storage connection	"To access your data in Microsoft Azure Data Lake Storage, create a connection asset for it.  
Azure Data Lake Storage (ADLS) is a scalable data storage and analytics service that is hosted in Azure, Microsoft's public cloud. The Microsoft Azure Data Lake Storage connection supports access to both Gen1 and Gen2 Azure Data Lake Storage repositories."
3481	Microsoft Azure Data Lake Storage connection	"Create a connection to Microsoft Azure Data Lake Storage
To create the connection asset, you need these connection details:  
WebHDFS URL: The WebHDFS URL for accessing HDFS. To connect to a Gen 2 ADLS, use the format, https://<account-name>.dfs.core.windows.net/<file-system> Where <account-name> is the name you used when you created the ADLS instance. For <file-system>, use the name of the container you created. For more information, see the Microsoft Data Lake Storage Gen2 documentation. Tenant ID: The Azure Active Directory tenant ID Client ID: The client ID for authorizing access to Microsoft Azure Data Lake Storage Client secret: The authentication key that is associated with the client ID for authorizing access to Microsoft Azure Data Lake Storage  
Select Server proxy to access the Azure Data Lake Storage data source through a proxy server. Depending on its setup, a proxy server can provide load balancing, increased security, and privacy. The proxy server settings are independent of the authentication credentials and the personal or shared credentials selection.  
Proxy host: The proxy URL. For example, https://proxy.example.com. Proxy port number: The port number to connect to the proxy server. For example, 8080 or 8443. The Proxy protocol selection for HTTP or HTTPS is optional.  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3482	Microsoft Azure Data Lake Storage connection	"Create a connection to Microsoft Azure Data Lake Storage
Next step: Add data assets from the connection
See Add data from a connection in a project."
3483	Microsoft Azure Data Lake Storage connection	"Where you can use this connection
You can use Microsoft Azure Data Lake Storage connections in the following workspaces and tools:  
Projects  
Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3484	Microsoft Azure Data Lake Storage connection	"Azure Data Lake Storage authentication setup
To set up authentication, you need a tenant ID, client (or application) ID, and client secret.  
Gen1: Gen2:  
Create an Azure Active Directory (Azure AD) web application, get an application ID, authentication key, and a tenant ID. Then, you must assign the Azure AD application to the Azure Data Lake Storage account file or folder. Follow Steps 1, 2, and 3 at Service-to-service authentication with Azure Data Lake Storage using Azure Active Directory.  
Follow instructions in Acquire a token from Azure AD for authorizing requests from a client application. These steps create a new identity. After you create the identity, set permissions to grant the application access to your ADLS. The Microsoft Azure Data Lake Storage connection will use the associated Client ID, Client secret, and Tenant ID for the application. Give the Azure App access to the storage container using Storage Explorer. For instructions, see Use Azure Storage Explorer to manage directories and files in Azure Data Lake Storage Gen2."
3485	Microsoft Azure Data Lake Storage connection	"Supported file types
The Microsoft Azure Data Lake Storage connection supports these file types: Avro, CSV, Delimited text, Excel, JSON, ORC, Parquet, SAS, SAV, SHP, and XML."
3486	Microsoft Azure Data Lake Storage connection	"Learn more
Azure Data Lake  
Parent topic: Supported connections"
3487	Workload identity federation examples	"Workload identity federation for the Google BigQuery and Google Cloud Storage connections is supported by any identity provider that supports OpenID Connect (OIDC) or SAML 2.0.  
These examples are for AWS with Amazon Cognito and for Microsoft Azure."
3488	Workload identity federation examples	"AWS
Configure workload identity federation in Amazon Cognito
CLIENT_ID=YourClientId ISSUER_URL=https://cognito-idp.YourRegion.amazonaws.com/YourPoolId POOL_ID=YourPoolId USERNAME=YourUsername PASSWORD=YourPassword GROUPNAME=YourGroupName aws cognito-idp admin-create-user --user-pool-id $POOL_ID --username $USERNAME --temporary-password Temp-Pass1 aws cognito-idp admin-set-user-password --user-pool-id $POOL_ID --username $USERNAME --password $PASSWORD --permanent aws cognito-idp create-group --group-name $GROUPNAME --user-pool-id $POOL_ID aws cognito-idp admin-add-user-to-group --user-pool-id $POOL_ID --username $USERNAME --group-name $GROUPNAME aws cognito-idp admin-initiate-auth --auth-flow ADMIN_USER_PASSWORD_AUTH --client-id $CLIENT_ID --auth-parameters USERNAME=$USERNAME,PASSWORD=$PASSWORD --user-pool-id $POOL_ID  
Create an OIDC identity provider (IdP) with Cognito by following the instructions in the Amazon documentation:  
Step 1. Create a user pool Step 2. Add an app client and set up the hosted UI  
For more information, see Getting started with Amazon Cognito.  
Create a group and user in the IdP with the AWS console. Or you can use AWS CLI:  
From the AWS console, click View Hosted UI and log in to the IDP UI in a browser to ensure that any new password challenge is resolved.  
Get an IdToken with the AWS CLI:  
For more information on the Amazon Cognito User Pools authentication flow, see AdminInitiateAuth."
3489	Workload identity federation examples	"AWS
Configure Google Cloud for Amazon Cognito
When you create the provider in Google Cloud, use these settings:  
Set Issuer (URL) to https://cognito-idp.YourRegion.amazonaws.com/YourPoolId. Set Allowed Audience to your client ID. Under Attribute Mapping, map google.subject to assertion.sub."
3490	Workload identity federation examples	"AWS
Create the connection with Amazon Cognito workload identity federation
//iam.googleapis.com/projects/PROJECT_NUMBER/locations/global/workloadIdentityPools/POOL_ID/providers/PROVIDER_ID  
Choose the Workload Identity Federation with access token authentication method.  
For the Security Token Service audience field, use this format:  
For the Service account e-mail, enter the email address of the Google service account to be impersonated. For more information, see Create a service account for the external workload.  
(Optional) Specify a value for the Service account token lifetime in seconds. The default lifetime of a service account access token is one hour. For more information, see URL-sourced credentials.  
Set Token format to Text  
Set Token type to ID token"
3491	Workload identity federation examples	"Azure
Configure workload identity federation in Azure
Create an Azure AD application and service principal.  
Set an Application ID URI for the application. You can use the default Application ID URI (api://APPID) or specify a custom URI. You can skip the instructions on creating a managed identity.  
Follow the instructions to create a new application secret to get an access token with the REST API.  
For more information, see Configure workload identity federation with Azure."
3492	Workload identity federation examples	"Azure
Configure Google Cloud for Azure
Follow the instructions: Configure workload identity federation. Follow the instructions: Create the workload identity pool and provider. When you configure the provider, use these settings:  
Set Issuer (URL) to https://sts.windows.net/TENANTID/, where TENANTID is the tenant ID that you received when you set up Azure Active Directory. Set the Allowed audience to the client ID that you received when you set up the app registration. Or specify another Application ID URI that you used when you set up the application identity in Azure. Under Attribute Mapping, map google.subject to assertion.sub."
3493	Workload identity federation examples	"Azure
Create the connection with Azure workload identity federation
//iam.googleapis.com/projects/PROJECT_NUMBER/locations/global/workloadIdentityPools/POOL_ID/providers/PROVIDER_ID  
Choose one of these authentication methods:  
Workload Identity Federation with access token Workload Identity Federation with token URL  
For the Security Token Service audience field, use the format that is described in Authenticate a workload that uses the REST API. For example:  
For the Service account e-mail, enter the email address of the Google service account to be impersonated. For more information, see Create a service account for the external workload.  
(Optional) Specify a value for the Service account token lifetime in seconds. The default lifetime of a service account access token is one hour. For more information, see URL-sourced credentials.  
If you specified Workload Identity Federation with token URL, use these values:  
Token URL: https://login.microsoftonline.com/TENANT_ID/oauth2/v2.0/token. This URL will fetch a token from Azure. HTTP method: POST HTTP headers: ""Content-Type""=""application/x-www-form-urlencoded;charset=UTF-8"",""Accept""=""application/json"" Request body: grant_type=client_credentials&client_id=CLIENT_ID&client_secret=CLIENT_SECRET&scope=APPLICATION_ID_URI/.default  
For Token type, select ID token for an identity provider that complies with the OpenID Connect (OIDC) specification. For information, see Token types.  
The Token format option depends on that authentication selection:  
Workload Identity Federation with access token: Select Text if you supplied the raw token value in the Access token field. Workload Identity Federation with token URL: For a response from the token URL in JSON format with the access token that is returned in a field named access_token, use these settings:  
Token format: JSON Token field name: access_token"
3494	Workload identity federation examples	"Learn more
Workload identity federation (Google Cloud) Configure workload identity federation on the identity provider (Google Cloud) Generate a credentials configuration file (Google Cloud)"
3495	Connectors for watsonx.ai	"You can add connections to a broad array of data sources for working with data in watsonx.ai. Source connections can be used to read data; target connections can be used to load (save) data. When you create a target connection, be sure to use credentials that have Write permission or you won't be able to save data to the target.  
From a project, you must create a connection to a data source before you can read data from it or load data to it.  
IBM services  
Third-party services  
Supported connectors by tool"
3496	Connectors for watsonx.ai	"IBM services
IBM Cloud Data Engine. Supports source connections only. IBM Cloud Databases for MongoDB. Supports source connections only. IBM Cloud Databases for MySQL IBM Cloud Databases for PostgreSQL IBM Cloud Object Storage IBM Cloud Object Storage (infrastructure) IBM Cloudant IBM Cognos Analytics. Supports source connections only. IBM Data Virtualization Manager for z/OS IBM Db2 IBM Db2 Big SQL IBM Db2 for i IBM Db2 for z/OS IBM Db2 on Cloud IBM Db2 Warehouse IBM Informix IBM Netezza Performance Server IBM Planning Analytics IBM watsonx.data. Supports source connections only."
3497	Connectors for watsonx.ai	"Third-party services
Many of the third-party services, data sources for working with data in watsonx.ai, are part of the ecosystem of watsonx technology partners. For information about watsonx integrations, see watsonx technology partners.  
Amazon RDS for MySQL Amazon RDS for Oracle Amazon RDS for PostgreSQL Amazon Redshift Amazon S3 Apache Cassandra Apache Derby Apache HDFS Apache Hive. Supports source connections only. Box Cloudera Impala. Supports source connections only. DataStax Enterprise Dremio. Supports source connections only. Dropbox Elasticsearch FTP Generic S3 Google BigQuery Google Cloud Storage Greenplum HTTP. Supports source connections only. Looker. Supports source connections only. MariaDB Microsoft Azure Blob Storage Microsoft Azure Cosmos DB Microsoft Azure Data Lake Storage Microsoft Azure File Storage Microsoft Azure SQL Database Microsoft SQL Server MongoDB. Supports source connections only. MySQL OData Oracle PostgreSQL Presto. Supports source connections only. Salesforce.com. Supports source connections only. SAP ASE SAP IQ. Supports source connections only. SAP OData SingleStoreDB Snowflake Tableau. Supports source connections only. Teradata Teradata JDBC Driver 17.00.00.03 Copyright (C) 2024 by Teradata. All rights reserved. IBM provides embedded usage of the Teradata JDBC Driver under license from Teradata solely for use as part of the IBM Watson service offering.."
3498	Connectors for watsonx.ai	"Supported connectors by tool
The following tools support connections:  
AutoAI Data Refinery Decision Optimization Notebooks SPSS Modeler Synthetic Data Generator"
3499	Connectors for watsonx.ai	"Learn more
Asset previews Profiles of assets Troubleshooting connections  
Parent topic: Preparing data"
3500	IBM Db2 for i connection	"To access your data in IBM Db2 for i, create a connection asset for it.  
Db2 for i is the relational database manager that is fully integrated on your system. Because it is integrated on the system, Db2 for i is easy to use and manage."
3501	IBM Db2 for i connection	"Supported versions
IBM DB2 for i 7.2+"
3502	IBM Db2 for i connection	"Prerequisites
Obtain the certificate file
A certificate file on the Db2 for i server is required to use this connection. To obtain an IBM Db2 Connect Unlimited Edition license certificate file, go to IBM Db2 Connect: Pricing and Installing the IBM Data Server Driver for JDBC and SQLJ. For installation instructions, see Activating the license certificate file for Db2 Connect Unlimited Edition."
3503	IBM Db2 for i connection	"Prerequisites
Run the bind command
Run the following commands from the Db2 client that is configured to access the Db2 for i server. You need to run the bind command only once per remote database per Db2 client version.  
For information about bind commands, see Binding applications and utilities."
3504	IBM Db2 for i connection	"Prerequisites
Run catalog commands
Run the following catalog commands from the Db2 client that is configured to access the Db2 for i server:  
db2 catalog tcpip node node_name remote hostname_or_address server port_no_or_service_name db2 catalog dcs database local_name as real_db_name db2 catalog database local_name as alias at node node_name authentication server  
Example: db2 catalog tcpip node db2i123 remote 192.0.2.0 server 446  
Example: db2 catalog dcs database db2i123 as db2i123  
Example: db2 catalog database db2i123 as db2i123 at node db2i123 authentication server  
For information about catalog commands, see CATALOG TCPIP NODE and CATALOG DCS DATABASE."
3505	IBM Db2 for i connection	"Create a connection to Db2 for i
To create the connection asset, you need these connection details:  
Hostname or IP address Port number Location: The unique name of the Db2 location you want to access Username and password SSL certificate (if required by the database server)  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3506	IBM Db2 for i connection	"Create a connection to Db2 for i
Next step: Add data assets from the connection
See Add data from a connection in a project."
3507	IBM Db2 for i connection	"Where you can use this connection
You can use Db2 for i connections in the following workspaces and tools:  
Projects  
SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3508	IBM Db2 for i connection	"Restriction
For SPSS Modeler, you can use this connection only to import data. You cannot export data to this connection or to a Db2 for i connection connected data asset."
3509	IBM Db2 for i connection	"Restriction
Running SQL statements
To ensure that your SQL statements run correctly, refer to the Db2 for i SQL reference for the correct syntax."
3510	IBM Db2 for i connection	"Learn more
IBM Db2 for i documentation  
Parent topic: Supported connections"
3511	IBM Data Virtualization Manager for z/OS connection	"To access your data in Data Virtualization Manager for z/OS, create a connection asset for it.  
Use the Data Virtualization Manager for z/OS connection to access data in your z/OS mainframe environment."
3512	IBM Data Virtualization Manager for z/OS connection	"Supported versions
IBM Data Virtualization Manager for z/OS 1.1.0"
3513	IBM Data Virtualization Manager for z/OS connection	"Create a connection to Data Virtualization Manager for z/OS
To create the connection asset, you need these connection details:  
Hostname or IP address Port number Username and password SSL certificate (if required by the database server)  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3514	IBM Data Virtualization Manager for z/OS connection	"Create a connection to Data Virtualization Manager for z/OS
Next step: Add data assets from the connection
See Add data from a connection in a project."
3515	IBM Data Virtualization Manager for z/OS connection	"Where you can use this connection
You can use Data Virtualization Manager for z/OS connections in the following workspaces and tools:  
Projects  
Data Refinery Synthetic Data Generator  
Catalogs  
Platform assets catalog  
Learn more  
IBM Data Virtualization Manager for z/OS  
Parent topic: Supported connections"
3516	IBM Db2 connection	"To access your data in an IBM Db2 database, create a connection asset for it.  
IBM Db2 is a database that contains relational data."
3517	IBM Db2 connection	"Supported versions
IBM Db2 10.1 and later"
3518	IBM Db2 connection	"Create a connection to Db2
To create the connection asset, you need the following connection details:  
Database  
Hostname or IP address  
Username and password  
Port  
Application name (optional): The name of the application that is currently using the connection. For information, see Client info properties support by the IBM Data Server Driver for JDBC and SQLJ.  
Client accounting information (optional): The value of the accounting string from the client information that is specified for the connection. For information, see Client info properties support by the IBM Data Server Driver for JDBC and SQLJ.  
Client hostname (optional): The hostname of the machine on which the application that is using the connection is running. For information, see Client info properties support by the IBM Data Server Driver for JDBC and SQLJ.  
Client user (optional): The name of the user on whose behalf the application that is using the connection is running. For information, see Client info properties support by the IBM Data Server Driver for JDBC and SQLJ.  
SSL certificate (if required by your database server)  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3519	IBM Db2 connection	"Create a connection to Db2
Next step: Add data assets from the connection
See Add data from a connection in a project."
3520	IBM Db2 connection	"Where you can use this connection
You can use Db2 connections in the following workspaces and tools:  
Projects  
Data Refinery Decision Optimization Notebooks. Click Read data on the Code snippets pane to get the connection credentials and load the data into a data structure. See Load data from data source connections. SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3521	IBM Db2 connection	"Where you can use this connection
Running SQL statements
To ensure that your SQL statements run correctly, refer to the Structured Query Language (SQL) topic in the IBM Db2 product documentation for the correct syntax."
3522	IBM Db2 connection	"Learn more
IBM Db2 product documentation  
Parent topic: Supported connections"
3523	MySQL connection	"To access your data in MySQL, create a connection asset for it.  
MySQL is an open-source relational database management system."
3524	MySQL connection	"Supported versions
MySQL Enterprise Edition 5.0+ MySQL Community Edition 4.1, 5.0, 5.1, 5.5, 5.6, 5.7"
3525	MySQL connection	"Create a connection to MySQL
To create the connection asset, you need these connection details:  
Database name Hostname or IP Address Port number Character Encoding Username and password SSL certificate (if required by the database server)  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3526	MySQL connection	"Create a connection to MySQL
Next step: Add data assets from the connection
See Add data from a connection in a project."
3527	MySQL connection	"Where you can use this connection
You can use MySQL connections in the following workspaces and tools:  
Projects  
Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3528	MySQL connection	"Where you can use this connection
Running SQL statements
To ensure that your SQL statements run correctly, refer to the MySQL documentation for the correct syntax."
3529	MySQL connection	"MySQL setup
MySQL Installation"
3530	MySQL connection	"Learn more
MySQL documentation  
Parent topic: Supported connections"
3531	IBM Cognos Analytics connection	"To access your data in Cognos Analytics, create a connection asset for it.  
Cognos Analytics is an AI-fueled business intelligence platform that supports the entire analytics cycle, from discovery to operationalization."
3532	IBM Cognos Analytics connection	"Supported versions
IBM Cognos Analytics 11"
3533	IBM Cognos Analytics connection	"Supported content types
Report (except Reports that require prompts) Query"
3534	IBM Cognos Analytics connection	"Create a connection to Cognos Analytics
To create the connection asset, you need the following connection details:  
Gateway URL SSL certificate (if required by the database server)"
3535	IBM Cognos Analytics connection	"Create a connection to Cognos Analytics
Credentials
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3536	IBM Cognos Analytics connection	"Create a connection to Cognos Analytics
Next step: Add data assets from the connection
See Add data from a connection in a project."
3537	IBM Cognos Analytics connection	"Where you can use this connection
You can use Cognos Analytics connections in the following workspaces and tools:  
Projects  
Data Refinery Notebooks. Click Read data on the Code snippets pane to get the connection credentials and load the data into a data structure. See Load data from data source connections. SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3538	IBM Cognos Analytics connection	"Cognos Analytics setup
Instructions for setting up Cognos Analytics: Getting started in Cognos Analytics."
3539	IBM Cognos Analytics connection	"Restrictions
You can use this connection only for source data. You cannot write to data or export data with this connection. Notebooks: Self-signed certificates are not supported for notebooks. The SSL certificate that is imported into the Cognos Analytics server must be signed by a trusted root authority. To confirm that the certificate is signed by a trusted root authority, enter the Cognos Analytics URL into a browser and verify that there is a padlock to the left of the URL. If the certificate is self-signed, the Cognos Analytics server administrator must replace it with a trusted TLS certificate."
3540	IBM Cognos Analytics connection	"Restrictions
Running SQL statements
To ensure that your SQL statements run correctly, refer to Working with Queries in SQL in the Cognos Analytics documentation for the correct syntax."
3541	IBM Cognos Analytics connection	"Learn more
Cognos Analytics documentation  
Parent topic: Supported connections"
3542	Creating jobs in Data Refinery	"You can create a job to run a Data Refinery flow directly in Data Refinery.  
To create a Data Refinery flow job:  
In Data Refinery, click the Jobs icon from the Data Refinery toolbar and select Save and create a job.  
Define the job details by entering a name and a description (optional).  
On the Configure page, select an environment runtime for the job, and optionally modify the job retention settings.  
On the Schedule page, you can optionally add a one-time or repeating schedule.  
If you define a start day and time without selecting Repeat, the job will run exactly one time at the specified day and time. If you define a start date and time and you select Repeat, the job will run for the first time at the timestamp indicated in the Repeat section.  
You can't change the time zone; you must set your job schedule in your web browser's time zone. The schedule will be translated to the time zone of the compute node where your job runs.  
If you exclude certain weekdays, the job might not run as you would expect. The reason might be due to a discrepancy between the time zone of the user who creates the schedule, and the time zone of the compute node where the job runs.  
Note: Your scheduled job can appear differently if your web browser’s time zone is set to your local time zone following Daylight Savings Time (DST). For example, your scheduled job appears at 3:00PM Eastern Standard Time Zone (EST) daily corresponding to 8:00PM Coordinated Universal Time (UTC). When your local time zone changes to Eastern Daylight Time Zone (EDT), your scheduled job continues to run at 8:00PM (UTC) which will now appear as 4:00PM (EDT) daily.  
Optional: Set up notifications for the job. You can select the type of alerts to receive.  
Review the job settings. Then, create the job and run it immediately, or create the job and run it later.  
The Data Refinery flow job is listed in the Jobs in your project."
3543	Creating jobs in Data Refinery	"Learn more
Compute resource options for Data Refinery in projects Viewing job details Refining data  
Parent topic: Jobs"
3544	Exporting a project	You can share assets in a project with others and copy a project by exporting them as a ZIP file to your desktop. The project readme file is added to the exported ZIP file by default.
3545	Exporting a project	"Requirements and restrictions
You cannot export assets larger than 500 MB  
If your project is marked as sensitive, you can't export data assets, connections or connected data from the project.  
Be mindful when selecting assets to always also include the dependencies of those assets, for example the data assets or connections for a data flow, a notebook, connected data, or jobs. There is no check for dependencies. If you don't include the dependencies, subsequent project imports do not work.  
You can only export and share assets across projects created in watsonx.ai. You can't export a project from Cloud Pak for Data as a Service and import it into watsonx.ai, or the other way around. You can however, move projects between Cloud Pak for Data as a Service and watsonx.ai. See Switching the platform for a project.  
Exporting a project from one region and importing the assets to a project or space in another region can result in an error creating the assets. The error message An unexpected response was returned when creating asset is a symptom of this restriction.  
Exporting a project is not available for all Watson Studio plans. See Watson Studio plans."
3546	Exporting a project	"Exporting a project to desktop
Exporting a project packs the project assets that you select into a single ZIP file that can be shared like any other file.  
To export project assets to desktop:  
Open the project you want to export assets from.  
Check whether the assets that you include in your export, for example notebooks or connections, don't contain credentials or other sensitive information that you don't want to share. You should remove this information before you begin the export. Only private connection credentials are removed.  
Optional. Add information to the readme on the Overview page of your project about the assets that you include in the export. For example, you can give a brief description of the analytics use case of the added assets and the data analysis methods that are used.  
Click from the project toolbar.  
Select the assets to add. You can filter by asset type or customize the project export settings by selecting preferences (the settings icon to the right of the window title) which are applied each time you export the project.  
Optional: Change the name of the project export file.  
Supply a password if you want to export connections that have shared credentials. Note that this password must be provided to decrypt these credentials on project import.  
Click Export. Do not leave the page while the export is running.  
When you export to desktop, the file is saved to the Downloads folder by default. If a ZIP file with the same name already exists, the existing file isn't overwritten.  
Ensure that your browser settings download the ZIP file to the desktop as a .zip file and not as a folder. Compressing this folder to enable project import leads to an error. Note also that you cannot manually add other assets to an exported project ZIP file on your desktop.  
The status of a project export is tracked on the project's Overview page."
3547	Exporting a project	"Learn more
Administering a project Importing a project  
Parent topic: Administering projects"
3548	IBM Cloud Databases for PostgreSQL connection	"To access your data in IBM Cloud Databases for PostgreSQL, create a connection asset for it.  
IBM Cloud Databases for PostgreSQL is an open source object-relational database that is highly customizable. It’s a feature-rich enterprise database with JSON support."
3549	IBM Cloud Databases for PostgreSQL connection	"Create a connection to IBM Cloud Databases for PostgreSQL
To create the connection asset, you need the following connection details:  
Database name Hostname or IP address of the database Port number Username and password SSL certificate (if required by the database server)  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3550	IBM Cloud Databases for PostgreSQL connection	"Create a connection to IBM Cloud Databases for PostgreSQL
Next step: Add data assets from the connection
See Add data from a connection in a project."
3551	IBM Cloud Databases for PostgreSQL connection	"Where you can use this connection
You can use IBM Cloud Databases for PostgreSQL connections in the following workspaces and tools:  
Projects  
Notebooks. Click Read data on the Code snippets pane to get the connection credentials and load the data into a data structure. For more information, see Load data from data source connections. SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3552	IBM Cloud Databases for PostgreSQL connection	"IBM Cloud Databases for PostgreSQL setup
IBM Cloud Databases for PostgreSQL setup"
3553	IBM Cloud Databases for PostgreSQL connection	"Restriction
For SPSS Modeler, you can use this connection only to import data. You cannot export data to this connection or to an IBM Cloud Databases for PostgreSQL connected data asset."
3554	IBM Cloud Databases for PostgreSQL connection	"Restriction
Running SQL statements
To ensure that your SQL statements run correctly, refer to the IBM Cloud Databases for PostgreSQL documentation for the correct syntax."
3555	IBM Cloud Databases for PostgreSQL connection	"Learn more
IBM Cloud Databases for PostgreSQL documentation  
Parent topic: Supported connections"
3556	Viewing jobs across projects	"You can view the jobs that exist across projects for assets that run in tools, such as notebooks, Data Refinery flows, and SPSS Modeler flows.  
To view the status of jobs or job runs in projects:  
From the navigation menu, select Projects > Jobs.  
Select a view scope:  
Jobs with finished runs: all jobs that contain finished runs Finished runs: all job runs that have finished Jobs with active runs: all jobs that contain that contain active runs Active runs: all job runs that are still active  
Click from the table toolbar to further narrow down the returned search results for the view scope you selected. The filter options vary depending the view scope selection, for example, for jobs with active runs, you can filter by run state, job type and project, whereas for finished runs by time, run state, whether the runs were started manually or by a schedule, job type, run duration and project.  
Parent topic: Jobs"
3557	DataStax Enterprise connection	"To access your data in DataStax Enterprise, create a connection asset for it.  
DataStax Enterprise is a NoSQL and vector database that is built on Apache Cassandra"
3558	DataStax Enterprise connection	"Supported versions
DataStax Enterprise 4.6 and later"
3559	DataStax Enterprise connection	"Create a connection to DataStax Enterprise
To create the connection asset, you need these connection details:  
Hostname or IP address Port number Keyspace Username and password Read consistency: Specifies the number of replicas that must respond to a read request before the data is returned to the client application. Write consistency: Specifies the number of replicas for which the write request must succeed before an acknowledgment is returned to the client application.  
all: Data is returned to the application after all replicas have responded. This setting provides the highest consistency and lowest availability. local_one: Data is returned from the closest replica in the local data center. local_quorum: Data is returned after a quorum of replicas in the same data center as the coordinator node has responded. This setting voids latency of inter -data center communication. local_serial: Data within a data center is read without proposing a new addition or update. Uncommitted transactions within the data center are committed as part of the read. one: Data is returned from the closest replica. This setting provides the highest availability, but increases the likelihood of stale data being read. quorum: (Default). Data is returned after a quorum of replicas has responded from any data center. serial: Data is read without proposing a new addition or update. Uncommitted transactions are committed as part of the read. three: Data is returned from three of the closest replicas. two: Data is returned from two of the closest replicas."
3560	DataStax Enterprise connection	"Create a connection to DataStax Enterprise
all: A write must succeed on all replica nodes in the cluster for that partition key. This setting provides the highest consistency and lowest availability. any: A write must succeed on at least one node. Even if all replica nodes for the given partition key are down, the write can succeed after a hinted handoff has been written. This setting provides the lowest consistency and highest availability. each_quorum: A write must succeed on a quorum of replica nodes across a data center. local_one: A write must succeed on at least one replica node in the local data center. local_quorum: A write must succeed on a quorum of replica nodes in the same data center as the coordinator node. This setting voids latency of inter -data center communication. local_serial: The driver prevents unconditional updates to achieve linearizable consistency for lightweight transactions within the data center. one: A write must succeed on at least one replica node. quorum: (Default). A write must succeed on a quorum of replica nodes. serial: The driver prevents unconditional updates to achieve linearizable consistency for lightweight transactions. three: A write must succeed on at least three replica nodes. two: A write must succeed on at least two replica nodes.  
   
SSL certificate (if required by the database server)  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3561	DataStax Enterprise connection	"Create a connection to DataStax Enterprise
Next step: Add data assets from the connection
See Add data from a connection in a project."
3562	DataStax Enterprise connection	"Where you can use this connection
You can use DataStax Enterprise connections in the following workspaces and tools:  
Projects  
Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3563	DataStax Enterprise connection	"DataStax Enterprise setup
Getting started with DataStax Enterprise"
3564	DataStax Enterprise connection	"Learn more
DataStax Enterprise DataStax Enterprise documentation  
Related connection: Apache Cassandra connection  
Parent topic: Supported connections"
3565	Elasticsearch connection	"To access your data in Elasticsearch, create a connection asset for it.  
Elasticsearch is a distributed, open source search and analytics engine. Use the Elasticsearch connection to access JSON documents in Elasticsearch indexes."
3566	Elasticsearch connection	"Supported versions
Elasticsearch version 6.0 or later"
3567	Elasticsearch connection	"Create a connection to Elasticsearch
To create the connection asset, you need these connection details:  
Username and password (Optional) Anonymous access SSL certificate (if required by the database server)  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3568	Elasticsearch connection	"Create a connection to Elasticsearch
Next step: Add data assets from the connection
See Add data from a connection in a project."
3569	Elasticsearch connection	"Where you can use this connection
You can use Elasticsearch connections in the following workspaces and tools:  
Projects  
Data Refinery SPSS Modeler  
Catalogs  
Platform assets catalog"
3570	Elasticsearch connection	"Elasticsearch setup
Set up Elasticsearch"
3571	Elasticsearch connection	"Restrictions
For Elasticsearch versions earlier than version 7, read is limited to 10,000 rows. For Data Refinery, the only supported action on the target file is to append all the rows of the Data Refinery flow output to the existing data set."
3572	Elasticsearch connection	"Restrictions
Running SQL statements
To ensure that your SQL statements run correctly, refer to the Elasticsearch Guide for SQL for the correct syntax."
3573	Elasticsearch connection	"Learn more
Elasticsearch Elastic Docs  
Parent topic: Supported connections"
3574	Creating SPSS Modeler jobs	"You can create a job to run an SPSS Modeler flow.  
To create an SPSS Modeler job:  
In SPSS Modeler, click the Create a job icon from the toolbar and select Create a job. A wizard will appear. Click Next to proceed through each page of the wizard as described here.  
Define the job details by entering a name and a description (optional). If desired, you can also specify retention settings for the job. Select Job run retention settings to set how long to retain finished job runs and job run artifacts such as logs. You can select one of the following retention methods. Be mindful when changing the default as too many job run files can quickly use up project storage.  
By duration (days). Specify the number of days to retain job runs and job artifacts. The retention value is set to 7 days by default (the last 7 days of job runs retained). By amount. Specify the last number of finished job runs and job artifacts to keep. The retention value is set to 200 jobs by default.  
On the Flow parameters page, you can set values for flow parameters if any exist for the flow. They are, in effect, user-defined variables that are saved and persisted with the flow. Parameters are often used in scripting to control the behavior of the script by providing information about fields and values that don't need to be hard coded in the script. See Setting properties for flows for more information.  
For example, your flow might contain a parameter called age_param that you choose to set to 40 here, and a parameter called bp_param you might set to HIGH.  
On the Configuration page, you can choose whether the job will run the entire flow or one or more branches of the flow.  
On the Schedule page, you can optionally add a one-time or repeating schedule.  
If you define a start day and time without selecting Repeat, the job will run exactly one time at the specified day and time. If you define a start date and time and you select Repeat, the job will run for the first time at the timestamp indicated in the Repeat section.  
You can't change the time zone; you must set your job schedule in your web browser's time zone. The schedule will be translated to the time zone of the compute node where your job runs."
3575	Creating SPSS Modeler jobs	"You can't change the time zone; you must set your job schedule in your web browser's time zone. The schedule will be translated to the time zone of the compute node where your job runs.  
If you exclude certain weekdays, the job might not run as you would expect. The reason might be due to a discrepancy between the time zone of the user who creates the schedule, and the time zone of the compute node where the job runs.  
Note: Your scheduled job can appear differently if your web browser’s time zone is set to your local time zone following Daylight Savings Time (DST). For example, your scheduled job appears at 3:00PM Eastern Standard Time Zone (EST) daily corresponding to 8:00PM Coordinated Universal Time (UTC). When your local time zone changes to Eastern Daylight Time Zone (EDT), your scheduled job continues to run at 8:00PM (UTC) which will now appear as 4:00PM (EDT) daily.  
Optionally turn on notifications for the job. You can select the type of alerts to receive.  
Review the job settings. Click Save to create the job.  
The SPSS Modeler job is listed under Jobs in your project."
3576	Creating SPSS Modeler jobs	"Learn more
Viewing job details SPSS Modeler documentation  
Parent topic: Jobs"
3577	Adding connections to data sources in a project	"You need to create a connection asset for a data source before you can access or load data to or from it. A connection asset contains the information necessary to establish a connection to a data source.  
Create connections to multiple types of data sources, including IBM Cloud services, other cloud services, on-prem databases, and more.  
See Connectors for the list of data sources.  
To create a new connection in a project:  
Go to the project page, and click the Assets tab.  
Click New asset > Connect to a data source.  
Choose the kind of connection:  
Select New connection (the default) to create a new connection in the project. Select Platform connections to select a connection that has already been created at the platform level. Select Deployed services to connect to a data source from a cloud service this is integrated with IBM watsonx.  
Choose a data source.  
Enter the connection information that is required for the data source. Typically, you need to provide information like the hostname, port number, username, and password. If you enter an asset name that is the same as an existing asset with the same data source type, then the existing asset is updated. If the data source type is not the same, then the existing connection asset is deleted and a new one is created.  
If prompted, specify whether you want to use personal or shared credentials. You cannot change this option after you create the connection. The credentials type for the connection, either Personal or Shared, is set by the account owner on the Account page. The default setting is Shared.  
Personal: With personal credentials, each user must specify their own credentials to access the connection. Each user's credentials are saved but are not shared with any other users. Use personal credentials instead of shared credentials to protect credentials. For example, if you use personal credentials and another user changes the connection properties (such as the hostname or port number), the credentials are invalidated to prevent malicious redirection. Shared: With shared credentials, all users access the connection with the credentials that you provide. Shared credentials can potentially be retrieved by a user who has access to the connection asset. Because the credentials are shared, it is difficult to audit access to the connection, to identify the source of data loss, or identify the source of a security breach."
3578	Adding connections to data sources in a project	"For Private connectivity: To connect to a database that is not externalized to the internet (for example, behind a firewall), see Securing connections.  
If available, click Test connection.  
Click Create. The connection appears on the Assets page. You can edit the connection by clicking the connection name on the Assets page.  
Add tables, files, or other types of data from the connection by creating a connected data asset.  
Connections with personal credentials are marked with a key icon on the Assets page and are locked. If you are authorized to access the connection, you can unlock it by entering your credentials the first time you select it. This is a one-time step that permanently unlocks the connection for you. After you unlock the connection, the key icon is no longer displayed. Connections with personal credentials are already unlocked if you created the connections yourself.  
Watch this video to see how to create a connection and add connected data to a project.  
Video disclaimer: Some minor steps and graphical elements in this video might differ from your platform.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Transcript  
Synchronize transcript with video"
3579	Adding connections to data sources in a project	"Video disclaimer: Some minor steps and graphical elements in this video might differ from your platform.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
Transcript  
Synchronize transcript with video  
Video transcript Time Transcript 00:00 This video shows you how to set up a connection to a data source and add connected data to a Watson Studio project. 00:08 If you have data stored in a data source, you can set up a connection to that data source from any project. 00:16 From here, you can add different elements to the project. 00:20 In this case, you want to add a connection. 00:24 You can create a new connection to an IBM service, such as IBM Db2 and Cloud Object Storage, or to a service from third parties, such as Amazon, Microsoft or Apache. 00:39 And you can filter the list based on compatible services. 00:45 You can also add a connection that was created at the platform level, which can be used across projects and catalogs. 00:54 Or you can create a connection to one of your provisioned IBM Cloud services. 00:59 In this case, select the provisioned IBM Cloud service for Db2 Warehouse on Cloud. 01:08 If the credentials are not prepopulated, you can get the credentials for the instance from the IBM Cloud service launch page. 01:17 First, test the connection and then create the connection. 01:25 The new connection now displays in the list of data assets. 01:30 Next, add connected data assets to this project. 01:37 Select the source - in this case, it's the Db2 Warehouse on Cloud connection just created. 01:43 Then select the schema and table. 01:50 You can see that this will add a reference to the data within this connection and include it in the target project. 01:58 Provide a name and a description and click ""Create"". 02:06 The data now displays in the list of data assets. 02:09 Open the data set to get a preview; and from here you can move directly into refining the data. 02:17 Find more videos in the Cloud Pak for Data as a Service documentation."
3580	Adding connections to data sources in a project	"Next step
Go to Data Refinery, and select the connection. Drill down to a schema, and table or view."
3581	Adding connections to data sources in a project	"Learn more
Adding platform connections Integrations with other cloud platforms Controlling access to Cloud Object Storage buckets  
Parent topic: Adding data to a project"
3582	MongoDB connection	"To access your data in MongoDB, create a connection asset for it.  
MongoDB is a distributed database that stores data in JSON-like documents."
3583	MongoDB connection	"Supported editions and versions
MongoDB editions
MongoDB Community IBM Cloud Databases for MongoDB. See IBM Cloud Databases for MongoDB connection for this data source. MongoDB Atlas WiredTiger Storage Engine"
3584	MongoDB connection	"Supported editions and versions
MongoDB versions
MongoDB 3.6 and later, 4.x, 5.x, and 6.x Microsoft Azure Cosmos DB for MongoDB 3.6 and later, 4.x"
3585	MongoDB connection	"Create a connection to MongoDB
To create the connection asset, you need these connection details:  
Database name Hostname or IP address Port number Authentication database: The name of the database in which the user was created. Username and password SSL certificate (if required by the database server)  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3586	MongoDB connection	"Create a connection to MongoDB
Next step: Add data assets from the connection
See Add data from a connection in a project."
3587	MongoDB connection	"Where you can use this connection
You can use MongoDB connections in the following workspaces and tools:  
Projects  
Data Refinery Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3588	MongoDB connection	"MongoDB setup
MongoDB installation"
3589	MongoDB connection	"Restrictions
You can only use this connection for source data. You cannot write to data or export data with this connection. MongoDB Query Language (MQL) is not supported."
3590	MongoDB connection	"Learn more
MongoDB tutorials mongodb.com  
Related connection: IBM Cloud Databases for MongoDB connection  
Parent topic: Supported connections"
3591	IBM Db2 Big SQL connection	"To access your data in IBM Db2 Big SQL, create a connection asset for it.  
IBM Db2 Big SQL is a high performance massively parallel processing (MPP) SQL engine for Hadoop that makes querying enterprise data from across the organization an easy and secure experience."
3592	IBM Db2 Big SQL connection	"Supported versions
Db2 Big SQL for Version 4.1+"
3593	IBM Db2 Big SQL connection	"Create a connection to IBM Db2 Big SQL
To create the connection asset, you need these connection details:  
Database name Hostname or IP address Port number Username and password SSL certificate (if required by the database server)  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3594	IBM Db2 Big SQL connection	"Create a connection to IBM Db2 Big SQL
Next step: Add data assets from the connection
See Add data from a connection in a project."
3595	IBM Db2 Big SQL connection	"Where you can use this connection
You can use IBM Db2 Big SQL connections in the following workspaces and tools:  
Projects  
Data Refinery Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3596	IBM Db2 Big SQL connection	"IBM Db2 Big SQL setup
Installing IBM Db2 Big SQL"
3597	IBM Db2 Big SQL connection	"IBM Db2 Big SQL setup
Running SQL statements
To ensure that your SQL statements run correctly, refer to the IBM Db2 Big SQL documentation for the correct syntax."
3598	IBM Db2 Big SQL connection	"Learn more
Db2 Big SQL documentation  
Parent topic: Supported connections"
3599	Switching the platform for a project	"You can switch the platform for some projects between the Cloud Pak for Data as a Service and the watsonx platform. When you switch the platform for a project, you can use the tools that are specific to that platform.  
For example, you might switch an existing Cloud Pak for Data as a Service project to watsonx so that you can use the Prompt Lab tool and create prompt and prompt session assets. See Comparison between watsonx and Cloud Pak for Data as a Service.  
Important:  
Foundation model inferencing with the Prompt Lab is available in the Dallas and Frankfurt regions. Your Watson Studio and Watson Machine Learning service instances are shared between watsonx and Cloud Pak for Data as a Service. If your Watson Studio and Watson Machine Learning service instances are provisioned in another region, you can't use foundation model inferencing or the Prompt Lab.  
Requirements Restrictions What happens when you switch a project Switch the platform for a project Switching multiple projects to watsonx"
3600	Switching the platform for a project	"Requirements
You can switch a project from one platform to the other if you have the required accounts and permissions.  
Watson Studio  
Watson Machine Learning"
3601	Switching the platform for a project	"Restrictions
To switch a project from Cloud Pak for Data as a Service to watsonx, all the assets in the project must have asset types that are supported by both platforms.  
Projects that contain any of the following asset types, but no other types of assets, are eligible to switch from Cloud Pak for Data as a Service to watsonx:  
AutoAI experiment COBOL copybook Connected data asset Connection Data asset from a file Data Refinery flow Decision Optimization experiment Federated Learning experiment Folder asset Jupyter notebook Model Python function Script SPSS Modeler flow Visualization  
You can’t switch a project that contains assets that are specific to Cloud Pak for Data as a Service. If you add any assets that you created with services other than Watson Studio and Watson Machine Learning to a project, you can't switch that project to watsonx. Although Pipelines assets are supported in both Cloud Pak for Data as a Service and watsonx projects, you can't switch a project that contains pipeline assets because pipelines can reference unsupported assets.  
You can switch a project that contains assets from watsonx to Cloud Pak for Data as a Service. However, assets that are only supported in watsonx are not available on Cloud Pak for Data as a Service. These assets include:  
Prompt Lab assets Synthetic data flows  
For more information about asset types, see Asset types and properties."
3602	Switching the platform for a project	"What happens when you switch the platform for a project
Switching a project between platforms has the following effects:"
3603	Switching the platform for a project	"Switch the platform for a project
You can switch the platform for a project from within the project on the original platform. You can switch between either Cloud Pak for Data as a Service and watsonx.  
To switch the platform for a project:  
On the original platform, go to the project's Manage tab, select the General page, and in the Controls section, click Switch platform. If you don't see a Switch platform button or the button is not active, you can't switch the project. Select the destination platform and click Switch platform."
3604	Switching the platform for a project	"Switching multiple projects to watsonx
You can switch one or more eligible projects to watsonx from Cloud Pak for Data as a Service from the watsonx home page.  
On the watsonx home page, click the Switch projects icon (). Select the projects that you want to switch. Only the projects that meet the requirements are listed. Optional. You can view the projects that contain unsupported asset types and the projects for which you don't have the Admin role. Click the Switch projects icon."
3605	Switching the platform for a project	"Learn more
Comparison between watsonx and Cloud Pak for Data as a Service Asset types and properties Switching the platform for a deployment space  
Parent topic: Administering projects"
3606	Adding task credentials	"A task credential is a form of user authentication that is required by some services to perform operations in projects and spaces, for example to run certain tasks in a service or to enable the execution of long operations such as scheduled jobs without interruption.  
In IBM watsonx, IBM Cloud API keys are used as task credentials. You can either provide an existing IBM Cloud API key, or you can generate a new key. Only one task credential can be stored per user, per IBM Cloud account, and is stored securely in a vault.  
You can generate and rotate API keys in Profile and settings > User API key.  
Any user with an IBM Cloud account can create an API key. The API key can be seen as a type of user name and password, enabling access to resources in your IBM Cloud account and should never be shared.  
If your service requires a task credential to perform an operation, you are prompted to provide it in the form of an existing or newly generated API key.  
Note that service administrators are responsible for defining a strategy to revoke task credentials when these are no longer required."
3607	Adding task credentials	"Learn more
Managing the user API key Understanding API keys  
Parent topic: Administering projects"
3608	IBM Informix connection	"To access your data in an IBM Informix database, create a connection asset for it.  
IBM Informix is a database that contains relational, object-relational, or dimensional data. You can use the Informix connection to access data from an on-prem Informix database server or from IBM Informix on Cloud."
3609	IBM Informix connection	"Supported Informix versions (on-prem)
Informix 14.10 and later. This version does not support the Progress DataDirect JDBC driver, which is used by the Informix connection. The Informix connection supports Informix 14.10 features that are comparable to previous Informix versions, but not the new features. Issues related to DataDirect's JDBC driver are not supported. Informix 12.10 and later Informix 11.0 and later Informix 10.0 and later Informix 9.2 and later"
3610	IBM Informix connection	"Create a connection to Informix
To create the connection asset, you need these connection details:  
Name of the database server Name of the database Hostname or IP address of the database Port number (Default is 1526) Username and password  
On-prem Informix database servers: For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3611	IBM Informix connection	"Create a connection to Informix
Next step: Add data assets from the connection
See Add data from a connection in a project."
3612	IBM Informix connection	"Where you can use this connection
You can use Informix connections in the following workspaces and tools:  
Projects  
Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3613	IBM Informix connection	"Informix setup
To set up Informix, see these topics:  
Informix on-prem: Creating a database server after installation Informix on Cloud: Getting started with Informix on Cloud"
3614	IBM Informix connection	"Informix setup
Running SQL statements
To ensure that your SQL statements run correctly, refer to the Guide to SQL: Syntax in the product documentation for the correct syntax."
3615	IBM Informix connection	"Learn more
Informix product documentation (on-prem) IBM Informix on Cloud IBM Informix on Cloud FAQ  
Parent topic: Supported connections"
3616	Microsoft Azure Blob Storage connection	"To access your data in Microsoft Azure Blob Storage, create a connection asset for it.  
Azure Blob Storage is used for storing large amounts of data in the cloud."
3617	Microsoft Azure Blob Storage connection	"Create a connection to Microsoft Azure Blob Storage
To create the connection asset, choose an authentication method.  
Connection string: Connection string from the storage account's Access keys page on the Microsoft Azure portal.  
Example connection string, which you can find in the ApiKeys section of the container:  
DefaultEndpointsProtocol=https;AccountName=sampleaccount;AccountKey=samplekey;EndpointSuffix=core.windows.net"
3618	Microsoft Azure Blob Storage connection	"Other properties
Container: The name of the container that contains the files to access."
3619	Microsoft Azure Blob Storage connection	"Other properties
Next step: Add data assets from the connection
See Add data from a connection in a project."
3620	Microsoft Azure Blob Storage connection	"Where you can use this connection
You can use Azure Blob Storage connections in the following workspaces and tools:  
Projects  
Data Refinery SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3621	Microsoft Azure Blob Storage connection	"Supported file types
The Azure Blob Storage connection supports these file types:  Avro, CSV, Delimited text, Excel, JSON, ORC, Parquet, SAS, SAV, SHP, and XML."
3622	Microsoft Azure Blob Storage connection	"Learn more
Microsoft Azure Storage account overview Quickstart: Upload, download, and list blobs with the Azure portal Manage storage account access keys  
Parent topic: Supported connections"
3623	IBM Planning Analytics connection	"To access your data in Planning Analytics, create a connection asset for it.  
Planning Analytics (formerly known as ""TM1"") is an enterprise performance management database that stores data in in-memory multidimensional OLAP cubes."
3624	IBM Planning Analytics connection	"Supported versions
IBM Planning Analytics, version 2.0.5 or later"
3625	IBM Planning Analytics connection	"Create a connection to Planning Analytics
To create the connection asset, you need these connection details:  
TM1 server API root URL Authentication type (Basic or CAM Credentials) Username and password SSL certificate (if required by the database server)  
For authentication setup information, see Authenticating and managing sessions."
3626	IBM Planning Analytics connection	"Create a connection to Planning Analytics
Next step: Add data assets from the connection
See Add data from a connection in a project."
3627	IBM Planning Analytics connection	"Where you can use this connection
You can use Planning Analytics connections in the following workspaces and tools:  
Projects  
Data Refinery Decision Optimization experiments Notebooks. Click Read data on the Code snippets pane to get the connection credentials and load the data into a data structure. See Load data from data source connections. SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3628	IBM Planning Analytics connection	"Planning Analytics setup
Enable TM1 REST APIs on the TM1 Server. See TMI REST API Installation and configuration."
3629	IBM Planning Analytics connection	"Cube dimension order
Versions earlier than TM1 11.4 For best performance, do not combine string and numeric data in a single cube. However, if the cube does include both string and numeric data, the string elements must be in the last dimension when the cube is created. Reordering dimensions later is ignored.  
Version TM1 11.4 or later The default setting in Planning Analytics for cube creation is current. This setting might cause errors or unexpected results when you use the Planning Analytics connection. Instead, set the interaction property use_creation_order value to true."
3630	IBM Planning Analytics connection	"Restriction
For Data Refinery, you can use this connection only as a source. You cannot use this connection as a target connection or as a target connected data asset."
3631	IBM Planning Analytics connection	"Learn more
Planning Analytics product documentation  
Parent topic: Supported connections"
3632	Administering a project	"If you have the Admin role in a project, you can perform administrative tasks for the project.  
Manage collaborators Mark data assets in project as sensitive Stop all active runtimes Export a project Manage project access tokens Remove assets Edit a locked asset Delete the project Copy a project Switch the platform for a project  
Note: In the activity log, the user ID for some activities might display `icp4d-dev` instead of `admin`."
3633	Administering a project	"Delete a project
If you have the Admin role in a project, you can delete it. All project assets, associated files in the project's storage, and the associated storage for the project are also deleted. Data in a remote data source that is accessed through a connection is not affected.  
To delete a project, choose Project > View All Projects and then choose Delete from the ACTIONS menu next to the project name."
3634	Administering a project	"Copy a project
You can copy an existing project by exporting it, and then importing it with a different name.  
Parent topic: Projects"
3635	IBM Cloud Data Engine connection	"To access your data in IBM Cloud Data Engine, create a connection asset for it.  
Important: The IBM Cloud Data Engine connector is deprecated and will be discontinued in a future release.  
IBM Cloud Data Engine is a service on IBM Cloud that you use to build, manage, and consume data lakes and their table assets in IBM Cloud Object Storage (COS). IBM Cloud Data Engine provides functions to load, prepare, and query big data that is stored in various formats. It also includes a metastore with table definitions. IBM Cloud Data Engine was formerly named ""IBM Cloud SQL Query."""
3636	IBM Cloud Data Engine connection	"Create a connection to IBM Cloud Data Engine
To create the connection asset, you need these connection details:  
The Cloud Resource Name (CRN) of the IBM Cloud Data Engine instance. Go to the IBM Cloud Data Engine service instance in your resources list in your IBM Cloud dashboard and copy the value of the CRN from the deployment details. Target Cloud Object Storage: A default location where IBM Cloud Data Engine stores query results. You can specify any Cloud Object Storage bucket that you have access to. You can also select the default Cloud Object Storage bucket that is created when you open the IBM Cloud Data Engine web console for the first time from IBM Cloud dashboard. See the Target location field in the IBM Cloud Data Engine web console. IBM Cloud API key: An API key for a user or service ID that has access to your IBM Cloud Data Engine and Cloud Object Storage services (for both the Cloud Object Storage data that you want to query and the default target Cloud Object Storage location).  
You can create a new API key for your own user:  
In the IBM Cloud console, go to Manage > Access (IAM). In the left navigation, select API keys. Select Create an IBM Cloud API Key."
3637	IBM Cloud Data Engine connection	"Create a connection to IBM Cloud Data Engine
Credentials
IBM Cloud Data Engine uses the SSO credentials that are specified as a single API key, which authenticates a user or service ID. The API key must have the following properties:  
Manage permission for the IBM Cloud Data Engine instance Read access to all Cloud Object Storage locations that you want to read from Write access to the default Cloud Object Storage target location Write access to the IBM Cloud Data Engine instance"
3638	IBM Cloud Data Engine connection	"Create a connection to IBM Cloud Data Engine
Next step: Add data assets from the connection
See Add data from a connection in a project."
3639	IBM Cloud Data Engine connection	"Where you can use this connection
You can use IBM Cloud Data Engine connections in the following workspaces and tools:  
Projects  
Data Refinery Notebooks. See the Notebook tutorial for using the IBM Cloud Data Engine (SQL Query) API to run SQL statements. SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3640	IBM Cloud Data Engine connection	"Restrictions
You can only use this connection for source data. You cannot write to data or export data with this connection."
3641	IBM Cloud Data Engine connection	"IBM Cloud Data Engine setup
To set up IBM Cloud Data Engine on IBM Cloud Object Storage, see Getting started with IBM Cloud Data Engine."
3642	IBM Cloud Data Engine connection	"Supported encryption
By default, all objects that are stored in IBM Cloud Object Storage are encrypted by using randomly generated keys and an all-or-nothing-transform (AONT). For details, see Encrypting your data. Additionally, you can use managed keys to encrypt the SQL query texts and error messages that are stored in the job information. See Encrypting SQL queries with Key Protect."
3643	IBM Cloud Data Engine connection	"Supported encryption
Running SQL statements
Video to learn how you can get started to run a basic query"
3644	IBM Cloud Data Engine connection	"Learn more
IBM Cloud Data Engine Connecting to a Cloud Data Lake with IBM Cloud Pak for Data  
Parent topic: Supported connections"
3645	Apache Derby connection	"To access your data in Apache Derby, create a connection asset for it.  
Apache Derby is a relational database management system developed by the Apache Software Foundation."
3646	Apache Derby connection	"Create a connection to Apache Derby
To create the connection asset, you need these connection details:  
Database name Hostname or IP address Port number Username and password SSL certificate (if required by the database server)  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3647	Apache Derby connection	"Create a connection to Apache Derby
Next step: Add data assets from the connection
See Add data from a connection in a project."
3648	Apache Derby connection	"Where you can use this connection
You can use Apache Derby connections in the following workspaces and tools:  
Projects  
Data Refinery SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3649	Apache Derby connection	"Apache Derby setup
Apache Derby installation"
3650	Apache Derby connection	"Apache Derby setup
Running SQL statements
To ensure that your SQL statements run correctly, refer to the Apache Derby documentation for the correct syntax."
3651	Apache Derby connection	"Learn more
Apache Derby documentation  
Parent topic: Supported connections"
3652	IBM Db2 for z/OS connection	"To access your data in IBM Db2 for z/OS, create a connection asset for it.  
Db2 for z/OS is an enterprise data server for IBM Z. It manages core business data across an enterprise and supports key business applications."
3653	IBM Db2 for z/OS connection	"Supported versions
IBM Db2 for z/OS version 11 and later"
3654	IBM Db2 for z/OS connection	"Prerequisites
Obtain the certificate file
A certificate file on the Db2 for z/OS server is required to use this connection.These steps must be done on the Db2 for z/OS server: Obtain an IBM Db2 Connect Unlimited Edition license certificate file from IBM Db2 Connect: Pricing and Installing the IBM Data Server Driver for JDBC and SQLJ. For installation instructions, see Activating the license certificate file for Db2 Connect Unlimited Edition."
3655	IBM Db2 for z/OS connection	"Prerequisites
Run the bind command
Run the following commands from the Db2 client that is configured to access the Db2 for z/OS server. You need to run the bind command only once per remote database per Db2 client version.  
For information about bind commands, see Binding applications and utilities (Db2 Connect Server)."
3656	IBM Db2 for z/OS connection	"Prerequisites
Run catalog commands
Run the following catalog commands from the Db2 client that is configured to access the Db2 for z/OS server:  
db2 catalog tcpip node node_name remote hostname_or_address server port_no_or_service_name db2 catalog dcs database local_name as real_db_name db2 catalog database local_name as alias at node node_name authentication server  
Example: db2 catalog tcpip node db2z123 remote 192.0.2.0 server 446  
Example: db2 catalog dcs database db2z123 as db2z123  
Example: db2 catalog database db2z123 as db2z123 at node db2z123 authentication server  
For information about catalog commands, see CATALOG TCPIP NODE and CATALOG DCS DATABASE."
3657	IBM Db2 for z/OS connection	"Create a connection to Db2 for z/OS
To create the connection asset, you need these connection details:  
Hostname or IP address Port number Collection ID: The ID of the collections of packages to use Location: The unique name of the Db2 location you want to access Username and password Application name (optional): The name of the application that is currently using the connection. For information, see Client info properties support by the IBM Data Server Driver for JDBC and SQLJ. Client accounting information (optional): The value of the accounting string from the client information that is specified for the connection. For information, see Client info properties support by the IBM Data Server Driver for JDBC and SQLJ. Client hostname (optional): The hostname of the machine on which the application that is using the connection is running. For information, see Client info properties support by the IBM Data Server Driver for JDBC and SQLJ. Client user (optional): The name of the user on whose behalf the application that is using the connection is running. For information, see Client info properties support by the IBM Data Server Driver for JDBC and SQLJ. SSL certificate (if required by the database server)  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3658	IBM Db2 for z/OS connection	"Create a connection to Db2 for z/OS
Next step: Add data assets from the connection
See Add data from a connection in a project."
3659	IBM Db2 for z/OS connection	"Where you can use this connection
You can use Db2 for z/OS connections in the following workspaces and tools:  
Projects  
Notebooks. Click Read data on the Code snippets pane to get the connection credentials and load the data into a data structure. See Load data from data source connections. Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3660	IBM Db2 for z/OS connection	"Restriction
For SPSS Modeler, you can use this connection only to import data. You cannot export data to this connection or to a Db2 for z/OS connected data asset."
3661	IBM Db2 for z/OS connection	"Restriction
Running SQL statements
To ensure that your SQL statements run correctly, refer to the Db2 for z/OS and SQL concepts for the correct syntax."
3662	IBM Db2 for z/OS connection	"Learn more
IBM Db2 for z/OS documentation  
Parent topic: Supported connections"
3663	Google BigQuery connection	"To access your data in Google BigQuery, create a connection asset for it.  
Google BigQuery is a fully managed, serverless data warehouse that enables scalable analysis over petabytes of data."
3664	Google BigQuery connection	"Create a connection to Google BigQuery
To create the connection asset, choose an authentication method. Choices include an authentication with or without workload identity federation.  
Without workload identity federation  
Account key (full JSON snippet): The contents of the Google service account key JSON file Client ID, Client secret, Access token, and Refresh token  
With workload identity federation You use an external identity provider (IdP) for authentication. An external identity provider uses Identity and Access Management (IAM) instead of service account keys. IAM provides increased security and centralized management. You can use workload identity federation authentication with an access token or with a token URL.  
You can configure a Google BigQuery connection for workload identity federation with any identity provider that complies with the OpenID Connect (OIDC) specification and that satisfies the Google Cloud requirements that are described in Prepare your external IdP. The requirements include:  
The identity provider must support OpenID Connect 1.0. The identity provider's OIDC metadata and JWKS endpoints must be publicly accessible over the internet. Google Cloud uses these endpoints to download your identity provider's key set and uses that key set to validate tokens. The identity provider is configured so that your workload can obtain ID tokens that meet these criteria:  
Tokens are signed with the RS256 or ES256 algorithm. Tokens contain an aud claim.  
For examples of the workload identity federation configuration steps for Amazon Web Services (AWS) and Microsoft Azure, see Workload identity federation examples.  
//iam.googleapis.com/projects/PROJECT_NUMBER/locations/global/workloadIdentityPools/POOL_ID/providers/PROVIDER_ID  
Access token: An access token from the identity provider to connect to BigQuery.  
Security Token Service audience: The security token service audience that contains the project ID, pool ID, and provider ID. Use this format:  
For more information, see Authenticate a workload by using the REST API.  
Service account email: The email address of the Google service account to be impersonated. For more information, see Create a service account for the external workload.  
Service account token lifetime (optional): The lifetime in seconds of the service account access token. The default lifetime of a service account access token is one hour. For more information, see URL-sourced credentials."
3665	Google BigQuery connection	"Create a connection to Google BigQuery
Service account token lifetime (optional): The lifetime in seconds of the service account access token. The default lifetime of a service account access token is one hour. For more information, see URL-sourced credentials.  
Token format: Text or JSON with the Token field name for the name of the field in the JSON response that contains the token.  
Token field name: The name of the field in the JSON response that contains the token. This field appears only when the Token format is JSON.  
Token type: AWS Signature Version 4 request, Google OAuth 2.0 access token, ID token, JSON Web Token (JWT), or SAML 2.0.  
//iam.googleapis.com/projects/PROJECT_NUMBER/locations/global/workloadIdentityPools/POOL_ID/providers/PROVIDER_ID  
Security Token Service audience: The security token service audience that contains the project ID, pool ID, and provider ID. Use this format:  
For more information, see Authenticate a workload using the REST API.  
Service account email: The email address of the Google service account to be impersonated. For more information, see Create a service account for the external workload.  
Service account token lifetime (optional): The lifetime in seconds of the service account access token. The default lifetime of a service account access token is one hour. For more information, see URL-sourced credentials.  
Token URL: The URL to retrieve a token.  
HTTP method: HTTP method to use for the token URL request: GET, POST, or PUT.  
Request body (for POST or PUT methods): The body of the HTTP request to retrieve a token.  
HTTP headers: HTTP headers for the token URL request in JSON or as a JSON body. Use format: ""Key1""=""Value1"",""Key2""=""Value2"".  
Token format: Text or JSON with the Token field name for the name of the field in the JSON response that contains the token.  
Token field name: The name of the field in the JSON response that contains the token. This field appears only when the Token format is JSON.  
Token type: AWS Signature Version 4 request, Google OAuth 2.0 access token, ID token, JSON Web Token (JWT), or SAML 2.0."
3666	Google BigQuery connection	"Create a connection to Google BigQuery
Other properties
Project ID (optional) The ID of the Google project.  
Output JSON string format: JSON string format for output values that are complex data types (for example, nested or repeated).  
Pretty: Values are formatted before sending them to output. Use this option to visually read a few rows. Raw: (Default) No formatting. Use this option for the best performance."
3667	Google BigQuery connection	"Create a connection to Google BigQuery
Permissions
The connection to Google BigQuery requires the following BigQuery permissions:  
bigquery.job.create bigquery.tables.get bigquery.tables.getData  
Use one of three ways to gain these permissions:  
Use the predefined BigQuery Cloud IAM role bigquery.admin, which includes these permissions; Use a combination of two roles, one from each column in the following table; or Create a custom role. See Create and manage custom roles.  
For more information about permissions and roles in Google BigQuery, see Predefined roles and permissions."
3668	Google BigQuery connection	"Create a connection to Google BigQuery
Next step: Add data assets from the connection
See Add data from a connection in a project."
3669	Google BigQuery connection	"Where you can use this connection
You can use Google BigQuery connections in the following workspaces and tools:  
Projects  
Data Refinery SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3670	Google BigQuery connection	"Google BigQuery setup
Quickstart by using the Cloud Console"
3671	Google BigQuery connection	"Learn more
Google BigQuery documentation Workload identity federation examples  
Parent topic: Supported connections"
3672	Amazon S3 connection	"To access your data in Amazon S3, create a connection asset for it.  
Amazon S3 (Amazon Simple Storage Service) is a service that is offered by Amazon Web Services (AWS) that provides object storage through a web service interface.  
For other types of S3-compliant connections, you can use the Generic S3 connection."
3673	Amazon S3 connection	"Create a connection to Amazon S3
To create the connection asset, you need these connection details:  
Bucket: Bucket name that contains the files. If your AWS credentials have permissions to list buckets and access all buckets, then you only need to supply the credentials. If your credentials don't have the privilege to list buckets and can only access a particular bucket, then you need to specify the bucket. Endpoint URL: Use for an AWS GovCloud instance. Include the region code. For example, https://s3.<region-code>.amazonaws.com. For the list of region codes, see AWS service endpoints. Region: Amazon Web Services (AWS) region. If you specify an Endpoint URL that is not for the AWS default region (us-west-2), then you should also enter a value for Region.  
Select Server proxy to access the Amazon S3 data source through a proxy server. Depending on its setup, a proxy server can provide load balancing, increased security, and privacy. The proxy server settings are independent of the authentication credentials and the personal or shared credentials selection.  
Proxy host: The proxy URL. For example, https://proxy.example.com. Proxy port number: The port number to connect to the proxy server. For example, 8080 or 8443. The Proxy username and Proxy password fields are optional."
3674	Amazon S3 connection	"Create a connection to Amazon S3
Credentials
The combination of Access key and Secret key is the minimum credentials.  
If the Amazon S3 account owner has set up temporary credentials or a Role ARN (Amazon Resource Name), enter the values provided by the Amazon S3 account owner for the applicable authentication combination:  
Access key, Secret key, and Session token Access key, Secret key, Role ARN, Role session name, and optional Duration seconds Access key, Secret key, Role ARN, Role session name, External ID, and optional Duration seconds  
For setup instructions for the Amazon S3 account owner, see Setting up temporary credentials or a Role ARN for Amazon S3."
3675	Amazon S3 connection	"Create a connection to Amazon S3
Next step: Add data assets from the connection
See Add data from a connection in a project."
3676	Amazon S3 connection	"Where you can use this connection
You can use Amazon S3 connections in the following workspaces and tools:  
Projects  
Data Refinery Decision Optimization Notebooks. Click Read data on the Code snippets pane to get the connection credentials and load the data into a data structure. See Load data from data source connections. SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3677	Amazon S3 connection	"Amazon S3 setup
See the Amazon Simple Storage Service User Guide for the setup steps."
3678	Amazon S3 connection	"Restriction
Folders cannot be named with the slash symbol (/) because the slash symbol is a delimiter for the file structure."
3679	Amazon S3 connection	"Supported file types
The Amazon S3 connection supports these file types:  Avro, CSV, Delimited text, Excel, JSON, ORC, Parquet, SAS, SAV, SHP, and XML."
3680	Amazon S3 connection	"Table formats
In addition to Flat file, the Amazon S3 connection supports these Data Lake table formats: Delta Lake and Iceberg."
3681	Amazon S3 connection	"Learn more
Amazon S3 documentation  
Related connection: Generic S3 connection  
Parent topic: Supported connections"
3682	Apache HDFS connection	"To access your data in Apache HDFS, create a connection asset for it.  
Apache Hadoop Distributed File System (HDFS) is a distributed file system that is designed to run on commodity hardware. Apache HDFS was formerly Hortonworks HDFS."
3683	Apache HDFS connection	"Create a connection to Apache HDFS
To create the connection asset, you need these connection details. The WebHDFS URL is required. The available properties in the connection form depend on whether you select Connect to Apache Hive so that you can write tables to the Hive data source.  
WebHDFS URL to access HDFS. Hive host: Hostname or IP address of the Apache Hive server. Hive database: The database in Apache Hive. Hive port number: The port number of the Apache Hive server. The default value is 10000. Hive HTTP path: The path of the endpoint such as gateway/default/hive when the server is configured for HTTP transport mode. SSL certificate (if required by the Apache Hive server).  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3684	Apache HDFS connection	"Create a connection to Apache HDFS
Next step: Add data assets from the connection
See Add data from a connection in a project."
3685	Apache HDFS connection	"Where you can use this connection
You can use Apache HDFS connections in the following workspaces and tools:  
Projects  
Data Refinery Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3686	Apache HDFS connection	"Apache HDFS setup
Install and set up a Hadoop cluster"
3687	Apache HDFS connection	"Supported file types
The Apache HDFS connection supports these file types:  Avro, CSV, Delimited text, Excel, JSON, ORC, Parquet, SAS, SAV, SHP, and XML."
3688	Apache HDFS connection	"Table formats
In addition to Flat file, the Apache HDFS connection supports these Data Lake table formats: Delta Lake and Iceberg."
3689	Apache HDFS connection	"Learn more
Apache HDFS Users Guide  
Parent topic: Supported connections"
3690	Managing assets in projects	"You can manage assets in a project by adding them, editing them, or deleting them.  
Add data assets You can add other types of assets by clicking New asset or Import assets on the project's Assets page. Edit assets Download assets Delete assets"
3691	Managing assets in projects	"Edit assets
You can edit the properties of all types of assets, such as the asset name, description, and tags. See Asset types and properties.  
The role you need to edit an asset depends on the asset type. See Project collaborator roles.  
Click the data asset name to open the asset. For some types of data, you can see an asset preview.  
To edit the data asset properties, such as its name and description, click the corresponding edit icon () on the information pane.  
To add or delete tags, click + (plus sign).  
To create or update a profile of relational data, click the Profile tab.  
To cleanse and shape relational data, click Prepare data to open the data asset in Data Refinery.  
When you change the name of data assets with file attachments that you uploaded into the project, the file attachments are also renamed. You must update any references to the data asset in code-based assets, like notebooks, to the new data asset name, otherwise, the code-based asset won't run.  
Click the connection asset name to edit the connection properties, such as the name, description, and connection details.  
Click the name of the asset on the Assets page to open it in its tool.  
On the Assets page of a project, the lock icon indicates that another collaborator is editing the asset or locked the asset to prevent editing by other users.  
Enabled lock: You can unlock the asset if you locked it or if you have the Admin role in the project. Disabled lock: You can't unlock a locked asset if you didn't lock it and you have the Editor or Viewer role in the project.  
When you unlock an asset that another collaborator is editing, you take control of the asset. The other collaborator is not notified and any changes made by that collaborator are overwritten by your edits."
3692	Managing assets in projects	"Delete an asset from a project
To delete an asset from a project, choose the Delete or the Remove option from the action menu next to the asset on the project Assets page. When you delete an asset, its associated file, if it has one, is also deleted. However, when you delete a connected data asset, the data in the associated data source is not affected.  
Depending on the type of asset, other related assets might also be deleted."
3693	Managing assets in projects	"Learn more
Asset types and properties  
Parent topic: Projects"
3694	Amazon RDS for Oracle connection	"To access your data in Amazon RDS for Oracle, create a connection asset for it.  
Amazon RDS for Oracle is an Oracle relational database that runs on the Amazon Relational Database Service (RDS)."
3695	Amazon RDS for Oracle connection	"Supported Oracle versions and editions
Oracle Database 19c (19.0.0.0) Oracle Database 12c Release 2 (12.2.0.1) Oracle Database 12c Release 1 (12.1.0.2)"
3696	Amazon RDS for Oracle connection	"Create a connection to Amazon RDS for Oracle
To create the connection asset, you'll need these connection details:  
Either the Oracle Service name or the Oracle System ID (SID) for the database. Hostname or IP address of the database Port number of the database. (Default is 1521)  
SSL certificate (if required by the database server)"
3697	Amazon RDS for Oracle connection	"Create a connection to Amazon RDS for Oracle
Next step: Add data assets from the connection
See Add data from a connection in a project."
3698	Amazon RDS for Oracle connection	"Where you can use this connection
Projects  
You can use Amazon RDS for Oracle connections in the following workspaces and tools:  
Data Refinery Decision Optimization SPSS Modeler  
Catalogs  
Platform assets catalog"
3699	Amazon RDS for Oracle connection	"Amazon RDS for Oracle setup
To set up the Oracle database on Amazon, see these topics:  
Creating an Amazon RDS DB Instance Creating an Oracle DB instance and connecting to a database on an Oracle DB instance Connecting to your Oracle DB instance"
3700	Amazon RDS for Oracle connection	"Learn more
Amazon RDS for Oracle  
Parent topic: Supported connections"
3701	HTTP connection	To access your data from a URL, create an HTTP connection asset for it.
3702	HTTP connection	"Supported file
Use the full path in the URL to the file that you want to read. You cannot browse for files."
3703	HTTP connection	"Certificates
Enter the SSL certificate of the host to be trusted. The SSL certificate is needed only when the host certificate is not signed by a known certificate authority.  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3704	HTTP connection	"Certificates
Next step: Add data assets from the connection
See Add data from a connection in a project."
3705	HTTP connection	"Where you can use this connection
You can use HTTP connections in the following workspaces and tools:  
Projects  
Data Refinery Notebooks. Click Read data on the Code snippets pane to get the connection credentials and load the data into a data structure. See Load data from data source connections. SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3706	HTTP connection	"Restriction
You can use this connection only for source data. You cannot write to data or export data with this connection."
3707	HTTP connection	"Supported file types
The HTTP connection supports these file types:  Avro, CSV, Delimited text, Excel, JSON, ORC, Parquet, SAS, SAV, SHP, and XML.  
Parent topic: Supported connections"
3708	Getting and preparing data in a project	"After you create a project, or join one, the next step is to add data to the project and prepare the data for analysis.  
You can add data assets from your local system, from a catalog, from the Resource hub, or from connections to data sources.  
You can add these types of data assets to a project:  
Data assets from files from your local system, including structured data, unstructured data, and images. The files are stored in the project's IBM Cloud Object Storage bucket. Connection assets that contain information for connecting to data sources. You can add connections to IBM or third-party data sources. See Connectors. Connected data assets that specify a table, view, or file that is accessed through a connection to a data source. Connected folder assets that specify a path in IBM Cloud Object Storage.  
To get started quickly, take a tutorial. See Quick start tutorials.  
To refine data by cleansing and shaping it, you can:  
Select the Prepare data tile on your watsonx home page. Add the data to the project, then open the data asset and click Prepare data.  
To manage feature groups for a data asset, open the data asset and go to its Feature group page.  
To create synthetic data, you can:  
Select the Prepare data tile on your watsonx home page. Select the Generate synthetic tabular data tile."
3709	Getting and preparing data in a project	"Learn more
Create a project Adding data to a project Refining data with Data Refinery Adding connections to the Platform assets catalog Manage feature groups (beta) Creating synthetic data"
3710	Creating jobs in the Notebook editor	"You can create a job to run a notebook directly in the Notebook editor.  
To create a notebook job:  
In the Notebook editor, click from the menu bar and select Create a job.  
Define the job details by entering a name and a description (optional).  
On the Configure page, select:  
A notebook version. The most recently saved version of the notebook is used by default. If no version of the notebook exists, you must create a version by clicking from the notebook action bar.  
A runtime. By default, the job uses the same environment template that was selected for the notebook.  
Advanced configuration to add environment variables and select the job run retention settings.  
The environment variables that are passed to the notebook when the job is started and affect the execution of the notebook.  
Each variable declaration must be made for a single variable in the following format VAR_NAME=foo and appear on its own line.  
For example, to determine which data source to access if the same notebook is used in different jobs, you can set the variable DATA_SOURCE to DATA_SOURCE=jdbc:db2//db2.server.com:1521/testdata in the notebook job that trains a model and to DATA_SOURCE=jdbc:db2//db2.server.com:1521/productiondata in the job where the model runs on real data. In another example, the variables BATCH_SIZE, NUM_CLASSES and EPOCHS that are required for a Keras model can be passed to the same notebook with different values in separate jobs.  
Select the job run result output. You can select:  
Log & notebook to store the output files of specific runs, the log file, and the resulting notebook. This is the default that is set for all new jobs. Select:  
To compare the results of different job runs, not just by viewing the log file. By keeping the output files of specific job runs, you can compare the results of job runs to fine tune your code. For example, by configuring different environment variables when the job is started, you can change the way the code in the notebook behaves and then compare these differences (including graphics) step by step between runs.  
Note:"
3711	Creating jobs in the Notebook editor	"Note:  
The job run retention value is set to 5 by default to avoid creating too many run output files. This means that the last 5 job run output files will be retained. You need to adjust this value if you want to compare more run output files. You cannot use the results of a specific job run to create a URL to enable ""Sharing by URL"". If you want to use a specific job result run as the source of what is shown via ""Share by URL"", you must create a new job and select Log & updated version.  
To view the logs.  
Log only to store the log file only. The resulting notebook is discarded. Select:  
To view the logs.  
Log & updated version to store the log file and update the output cells of the version you used as input to this task. Select:  
To view the logs. To share the result of a job run via ""Share by URL"".  
Retention configuration to set how long to retain finished job runs and job run artifacts like logs or notebook results. You can either select the number of days to retain the job runs or the last number of job runs to keep. The retention value is set to 5 by default (the last 5 job run output files are retained).  
Be mindful when changing the default as too many job run files can quickly use up project storage.  
On the Schedule page, you can optionally add a one-time or repeating schedule.  
If you define a start day and time without selecting Repeat, the job will run exactly one time at the specified day and time. If you define a start date and time and you select Repeat, the job will run for the first time at the timestamp indicated in the Repeat section.  
You can't change the time zone; you must set your job schedule in your web browser's time zone. The schedule will be translated to the time zone of the compute node where your job runs.  
If you exclude certain weekdays, the job might not run as you would expect. The reason might be due to a discrepancy between the time zone of the user who creates the schedule, and the time zone of the compute node where the job runs."
3712	Creating jobs in the Notebook editor	"Note: Your scheduled job can appear differently if your web browser’s time zone is set to your local time zone following Daylight Savings Time (DST). For example, your scheduled job appears at 3:00PM Eastern Standard Time Zone (EST) daily corresponding to 8:00PM Coordinated Universal Time (UTC). When your local time zone changes to Eastern Daylight Time Zone (EDT), your scheduled job continues to run at 8:00PM (UTC) which will now appear as 4:00PM (EDT) daily.  
An API key is generated when you create a scheduled job, and future runs will use this API key. If you didn't create a scheduled job but choose to modify one, an API key is generated for you when you modify the job and future runs will use this API key.  
Optionally set to see notifications for the job. You can select the type of alerts to receive.  
Review the job settings. Then create the job and run it immediately, or create the job and run it later. All notebook code cells are run and all output cells are updated.  
The notebook job is listed under Jobs in your project. To view the notebook run output, click the job and then Run result on the Job run details page."
3713	Creating jobs in the Notebook editor	"Learn more
Viewing job details Coding and running notebooks Environments for the Notebook editor  
Parent topic: Creating and managing jobs"
3714	Your sandbox project	"A project is where you work with data and models by using tools. When you sign up for watsonx.ai, your sandbox project is created automatically, and you can start working in it immediately.  
Initially, your sandbox project is empty. To start working, click a task tile on the home page or go to the Assets page in your project, click New asset, and select a task. Each task can result in an asset that is saved in the project. Many tasks include samples that you can use. You can find sample prompts, notebooks, data sets, and other assets in the Resource hub from the home page. You can share your work by adding collaborators to your project. If you need to work with data, you can add data assets to your project.  
If your sandbox project is your only project, then any task that you select occurs in the context of your sandbox project. When you have multiple projects, you can change the default project by selecting a project from the Open in list on the home page.  
Other projects that you create have the same functionality as your sandbox project, except that your Watson Machine Learning service instance is automatically associated with your sandbox project. You must manually associate your Watson Machine Learning service instance with other projects."
3715	Your sandbox project	"Manually creating a sandbox project
If you switch from Cloud Pak for Data as a Service to watsonx, you can create a sandbox project from the watsonx home page when the following conditions are met:  
You have one or more instances of the Watson Machine Learning service. You have exactly one instance of the IBM Cloud Object Storage service.  
To manually create a sandbox project, click Create sandbox in the Projects section.  
Otherwise, you can create a different project. See Creating a project. You are guided through associating a Watson Machine Learning service with the project when you open certain tools.  
You can switch an existing project from the Cloud Pak for Data as a Service to watsonx. See Switching the platform for a project."
3716	Your sandbox project	"Learn more
Asset types and properties Adding collaborators Add data assets Manage assets Adding associated services to a project Object storage for workspaces  
Parent topic: Projects"
3717	SAP ASE connection	"To access your data in SAP ASE, create a connection asset for it.  
SAP ASE is a relational model database server. SAP ASE was formerly Sybase."
3718	SAP ASE connection	"Supported versions
SAP Sybase ASE 11.5+, 16.0+"
3719	SAP ASE connection	"Create a connection to SAP ASE
To create the connection asset, you need these connection details:  
Database name Hostname or IP address Port number Username and password SSL certificate (if required by the database server)  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3720	SAP ASE connection	"Create a connection to SAP ASE
Next step: Add data assets from the connection
See Add data from a connection in a project."
3721	SAP ASE connection	"Where you can use this connection
You can use SAP ASE connections in the following workspaces and tools:  
Projects  
SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3722	SAP ASE connection	"SAP ASE setup
Get Started with SAP ASE"
3723	SAP ASE connection	"SAP ASE setup
Running SQL statements
To ensure that your SQL statements run correctly, refer to the SAP ASE documentation for the correct syntax."
3724	SAP ASE connection	"Learn more
SAP ASE technical information  
Parent topic: Supported connections"
3725	Microsoft SQL Server connection	"Create a connection asset for Microsoft SQL Server.  
Microsoft SQL Server is a relational database management system."
3726	Microsoft SQL Server connection	"Supported versions
Microsoft SQL Server 2000+ Microsoft SQL Server 2000 Desktop Engine (MSDE 2000) Microsoft SQL Server 7.0"
3727	Microsoft SQL Server connection	"Create a connection to Microsoft SQL Server
To create the connection asset, you need the following connection details:  
Database name Hostname or IP address Either the Port number or the Instance name. If the server is configured for dynamic ports, use the Instance name. Username and password Select Use Active Directory if the Microsoft SQL Server has been set up in a domain that uses NTLM (New Technology LAN Manager) authentication. Then enter the name of the domain that is associated with the username and password SSL certificate (if required by the database server)  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3728	Microsoft SQL Server connection	"Create a connection to Microsoft SQL Server
Next step: Add data assets from the connection
See Add data from a connection in a project."
3729	Microsoft SQL Server connection	"Where you can use this connection
You can use Microsoft SQL Server connections in the following workspaces and tools:  
Projects  
Data Refinery Decision Optimization Notebooks. Click Read data on the Code snippets pane to get the connection credentials and load the data into a data structure. For more information, see Load data from data source connections. SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3730	Microsoft SQL Server connection	"Microsoft SQL Server setup
Microsoft SQL Server installation"
3731	Microsoft SQL Server connection	"Restriction
Except for NTLM authentication, Windows Authentication is not supported."
3732	Microsoft SQL Server connection	"Restriction
Running SQL statements
To ensure that your SQL statements run correctly, refer to the Transact-SQL Reference for the correct syntax."
3733	Microsoft SQL Server connection	"Learn more
Microsoft SQL Server documentation  
Parent topic: Supported connections"
3734	Adding data to a project	"After you create a project, the next step is to add data assets to it so that you can work with data. All the collaborators in the project are automatically authorized to access the data in the project.  
Different asset types can have duplicate names. However, you can't add an asset type with the same name multiple times.  
You can use the following methods to add data assets to projects:"
3735	Adding data to a project	"Add local files
You can add a file from your local system as a data asset in a project.  
You must have the Editor or Admin role in the project.  
The file cannot be empty.  
The file name can't exceed 255 characters.  
The maximum size for files that you can load with the UI is 5 GB. You can load larger files to a project with APIs.  
Important: You can't add executable files to a project. All other types files that you add to a project are not checked for malicious code. You must ensure that your files do not contain malware or other types of malicious software that other collaborators might download.  
To add data files to a project:  
From your project's Assets page, click the Upload asset to project icon . You can also click the same icon from within a notebook or canvas.  
In the pane that opens, browse for the files or drag them onto the pane. You must stay on the page until the load is complete.  
The files are saved in the object storage that is associated with your project and are listed as data assets on the Assets page of your project.  
When you click the data asset name, you can see this information about data assets from files:  
The asset name and description The tags for the asset The name of the person who created the asset The size of the data The date when the asset was added to the project The date when the asset was last modified A preview of the data, for CSV, Avro, Parquet, TSV, Microsoft Excel, PDF, text, JSON, and image files A profile of the data, for CSV, Avro, Parquet, TSV, and Microsoft Excel files  
You can update the contents of a data asset from a file by adding a file with the same name and format to the project and then choosing to replace the existing data asset.  
You can remove the data asset by choosing the Delete option from the action menu next to the asset name. Choose the Prepare data option to refine the data with Data Refinery."
3736	Adding data to a project	"Add Resource hub data sets
You can add data sets from Resource hub to your project:  
In Resource hub, find the card for the data set that you want to add. Click the Add to Project icon from the action bar, select the project, and click Add.  
This video provides a visual method to learn the concepts and tasks in this documentation."
3737	Adding data to a project	"Convert files in project storage to assets
The storage for the project contains the data assets that you uploaded to the project, but it can also contain other files. For example, you can save a DataFrame in a notebook in the project environment storage. You can convert files in project storage to assets.  
To convert files in project storage to assets:  
From the Assets tab of your project, click Import asset. Select Project files. Select the data_asset folder. Select the asset and click Import."
3738	Adding data to a project	"Next steps
Refine the data Analyze the data and work with models"
3739	Adding data to a project	"Learn more
Downloading data assets  
Parent topic: Preparing data"
3740	Dropbox connection	"To access your data in Dropbox, create a connection asset for it.  
Dropbox is a cloud storage service where you can you host and synchronize files on your devices."
3741	Dropbox connection	"Create a connection to Dropbox
To create the connection asset, you need an access token or a refresh token.  
Authentication method: You can use an access token for short-term access or a refresh token for long-term access to Dropbox.  
Access token (short-lived): The OAuth2 access token that you obtained from the Dropbox App Console or by following the instructions at the OAuth Guide in the Dropbox documentation.  
Refresh token (long-lived): For setup instructions for the token, see the Implement refresh tokens information at Migrating App Permissions and Access Tokens in the Dropbox documentation. You can find the values for the App key (Client ID) and App secret (Client secret) in the Dropbox App Console after you select your app."
3742	Dropbox connection	"Create a connection to Dropbox
Next step: Add data assets from the connection
See Add data from a connection in a project."
3743	Dropbox connection	"Where you can use this connection
You can use Dropbox connections in the following workspaces and tools:  
Projects  
Data Refinery SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3744	Dropbox connection	"Dropbox setup
Dropbox plans"
3745	Dropbox connection	"Supported file types
The Dropbox connection supports these file types:  Avro, CSV, Delimited text, Excel, JSON, ORC, Parquet, SAS, SAV, SHP, and XML."
3746	Dropbox connection	"Learn more
Dropbox quick start guides Getting started with the Dropbox API  
Parent topic: Supported connections"
3747	Snowflake connection	"To access your data in Snowflake, create a connection asset for it.  
Snowflake is a cloud-based data storage and analytics service."
3748	Snowflake connection	"Create a connection to Snowflake
To create the connection asset, you need the following connection details:  
Account name: The full name of your account Database name Role: The default access control role to use in the Snowflake session Warehouse: The virtual warehouse"
3749	Snowflake connection	"Create a connection to Snowflake
Credentials
Authentication method:  
Username and password  
Key-Pair: Enter the contents of the private key and the key passphrase (if configured). These properties must be set up by the Snowflake administrator. For information, see Key Pair Authentication & Key Pair Rotation in the Snowflake documentation.  
Okta URL endpoint: If your company uses native Okta SSO authentication, enter the Okta URL endpoint for your Okta account. Example: https://<okta_account_name>.okta.com. Leave this field blank if you want to use the default authentication of Snowflake. For information about federated authentication provided by Okta, see Native SSO."
3750	Snowflake connection	"Create a connection to Snowflake
Next step: Add data assets from the connection
See Add data from a connection in a project."
3751	Snowflake connection	"Where you can use this connection
You can use Snowflake connections in the following workspaces and tools:  
Projects  
Data Refinery Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3752	Snowflake connection	"Snowflake setup
General Configuration"
3753	Snowflake connection	"Snowflake setup
Running SQL statements
To ensure that your SQL statements run correctly, refer to the Snowflake SQL Command Reference for the correct syntax."
3754	Snowflake connection	"Learn more
Snowflake in 20 Minutes  
Parent topic: Supported connections"
3755	Amazon Redshift connection	"To access your data in Amazon Redshift, create a connection asset for it.  
Amazon Redshift is a data warehouse product that forms part of the larger cloud-computing platform Amazon Web Services (AWS)."
3756	Amazon Redshift connection	"Create a connection to Amazon Redshift
To create the connection asset, you need these connection details:  
Database name Hostname or IP address Port number Username and password SSL certificate (if required by the database server)"
3757	Amazon Redshift connection	"Create a connection to Amazon Redshift
Next step: Add data assets from the connection
See Add data from a connection in a project."
3758	Amazon Redshift connection	"Where you can use this connection
You can use Amazon Redshift connections in the following workspaces and tools:  
Projects  
Data Refinery SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3759	Amazon Redshift connection	"Amazon Redshift setup
See Amazon Redshift setup prerequisites for setup information."
3760	Amazon Redshift connection	"Amazon Redshift setup
Running SQL statements
To ensure that your SQL statements run correctly, refer to the Amazon Redshift documentation for the correct syntax."
3761	Amazon Redshift connection	"Learn more
Amazon Redshift documentation  
Parent topic: Supported connections"
3762	MariaDB connection	"To access your data in MariaDB, create a connection asset for it.  
MariaDB is an open source relational database. You can use the MariaDB connection to connect to either a MariaDB server or to a Microsoft Azure Database for MariaDB service in the cloud."
3763	MariaDB connection	"Supported versions
MariaDB server: 10.5.5 Microsoft Azure Database for MariaDB: 10.3"
3764	MariaDB connection	"Create a connection to MariaDB
To create the connection asset, you need these connection details:  
Database name Hostname or IP address Port number Username and password SSL certificate (if required by the database server)  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3765	MariaDB connection	"Create a connection to MariaDB
Next step: Add data assets from the connection
See Add data from a connection in a project."
3766	MariaDB connection	"Where you can use this connection
You can use MariaDB connections in the following workspaces and tools:  
Projects  
Data Refinery Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3767	MariaDB connection	"MariaDB setup
Setup depends on whether you are connecting from a local MariaDB server or a Microsoft Azure Database for MariaDB database service in the cloud.  
MariaDB server: MariaDB Administration Microsoft Azure Database for MariaDB: Quickstart: Create an Azure Database for MariaDB server by using the Azure portal"
3768	MariaDB connection	"Learn more
MariaDB Foundation Microsoft Azure Database for MariaDB  
Parent topic: Supported connections"
3769	Microsoft Azure SQL Database connection	"To access your data in a Microsoft Azure SQL Database, create a connection asset for it.  
Microsoft Azure SQL Database is a managed cloud database provided as part of Microsoft Azure."
3770	Microsoft Azure SQL Database connection	"Create a connection to Microsoft Azure SQL Database
To create the connection asset, you need these connection details:  
Database name Hostname or IP address Port number Select Use Active Directory if the server has been set up to use Azure Active Directory authentication (Azure AD). Enter your Azure AD user and password. Username and password SSL certificate (if required by the database server)  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3771	Microsoft Azure SQL Database connection	"Create a connection to Microsoft Azure SQL Database
Next step: Add data assets from the connection
See Add data from a connection in a project."
3772	Microsoft Azure SQL Database connection	"Where you can use this connection
You can use Microsoft Azure SQL Database connections in the following workspaces and tools:  
Projects  
Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3773	Microsoft Azure SQL Database connection	"Where you can use this connection
Running SQL statements
To ensure that your SQL statements run correctly, refer to the Azure SQL Database documentation for the correct syntax."
3774	Microsoft Azure SQL Database connection	"Microsoft Azure SQL Database setup
Getting started with single databases in Azure SQL Database"
3775	Microsoft Azure SQL Database connection	"Learn more
Azure SQL Database documentation  
Parent topic: Supported connections"
3776	Downloading data assets from a project	"You can download data assets from a project to your local system.  
Important: Take care when you download assets. Collaborators can upload any type of file into a project, including files that have malware or other types of malicious code."
3777	Downloading data assets from a project	"Download a data asset
To download a data asset that is in the project's storage, select Download from the ACTION menu next to the asset name.  
For an alternate method of downloading data assets for a project, select Files in the Data side panel. Checkmark the data asset and select Download from the ACTION menu in the side panel."
3778	Downloading data assets from a project	"Download a connected data asset
To download a connected data asset, use Data Refinery to run a job that saves the file as the output of a Data Refinery flow. The output of the Data Refinery flow is a new CSV file in the project’s storage.  
Click the asset name to open it. Click Prepare data. From the Jobs menu, click Save and create a job. Enter a job name and click Create and Run. Go back to the Assets page. Refresh the page. By default, the downloadable asset is named table-name_shaped.csv. Choose Download from the ACTION menu next to the asset name."
3779	Downloading data assets from a project	"Learn more
Data Refinery  
Parent topic: Projects"
3780	Dremio connection	"To access your data in Dremio, create a connection asset for it.  
Dremio is an open data lake platform. It supports all the major third-party data sources. You can connect to an instance on Dremio Cloud or Dremio Software (on-prem)."
3781	Dremio connection	"Create a connection to Dremio
To create the connection asset, you need these connection details:  
Hostname or IP address: You can create a Dremio Cloud instance only in the European Union (EU) or the United States (US). Use data.eu.dremio.cloud for the EU and use data.dremio.cloud for the US. Dremio Software can be hosted anywhere.  
Port number: The default port for Dremio Cloud instances is 443 and for Dremio Software it is 32010.  
Authentication method:  
To connect to Dremio Cloud, you must use a Personal Access Token for authentication. Select Port is SSL-enabled and enter the Personal Access Token. To generate a Personal Access Token, see the instructions Personal Access Tokens for Dremio Cloud. To connect to Dremio Software, you can use a username and password or you can use a Personal Access Token for authentication. To use a Personal Access Token, select Port is SSL-enabled and enter the Personal Access Token. To generate a Personal Access Token, see the instructions Personal Access Tokens for Dremio Software.  
SSL certificate:  
Select Port is SSL-enabled to connect to Dremio Cloud. Select Port is SSL-enabled and enter an SSL certificate if you want to connect to Dremio Software with SSL."
3782	Dremio connection	"Create a connection to Dremio
Choose the method for creating a connection based on where you are in the platform
In a project Click Assets > New asset > Connect to a data source. See Adding a connection to a project.  
In a deployment space Click Add to space > Connection. See Adding connections to a deployment space."
3783	Dremio connection	"Create a connection to Dremio
Next step: Add data assets from the connection
See Add data from a connection in a project."
3784	Dremio connection	"Where you can use this connection
You can use the Dremio connection in the following workspaces and tools:  
Projects  
Data Refinery SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3785	Dremio connection	"Dremio setup
Dremio can be set up in various deployments, see Dremio Cluster Deployment. To set up Dremio Cloud, see Dremio Cloud."
3786	Dremio connection	"Restriction
You can use this connection only for reading data. You cannot write data or export data with this connection."
3787	Dremio connection	"Restriction
Running SQL statements
To ensure that your SQL statements run correctly, refer to the Dremio SQL Reference for the correct syntax."
3788	Dremio connection	"Learn more
Dremio Software documentation Dremio Cloud documentation  
Parent topic: Supported connections"
3789	IBM Cloudant connection	"To access your data in IBM Cloudant, create a connection asset for it.  
Cloudant is a JSON document database available in IBM Cloud."
3790	IBM Cloudant connection	"Create a connection to Cloudant
To create the connection asset, you need these connection details:  
URL to the Cloudant database Database name Username and password"
3791	IBM Cloudant connection	"Create a connection to Cloudant
Next step: Add data assets from the connection
See Add data from a connection in a project."
3792	IBM Cloudant connection	"Where you can use this connection
You can use Cloudant connections in the following workspaces and tools:  
Projects  
Data Refinery Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3793	IBM Cloudant connection	"Cloudant setup
To set up the Cloudant database on IBM Cloud, see Getting started with IBM Cloudant. When you create your Cloudant service, for Authentication method, select IAM and legacy credentials."
3794	IBM Cloudant connection	"Restriction
IBM Cloud Query (CQ) is not supported."
3795	IBM Cloudant connection	"Learn more
IBM Cloudant docs  
Parent topic: Supported connections"
3796	Connecting to data behind a firewall	"To connect to a database that is not accessible via the internet (for example, behind a firewall), you must set up a secure communication path between your on-premises data source and IBM Cloud. Use a Satellite Connector, a Satellite location, or a Secure Gateway instance for the secure communication path.  
Set up a Satellite Connector: Satellite Connector is the replacement for Secure Gateway. Satellite Connector uses a lightweight Docker-based communication that creates secure and auditable communications from your on-prem, cloud, or Edge environment back to IBM Cloud. Your infrastructure needs only a container host, such as Docker. For more information, see Satellite Connector overview.  
Set up a Satellite location: A Satellite location provides the same secure communications to IBM Cloud as a Satellite Connector but adds high availability access by default plus the ability to communicate from IBM Cloud to your on-prem location. It supports managed cloud services on on-premises, such as Managed OpenShift and Managed Databases, supported remotely by IBM Cloud PaaS SRE resources. A Satellite location requires at least three x86 hosts in your infrastructure for the HA control plane. A Satellite location is a superset of the capabilities of the Satellite Connector. If you need only client data communication, set up a Satellite Connector.  
Configure a Secure Gateway: Secure Gateway is IBM Cloud's former solution for communication between on-prem or third-party cloud environments. Secure Gateway is now deprecated by IBM Cloud. For a new connection, set up a Satellite Connector instead."
3797	Connecting to data behind a firewall	"Set up a Satellite Connector
To set up a Satellite Connector, you create the Connector in your IBM Cloud account. Next, you configure agents to run in your local Docker host platform on-premises. Finally, you create the endpoints for your data source that IBM watsonx uses to access the data source from IBM Cloud."
3798	Connecting to data behind a firewall	"Set up a Satellite Connector
Setting up a Satellite Connector
Note: Not all connections support Satellite. If the connection supports Satellite, the IBM Cloud Satellite tile will be available in the Private Connectivity section of the Create connection form. Alternatively, you can filter all the connections that support Satellite in the New connection page.  
These steps automatically add the Satellite Connector user endpoints that correspond to the connection that you create in IBM watsonx.  
Access the Create connector page in IBM Cloud from one of these places:  
Log in to the Connectors page in IBM Cloud. In IBM watsonx:  
Go to the project page. Click the Assets tab. Click New asset > Connect to a data source. Select the IBM watsonx connector. In the Create connection page, scroll down to the Private connectivity section, and click the IBM Cloud Satellite tile. Click Configure Satellite and then log in to IBM Cloud. Click Create connector.  
Follow the steps for Creating a Connector.  
Set up the Connector agent containers in your local Docker host environment. For high availability, use three agents per connector that are deployed on separate Docker hosts. It is best to use a separate infrastructure and network connectivity for each agent. Follow the steps for Running a Connector agent. The agents will appear in the Active Agents list for the connector.  
In IBM watsonx, go back to the Create connection page. In the Private connectivity section, click Reload, and then select the Satellite Connector that you created.  
Click Test Connection to verify that you can connect from IBM watsonx to the data source using the Satellite Connector.  
Save the connection. A Satellite Connector endpoint corresponding to the connection is automatically created.  
In the Satellite Connectors dashboard in IBM Cloud, for each connection that you create, a user endpoint is added in the Satellite Connector. The name of the user endpoint that is automatically added will look like ep-<XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX>. To see which endpoint corresponds to the connection that you created in IBM watsonx, check the Destination FQDN or IP and Destination port values in the endpoint details."
3799	Connecting to data behind a firewall	"Set up a Satellite location
Use the Satellite location feature of IBM Cloud Satellite to securely connect to a Satellite location that you configure for your IBM Cloud account."
3800	Connecting to data behind a firewall	"Set up a Satellite location
Requirements for a Satellite location
Note: Not all connections support Satellite. If the connection supports Satellite, the IBM Cloud Satellite tile will be available in the Private Connectivity section of the Create connection form. Alternatively, you can filter all the connections that support Satellite in the New connection page."
3801	Connecting to data behind a firewall	"Set up a Satellite location
Setting up a Satellite location
Configure the Satellite location in IBM Cloud.  
Task 1: Create a Satellite location Task 2: Attach the hosts to the Satellite location Task 3: Assign the hosts to the control plane Task 4: Create the connection secured with a Satellite location Maintaining the Satellite location"
3802	Connecting to data behind a firewall	"Set up a Satellite location
Task 1: Create a Satellite location
A Satellite location is a representation of an environment in your infrastructure provider, such as an on-prem data center or cloud. To connect to data sources in IBM watsonx, you need three computers or virtual machines. To create the Satellite location:  
Access the Create a Satellite location setup page in IBM Cloud from one of these places:  
Log in to IBM Cloud, and select Create location. In IBM watsonx:  
Go to the project page. Click the Assets tab. Click New asset > Connect to a data source. Select the connector. In the Create connection page, scroll down to the Private connectivity section, and click the IBM Cloud Satellite tile. Click Configure Satellite and then log in to IBM Cloud. Click Create location. These instructions follow the On-premises & edge template. Depending on your infrastructure, you can select a different template. Refer to the template instructions and the information at Understanding Satellite location and hosts in the IBM Cloud docs.  
Click Edit to modify the Satellite location information:  
Name: You can use this field to differentiate between different networks such as my US East network or my Japan network.  
The Tags and Description fields are optional.  
Managed from: Select the IBM Cloud region that is closest to where your host machines physically reside.  
Resource group: is set to default by default.  
Zones: IBM automatically spreads the control plane instances across three zones within the same IBM Cloud multizone metro. For example, if you manage your location from the wdc metro in the US East region, your Satellite location control plane instances are spread across the us-east-1, us-east-2, and us-east-3 zones. This zonal spread ensures that your control plane is available, even if one zone becomes unavailable.  
Red Hat CoreOS: Do not select this option. Leave it cleared or as No.  
Object storage: Click Edit to enter the exact name of an existing IBM Cloud Object Storage bucket that you want to use to back up Satellite location control plane data. Otherwise, a new bucket is automatically created in an Object Storage instance in your account.  
Review your order details, and then click Create location."
3803	Connecting to data behind a firewall	"Set up a Satellite location
Task 1: Create a Satellite location
Review your order details, and then click Create location.  
A location control plane is deployed to one of the zones that are located in the IBM Cloud region that you selected. The control plane is ready for you to attach hosts to it."
3804	Connecting to data behind a firewall	"Set up a Satellite location
Task 2: Attach the hosts to the Satellite location
Attach three hosts that conform to the host requirements to the Satellite location.  
Satellite hosts are dedicated servers and cannot be shared with other applications. You cannot log in to a host with SSH. The root password will be changed. You need only three hosts for IBM watsonx connections. Worker nodes are not required. Only control plane hosts are needed for IBM watsonx connections. The Red Hat OpenShift Container Platform (OCP) is not needed for IBM watsonx connections. Container Linux CoreOS Linux is not needed for IBM watsonx connections. Hosts connect to IBM Cloud with the TLS 1.3 protocol.  
To attach the hosts to the Satellite location:  
From the Satellite Locations dashboard, click the name of your location.  
Click Attach Hosts to generate and download a script.  
Run the script on all the hosts to be attached to the Satellite location.  
Save the attach script in case you attach more hosts to the location in the future. The token in the attach script is an API key, which must be treated and protected as sensitive information. See Maintaining the Satellite location."
3805	Connecting to data behind a firewall	"Set up a Satellite location
Task 3: Assign the hosts to the control plane
To assign the hosts:  
From the Satellite Locations dashboard, click the name of your location.  
For each host, click the overflow menu () and then select Assign. Assign one host to each zone."
3806	Connecting to data behind a firewall	"Set up a Satellite location
Task 4: Create the connection secured with a Satellite location
To create the secure connection:  
In IBM watsonx, go to the project page. Click the Assets tab.  
Click New asset > Connect to a data source.  
Select the connector.  
In the Create connection form, complete the connection details. The hostname or IP address and the port of the data source must be available from each host that is attached to the Satellite location.  
Click Reload, and then select the Satellite location that you created.  
In the Satellite Locations dashboard in IBM Cloud, for each connection that you create, a link endpoint is created with Destination type Location, and Created by Connectivity in the Satellite location. The name of the user endpoint that is automatically added will look like ep-<XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX>. To see which endpoint corresponds to the connection that you created in IBM watsonx, check the Destination FQDN or IP and Destination port values in the endpoint details."
3807	Connecting to data behind a firewall	"Set up a Satellite location
Maintaining the Satellite location
The host attach script expires one year from the creation date. To make sure that the hosts don't have authentication problems, download a new copy of the host attach script at least once per year. Save the attach script in case you attach more hosts to the location in the future. If you generate a new host attach script, it detaches all the existing hosts. Hosts can be reclaimed by detaching them from the Satellite location and reloading the operating system in the infrastructure provider."
3808	Connecting to data behind a firewall	"Configure a Secure Gateway
The IBM Cloud Secure Gateway service provides a remote client to create a secure connection to a database that is not externalized to the internet. You can provision a Secure Gateway service in one service region and use it in service instances that you provisioned in other regions. After you create an instance of the Secure Gateway service, you add a Secure Gateway.  
Important: Secure Gateway is deprecated by IBM Cloud. For information see Secure Gateway deprecation overview and timeline.  
Note: Not all connections support Secure Gateway. If the connection supports Secure Gateway, the IBM Cloud Secure Gateway tile will be available in the Private Connectivity section of the Create connection form. Alternatively, you can filter all the connections that support Secure Gateway in the New connection page.  
To configure a secure gateway:  
Configure a secure gateway from the Create connection screen: Select a region and a pricing plan. Under Configure your resource, enter a service name and optional tags. Click Create. On the Services instances page, find the Secure Gateway service and click its name. Follow the instructions to add a gateway Adding a gateway. To maintain security for the connection, make sure that you configure the Secure Gateway to require a security token. Make sure you copy your Gateway ID and security token. From within your new gateway, on the Clients tab, click Connect Client to open the Connect Client pane. Select the client download for your operating system. Follow the instructions for installing the Client. Depending on the resource authentication protocol that you specify, you might need to upload a certificate. A destination is created when the connection is first established. In IBM watsonx, go to the project page. Click the Assets tab. In the Private connectivity section, click Reload, and then select the secure gateway that you created.  
Click the IBM Cloud Secure Gateway tile. Click Create a new instance of Secure Gateway. Alternatively, from the main menu in IBM watsonx, choose Administration > Services > Services catalog and then select Secure Gateway."
3809	Connecting to data behind a firewall	"Learn more
Getting started with IBM Cloud Satellite Secure Gateway deprecation  
Parent topic: Adding data to a project"
3810	IBM Netezza Performance Server connection	"To access your data in IBM Netezza Performance Server, create a connection asset for it.  
Netezza Performance Server is a platform for high-performance data warehousing and analytics."
3811	IBM Netezza Performance Server connection	"Supported versions
IBM Netezza Performance Server 11.x IBM Netezza appliance software 7.0.x, 7.1.x, 7.2.x"
3812	IBM Netezza Performance Server connection	"Create a connection to Netezza Performance Server
To create the connection asset, you need the following connection details:  
Database name Hostname or IP address Port number Username and password SSL certificate (if required by the database server)  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3813	IBM Netezza Performance Server connection	"Create a connection to Netezza Performance Server
Next step: Add data assets from the connection
See Add data from a connection in a project."
3814	IBM Netezza Performance Server connection	"Where you can use this connection
You can use Netezza Performance Server connections in the following workspaces and tools:  
Projects  
Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3815	IBM Netezza Performance Server connection	"Netezza Performance Server setup
Netezza Performance Server Getting started PureData System for Analytics Initial system setup"
3816	IBM Netezza Performance Server connection	"Netezza Performance Server setup
Running SQL statements
To ensure that your SQL statements run correctly, refer to the product documentation:  
Netezza Performance Server SQL command reference PureData System for Analytics IBM Netezza SQL Extensions toolkit"
3817	IBM Netezza Performance Server connection	"Learn more
IBM Netezza Performance Server documentation IBM PureData System for Analytics documentation  
Parent topic: Supported connections"
3818	Managing feature groups (beta)	"Create a feature group to preserve a set of columns of a data asset along with associated metadata for use with Machine Learning models.  
Watson Studio (for projects)  
To view this page, you can have any role in a project.  
To edit or update information on this page, you must have the Editor or Admin role in the project.  
You can view the asset feature group in these workspaces:  
Projects  
These types of assets can have a feature group:  
Tabular: CSV, TSV, Parquet, xls, xslx, avro, text, json files Connected data types that are structured and supported in Watson Studio.  
No limit"
3819	Managing feature groups (beta)	"Feature groups (beta)
Create a feature group to preserve a set of columns of a particular data asset along with the metadata used for Machine Learning. For example, if you have a set of features for a credit approval model, you can preserve the features used to train the model, as well as some metadata, including which column is used as the prediction target, and which columns are used for bias detection. Feature groups make it simple to preserve the metadata for the features used to train a machine learning model so other data scientists can use the same features. You can see the feature group tab when you preview a particular asset.  
Creating a feature group Editing a feature group Removing features or a feature group Using the Python API for feature groups"
3820	Managing feature groups (beta)	"Feature groups (beta)
Creating a feature group in a project
If you create a profile for the data asset before creating a feature group you can select profile metadata to add values to the feature.  
You can select particular columns of data assets to form a feature group.  
In the project Assets tab, click the name of the relevant asset to open the preview and select the Feature group tab. Here you can create a feature group or view and edit an existing one. An asset can have only one feature group. Click New feature group.  
Select the columns that you want to be used in the feature group. Select the Name checkbox to include all the columns as features."
3821	Managing feature groups (beta)	"Feature groups (beta)
Editing a feature group
When you have selected the columns of the data asset to be used in the feature group, you can then view each feature and edit it to specify the role it will have in Machine Learning models.  
Click a feature name and click Edit this feature. A window opens displaying the following tabs:  
Details - provide the following information about the feature.  
Select a Role to be assigned to the feature:  
Input: the feature can be used as input for training a Machine Learning model. Target: the feature to be used as the prediction target when the data is used to train a Machine Learning model. Identifier: the primary key, such as customer ID, used to identify the input data.  
Enter a Description, Recipe (any method or formula used to create values for the feature) and any Tags.  
Value descriptions  
Value descriptions allow you to clarify the meaning of specific values. For example, consider a column ""credit evaluation"" with the values -1, 0 and 1. You can use value descriptions to provide meaning for these values. For example, -1 might mean ""evaluation rejected"". You can enter descriptions for particular values. For numerical values, you can also specify a range. To specify a range of numerical values, enter the following text [n,m] where n is the start and m is the end of the range, surrounded by brackets, and click Add. For example, to describe all age values between 18 and 24 as ""millenials"", enter [18,24] as the value and millenials as the description. If you have a profile defined, the profile values are displayed in the value descriptions list. From here you can select one value or multiple values.  
Fairness information  
You can define Monitor or Reference groups of values for monitoring bias. The values that are more at risk of biased outcomes can be placed in the Monitor group. These values are then compared to values in the Reference group. To specify a range of numerical values, enter the following text [n,m] where n is the start and m is the end of the range, surrounded by brackets. For example, to monitor all age values between 18 and 35, enter [18,35]. Then select Monitor or Reference and click Add. You can also specify Favorable outcomes. See Fairness in AutoAI experiments for more information about fairness."
3822	Managing feature groups (beta)	"Feature groups (beta)
Editing a feature group
When you have edited the feature, click Save. You can now see your changes in the Feature Details window. Close this window to return to the feature group."
3823	Managing feature groups (beta)	"Feature groups (beta)
Removing features from a group
To remove a feature from a group:  
Preview the asset in the project and select the Feature group tab.  
In the Features table that is displayed, select the feature (or features) that you want to remove.  
In the toolbar that appears, select Remove from group.  
The feature, or feature group if you selected all the features, is removed."
3824	Managing feature groups (beta)	"Feature groups (beta)
Using the Python API to create and use feature groups
You can also use the assetframe-lib Python library in notebooks to create and edit feature groups. This library also allows you use feature metadata like fairness information when creating machine learning models."
3825	Managing feature groups (beta)	"Learn more
For examples on how to create and use feature groups in notebooks:  
Creating and using feature store data sample project in the Resource hub"
3826	Microsoft Azure Cosmos DB connection	"To access your data in Microsoft Azure Cosmos DB, create a connection asset for it.  
Azure Cosmos DB is a fully managed NoSQL database service."
3827	Microsoft Azure Cosmos DB connection	"Create a connection to Microsoft Azure Cosmos DB
To create the connection asset, you need these connection details:  
Hostname Port number Master key: The Azure Cosmos Database primary read-write key  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3828	Microsoft Azure Cosmos DB connection	"Create a connection to Microsoft Azure Cosmos DB
Next step: Add data assets from the connection
See Add data from a connection in a project."
3829	Microsoft Azure Cosmos DB connection	"Where you can use this connection
You can use Microsoft Azure Cosmos DB connections in the following workspaces and tools:  
Projects  
Data Refinery Decision Optimization Notebooks. Click Read data on the Code snippets pane to get the connection credentials and load the data into a data structure. See Load data from data source connections. SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3830	Microsoft Azure Cosmos DB connection	"Azure Cosmos DB setup
Set up Azure Cosmos DB: Azure portal Secure access to data in Azure Cosmos DB: Master keys"
3831	Microsoft Azure Cosmos DB connection	"Restrictions
Only the Core (SQL) API is supported."
3832	Microsoft Azure Cosmos DB connection	"Restrictions
Running SQL statements
To ensure that your SQL statements run correctly, refer to the Azure Cosmos DB documentation for the correct syntax."
3833	Microsoft Azure Cosmos DB connection	"Learn more
Azure Cosmos DB  
Parent topic: Supported connections"
3834	Microsoft Azure File Storage connection	"To access your data in Microsoft Azure File Storage, create a connection asset for it.  
Azure Files are Microsoft's cloud file system. They are managed file shares that are accessible via the Server Message Block (SMB) protocol or the Network File System (NFS) protocol."
3835	Microsoft Azure File Storage connection	"Create a connection to Microsoft Azure File Storage
To create the connection asset, you need these connection details:  
Connection string: Authentication is managed by the Azure portal access keys."
3836	Microsoft Azure File Storage connection	"Create a connection to Microsoft Azure File Storage
Next step: Add data assets from the connection
See Add data from a connection in a project."
3837	Microsoft Azure File Storage connection	"Where you can use this connection
You can use Microsoft Azure File Storage connections in the following workspaces and tools:  
Projects  
Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3838	Microsoft Azure File Storage connection	"Azure File Storage setup
Set up storage and access keys on the Microsoft Azure portal. For instructions see Manage storage account access keys. Example connection string, which you can find in the ApiKeys section of the container:  
DefaultEndpointsProtocol=https;AccountName=sampleaccount;AccountKey=samplekey;EndpointSuffix=core.windows.net  
Choose the method to create and manage your Azure Files:  
Quickstart: Create and manage Azure Files share with Windows virtual machines Quickstart: Create and manage Azure file shares with the Azure portal Quickstart: Create and manage an Azure file share with Azure PowerShell Quickstart: Create and manage Azure file shares using Azure CLI Quickstart: Create and manage Azure file shares with Azure Storage Explorer"
3839	Microsoft Azure File Storage connection	"Restriction
Microsoft Azure's maximum file size is 1 TB."
3840	Microsoft Azure File Storage connection	"Supported file types
The Azure File Storage connection supports these file types:  Avro, CSV, Delimited text, Excel, JSON, ORC, Parquet, SAS, SAV, SHP, and XML."
3841	Microsoft Azure File Storage connection	"Known issue
During the upload, the data is appended in portions to a temporary blob and then converted into the file. Depending on the size of the streamed content, there might be a delay in creating the file. Wait until all the data is uploaded."
3842	Microsoft Azure File Storage connection	"Learn more
Azure Files  
Parent topic: Supported connections"
3843	Cloudera Impala connection	"To access your data in Cloudera Impala, create a connection asset for it.  
Cloudera Impala provides SQL queries directly on your Apache Hadoop data stored in HDFS or HBase."
3844	Cloudera Impala connection	"Supported versions
Cloudera Impala 1.3+"
3845	Cloudera Impala connection	"Create a connection to Cloudera Impala
To create the connection asset, you need these connection details:  
Database name Hostname or IP address Port number Username and password SSL certificate (if required by the database server)"
3846	Cloudera Impala connection	"Create a connection to Cloudera Impala
Next step: Add data assets from the connection
See Add data from a connection in a project."
3847	Cloudera Impala connection	"Where you can use this connection
You can use Cloudera Impala connections in the following workspaces and tools:  
Projects  
Data Refinery SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3848	Cloudera Impala connection	"Cloudera Impala setup
Cloudera Impala installation"
3849	Cloudera Impala connection	"Restriction
You can use this connection only for source data. You cannot write to data or export data with this connection."
3850	Cloudera Impala connection	"Restriction
Running SQL statements
To ensure that your SQL statements run correctly, refer to the Impala SQL Language Reference for the correct syntax."
3851	Cloudera Impala connection	"Learn more
Cloudera Impala documentation  
Parent topic: Supported connections"
3852	Importing a project	You can create a project that is preloaded with assets by importing the project.
3853	Importing a project	"Requirements
Importing a project from a local file is a method of copying a project. You can import a project from a file on your local system only if the ZIP file that you select was exported from a IBM watsonx project as a compressed file. You can import only projects that you exported from watsonx.ai. You cannot import a compressed file that was exported from a Cloud Pak for Data as a Service project.  
If the exported file that you select to import was encrypted, you must enter the password that was used for encryption to enable decrypting sensitive connection properties.  
You can create a project from a project sample to learn how to work with data in tools, such as notebooks to prepare data, analyze data, build and train models, and visualize analysis results.  
The sample projects show how to accomplish goals, for example, to load and explore data, to create and train machine learning models for predictive analysis. Each project includes the required assets, such as notebooks, and all the data sets that you need to complete the example use case."
3854	Importing a project	"Importing a project from a local file or sample
To import a project:  
Click New project on the home page or on your Projects page. Choose whether to create a project based on an exported project file or a sample project. Upload a project file or select a sample project. On the New project screen, add a name and optional description for the project. If the project file that you select to import is encrypted, you must enter the password that was used for encryption to enable decrypting sensitive connection properties. If you enter an incorrect password, the project file imports successfully, but sensitive connection properties are falsely decrypted. Select the Restrict who can be a collaborator checkbox to restrict collaborators to members of your organization. You can't change this setting after you create the project. Choose an existing object storage service instance or create a new one. Click Create. You can start adding resources if your project is empty, or begin working with the resources you imported."
3855	Importing a project	"Learn more
Administering a project Exporting project  
Parent topic: Creating a project"
3856	Looker connection	"To access your data in Looker, create a connection asset for it.  
Looker is a business intelligence software and big data analytics platform that helps you explore, analyze and share real-time business analytics."
3857	Looker connection	"Create a connection to Looker
To create the connection asset, you need these connection details:  
Hostname or IP address Port number of the Looker server Client ID and Client secret  
Before you configure the connection, set up API3 credentials for your Looker instance. For details, see Looker API Authentication."
3858	Looker connection	"Create a connection to Looker
Next step: Add data assets from the connection
See Add data from a connection in a project."
3859	Looker connection	"Where you can use this connection
You can use Looker connections in the following workspaces and tools:  
Projects  
SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3860	Looker connection	"Looker setup
Set up and administer Looker"
3861	Looker connection	"Restriction
You can use this connection only for source data. You cannot write to data or export data with this connection."
3862	Looker connection	"Restriction
Running SQL statements
To ensure that your SQL statements run correctly, refer to the Looker documentation, Using SQL Runner, for the correct syntax."
3863	Looker connection	"Supported file types
The Looker connection supports these file types:  CSV, Delimited text, Excel, JSON."
3864	Looker connection	"Learn more
Looker documentation  
Parent topic: Supported connections"
3865	Marking a project as sensitive	"When you create a project, you can mark the project as sensitive to prevent project collaborators from moving sensitive data out of the project.  
Marking a project as sensitive prevents collaborators of a project, including administrators, from downloading or exporting data assets, connections, or connected data from a project. These sensitive assets cannot be added to a catalog or promoted to a space either. Project collaborators with Admin or Editor role can export assets like notebooks or models from the project.  
When users open a project that is marked as sensitive, a notification is displayed stating that no data assets can be downloaded or exported from the project."
3866	Marking a project as sensitive	"Restrictions
You cannot mark a project as sensitive after the project is created. You cannot mark projects that use Git integration as sensitive.  
Parent topic: Administering projects"
3867	Creating jobs for Pipelines	"You can create jobs for Pipelines.  
To create a Pipelines job:  
Open your Pipelines asset from the project. Click Run pipeline > Create a job. On the Create a job page, you can choose the asset version that you'd like to run. The most recently saved version of the Pipelines is used by default. Give a name and optional description for your job. Click next. Define your IAM API key. The most recently used API key is used by default. If you'd like to use a new API key, click Generate new API key. Click next. You can schedule your job by toggling Schedule off to Schedule to run. You can choose either or both options: Review your job settings and click Create. The Pipelines job is listed under Jobs in your project.  
Start on: Choose a date for your scheduled job to run. The time zone is GMT-0400 (Eastern Daylight Time). If you do not choose a start date, the job will never run automatically and must be started manually. Repeat: You can choose to schedule the repeated frequency (every minute to every month), exclude running the job on certain days, and choose an end date. If you do not choose to repeat the job, it runs one time if a start date is given, or does not run."
3868	Creating jobs for Pipelines	"Learn more
Viewing jobs across projects  
Parent topic: Creating and managing jobs"
3869	Working in projects	"A project is a collaborative workspace where you work with data and other assets to accomplish a particular goal.  
By default, your sandbox project is created automatically when you sign up for watsonx.ai.  
Your project can include these types of resources:  
Collaborators are the people who you work with in your project. Data assets are what you work with. Data assets often consist of raw data that you work with to refine. Tools and their associated assets are how you work with data. Environments are how you configure compute resources for running assets in tools. Jobs are how you manage and schedule the running of assets in tools. Project documentation and notifications are how you stay informed about what's happening in the project. Asset storage is where project information and files are stored. Integrations are how you incorporate external tools.  
You can customize projects to suit your goals. You can change the contents of your project and almost all of its properties at any time. However, you must make these choices when you create the project because you can't change them later:  
The instance of IBM Cloud Object Storage to use for project storage.  
You can view projects that you create and collaborate in by selecting Projects > View all projects in the navigation menu, or by viewing the Projects pane on the main page."
3870	Working in projects	"Collaboration in projects
As a project creator, you can add other collaborators and assign them roles that control which actions they can take. You automatically have the Admin role in the project, and if you give other collaborators the Admin role, they can add collaborators too. See Adding collaborators and Project collaborator roles.  
Tip: If appropriate, add at least one other user as a project administrator to ensure that someone is able to manage the project if you are unavailable."
3871	Working in projects	"Collaboration in projects
Collaboration on assets
All collaborators work with the same copy of each asset. Only one collaborator can edit an asset at a time. While a collaborator is editing an asset in a tool, that asset is locked. Other collaborators can view a locked asset, but not edit it. See Managing assets."
3872	Working in projects	"Data assets
You can add these types of data assets to projects:  
Data assets from local files or the Resource hub Connections to cloud and on-premises data sources Connected data assets from an existing connection asset that provide read-only access to a table or file in an external data source Folder data assets to view the files within a folder in a file system  
Learn more about data assets:  
Connectors Adding data to a project Data asset types and their properties"
3873	Working in projects	"Tools and their associated assets
When you run a tool, you create an asset that contains the information for a specific goal. For example, when you run the Data Refinery tool, you create a Data Refinery flow asset that defines the set of ordered operations to run on a specific data asset. Each tool has one or more types of associated assets that run in the tool.  
For a mapping of assets to the tools that you use to create them, see Asset types and properties."
3874	Working in projects	"Environments
Environments control your compute resources. An environment template specifies hardware and software resources to instantiate the environment runtimes that run your assets in tools.  
Some tools have an automatically selected environment template. However, for other tools, you can choose between multiple environments. When you create an asset in a tool, you assign an environment to it. You can change the environment for an asset when you run it.  
Watson Studio includes a set of default environment templates that vary by coding language, tool, and compute engine type. You can also create custom environment templates or add services that provide environment templates.  
The compute resources that you consume in a project are tracked. Depending on your offering plan, you have a limit to your monthly compute resources or you pay for all compute resources.  
See Environments."
3875	Working in projects	"Jobs
A job is a single run of an asset in a tool with a specified environment runtime. You can schedule one or repeating jobs, monitor, edit, stop, or cancel jobs. See Jobs."
3876	Working in projects	"Asset storage
Each project has a dedicated, secure storage bucket that contains:  
Data assets that you upload to the project as files. Data assets from files that you copy from another workspace. Files that you save to the project with a tool. Files for assets that run in tools, such as notebooks. Saved models. The project readme file and internal project files.  
When you create a project, you must select an instance of IBM Cloud Object Storage or create a new instance. You cannot change the IBM Cloud Object Storage instance after you create the workspace. See Object storage.  
When you delete a project, its storage bucket is also deleted."
3877	Working in projects	"Integrations with external tools
Integrations provide a method to interact with tools that are external to the project.  
You can integrate with a Git repository to publish notebooks."
3878	Working in projects	"Project documentation and notifications
While you create a project, you can add a short description to document the purpose or goal of the project. You can edit the description later, on the project's Settings page.  
You can mark the project as sensitive. When users open a project that is marked as sensitive, a notification is displayed stating that no data assets can be downloaded or exported from the project.  
The Overview page of a project contains a readme file where you can document the status or results of the project. The readme file uses standard Markdown formatting. Collaborators with the Admin or Editor role can edit the readme file.  
You can view recent asset activity in the Assets pane on the Overview page, and filter the assets by selecting By you or By all using the dropdown. By you lists assets that you edited, ordered by most recent. By all lists assets that are edited by others and also by you, ordered by most recent.  
All collaborators in a project are notified when a collaborator changes an asset."
3879	Working in projects	"Learn more
Your sandbox project Creating a project Administering a project Adding collaborators Managing assets Downloading data assets Environments Jobs Adding data Marking a project as sensitive"
3880	IBM Db2 Warehouse connection	"To access your data in IBM Db2 Warehouse, create a connection asset for it.  
IBM Db2 Warehouse is an analytics data warehouse that gives you a high level of control over your data and applications. You can use the IBM Db2 Warehouse connection to connect to a database in these products:  
IBM Db2 Warehouse in IBM Cloud IBM Db2 Warehouse on-prem"
3881	IBM Db2 Warehouse connection	"Create a connection to Db2 Warehouse
To create the connection asset, you need these connection details:  
Database name Hostname or IP address of the database server Port number API key or Username and password  
Application name (optional): The name of the application that is currently using the connection. For information, see Client info properties support by the IBM Data Server Driver for JDBC and SQLJ. Client accounting information (optional): The value of the accounting string from the client information that is specified for the connection. For information, see Client info properties support by the IBM Data Server Driver for JDBC and SQLJ. Client hostname (optional): The hostname of the machine on which the application that is using the connection is running. For information, see Client info properties support by the IBM Data Server Driver for JDBC and SQLJ. Client user (optional): The name of the user on whose behalf the application that is using the connection is running. For information, see Client info properties support by the IBM Data Server Driver for JDBC and SQLJ.  
SSL certificate (if required by the database server)"
3882	IBM Db2 Warehouse connection	"Create a connection to Db2 Warehouse
Credentials
For Credentials, you can enter either an API key or a username and password  
You can use an API key to authenticate to Db2 Warehouse in IBM Cloud.  
Db2 Warehouse in IBM Cloud First add the user ID as an IAM user or as a service ID. For instructions, see the Console user experience section of the Identity and access management (IAM) on IBM Cloud topic.  
If users want to authenticate with Db2 Warehouse with an IAM API key, the administrator of the Db2 Warehouse instance can add the IAM users by using the User management console, and then the users can each create an API key for themselves by using the IAM access management console.  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3883	IBM Db2 Warehouse connection	"Create a connection to Db2 Warehouse
Next step: Add data assets from the connection
See Add data from a connection in a project."
3884	IBM Db2 Warehouse connection	"Where you can use this connection
You can use Db2 Warehouse connections in the following workspaces and tools:  
Projects  
Data Refinery Decision Optimization Notebooks. Click Read data on the Code snippets pane to get the connection credentials and load the data into a data structure. See Load data from data source connections. SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3885	IBM Db2 Warehouse connection	"Db2 Warehouse setup
IBM Db2 Warehouse on Cloud: Getting started with Db2 Warehouse on Cloud IBM Db2 Warehouse on-prem: Setting up Db2 Warehouse"
3886	IBM Db2 Warehouse connection	"Db2 Warehouse setup
Running SQL statements
To ensure that your SQL statements run correctly, refer to the product documentation in Learn more for the correct syntax."
3887	IBM Db2 Warehouse connection	"Known issue
On Data Refinery, system-level schemas aren’t filtered out."
3888	IBM Db2 Warehouse connection	"Learn more
IBM Db2 Warehouse on Cloud product documentation (IBM Cloud) IBM Db2 Warehouse on-prem product documentation  
Parent topic: Supported connections"
3889	FTP (remote file system) connection	"To access your data with the FTP protocol, create a connection asset for it.  
FTP is a standard communication protocol that is used to transfer files from a server to a client on a computer network."
3890	FTP (remote file system) connection	"Create an FTP connection
To create the connection asset, you need these connection details:  
Connection mode: The connection method configured on the FTP server: Hostname or IP address Port number of the FTP server SSH mode: Private key and Key passphrase  
Anonymous Basic authentication (with username and password) SFTP Tectia: Transfer data sets that are in Multiple Virtual Storage (MVS) format to or from an IBM z/OS mainframe computer. MVS data sets use a period (.) to separate the qualifiers in the data set names. To write to an MVS data set, select Access MVS Dataset and enter the file transfer advice (FTADV) strings in key-value pairs separated by commas. For information, see the Tectia documentation. SSH: File transfer over a secure channel that uses the Secure Shell protocol. Also requires username and password. SSL: File transfer that uses File Transport Protocol (FTP), which supports secure transmission via SSL (sslTLSv2) protocol. Also requires username and password.  
Authentication method:  
Username and password Username, password, private key. If you use an encrypted private key, you will need a key passphrase. Username and private key. If you use an encrypted private key, you will need a key passphrase.  
If you use a private key, make sure that the key is an RSA private key that is generated by the ssh-keygen tool. The private key must be in the PEM format.  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection. This selection is available for the SSH connection mode only."
3891	FTP (remote file system) connection	"Create an FTP connection
Next step: Add data assets from the connection
See Add data from a connection in a project."
3892	FTP (remote file system) connection	"Where you can use this connection
You can use FTP connections in the following workspaces and tools:  
Projects  
Data Refinery SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3893	FTP (remote file system) connection	"Supported file types
The FTP connection supports these file types:  Avro, CSV, Delimited text, Excel, JSON, ORC, Parquet, SAS, SAV, SHP, and XML.  
Parent topic: Supported connections"
3894	Adding a connected folder asset to a project	"You can create a connected folder asset based on a path within an IBM Cloud Object Storage system that is accessed through a connection. You can view the files and subfolders that share the path with the connected folder asset. The files that you can view within the connected folder asset are not themselves data assets. For example, you can create a connected folder asset for a path that contains news feeds that are continuously updated.  
Watch this video to see how to add a connected folder asset in a project, then follow the steps below the video.  
Video disclaimer: Some minor steps and graphical elements in this video might differ from your platform.  
This video provides a visual method to learn the concepts and tasks in this documentation.  
To add a connected folder asset from a connection to a project:  
If necessary, create a connection asset. Include an Access Key and a Secret Key to your IBM Cloud Object Storage connection to enable the downloading of files within the connected folder asset. If you're using an existing IBM Cloud Object Storage connection asset that doesn't have an Access Key and Secret Key, edit the connection asset and add them.  
Click Import assets > Connected data.  
Select an existing connection asset as the source of the data.  
Select the folder you want and click Import.  
Type a name and description.  
Click Create. The connected folder asset appears on the project Assets page in the Data assets category.  
Click the connected folder asset name to view the contents of the connected folder asset. Click the eye icon next to a file name to view the contents of the files within the folder that have these formats:  
CSV JSON Parquet  
You can refine the files within a connected folder asset and then save the result as a data asset. While viewing the connected folder asset, select a file and then click Prepare data.  
You can view the files within the connected folder asset if the IBM Cloud Object Storage connection asset that's associated with the connected folder asset has an Access Key and a Secret Key (also known as HMAC credentials). For more information about HMAC credentials, see IBM Cloud Object Storage Service credentials."
3895	Adding a connected folder asset to a project	"Next steps
Refining a file within the folder  
Parent topic: Adding data to a project"
3896	OData connection	"To access your data in OData, create a connection asset for it.  
The OData (Open Data) protocol is a REST-based data access protocol. The OData connection reads data from a data source that uses the OData protocol."
3897	OData connection	"Supported versions
The OData connection is supported on OData protocol version 2 or version 4."
3898	OData connection	"Create a connection to OData
To create the connection asset, you need these connection details:  
Service root URL: The URL to access the service root of a site that has implemented the OData protocol. Consult the client product's documentation to confirm the Service root URL. Timeout seconds: Timeout value for the HTTP calls.  
Credentials type:  
API Key Basic None  
Encryption: SSL certificate (if required by the database server)  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3899	OData connection	"Create a connection to OData
Next step: Add data assets from the connection
See Add data from a connection in a project."
3900	OData connection	"Where you can use this connection
You can use the OData connection in the following workspaces and tools:  
Projects  
Data Refinery SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3901	OData connection	"OData setup
To set up the OData service, see How to Use Web API OData to Build an OData V4 Service without Entity Framework."
3902	OData connection	"Restrictions
For Data Refinery, you can use this connection only as a source. You cannot use this connection as a target connection or as a target connected data asset. For SPSS Modeler, you cannot create new entity sets."
3903	OData connection	"Learn more
www.odata.org  
Parent topic: Supported connections"
3904	IBM Cloud Object Storage connection	"To access your data in IBM Cloud Object Storage (COS), create a connection asset for it.  
IBM Cloud Object Storage on IBM Cloud provides unstructured data storage for cloud applications. Cloud Object Storage offers S3 API and application binding with regional and cross-regional resiliency."
3905	IBM Cloud Object Storage connection	"Create a connection to IBM Cloud Object Storage
To create the connection asset, you need these connection details:  
Bucket name. (Optional. If you do not enter the bucket name, then the credentials must have permission to list all the buckets.) Login URL. To find the Login URL:  
Go to the Cloud Object Storage Resource list at https://cloud.ibm.com/resources. Expand the Storage resource. Click the Cloud Object Storage service. From the menu, select Endpoints. Optional: Use the Select resiliency and Select location menus to filter the choices. Copy the value of the public endpoint that is in the same region as the bucket that you want to use.  
SSL certificate: (Optional). A self-signed certificate that was created by a tool such as OpenSSL."
3906	IBM Cloud Object Storage connection	"Create a connection to IBM Cloud Object Storage
Credentials
Use one of the following combination of values for authentication:  
Service credentials  
Resource instance ID and API key  
Resource instance ID, API key, Access key, and Secret key (In this combination, the Resource instance ID and API key are used for authentication. The Access key and Secret key are stored.)  
Access key and Secret key  
To find the value for Service credentials:  
Go to the Cloud Object Storage Resource list at https://cloud.ibm.com/resources. Expand the Storage resource. Click the Cloud Object Storage service, and then click the Service credentials tab. Expand the Key name that you want to use. Copy the entire JSON file. Include the opening and closing braces { } symbols.  
To find the values for the API key, Access key, Secret key, and the Resource instance ID:  
Go to the Cloud Object Storage Resource list at https://cloud.ibm.com/resources. Expand the Storage resource. Click the Cloud Object Storage service, and then click the Service credentials tab. Expand the Key name that you want to use. Copy the values without the quotation marks:  
API key: apikey Access key: access_key_id Secret key: secret_access_key Resource instance ID: resource_instance_id"
3907	IBM Cloud Object Storage connection	"Create a connection to IBM Cloud Object Storage
Next step: Add data assets from the connection
See Add data from a connection in a project."
3908	IBM Cloud Object Storage connection	"Where you can use this connection
You can use IBM Cloud Object Storage connections in the following workspaces and tools:  
Projects  
AutoAI Data Refinery Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3909	IBM Cloud Object Storage connection	"Connecting to the Cloud Object Storage service with the S3 API
To connect to Cloud Object Storage with the S3 API, you need the Login URL, an Access key and a Secret key.  
The API key is a token that is used to call the Watson IoT Platform HTTP APIs. Users are assigned roles and they can generate an API key that they can use to authorize calls to API endpoints. For more information, see the IBM Cloud Object Storage S3 API documentation."
3910	IBM Cloud Object Storage connection	"IBM Cloud Object Storage setup
Getting started with IBM Cloud Object Storage"
3911	IBM Cloud Object Storage connection	"Supported file types
The IBM Cloud Object Storage connection supports these file types:  Avro, CSV, Delimited text, Excel, JSON, ORC, Parquet, SAS, SAV, SHP, and XML."
3912	IBM Cloud Object Storage connection	"Learn more
Controlling access to COS buckets  
Parent topic: Supported connections"
3913	Apache Cassandra connection	"To access your data in Apache Cassandra, create a connection asset for it.  
Apache Cassandra is an open source, distributed, NoSQL database."
3914	Apache Cassandra connection	"Supported versions
Apache Cassandra 2.0 or later"
3915	Apache Cassandra connection	"Create a connection to Apache Cassandra
To create the connection asset, you need these connection details:  
Hostname or IP address Port number Keyspace Username and password Read consistency: Specifies the number of replicas that must respond to a read request before the data is returned to the client application. Write consistency: Specifies the number of replicas for which the write request must succeed before an acknowledgment is returned to the client application.  
all: Data is returned to the application after all replicas have responded. This setting provides the highest consistency and lowest availability. local_one: Data is returned from the closest replica in the local data center. local_quorum: Data is returned after a quorum of replicas in the same data center as the coordinator node has responded. This setting voids latency of inter -data center communication. local_serial: Data within a data center is read without proposing a new addition or update. Uncommitted transactions within the data center are committed as part of the read. one: Data is returned from the closest replica. This setting provides the highest availability, but increases the likelihood of stale data being read. quorum: (Default). Data is returned after a quorum of replicas has responded from any data center. serial: Data is read without proposing a new addition or update. Uncommitted transactions are committed as part of the read. three: Data is returned from three of the closest replicas. two: Data is returned from two of the closest replicas."
3916	Apache Cassandra connection	"Create a connection to Apache Cassandra
all: A write must succeed on all replica nodes in the cluster for that partition key. This setting provides the highest consistency and lowest availability. any: A write must succeed on at least one node. Even if all replica nodes for the given partition key are down, the write can succeed after a hinted handoff has been written. This setting provides the lowest consistency and highest availability. each_quorum: A write must succeed on a quorum of replica nodes across a data center. local_one: A write must succeed on at least one replica node in the local data center. local_quorum: A write must succeed on a quorum of replica nodes in the same data center as the coordinator node. This setting voids latency of inter -data center communication. local_serial: The driver prevents unconditional updates to achieve linearizable consistency for lightweight transactions within the data center. one: A write must succeed on at least one replica node. quorum: (Default). A write must succeed on a quorum of replica nodes. serial: The driver prevents unconditional updates to achieve linearizable consistency for lightweight transactions. three: A write must succeed on at least three replica nodes. two: A write must succeed on at least two replica nodes.  
   
SSL certificate (if required by the database server)  
For Private connectivity, to connect to a database that is not externalized to the internet (for example, behind a firewall), you must set up a secure connection."
3917	Apache Cassandra connection	"Create a connection to Apache Cassandra
Next step: Add data assets from the connection
See Add data from a connection in a project."
3918	Apache Cassandra connection	"Where you can use this connection
You can use Apache Cassandra connections in the following workspaces and tools:  
Projects  
Data Refinery Decision Optimization Notebooks. Click Read data on the Code snippets pane to get the connection credentials and load the data into a data structure. See Load data from data source connections. SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3919	Apache Cassandra connection	"Apache Cassandra setup
Installing Cassandra Configuring Cassandra CREATE KEYSPACE"
3920	Apache Cassandra connection	"Learn more
cassandra.apache.org Cassandra Documentation  
Parent topic: Supported connections"
3921	Presto connection	"To access your data in Presto, create a connection asset for it.  
Presto is a fast and reliable SQL engine for Data Analytics and the Open Lakehouse."
3922	Presto connection	"Supported versions
Version 0.279 and earlier"
3923	Presto connection	"Create a connection to Presto
To create the connection asset, you need these connection details:  
Hostname or IP address Port Username Password (required if you connect to Presto with SSL enabled) SSL certificate (if required by the Presto server)"
3924	Presto connection	"Create a connection to Presto
Connecting to Presto within IBM watsonx.data
To connect to a Presto server within watsonx.data on IBM Cloud, use these connection details:  
Username: ibmlhapikey Password (for SSL-enabled, which is the default): An IBM Cloud API key. For more information, see Connecting to Presto server.  
To connect to a Presto server within watsonx.data on Cloud Pak for Data or stand-alone watsonx.data, use the username and password that you use for the watsonx.data console."
3925	Presto connection	"Create a connection to Presto
Next step: Add data assets from the connection
See Add data from a connection in a project."
3926	Presto connection	"Where you can use this connection
You can use the Presto connection in the following workspaces and tools:  
Projects  
Data Refinery Decision Optimization SPSS Modeler Synthetic Data Generator  
Catalogs  
Platform assets catalog"
3927	Presto connection	"Presto setup
To set up Presto, see Presto installation."
3928	Presto connection	"Restriction
You can use this connection only for source data. You cannot write to data or export data with this connection."
3929	Presto connection	"Limitation
The Presto connection does not support the Apache Cassandra Time data type."
3930	Presto connection	"Limitation
Running SQL statements
To ensure that your SQL statements run correctly, refer to the SQL Statement Syntax for the correct syntax."
3931	Presto connection	"Learn more
Presto documentation  
Related connection: IBM watsonx.data connection  
Parent topic: Supported connections"
3932	Time series lazy evaluation	"Lazy evaluation is an evaluation strategy that delays the evaluation of an expression until its value is needed. When combined with memoization, lazy evaluation strategy avoids repeated evaluations and can reduce the running time of certain functions by a significant factor.  
The time series library uses lazy evaluation to process data. Notionally an execution graph is constructed on time series data whose evaluation is triggered only when its output is materialized. Assuming an object is moving in a one dimensional space, whose location is captured by x(t). You can determine the harsh acceleration/braking (h(t)) of this object by using its velocity (v(t)) and acceleration (a(t)) time series as follows:  
This results in a simple execution graph of the form:  
Evaluations are triggered only when an action is performed, such as compute h(5...10), i.e. compute h(5), ..., h(10). The library captures narrow temporal dependencies between time series. In this example, h(5...10) requires a(5...10), which in turn requires v(4...10), which then requires x(3...10). Only the relevant portions of a(t), v(t) and x(t) are evaluated.  
Furthermore, evaluations are memoized and can thus be reused in subsequent actions on h. For example, when a request for h(7...12) follows a request for h(5...10), the memoized values h(7...10) would be leveraged; further, h(11...12) would be evaluated using a(11...12), v(10...12) and x(9...12), which would in turn leverage v(10) and x(9...10) memoized from the prior computation.  
In a more general example, you could define a smoothened velocity timeseries as follows:  
In this example h(l...u) has the following temporal dependency. Evaluation of h(l...u) would strictly adhere to this temporal dependency with memoization."
3933	Time series lazy evaluation	"An Example
The following example shows a python code snippet that implements harsh acceleration on a simple in-memory time series. The library includes several built-in transforms. In this example the difference transform is applied twice to the location time series to compute acceleration time series. A map operation is applied to the acceleration time series using a harsh lambda function, which is defined after the code sample, that maps acceleration to either +1 (harsh acceleration), -1 (harsh braking) and 0 (otherwise). The filter operation selects only instances wherein either harsh acceleration or harsh braking is observed. Prior to calling get_values, an execution graph is created, but no computations are performed. On calling get_values(5, 10), the evaluation is performed with memoization on the narrowest possible temporal dependency in the execution graph.  
The harsh lambda is defined as follows:"
3934	Time series lazy evaluation	"Learn more
To use the tspy Python SDK, see the tspy Python SDK documentation.  
Parent topic: Time series analysis"
3935	Parquet modular encryption	"If your data is stored in columnar format, you can use Parquet modular encryption to encrypt sensitive columns when writing Parquet files, and decrypt these columns when reading the encrypted files. Encrypting data at the column level, enables you to decide which columns to encrypt and how to control the column access.  
Besides ensuring privacy, Parquet modular encryption also protects the integrity of stored data. Any tampering with file contents is detected and triggers a reader-side exception.  
Key features include:  
sc.hadoopConfiguration.set(""<parameter name>"" , ""<parameter value>"") <data frame name>.write .option(""<parameter name>"" , ""<parameter value>"") .parquet(""<write path>"")  
Parquet modular encryption and decryption is performed on the Spark cluster. Therefore, sensitive data and the encryption keys are not visible to the storage.  
Standard Parquet features, such as encoding, compression, columnar projection and predicate push-down, continue to work as usual on files with Parquet modular encryption format.  
You can choose one of two encryption algorithms that are defined in the Parquet specification. Both algorithms support column encryption, however:  
The default algorithm AES-GCM provides full protection against tampering with data and metadata parts in Parquet files. The alternative algorithm AES-GCM-CTR supports partial integrity protection of Parquet files. Only metadata parts are protected against tampering, not data parts. An advantage of this algorithm is that it has a lower throughput overhead compared to the AES-GCM algorithm.  
You can choose which columns to encrypt. Other columns won't be encrypted, reducing the throughput overhead.  
Different columns can be encrypted with different keys.  
By default, the main Parquet metadata module (the file footer) is encrypted to hide the file schema and list of sensitive columns. However, you can choose not to encrypt the file footers in order to enable legacy readers (such as other Spark distributions that don't yet support Parquet modular encryption) to read the unencrypted columns in the encrypted files.  
Encryption keys can be managed in one of two ways:  
Directly by your application. See Key management by application. By a key management system (KMS) that generates, stores and destroys encryption keys used by the Spark service. These keys never leave the KMS server, and therefore are invisible to other components, including the Spark service. See Key management by KMS."
3936	Parquet modular encryption	"Note: Only master encryption keys (MEKs) need to be managed by your application or by a KMS.  
For each sensitive column, you must specify which master key to use for encryption. Also, a master key must be specified for the footer of each encrypted file (data frame). By default, the footer key will be used for footer encryption. However, if you choose a plain text footer mode, the footer won’t be encrypted, and the key will be used only for integrity verification of the footer.  
The encryption parameters can be passed via the standard Spark Hadoop configuration, for example by setting configuration values in the Hadoop configuration of the application's SparkContext:  
Alternatively, you can pass parameter values through write options:"
3937	Parquet modular encryption	"Running with Parquet modular encryption
Parquet modular encryption is available only in Spark notebooks that are run in an IBM Analytics Engine service instance. Parquet modular encryption is not supported in notebooks that run in a Spark environment.  
To enable Parquet modular encryption, set the following Spark classpath properties to point to the Parquet jar files that implement Parquet modular encryption, and to the key management jar file:  
spark.driver.extraClassPath=/home/common/lib/parquetEncryption/ibm-parquet-kms-<latestversion>-jar-with-dependencies.jar:/home/common/lib/parquetEncryption/parquet-format-<latestversion>.jar:/home/common/lib/parquetEncryption/parquet-hadoop-<latestversion>.jar spark.executor.extraClassPath=/home/common/lib/parquetEncryption/ibm-parquet-<latestversion>-jar-with-dependencies.jar:/home/common/lib/parquetEncryption/parquet-format-<latestversion>.jar:/home/common/lib/parquetEncryption/parquet-hadoop-<latestversion>.jar  
Navigate to Ambari > Spark > Config -> Custom spark2-default.  
Add the following two parameters to point explicitly to the location of the JAR files. Make sure that you edit the paths to use the actual version of jar files on the cluster."
3938	Parquet modular encryption	"Mandatory parameters
The following parameters are required for writing encrypted data:  
parameter name: ""encryption.column.keys"" parameter value: ""<master key ID>:<column>,<column>;<master key ID>:<column>,.."" parameter name: ""encryption.footer.key"" parameter value: ""<master key ID>"" dataFrame.write .option(""encryption.footer.key"" , ""k1"") .option(""encryption.column.keys"" , ""k2:SSN,Address;k3:CreditCard"") .parquet(""<path to encrypted files>"")  
List of columns to encrypt, with the master encryption keys:  
The footer key:  
For example:  
Important:  
If neither the encryption.column.keys parameter nor the encryption.footer.key parameter is set, the file will not be encrypted. If only one of these parameters is set, an exception is thrown, because these parameters are mandatory for encrypted files."
3939	Parquet modular encryption	"Optional parameters
The following optional parameters can be used when writing encrypted data:  
parameter name: ""encryption.algorithm"" parameter value: ""AES_GCM_CTR_V1"" parameter name: ""encryption.plaintext.footer"" parameter value: ""true""  
The encryption algorithm AES-GCM-CTR  
By default, Parquet modular encryption uses the AES-GCM algorithm that provides full protection against tampering with data and metadata in Parquet files. However, as Spark 2.3.0 runs on Java 8, which doesn’t support AES acceleration in CPU hardware (this was only added in Java 9), the overhead of data integrity verification can affect workload throughput in certain situations.  
To compensate this, you can switch off the data integrity verification support and write the encrypted files with the alternative algorithm AES-GCM-CTR, which verifies the integrity of the metadata parts only and not that of the data parts, and has a lower throughput overhead compared to the AES-GCM algorithm.  
Plain text footer mode for legacy readers  
By default, the main Parquet metadata module (the file footer) is encrypted to hide the file schema and list of sensitive columns. However, you can decide not to encrypt the file footers in order to enable other Spark and Parquet readers (that don't yet support Parquet modular encryption) to read the unencrypted columns in the encrypted files. To switch off footer encryption, set the following parameter:  
Important:  
The encryption.footer.key parameter must also be specified in the plain text footer mode. Although the footer is not encrypted, the key is used to sign the footer content, which means that new readers could verify its integrity. Legacy readers are not affected by the addition of the footer signature."
3940	Parquet modular encryption	"Usage examples
The following sample code snippets for Python show how to create data frames, written to encrypted parquet files, and read from encrypted parquet files.  
from pyspark.sql import Row squaresDF = spark.createDataFrame( sc.parallelize(range(1, 6)) .map(lambda i: Row(int_column=i, square_int_column=i ** 2))) sc._jsc.hadoopConfiguration().set(""encryption.key.list"", ""key1: AAECAwQFBgcICQoLDA0ODw==, key2: AAECAAECAAECAAECAAECAA=="") sc._jsc.hadoopConfiguration().set(""encryption.column.keys"", ""key1:square_int_column"") sc._jsc.hadoopConfiguration().set(""encryption.footer.key"", ""key2"") encryptedParquetPath = ""squares.parquet.encrypted"" squaresDF.write.parquet(encryptedParquetPath) sc._jsc.hadoopConfiguration().set(""encryption.key.list"", ""key1: AAECAwQFBgcICQoLDA0ODw==, key2: AAECAAECAAECAAECAAECAA=="") encryptedParquetPath = ""squares.parquet.encrypted"" parquetFile = spark.read.parquet(encryptedParquetPath) parquetFile.show()  
Python: Writing encrypted data:  
Python: Reading encrypted data:  
The contents of the Python job file InMemoryKMS.py is as follows:"
3941	Parquet modular encryption	"Internals of encryption key handling
When writing a Parquet file, a random data encryption key (DEK) is generated for each encrypted column and for the footer. These keys are used to encrypt the data and the metadata modules in the Parquet file.  
The data encryption key is then encrypted with a key encryption key (KEK), also generated inside Spark/Parquet for each master key. The key encryption key is encrypted with a master encryption key (MEK) locally.  
Encrypted data encryption keys and key encryption keys are stored in the Parquet file metadata, along with the master key identity. Each key encryption key has a unique identity (generated locally as a secure random 16-byte value), also stored in the file metadata.  
When reading a Parquet file, the identifier of the master encryption key (MEK) and the encrypted key encryption key (KEK) with its identifier, and the encrypted data encryption key (DEK) are extracted from the file metadata.  
The key encryption key is decrypted with the master encryption key locally. Then the data encryption key (DEK) is decrypted locally, using the key encryption key (KEK)."
3942	Parquet modular encryption	"Learn more
Parquet modular encryption"
3943	Time series analysis	"A time series is a sequence of data values measured at successive, though not necessarily regular, points in time. The time series library allows you to perform various key operations on time series data, including segmentation, forecasting, joins, transforms, and reducers.  
The library supports various time series types, including numeric, categorical, and arrays. Examples of time series data include:  
Stock share prices and trading volumes Clickstream data Electrocardiogram (ECG) data Temperature or seismographic data Network performance measurements Network logs Electricity usage as recorded by a smart meter and reported via an Internet of Things data feed  
An entry in a time series is called an observation. Each observation comprises a time tick, a 64-bit integer that indicates when the observation was made, and the data that was recorded for that observation. The recorded data can be either numerical, for example, a temperature or a stock share price, or categorical, for example, a geographic area. A time series can but must not necessarily be associated with a time reference system (TRS), which defines the granularity of each time tick and the start time.  
The time series library is Python only."
3944	Time series analysis	"Next step
Using the time series library"
3945	Time series analysis	"Learn more
Time series key functionality"
3946	Key management by application	This topic describes how to manage column encryption keys by application. It explains how to provide master keys and how to write and read encrypted data using these master keys.
3947	Key management by application	"Providing master keys
To provide master keys:  
parameter name: ""encryption.key.list"" parameter value: ""<master key ID>:<master key (base64)> , <master key ID>:<master key (base64)>.."" sc.hadoopConfiguration.set(""encryption.key.list"" , ""k1:iKwfmI5rDf7HwVBcqeNE6w== , k2:LjxH/aXxMduX6IQcwQgOlw== , k3:rnZHCxhUHr79Y6zvQnxSEQ=="")  
Pass the explicit master keys, in the following format:  
For example:  
The length of master keys before base64 encoding can be 16, 24 or 32 bytes (128, 192 or 256 bits)."
3948	Key management by application	"Writing encrypted data
To write encrypted data:  
parameter name: ""encryption.column.keys"" parameter value: ""<master key ID>:<column>,<column>;<master key ID>:<column> .."" parameter name: ""encryption.footer.key"" parameter value: ""<master key ID>"" dataFrame.write .option(""encryption.footer.key"" , ""k1"") .option(""encryption.column.keys"" , ""k2:SSN,Address;k3:CreditCard"") .parquet(""<path to encrypted files>"")  
Specify which columns to encrypt, and which master keys to use:  
Specify the footer key:  
For example:  
Note: `"" ""` must contain the string `.encrypted` in the URL, for example `/path/to/my_table.parquet.encrypted`. If either the `""encryption.column.keys""` parameter or the `""encryption.footer.key""` parameter is not set, an exception will be thrown."
3949	Key management by application	"Reading encrypted data
The required metadata is stored in the encrypted Parquet files.  
To read the encrypted data:  
sc.hadoopConfiguration.set(""encryption.key.list"" , ""k1:iKwfmI5rDf7HwVBcqeNE6w== , k2:LjxH/aXxMduX6IQcwQgOlw== , k3:rnZHCxhUHr79Y6zvQnxSEQ=="") val dataFrame = spark.read.parquet(""<path to encrypted files>"")  
Provide the encryption keys:  
Call the regular parquet read commands, such as:  
Note: `"" ""` must contain the string `.encrypted` in the URL, for example `/path/to/my_table.parquet.encrypted`.  
Parent topic: Parquet encryption"
3950	Time series key functionality	"The time series library provides various functions on univariate, multivariate, multi-key time series as well as numeric and categorical types.  
The functionality provided by the library can be broadly categorized into:  
Time series I/O, for creating and saving time series data Time series functions, transforms, windowing or segmentation, and reducers Time series SQL and SQL extensions to Spark to enable executing scalable time series functions  
Some of the key functionality is shown in the following sections using examples."
3951	Time series key functionality	"Time series I/O
The primary input and output (I/O) functionality for a time series is through a pandas DataFrame or a Python list. The following code sample shows constructing a time series from a DataFrame:  
To revert from a time series back to a pandas DataFrame, use the to_df function:"
3952	Time series key functionality	"Data model
Time series data does not have any standards for the model and data types, unlike some data types such as spatial, which are governed by a standard such as Open Geospatial Consortium (OGC). The challenge with time series data is the wide variety of functions that need to be supported, similar to that of Spark Resilient Distributed Datasets (RDD).  
The data model allows for a wide variety of operations ranging across different forms of segmentation or windowing of time series, transformations or conversions of one time series to another, reducers that compute a static value from a time series, joins that join multiple time series, and collectors of time series from different time zones. The time series library enables the plug-and-play of new functions while keeping the core data structure unchangeable. The library also support numeric and categorical typed timeseries.  
With time zones and various human readable time formats, a key aspect of the data model is support for Time Reference System (TRS). Every time series is associated with a TRS (system default), which can be remapped to any specific choice of the user at any time, enabling easy transformation of a specific time series or a segment of a time series. See Using time reference system.  
Further, with the need for handling large scale time series, the library offers a lazy evaluation construct by providing a mechanism for identifying the maximal narrow temporal dependency. This construct is very similar to that of a Spark computation graph, which also loads data into memory on as needed basis and realizes the computations only when needed."
3953	Time series key functionality	"Time series data types
You can use multiple data types as an element of a time series, spanning numeric, categorical, array, and dictionary data structures.  
The following data types are supported in a time series:"
3954	Time series key functionality	"Time series functions
You can use different functions in the provided time series packages to analyze time series data to extract meaningful information with which to create models that can be used to predict new values based on previously observed values. See Time series functions."
3955	Time series key functionality	"Learn more
To use the tspy Python SDK, see the tspy Python SDK documentation.  
Parent topic: Time series analysis"
3956	Key management by KMS	Parquet modular encryption can work with arbitrary Key Management Service (KMS) servers. A custom KMS client class, able to communicate with the chosen KMS server, has to be provided to the Analytics Engine powered by Apache Spark instance. This class needs to implement the KmsClient interface (part of the Parquet modular encryption API). Analytics Engine powered by Apache Spark includes the VaultClient KmsClient, that can be used out of the box if you use Hashicorp Vault as the KMS server for the master keys. If you use or plan to use a different KMS system, you can develop a custom KmsClient class (taking the VaultClient code as an example).
3957	Key management by KMS	"Custom KmsClient class
Parquet modular encryption provides a simple interface called org.apache.parquet.crypto.keytools.KmsClient with the following two main functions that you must implement:  
In addition, the interface provides the following initialization function that passes KMS parameters and other configuration:  
See Example of KmsClient implementation to learn how to implement a KmsClient.  
After you have developed the custom KmsClient class, add it to a jar supplied to Analytics Engine powered by Apache Spark, and pass its full name in the Spark Hadoop configuration, for example:"
3958	Key management by KMS	"Key management by Hashicorp Vault
If you decide to use Hashicorp Vault as the KMS server, you can use the pre-packaged VaultClient:"
3959	Key management by KMS	"Key management by Hashicorp Vault
Creating master keys
Consult the Hashicorp Vault documentation for the specifics about actions on Vault. See:  
Transit Secrets Engine Encryption as a Service: Transit Secrets Engine  
Enable the Transit Engine either at the default path or providing a custom path. Create named encryption keys. Configure access policies with which a user or machine is allowed to access these named keys."
3960	Key management by KMS	"Key management by Hashicorp Vault
Writing encrypted data
val k1 = ""key1"" val k2 = ""key2"" val k3 = ""key3"" dataFrame.write .option(""parquet.encryption.footer.key"" , k1) .option(""parquet.encryption.column.keys"" , k2+"":SSN,Address;""+k3+"":CreditCard"") .parquet(""<path to encrypted files>"")  
Pass the following parameters:  
sc.hadoopConfiguration.set(""parquet.ecnryption.kms.client.class"", ""com.ibm.parquet.key.management.VaultClient"") sc.hadoopConfiguration.set(""parquet.encryption.kms.instance.id"" , ""north/transit1"") sc.hadoopConfiguration.set(""parquet.encryption.kms.instance.url"" , ""https://<hostname>:8200"") sc.hadoopConfiguration.set(""parquet.encryption.key.access.token"" , ""<token string>"") val token = scala.io.Source.fromFile(""<token file>"").mkStringsc.hadoopConfiguration.set(""parquet.encryption.key.access.token"" , token)  
Set ""parquet.encryption.kms.client.class"" to ""com.ibm.parquet.key.management.VaultClient"":  
Optional: Set the custom path ""parquet.encryption.kms.instance.id"" to your transit engine:  
Set ""parquet.encryption.kms.instance.url"" to the URL of your Vault instance:  
Set ""parquet.encryption.key.access.token"" to a valid access token with the access policy attached, which provides access rights to the required keys in your Vault instance:  
If the token is located in a local file, load it:  
Specify which columns need to be encrypted, and with which master keys. You must also specify the footer key. For example:  
Note: If either the ""parquet.encryption.column.keys"" or the ""parquet.encryption.footer.key"" parameter is not set, an exception will be thrown."
3961	Key management by KMS	"Reading encrypted data
The required metadata, including the ID and URL of the Hashicorp Vault instance, is stored in the encrypted Parquet files.  
To read the encrypted metadata:  
sc.hadoopConfiguration.set(""parquet.ecnryption.kms.client.class"", ""com.ibm.parquet.key.management.VaultClient"") sc.hadoopConfiguration.set(""parquet.encryption.key.access.token"" , ""<token string>"") val dataFrame = spark.read.parquet(""<path to encrypted files>"")  
Set KMS client to the Vault client implementation:  
Provide the access token with policy attached that grants access to the relevant keys:  
Call the regular Parquet read commands, such as:"
3962	Key management by KMS	"Key rotation
If key rotation is required, an administrator with access rights to the KMS key rotation actions must rotate master keys in Hashicorp Vault using the procedure described in the Hashicorp Vault documentation. Thereafter the administrator can trigger Parquet key rotation by calling:  
To enable Parquet key rotation, the following Hadoop configuration properties must be set:  
The parameters ""parquet.encryption.key.access.token"" and ""parquet.encryption.kms.instance.url"" must set set, and optionally ""parquet.encryption.kms.instance.id"" The parameter ""parquet.encryption.key.material.store.internally"" must be set to ""false"". The parameter ""parquet.encryption.kms.client.class"" must be set to ""com.ibm.parquet.key.management.VaultClient""  
For example:  
Parent topic: Parquet encryption"
3963	Time series functions	"Time series functions are aggregate functions that operate on sequences of data values measured at points in time.  
The following sections describe some of the time series functions available in different time series packages."
3964	Time series functions	"Transforms
Transforms are functions that are applied on a time series resulting in another time series. The time series library supports various types of transforms, including provided transforms (by using from tspy.functions import transformers) as well as user defined transforms.  
The following sample shows some provided transforms:"
3965	Time series functions	"Segmentation
Segmentation or windowing is the process of splitting a time series into multiple segments. The time series library supports various forms of segmentation and allows creating user-defined segments as well."
3966	Time series functions	"Segmentation
>>> import tspy >>> ts_orig = tspy.builder() .add(tspy.observation(1,1.0)) .add(tspy.observation(2,2.0)) .add(tspy.observation(6,6.0)) .result().to_time_series() >>> ts_orig timestamp: 1 Value: 1.0 timestamp: 2 Value: 2.0 timestamp: 6 Value: 6.0 >>> ts = ts_orig.segment_by_time(3,1) >>> ts timestamp: 1 Value: original bounds: (1,3) actual bounds: (1,2) observations: [(1,1.0),(2,2.0)] timestamp: 2 Value: original bounds: (2,4) actual bounds: (2,2) observations: [(2,2.0)] timestamp: 3 Value: this segment is empty timestamp: 4 Value: original bounds: (4,6) actual bounds: (6,6) observations: [(6,6.0)] >>> import tspy >>> ts_orig = tspy.time_series([1.0, 2.0, 3.0, 4.0, 5.0]) >>> ts_orig timestamp: 0 Value: 1.0 timestamp: 1 Value: 2.0 timestamp: 2 Value: 3.0 timestamp: 3 Value: 4.0 timestamp: 4 Value: 5.0 >>> ts = ts_orig.segment_by_anchor(lambda x: x % 2 == 0, 1, 2) >>> ts timestamp: 1 Value: original bounds: (0,3) actual bounds: (0,3) observations: [(0,1.0),(1,2.0),(2,3.0),(3,4.0)] timestamp: 3 Value: original bounds: (2,5) actual bounds: (2,4) observations: [(2,3.0),(3,4.0),(4,5.0)] >>> ts = tspy.time_series([1.0,2.0,3.0,4.0,5.0,2.0,1.0,-1.0,50.0,53.0,56.0]) >>> max_error = .5 >>> skip = 1 >>> reg_sts ="
3967	Time series functions	"Segmentation
>>> max_error = .5 >>> skip = 1 >>> reg_sts = ts.to_segments(segmenters.regression(max_error,skip,use_relative=True)) >>> reg_sts timestamp: 0 Value: range: (0, 4) outliers: {} timestamp: 5 Value: range: (5, 7) outliers: {} timestamp: 8 Value: range: (8, 10) outliers: {}"
3968	Time series functions	"Segmentation
Window based segmentation  
This type of segmentation of a time series is based on user specified segment sizes. The segments can be record based or time based. There are options that allow for creating tumbling as well as sliding window based segments.  
Anchor based segmentation  
Anchor based segmentation is a very important type of segmentation that creates a segment by anchoring on a specific lambda, which can be a simple value. An example is looking at events that preceded a 500 error or examining values after observing an anomaly. Variants of anchor based segmentation include providing a range with multiple markers.  
Segmenters  
There are several specialized segmenters provided out of the box by importing the segmenters package (using from tspy.functions import segmenters). An example segmenter is one that uses regression to segment a time series:"
3969	Time series functions	"Reducers
A reducer is a function that is applied to the values across a set of time series to produce a single value. The time series reducer functions are similar to the reducer concept used by Hadoop/Spark. This single value can be a collection, but more generally is a single object. An example of a reducer function is averaging the values in a time series.  
Several reducer functions are supported, including:  
>>> from tspy.functions import * >>> ts = tspy.time_series([1.0, 2.0, 3.0, 4.0, 5.0, 6.0]) >>> ts2 = ts.transform(transformers.awgn(sd=.3)) >>> dtw_distance = ts.reduce(ts2,reducers.dtw(lambda obs1, obs2: abs(obs1.value - obs2.value))) >>> print(dtw_distance) 1.8557981638880405 >>> from tspy.functions import * >>> ts = tspy.time_series([1.0, 2.0, 3.0, 4.0, 5.0, 6.0]) >>> ts2 = ts.transform(transformers.awgn(sd=.3)) >>> corr = ts.reduce(ts2, reducers.correlation()) >>> print(corr) 0.9938941942380525 >>> adf = ts.reduce(reducers.adf()) >>> print(adf) pValue: -3.45 satisfies test: false >>> ts2 = ts.transform(transformers.awgn(sd=.3)) >>> granger = ts.reduce(ts2, reducers.granger(1)) >>> print(granger) #f_stat, p_value, R2 -1.7123613937876463,-3.874412217575385,1.0 >>> desc = ts.describe() >>> print(desc) min inter-arrival-time: 1 max inter-arrival-time: 1 mean inter-arrival-time: 1.0 top: null unique: 6 frequency: 1 first: TimeStamp: 0 Value: 1.0 last: TimeStamp: 5 Value: 6.0 count: 6 mean:3.5 std:1.707825127659933 min:1.0 max:6.0 25%:1.75 50%:3.5 75%:5.25  
Distance reducers"
3970	Time series functions	"Reducers
Distance reducers  
Distance reducers are a class of reducers that compute the distance between two time series. The library supports numeric as well as categorical distance functions on sequences. These include time warping distance measurements such as Itakura Parallelogram, Sakoe-Chiba Band, DTW non-constrained and DTW non-time warped contraints. Distribution distances such as Hungarian distance and Earth-Movers distance are also available.  
For categorical time series distance measurements, you can use Damerau Levenshtein and Jaro-Winkler distance measures.  
Math reducers  
Several convenient math reducers for numeric time series are provided. These include basic ones such as average, sum, standard deviation, and moments. Entropy, kurtosis, FFT and variants of it, various correlations, and histogram are also included. A convenient basic summarization reducer is the describe function that provides basic information about the time series.  
Another basic reducer that is very useful for getting a first order understanding of the time series is the describe reducer. The following illustrates this reducer:"
3971	Time series functions	"Temporal joins
The library includes functions for temporal joins or joining time series based on their timestamps. The join functions are similar to those in a database, including left, right, outer, inner, left outer, right outer joins, and so on. The following sample codes shows some of these join functions:"
3972	Time series functions	"Forecasting
A key functionality provided by the time series library is forecasting. The library includes functions for simple as well as complex forecasting models, including ARIMA, Exponential, Holt-Winters, and BATS. The following example shows the function to create a Holt-Winters:"
3973	Time series functions	"Time series SQL
The time series library is tightly integrated with Apache Spark. By using new data types in Spark Catalyst, you are able to perform time series SQL operations that scale out horizontally using Apache Spark. This enables you to easily use time series extensions in IBM Analytics Engine or in solutions that include IBM Analytics Engine functionality like the Watson Studio Spark environments.  
SQL extensions cover most aspects of the time series functions, including segmentation, transformations, reducers, forecasting, and I/O. See Analyzing time series data."
3974	Time series functions	"Learn more
To use the tspy Python SDK, see the tspy Python SDK documentation.  
Parent topic: Time series analysis"
3975	Using the time series library	"To get started working with the time series library, import the library to your Python notebook or application.  
Use this command to import the time series library:"
3976	Using the time series library	"Creating a time series
To create a time series and use the library functions, you must decide on the data source. Supported data sources include:  
In-memory lists pandas DataFrames In-memory collections of observations (using the ObservationCollection construct) User-defined readers (using the TimeSeriesReader construct)  
The following example shows ingesting data from an in-memory list:  
The output is as follows:  
You can also operate on many time-series at the same time by using the MultiTimeSeries construct. A MultiTimeSeries is essentially a dictionary of time series, where each time series has its own unique key. The time series are not aligned in time.  
The MultiTimeSeries construct provides similar methods for transforming and ingesting as the single time series construct:  
The output is the following:"
3977	Using the time series library	"Interpreting time
By default, a time series uses a long data type to denote when a given observation was created, which is referred to as a time tick. A time reference system is used for time series with timestamps that are human interpretable. See Using time reference system.  
The following example shows how to create a simple time series where each index denotes a day after the start time of 1990-01-01:  
The output is as follows:"
3978	Using the time series library	"Performing simple transformations
Transformations are functions which, when given one or more time series, return a new time series.  
For example, to segment a time series into windows where each window is of size=3, sliding by 2 records, you can use the following method:  
The output is as follows:  
This example shows adding 1 to each value in a time series:  
The output is as follows:  
Or you can temporally left join a time series, for example ts with another time series ts2:  
The output is as follows:"
3979	Using the time series library	"Performing simple transformations
Using transformers
A rich suite of built-in transformers is provided in the transformers package. Import the package to use the provided transformer functions:  
After you have added the package, you can transform data in a time series be using the transform method.  
For example, to perform a difference on a time-series:  
Here the output is:"
3980	Using the time series library	"Performing simple transformations
Using reducers
Similar to the transformers package, you can reduce a time series by using methods provided by the reducers package. You can import the reducers package as follows:  
After you have imported the package, use the reduce method to get the average over a time-series for example:  
This outputs:  
Reducers have a special property that enables them to be used alongside segmentation transformations (hourly sum, avg in the window prior to an error occurring, and others). Because the output of a segmentation + reducer is a time series, the transform method is used.  
For example, to segment into windows of size 3 and get the average across each window, use:  
This results in:"
3981	Using the time series library	"Graphing time series
Lazy evaluation is used when graphing a time series. When you graph a time series, you can do one of the following:  
Collect the observations of the time series, which returns an BoundTimeSeries Reduce the time series to a value or collection of values Perform save or print operations  
For example, to collect and return all of the values of a timeseries:  
This results in:  
To collect a range from a time series, use:  
Here the output is:  
Note that a time series is optimized for range queries if the time series is periodic in nature.  
Using the describe on a current time series, also graphs the time series:  
The output is:"
3982	Using the time series library	"Learn more
Time series key functionality Time series functions Time series lazy evaluation Using time reference system tspy Python SDK documentation  
Parent topic: Time series analysis"
3983	Time reference system	"Time reference system (TRS) is a local, regional or global system used to identify time.  
A time reference system defines a specific projection for forward and reverse mapping between a timestamp and its numeric representation. A common example that most users are familiar with is UTC time, which maps a timestamp, for example, (1 Jan 2019, 12 midnight (GMT) into a 64-bit integer value (1546300800000), which captures the number of milliseconds that have elapsed since 1 Jan 1970, 12 midnight (GMT). Generally speaking, the timestamp value is better suited for human readability, while the numeric representation is better suited for machine processing.  
In the time series library, a time series can be associated with a TRS. A TRS is composed of a:  
Time tick that captures time granularity, for example 1 minute Zoned date time that captures a start time, for example 1 Jan 2019, 12 midnight US Eastern Daylight Savings time (EDT). A timestamp is mapped into a numeric representation by computing the number of elapsed time ticks since the start time. A numeric representation is scaled by the granularity and shifted by the start time when it is mapped back to a timestamp.  
Note that this forward + reverse projection might lead to time loss. For instance, if the true time granularity of a time series is in seconds, then forward and reverse mapping of the time stamps 09:00:01 and 09:00:02 (to be read as hh:mm:ss) to a granularity of one minute would result in the time stamps 09:00:00 and 09:00:00 respectively. In this example, a time series, whose granularity is in seconds, is being mapped to minutes and thus the reverse mapping looses information. However, if the mapped granularity is higher than the granularity of the input time series (more specifically, if the time series granularity is an integral multiple of the mapped granularity) then the forward + reverse projection is guaranteed to be lossless. For example, mapping a time series, whose granularity is in minutes, to seconds and reverse projecting it to minutes would result in lossless reconstruction of the timestamps."
3984	Time reference system	"Setting TRS
When a time series is created, it is associated with a TRS (or None if no TRS is specified). If the TRS is None, then the numeric values cannot be mapped to timestamps. Note that TRS can only be set on a time series at construction time. The reason is that a time series by design is an immutable object. Immutability comes in handy when the library is used in multi-threaded environments or in distributed computing environments such as Apache Spark. While a TRS can be set only at construction time, it can be changed using the with_trs method as described in the next section. with_trs produces a new time series and thus has no impact on immutability.  
Let us consider a simple time series created from an in-memory list:  
This returns:  
At construction time, the time series can be associated with a TRS. Associating a TRS with a time series allows its numeric timestamps to be as per the time tick and offset/timezone. The following example shows 1 minute and 1 Jan 2019, 12 midnight (GMT):  
This returns:  
Here is another example where the numeric timestamps are reinterpreted with a time tick of one hour and offset/timezone as 1 Jan 2019, 12 midnight US Eastern Daylight Savings time (EDT).  
This returns:  
Note that the timestamps now indicate an offset of -4 hours from GMT (EDT timezone) and captures the time tick of one hour. Also note that setting a TRS does NOT change the numeric timestamps - it only specifies a way of interpreting numeric timestamps.  
This returns:"
3985	Time reference system	"Changing TRS
You can change the TRS associated with a time series using the with_trs function. Note that this function will throw an exception if the input time series is not associated with a TRS (if TRS is None). Using with_trs changes the numeric timestamps.  
The following code sample shows TRS set at contructions time without using with_trs:  
This returns:  
The following example shows how to apply with_trs to change granularity to one minute and retain the original time offset (1 Jan 1970, 12 midnight GMT):  
This returns:  
Now apply with_trs to change granularity to one minute and the offset to 1 Jan 2019, 12 midnight GMT:  
This returns:  
To better understand how it impacts post processing, let's examine the following. Note that materialize on numeric timestamps operates on the underlying numeric timestamps associated with the time series.  
This returns:  
The method materialize can also be applied to datetime objects. This results in an exception if the underlying time series is not associated with a TRS (if TRS is None). Assuming the underlying time series has a TRS, the datetime objects are mapped to a numeric range using the TRS."
3986	Time reference system	"Duplicate timestamps
Changing the TRS can result in duplicate timestamps. The following example changes the granularity to one hour which results in duplicate timestamps. The time series library handles duplicate timestamps seamlessly and provides convenience combiners to reduce values associated with duplicate timestamps into a single value, for example by calculating an average of the values grouped by duplicate timestamps.  
This returns:  
Duplicate timestamps can be optionally combined as follows:  
This returns:"
3987	Time reference system	"Learn more
To use the tspy Python SDK, see the tspy Python SDK documentation.  
Parent topic: Time series analysis"
3988	Stop using services or IBM watsonx	"You can stop using any services or IBM watsonx at any time, whether you are accessing the services from your own or someone else's IBM Cloud account.  
The method you choose to stop using IBM watsonx depends on your goal:  
To remove your access to IBM watsonx in all IBM Cloud accounts that you belong to, leave IBM watsonx. To stop the use of a service in your IBM Cloud account, delete your service in your IBM Cloud account. To stop all use of all IBM Cloud services in your account, delete your IBM Cloud account.  
When other users in your account stop using IBM watsonx, they are cleaned up appropriately."
3989	Stop using services or IBM watsonx	"Leave IBM watsonx
If you want to leave IBM watsonx:  
Log in to IBM watsonx. Click your avatar and then Profile. On the Profile page, click Leave watsonx. If you change your mind about leaving, you can sign up to re-activate your profile.  
Use this process when you want to stop using IBM watsonx, you are not the account owner, and you want to keep your IBM Cloud account.  
These are the results when you leave IBM watsonx:  
Your profile is deleted and you can't log in to IBM watsonx. Your projects and deployment spaces remain until you delete your services. Your IBM Cloud account remains active. Your IBM Cloud services are not affected."
3990	Stop using services or IBM watsonx	"Delete a service
To remove any of your services:  
Log in to IBM watsonx. Click Administration > Services > Service instances. Click the menu next to the service you want to remove and choose Delete.  
This action is the same as deleting the service in IBM Cloud. If you change your mind within 30 days, you can get your services and data back by reprovisioning the service.  
These are the results when you delete the Watson Studio service:  
Your IBM watsonx profile remains. You can no longer access that service from that IBM Cloud account. You can still access your services from other accounts. Your billing for that service stops. Your data in IBM Cloud Object Storage remains. Your projects remain. You remain a collaborator in all your projects in other IBM Cloud accounts."
3991	Stop using services or IBM watsonx	"Closing a IBM Cloud account
If you want to stop using IBM Cloud services altogether and delete all your data, you can deactivate your IBM Cloud account. Follow these steps to close your Lite account:  
Sign in to your IBM Cloud account. In the IBM Cloud console, go to the Manage > Account > Account settings page. Click Close Account. After an account is closed for 30 days, all data is deleted and all services are removed.  
If you are not the owner of the account, you do not see a Close Account button.  
These are the results when your IBM Cloud account is in the Canceled state:  
All your data in IBM Cloud is permanently deleted in 30 days. The projects and catalogs in your account are deleted. Your IBM watsonx profile and your IBM Cloud profile are deleted. All the IBM Cloud services in you account are deleted in 30 days. You are removed as a collaborator from projects and catalogs in other accounts within 30 days.  
If you want to close a Pay-As-You-Go or Subscription account, contact Support."
3992	Stop using services or IBM watsonx	"Learn more
Removing users from the account or from the workspace IBM Cloud docs: Leaving an account IBM Cloud docs: Managing your account settings  
Parent topic: Managing IBM watsonx"
3993	Managing IBM watsonx	"As the owner or an administrator of the IBM Cloud account, you can monitor and manage services and the platform.  
Configuring services  
An IBM Cloud account administrator is a user in the account who was assigned the Administrator role in IBM Cloud for the All Identity and Access enabled services option in IAM. If you're not sure of your roles, see Determine your roles.  
You perform some administrative tasks within IBM watsonx, and others in IBM Cloud. Some tasks require steps in both areas, depending on your goals."
3994	Managing IBM watsonx	"Configuring services
The services that are included in watsonx.ai are Watson Studio and Watson Machine Learning."
3995	Managing IBM watsonx	"Switch service region
The platform and services are available in multiple IBM Cloud service regions and you can have services in more than one region. Your projects, catalogs, and data are specific to the region in which they were saved and can be accessed only from your services in that region. If you provision Watson Studio services in both the Dallas and the Frankfurt regions, you can't access projects that you created in the Frankfurt region from the Dallas region.  
To switch your service region:  
Log in to IBM watsonx. Click the Region Switcher in the home page header. Select the region that contains your services and projects.  
For wider browsers, you can select the region from the dropdown menu."
3996	Managing IBM watsonx	"Learn more
Watson Studio offering plans Watson Machine Learning plans and compute usage Roles in the platform  
Parent topic: Administration"
3997	Setting up IBM Cloud Object Storage for use with IBM watsonx	"An IBM Cloud Object Storage service instance is provisioned automatically with a Lite plan when you join IBM watsonx. Workspaces, such as projects, require IBM Cloud Object Storage to store files that are related to assets, including uploaded data files or notebook files.  
You can also connect to IBM Cloud Object Storage as a data source. See IBM Cloud Object Storage connection."
3998	Setting up IBM Cloud Object Storage for use with IBM watsonx	"Overview of setting up Cloud Object Storage
To set up Cloud Object Storage, complete these tasks:  
Generate an administrative key. Ensure that Global location is set in each user's profile. Provide access to Cloud Object Storage. Optional: Protect sensitive data. Optional: Encrypt your IBM Cloud Object Storage instance with your own key.  
Assign roles to enable access. Enable storage delegation.  
Watch the following video to see how administrators set up Cloud Object Storage for use with Cloud Pak for Data as a Service.  
This video provides a visual method to learn the concepts and tasks in this documentation."
3999	Setting up IBM Cloud Object Storage for use with IBM watsonx	"Generate an administrative key
You generate an administrative key for Cloud Object Storage by creating an initial test project. The test project can be deleted after its creation. Its sole purpose is to generate the key.  
To automatically generate the administrative key for your Cloud Object Storage instance:  
From the IBM watsonx main menu, select Projects > View all projects and then click New project. Specify to create an empty project. Enter a project name, such as ""Test Project"". Select your Cloud Object Storage instance. Click Create. The administrative key is generated. Delete the test project."
4000	Setting up IBM Cloud Object Storage for use with IBM watsonx	"Ensure that Global location is set for Cloud Object Storage in each user's profile
Cloud Object Storage requires the Global location to be configured in each user's profile. The Global location is configured automatically, but it might be changed by mistake. An error occurs when a project is created if the Global location is not enabled in the user's profile. Ask users to check that Global location is enabled.  
Check for the Global location in each user's profile."
4001	Setting up IBM Cloud Object Storage for use with IBM watsonx	"Provide access to Cloud Object Storage
You can provide different levels of access to Cloud Object Storage for people who need to work in IBM watsonx. Using the storage delegation setting on the Cloud Object Storage instance, you can provide quick access to most users to create projects and catalogs. However, another option is to provide targeted access by using IAM roles and access groups. Role-based access enacts stricter controls for viewing the Cloud Object Storage instance directly and for creating projects and catalogs. If you decide to provide controlled access with IAM roles and access groups, you must disable storage delegation for the Cloud Object Storage instance.  
You enable storage delegation for the Cloud Object Storage instance to provide access to nonadministrative users. Users with minimal IAM permissions can create projects and catalogs, which automatically create buckets in the Cloud Object Storage instance. See Enable storage delegation for nonadministrative users.  
You provide more controlled access with IAM roles and access groups. For example, the Cloud Object Storage Manager role provides permissions to create projects and spaces together with the corresponding buckets in the Cloud Object Storage instance. It also provides permissions to view all buckets and encryption root keys in the Cloud Object Storage instance, to view the metadata for a bucket and delete buckets, and to perform other administrative tasks that are related to buckets. See Assign roles to enable access.  
No role assignments are needed for collaborators who work with the data in a project or catalog. Users who are given collaborator roles can work in the project or catalog without storage delegation or an IAM role. See Project collaborator roles and permissions."
4002	Setting up IBM Cloud Object Storage for use with IBM watsonx	"Provide access to Cloud Object Storage
Assign roles to enable access
The IBM Cloud account owner or administrator assigns appropriate roles to users to provide access to Cloud Object Storage. Storage delegation must be disabled when using role-based access.  
Rather than assigning each individual user a set of roles, you can create an access group. Access groups expedite role assignments by grouping permissions. For instructions on creating access groups, see IBM Cloud docs: Setting up access groups."
4003	Setting up IBM Cloud Object Storage for use with IBM watsonx	"Provide access to Cloud Object Storage
Enable storage delegation
Storage delegation for the Cloud Object Storage instance allows nonadministrative users to create projects, the Platform assets catalog, and the corresponding Cloud Object Storage buckets. Storage delegation provides wide access to Cloud Object Storage and allows users with minimal permissions to create projects. Storage delegation for projects also includes deployment spaces.  
To enable storage delegation for the Cloud Object Storage instance:  
From the navigation menu, select Administration > Configurations and settings > Storage delegation. Set storage delegation for Projects to on. Optional. If you want a non-administrative user to create the Platform assets catalog, set storage delegation for Catalogs to on."
4004	Setting up IBM Cloud Object Storage for use with IBM watsonx	"Optional: Encrypt your IBM Cloud Object Storage instance with your own key
Encryption protects the data for your projects and catalogs. Data at rest in Cloud Object Storage is encrypted by default with randomly generated keys that are managed by IBM. For increased protection, you can create and manage your own encryption keys with IBM Key Protect. IBM Key Protect for IBM Cloud is a centralized key management system for generating, managing, and deleting encryption keys used by IBM Cloud services.  
For more information, see IBM Cloud docs: IBM Key Protect for IBM Cloud.  
Not all Watson Studio service plans support the use of your own encryption keys. Check your specific plan for details.  
To encrypt your Cloud Object Storage instance with your own key, you need an instance of the IBM Key Project service. Although Key Protect is a paid service, each account is allowed five keys without charge.  
In IBM Cloud, provision Key Protect and generate a key:  
Create an instance of Key Protect for your account from the IBM Cloud catalog. See IBM Cloud docs: Provisioning the Key Protect service. Grant a service authorization between your Key Protect instance and your Cloud Object Storage instance. Do not associate a key with a bucket. If you don't grant the authorization, users cannot create projects and catalogs with the Cloud Object Storage instance. For more information, see IBM Cloud docs: Using authorizations to grant access between services. You can also grant a service authorization for a root key from Watson Studio, by choosing Manage > Access (IAM). Create a root key to protect your Cloud Object Storage instance. See IBM Cloud docs: Creating root keys.  
In IBM watsonx, add the key to the Cloud Object Storage instance:  
Select Administration > Configurations and settings > Storage delegation. Slide the toggle for Projects, Catalogs, or both to select data for encryption with your key. Click Add... under Encryption keys to add an encryption key. Select the Key Protect instance and the Key Protect key. Click OK to add the encryption key.  
Important: If you change or remove the key, you lose access to existing encrypted data in the Cloud Object Storage instance."
4005	Setting up IBM Cloud Object Storage for use with IBM watsonx	"Optional: Protect sensitive data stored on Cloud Object Storage
When you join IBM watsonx, a single Cloud Object Storage instance is automatically provisioned for you. The Cloud Object Storage instance contains separate buckets for each project to store data assets and related files. The ability to create projects and thus to add buckets to Cloud Object Storage is available only to users with the Platform Administrator role and the Manager role for the Cloud Object Storage Service. Although only users with these roles can create projects and their accompanying buckets, any user with the Editor or Viewer role can see the data files. For some businesses, the data files contain sensitive information and require stricter access controls."
4006	Setting up IBM Cloud Object Storage for use with IBM watsonx	"Optional: Protect sensitive data stored on Cloud Object Storage
Control access to Cloud Object Storage with multiple instances
For paid plans, you can control access to sensitive data files by creating one or more Cloud Object Storage instances and assigning access to specific users. Project creators select the appropriate Cloud Object Storage instance when they create a project. The data assets and files for the project are stored in a bucket in the selected instance. Users with Editor or Viewer roles can work in the projects, but they cannot see the assets directly in the related Cloud Object Storage bucket. You can assign access to a specific Cloud Object Storage instance either to an individual user or to an access group. You must be the account owner or administrator to create service instances and assign access.  
Extra fees are not incurred by creating more than one Cloud Object Storage instances because charges are determined by overall storage utilization. The number of instances is not a factor for Cloud Object Storage fees.  
Only one instance of Cloud Object Storage is allowed for the Lite plan. You can change your pricing plan from the IBM Cloud catalog.  
To create a Cloud Object Storage instance and assign access:  
Select Services > Services catalog from the navigation menu. Select Storage > Cloud Object Storage. Click Create. A Service name is generated for you on IBM Cloud. Select Manage > Access(IAM). Select Users or Access groups. Click Assign access. In the Services list, choose Cloud Object Storage. For Resources, choose: For Roles and actions, choose: Click Add and Assign.  
Scope = Specific resources Attribute type = Service instance Operator = string equals Value = name of Cloud Object Storage  
Service access = Manager Platform access = Administrator  
The specified Cloud Object Storage instance can be accessed only by the user or access group with the Service role of Manager and the Platform role of Administrator. Other users can work in the projects but cannot create projects or view assets directly in the bucket."
4007	Setting up IBM Cloud Object Storage for use with IBM watsonx	"Next step
Finish the remaining steps for setting up the platform."
4008	Setting up IBM Cloud Object Storage for use with IBM watsonx	"Learn more
Security for IBM watsonx Data security  
Parent topic: Setting up the platform"
4009	Watson Studio on IBM watsonx	Description Quick links
4010	Watson Studio on IBM watsonx	"Description
Watson Studio is part of IBM® watsonx.ai. Watson Studio provides the environment and tools for your team to collaboratively solve your business problems. You can choose the tools you need to analyze and visualize data, to cleanse and shape data, to experiment with prompting foundation models, to tune foundation models, or to create and train machine learning models.  
Watson Studio provides the following tools:  
Data Refinery: Prepare and visualize data. Prompt Lab: Experiment with prompting foundation models. Tuning Studio: Tune a foundation model to guide the foundation model to return useful output. Jupyter notebook editor: Code Jupyter notebooks in Python or R. SPSS® Modeler: Automate the flow of data through a model with SPSS algorithms. Decision Optimization model builder: Optimize solving business problem scenarios. Federated learning: Train models on remote parties without sharing data. RStudio®: Code Jupyter notebooks in R. Pipelines: Automate end-to-end flows of data or models. Synthetic Data Generator: Generate tabular data to use in training models."
4011	Watson Studio on IBM watsonx	"Quick links
Administer: Manage and maintain the service Use: Work with the service What's new: See what's new each week Known issues: View limitations Offering plans: Capabilities for each plan"
4012	Watson Studio on IBM watsonx	Parent topic: IBM Cloud services in the IBM watsonx services catalog
4013	Databases for PostgreSQL on IBM® watsonx	Description Quick links
4014	Databases for PostgreSQL on IBM® watsonx	"Description
The Databases for PostgreSQL service provides the PostgreSQL database. PostgreSQL is an object-relational database management system (ORDBMS) with an emphasis on extensibility and on standards-compliance.  
This service provides a database that you can connect to from Watson Studio ."
4015	Databases for PostgreSQL on IBM® watsonx	"Quick links
Use: Work with the service Create: Create the service instance Connect: Create a connection"
4016	Databases for PostgreSQL on IBM® watsonx	Parent topic: IBM Cloud services in the IBM watsonx services catalog
4017	watsonx.governance on IBM® watsonx	Description Quick links
4018	watsonx.governance on IBM® watsonx	"Description
Use watsonx.governance to analyze your AI with trust and transparency and understand how your AI models make decisions. Detect and mitigate bias and drift. Increase the quality and accuracy of your predictions. Explain transactions and perform what-if analysis. Extend the governance capabilities with support for tracking predictive and generative model assets in AI use cases. Details about the tracked assets are recorded in fact sheets to use for your governance and compliance requirements.  
Watsonx.governance provides an enterprise-grade environment for AI applications that provides your enterprise visibility into how your AI is built, is used, and delivers return on investment. Its open platform enables businesses to operate and automate AI at scale with transparent, explainable outcomes that are free from harmful bias and drift. Watsonx.governance supports external models that are developed in third-party providers, including Amazon Web Services or Microsoft Azure."
4019	watsonx.governance on IBM® watsonx	"Quick links
Administer: Manage and maintain the service Use: Work with the service Develop: Write code and build applications What's new: See what's new each week Troubleshoot: Find solutions to problems"
4020	watsonx.governance on IBM® watsonx	Parent topic: IBM Cloud services in the IBM watsonx services catalog
4021	IBM Cloud services in the IBM watsonx services catalog	"Storage service Security service Data source services Learn more  
You can provision IBM® Cloud service instances for the watsonx platform.  
The IBM watsonx.ai component provides the following services that provide key functionality, including tools and compute resources:  
Watson™ Studio Watson Machine Learning  
If you signed up for watsonx.ai, you already have these services. Otherwise, you can create instances of these services from the Services catalog.  
If you signed up for watsonx.governance, you already have this service. Otherwise, you can create an instance of this service from the Services catalog.  
The IBM Cloud Object Storage provides storage for projects and deployment spaces on the IBM watsonx platform.  
The Secure Gateway service provides secure connections to on-premises date sources.  
These services provide databases that you can access in IBM watsonx by creating connections:  
IBM Analytics Engine Cloudant Databases for Elasticsearch Databases for EDB Databases for MongoDB Databases for PostgreSQL Db2® Db2 Warehouse watsonx.data"
4022	IBM Cloud services in the IBM watsonx services catalog	"Learn more
Creating services Securing connections to services with private service endpoints"
4023	IBM Cloud services in the IBM watsonx services catalog	Parent topic: Services and integrations
4024	Cloudant on IBM® watsonx	Description Quick links
4025	Cloudant on IBM® watsonx	"Description
IBM Cloudant is a fully managed JSON document database that offers independent serverless scaling of provisioned throughput capacity and storage. Cloudant is compatible with Apache CouchDB and accessible through a simple to use HTTPS API for web, mobile, and IoT applications.  
This service provides a database that you can connect to from Watson Studio."
4026	Cloudant on IBM® watsonx	"Quick links
Administer: Manage and maintain the service Use: Work with the service Develop: Write code and build applications Create: Create the service instance Connect: Create a connection"
4027	Cloudant on IBM® watsonx	Parent topic: IBM Cloud services in the IBM watsonx services catalog
4028	Watson Machine Learning on IBM watsonx	"Quick links  
Watson Machine Learning is part of IBM® watsonx.ai. Watson Machine Learning provides a full range of tools for your team to build, train, and deploy Machine Learning models. You can choose the tool with the level of automation or autonomy that matches your needs.  
Watson Machine Learning provides the following tools:  
AutoAI experiment builder for automatically processing structured data to generate model-candidate pipelines. The best-performing pipelines can be saved as a machine learning model and deployed for scoring. Deployment spaces give you the tools to view and manage model deployments. Tools to view and manage model deployments."
4029	Watson Machine Learning on IBM watsonx	"Quick links
Use: Work with the service What's new: See what's new each week Known issues: View limitations Troubleshoot: Find solutions to problems Offering plans: Capabilities for each plan"
4030	Watson Machine Learning on IBM watsonx	Parent topic: IBM Cloud services in the IBM watsonx services catalog
4031	Databases for EDB on IBM watsonx	Description Quick links
4032	Databases for EDB on IBM watsonx	"Description
The Databases for EDB service provides the EDB Postgres Advanced Server database engine that optimizes the built-in features of PostgreSQL. EDB Postgres Advanced Server (formerly known as EnterpriseDB) is a PostgreSQL-based database engine optimized for performance, developer productivity, and compatibility with Oracle. Databases for EDB is a fully managed offering with 24x7 operations and support. Features include high availability, automated backup orchestration, and de-coupled scaling of storage, RAM, and vCPUs.  
IBM® watsonx does not yet include a connection to EDB Postgres Advanced Server, however, you can use the PostgreSQL connector to create connections to EDB Postgres Advanced Server from Watson Studio.  
This service provides a database that you can connect to from Watson Studio.  
Restriction: When you connect to EDB through IBM watsonx, you can use PostgreSQL features, but not EDB Postgres Advanced Server features. See the list of differences between PostgreSQL and EDB Postgres Advanced Server. This restriction applies only when you access EDB through IBM watsonx. This restriction does not apply when you have an external service that uses the EDB Postgres Advanced Server driver to access the EDB Postgres Advanced Server service provisioned through IBM watsonx."
4033	Databases for EDB on IBM watsonx	"Quick links
Use: Work with the service Develop: Write code and build applications Create: Create the service instance Connect: Create a connection"
4034	Databases for EDB on IBM watsonx	Parent topic: IBM Cloud services in the IBM watsonx services catalog
4035	Db2 on IBM watsonx	Description Quick links
4036	Db2 on IBM watsonx	"Description
IBM Db2 provides advanced data management and analytics capabilities for transactional workloads. Db2 has no processor, memory, or database size limits, which makes it ideal for any size workload. The Db2 service enables you to create these databases in your IBM® watsonx cluster so that you can govern the data and use it for more in-depth analysis.  
Integrating a Db2 database with IBM watsonx can be useful in the following situations:  
You need your transactional data to be governed, such as data from a website, bank, or retail store. You want to create a replica of your transactional database so that you can run analytics without impacting regular business operations. You need to ensure the integrity of your data by using an ACID-compliant database. You need a low-latency database. You need real-time insight into your business operations.  
This service provides a database that you can connect to from Watson Studio ."
4037	Db2 on IBM watsonx	"Quick links
Use: Work with the service Develop: Write code and build applications Administer: Manage and maintain the service Create: Create the service instance Connect: Create a connection"
4038	Db2 on IBM watsonx	Parent topic: IBM Cloud services in the IBM watsonx services catalog
4039	Secure Gateway on IBM® watsonx	Description Quick links
4040	Secure Gateway on IBM® watsonx	"Description
Attention: IBM Cloud® is announcing the full deprecation of Secure Gateway. See the deprecation dates, details, and specific implications.  
IBM Secure Gateway for IBM Cloud service provides a quick, easy, and secure solution for connecting anything to anything. By deploying the light-weight and natively installed Secure Gateway Client, a secure, persistent connection can be established between your environment and the cloud. With this, you can safely connect all of your applications and resources regardless of their location. Rather than bridging your environments at the network level like a traditional VPN, Secure Gateway provides granular resource control operating with the principle of least privilege as its core tenet.  
This service provides secure connections to on-premises data sources."
4041	Secure Gateway on IBM® watsonx	"Quick links
Administer: Manage and maintain the service Use: Work with the service Develop: Write code and build applications Create: Create the service instance"
4042	Secure Gateway on IBM® watsonx	Parent topic: IBM Cloud services in the IBM watsonx services catalog
4043	watsonx.data on IBM® watsonx	Description Quick links
4044	watsonx.data on IBM® watsonx	"Description
IBM watsonx.data is a new open lakehouse architecture that combines the elements of the data warehouse and data lake models. The best-in-class features and optimizations available on watsonx.data make it an optimal choice for next generation data analytics and automation."
4045	watsonx.data on IBM® watsonx	"Quick links
Use: Work with the service Develop: Write code and build applications What's new: See what's new each week Known issues: View limitations Troubleshoot: Find solutions to problems"
4046	watsonx.data on IBM® watsonx	Parent topic: IBM Cloud services in the IBM watsonx services catalog
4047	Cloud Object Storage on IBM® watsonx	Description Quick links
4048	Cloud Object Storage on IBM® watsonx	"Description
IBM Cloud Object Storage provides storage for projects, catalogs, and deployment spaces. You can also create connections to access data in an IBM Cloud Object Storage instance.  
When you create a project, catalog, or deployment space, you must choose an IBM Cloud Object Storage instance. You can use a single instance for all projects, catalogs, and deployment spaces, or you can use multiple instances. Each project, catalog, and deployment space has its own dedicated bucket. The bucket stores files for assets, such as uploaded data files or notebook files. The connection between the project, catalog, or deployment space and its storage bucket is implicit.  
You create an IBM Cloud Object Storage connection when you want to connect to data stored in any IBM Cloud Object Storage instance that you have access to. After you create a connection to a Cloud Object Storage bucket, you can access the files in the bucket by creating connected data assets.  
This service provides storage for projects, catalogs, and deployment spaces. The Lite plan instance is free to use for storage capacity up to 25 GB per month."
4049	Cloud Object Storage on IBM® watsonx	"Quick links
Administer: Manage and maintain the service Use: Work with the service Create: Create the service instance Connect: Create a connection"
4050	Cloud Object Storage on IBM® watsonx	Parent topic: IBM Cloud services in the IBM watsonx services catalog
4051	Databases for MongoDB on IBM watsonx	Description Quick links
4052	Databases for MongoDB on IBM watsonx	"Description
MongoDB Enterprise Advanced is a cross-platform, document-oriented, NoSQL database. Rather than table-based relational database structure, it uses JSON-like documents with dynamic schemas, which makes it easier and faster to integrate data in certain types of applications. With the Databases for MongoDB service you can create highly performant, highly available databases with automatic scaling in your IBM® watsonx cluster so that you can govern the data and use it for in-depth analysis.  
Integrating a MongoDB database into IBM watsonx can be useful in the following situations:  
You need an operational database that supports a rapidly changing data model. You want lightweight, low-latency analytics integrated into your operational database. You need real-time views of your business, even if your data is in silos. You develop applications and need a database that can: You need to store large amounts of data from Internet of Things devices or sensors. You need to maintain a catalog. You need to store and serve many different types of content.  
Store large amounts of data with different data types, such as structure, unstructured, and polymorphic data Support millions of users. Personalize the content that you deliver to customers.  
This service provides a database that you can connect to from Watson Studio ."
4053	Databases for MongoDB on IBM watsonx	"Quick links
Use: Work with the service Create: Create the service instance Connect: Create a connection"
4054	Databases for MongoDB on IBM watsonx	Parent topic: IBM Cloud services in the IBM watsonx services catalog
4055	IBM Analytics Engine on IBM® watsonx	Description Quick links
4056	IBM Analytics Engine on IBM® watsonx	"Description
With IBM Analytics Engine, you can run Jupyter notebooks and jobs from tools in Watson Studio by selecting IBM Analytics Engine as your runtime environment. You are offered Hortonworks Data Platform on IBM Cloud. You get one VM per cluster compute node and your own local HDFS. You get Spark and the entire Hadoop ecosystem. You are given shell access and can also create notebooks.  
This service adds a tool and compute resources in projects."
4057	IBM Analytics Engine on IBM® watsonx	"Quick links
Use: Work with the service Administer: Manage and maintain the service Develop: Write code and build applications Create: Create the service instance"
4058	IBM Analytics Engine on IBM® watsonx	Parent topic: IBM Cloud services in the IBM watsonx services catalog
4059	Databases for Elasticsearch on IBM® watsonx	Description Quick links
4060	Databases for Elasticsearch on IBM® watsonx	"Description
Elasticsearch is an open source search and analytics engine based on the Apache Lucene library. It combines the power of a scalable full text search engine with the indexing strengths of a schema-free JSON document database.  
Elasticsearch is a powerful tool for rich data analysis of large volumes of data, catalogs, autocompletion, log analysis, monitoring, blockchain analysis and more. IBM Cloud Databases for Elasticsearch provide the benefits of high availability, automated backup orchestration, autoscaling, and de-coupled allocation of storage, RAM, and vCPUs. Databases for Elasticsearch pricing is based on underlying disk, RAM, and optional vCPU allocation, as well as backup storage usage.  
This service provides a database that you can connect to from Watson Studio."
4061	Databases for Elasticsearch on IBM® watsonx	"Quick links
Administer: Manage and maintain the service Use: Work with the service Develop: Write code and build applications Create: Create the service instance Connect: Create a connection"
4062	Databases for Elasticsearch on IBM® watsonx	Parent topic: IBM Cloud services in the IBM watsonx services catalog
4063	Db2 Warehouse on IBM watsonx	Description Quick links
4064	Db2 Warehouse on IBM watsonx	"Description
IBM Db2 Warehouse is an analytics data warehouse that features in-memory data processing and in-database analytics. It is client-managed and optimized for fast and flexible deployment, with automated scaling that supports analytics workloads. Based on the number of worker nodes selected, IBM® watsonx automatically creates the appropriate data warehouse environment. For a single node, the warehouse uses symmetric multiprocessing (SMP) architecture for cost-efficiency. For two or more nodes, the warehouse is deployed using a massively parallel processing (MPP) architecture for high availability and improved performance.  
Integrating a Db2 Warehouse database with IBM watsonx can be useful in the following situations:  
You have developers who need to create small-scale database management systems for development and test work. For example, if you need to test new applications and data sources in a development environment before you move them to a production environment. You want to accelerate line-of-business analytics projects by creating a data mart service that combines a governed data source with analytic techniques. You need to deliver self-service analytics solutions and applications that leverage data that is generated from new sources and is ingested directly into the private cloud warehouse. You want to migrate a subset of applications or data from an on-premises data warehouse to a private cloud. You want to save money or improve performance by migrating on-premises data marts or an on-premises data warehouse to a cloud-native data warehouse. You want to support data scientists who are coding and need to store data locally and need to use a logical representation. You want to reduce network traffic and improve analytic performance by storing your data near your analytics engine. You have multiple departments, and each department requires their own database management system.  
This service provides a database that you can connect to from Watson Studio."
4065	Db2 Warehouse on IBM watsonx	"Quick links
Use: Work with the service Administer: Manage and maintain the service Develop: Write code and build applications Create: Create the service instance Connect: Create a connection"
4066	Db2 Warehouse on IBM watsonx	Parent topic: IBM Cloud services in the IBM watsonx services catalog
